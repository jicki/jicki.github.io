[{"categories":["golang"],"content":"Go 语言基础","date":"2020-01-01","objectID":"/golang-study-notes/","tags":["golang"],"title":"Go 语言基础","uri":"/golang-study-notes/"},{"categories":["golang"],"content":"Go 简介 Go（又称Golang）是Google开发的一种静态强类型、编译型、并发型，并具有垃圾回收功能的编程语言。 罗伯特·格瑞史莫，罗勃·派克（Rob Pike）及肯·汤普逊于2007年9月开始设计Go，稍后 Ian Lance Taylor、Russ Cox加入项目。 Go是基于Inferno操作系统所开发的。 Go于2009年11月正式宣布推出，成为开放源代码项目，支持Linux、Mac OS X、Windows等操作系统。 Go在2016年，Go被软件评价公司TIOBE 选为“TIOBE 2016 年最佳语言”。 Go 代理 为解决 go get 下载慢或者下载不到的问题 # Go 1.13 中使用 goproxy.cn 作为代理 # 执行如下命令 go env -w GOPROXY=https://goproxy.cn,https://mirrors.aliyun.com/goproxy,direct Go 学习目录 【Go学习笔记 - Go 语言结构】 【Go学习笔记 - Go 常量变量 \u0026\u0026 数据类型】 【Go学习笔记 - Go 运算符 \u0026\u0026 流程控制】 【Go学习笔记 - Go 数组 切片 Map】 【Go学习笔记 - Go 值类型与引用类型】 【Go学习笔记 - Go 函数 闭包 panic 指针】 【Go学习笔记 - Go 结构体 方法 Json序列化】 【Go学习笔记 - Go 包package 】 【Go学习笔记 - Go 文件操作 】 【Go学习笔记 - Go 接口 interface 类型断言 】 【Go学习笔记 - 反射】 【Go学习笔记 - 并发 goroutine \u0026\u0026 锁】 【Go学习笔记 - 网络编程-HTTP-TCP-UDP】 【Go学习笔记 - Golang 性能调优】 【Go学习笔记 - 基础库 flag 库】 【Go学习笔记 - regexp 库 正则】 ","date":"2020-01-01","objectID":"/golang-study-notes/:0:0","tags":["golang"],"title":"Go 语言基础","uri":"/golang-study-notes/"},{"categories":["golang"],"content":"Go Web编程","date":"2020-01-01","objectID":"/golang-study-web-notes/","tags":["golang"],"title":"Go Web编程","uri":"/golang-study-web-notes/"},{"categories":["golang"],"content":"Go 学习目录 Go Web编程 - PV UV IP PR 并发 Go Web编程 - 通讯协议 TLV Go Web编程 - RESTful 框架 Go Web编程 - MVC 框架 Go Web编程 - Gin 简介 Go Web编程 - Go HTML 基础 Go Web编程 - Gin HTML 应用 Go Web编程 - GORS 解决跨域 Go Web编程 - Go 操作 Mysql Go Web编程 - Go GORM MYSQL Go Web编程 - Go 操作 Redis Go Web编程 - Go Logrus 日志库 Go Web编程 - Uber Zap Logger 库 Go Web编程 - Gin 框架使用 Zap 日志库 Go Web编程 - Cookie 与 Session Go Web编程 - Sonyflake 生成唯一ID Go Web编程 - Go Context Go Web编程 - Go 操作 Kafka Go Web编程 - Go 使用 Protobuf 数据传输 Go Web编程 - Go 使用 viper 管理配置 Go Web编程 - Go 使用 validator 检验参数 Go Web编程 - Go 使用 Jwt 实现 Token 认证 ","date":"2020-01-01","objectID":"/golang-study-web-notes/:0:0","tags":["golang"],"title":"Go Web编程","uri":"/golang-study-web-notes/"},{"categories":["linux","ansible"],"content":"ansible playbook 从入门到放弃","date":"2020-08-10","objectID":"/ansible-playbook/","tags":["linux","ansible"],"title":"ansible playbook 从入门到放弃","uri":"/ansible-playbook/"},{"categories":["linux","ansible"],"content":" 前言 运维自动化的发展历程以及技术应用 本地部署 On-Premises 如下都需要自己 部署/配置 以及维护。 Application Data Runtime Middleware OS Virtualization Servers Storage NetWorking 基础设施既服务 ( 如 阿里云 ) IaaS - Infrastructure as a Service 如下需要自己 部署/配置 以及维护。 Application Data Runtime Middleware OS 平台既服务 ( 如 阿里云- ACK 容器服务 ) PaaS - Platform as a Service 如下需要自己 部署/配置 以及维护。 Application Data 软件既服务 ( 如 各类软件 微信、钉钉、邮箱 ) SaaS - Software as a Service 所有的服务软件都不需要自己维护, 直接使用既可。 ansible ","date":"2020-08-10","objectID":"/ansible-playbook/:0:0","tags":["linux","ansible"],"title":"ansible playbook 从入门到放弃","uri":"/ansible-playbook/"},{"categories":["linux","ansible"],"content":"ansible 简介 ansible 是基于 python2-paramiko 模块开发的自动化运维工具。 实现了批量系统配置, 批量程序部署, 批量运行命令等功能。ansible 是基于模块工作的, 本身没有批量部署的能力。真正具有批量部署的是 ansible 所运行的模块, ansible 只是提供了一种框架。 ansible 集合了众多运维工具（ pupet、cfengine、chef、func、fabric、saltstack ）的优点 ansible 发展史 ansible 作者是 Michael DeHaan 同时他也是 Cobbler 与 Func 作者。 2012-03-09 发布 0.0.1 版本。 2015-10-17 被 Red Hat 收购。 ansible 特性 基于 Python 开发 模块化: 调用特定的模块(如: Paramiko、PyYAML、jinja2 等), 完成特定的任务。 支持自定义模块 部署简单, 基于Linux内置的 Python 、Open-SSH 和另一个 agentless 组件. 支持 PlayBook 编排任务 幂等性: 任务重复执行等于只执行一次, 不会重复执行多次相同命令。 无需代理不依赖PKI. 支持多语言模块编写. YAML格式编排任务,支持丰富的数据结构. ","date":"2020-08-10","objectID":"/ansible-playbook/:1:0","tags":["linux","ansible"],"title":"ansible playbook 从入门到放弃","uri":"/ansible-playbook/"},{"categories":["linux","ansible"],"content":"ansible 架构 ansible 主要组成部分: ansible playbooks: 任务剧本(任务集), 通过编排定义ansible任务集合的配置文件, 由ansible 顺序依次执行, 文件通常是 JSON格式的YML文件。 Roles: 角色, 多个 ansilbe playbooks 的集合. Inventory: ansible 管理主机的清单 默认为 /etc/ansible/hosts 文件。 Modules: ansible 执行命令的功能模块, 一般为ansible内置核心模块, 也可以自定义第三方模块. plugins: ansible 功能插件, 是功能模块的补充. 如: 连接类型插件、循环插件、变量插件、过滤插件等等. Api: 提供第三方程序调用的开放接口. ansible: ansible 的客户端命令工具. 执行 ansible 命令的主要程序. Ad-Hoc 用户执行单条或多条的 ansible 命令. ansible playbook 通过编写编排文件 ansible 执行命令集合. ansible playbook 执行过程 -\u003e 将编排好的任务写入 ansible-playbook 编排文件中 -\u003e 通过 ansible-playbook 命令拆分任务集合, 然后按照预定的顺序逐条执行ansible 命令. ","date":"2020-08-10","objectID":"/ansible-playbook/:2:0","tags":["linux","ansible"],"title":"ansible playbook 从入门到放弃","uri":"/ansible-playbook/"},{"categories":["linux","ansible"],"content":"ansible 安装 ansible 安装很简单, 也有很多种方式. 安装 epel 源以后 执行 yum -y install ansible 使用 源码包 进行 安装 通过 git clone https://github.com/ansible/ansible 以后进行安装 使用 pip 命令安装 [root@jicki ~]# ansible --version ansible 2.9.10 config file = /etc/ansible/ansible.cfg configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules'] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Apr 2 2020, 13:16:51) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] ansible 相关说明 /etc/ansible/ansible.cfg 主配置文件, 配置ansible的工作特性. /etc/ansible/hosts 主机清单. /etc/ansible/roles/ 存放(roles)角色的目录. /usr/bin/ansible 二进制执行文件, ansible 主程序. /usr/bin/ansilbe-doc 配置文档, 模块功能查看工具. /usr/bin/ansible-galaxy 用于上传/下载 roles 模块到官方平台的工具. /usr/bin/ansible-playbook 自动化任务、编排剧本工具/usr/bin/ansible-pull 远程执行命令的工具. /usr/bin/ansible-vault 文件(如: playbook 文件) 加密工具. /usr/bin/ansible-console 基于 界面的用户交互执行工具. ","date":"2020-08-10","objectID":"/ansible-playbook/:3:0","tags":["linux","ansible"],"title":"ansible playbook 从入门到放弃","uri":"/ansible-playbook/"},{"categories":["kubernetes","docker","devops"],"content":"GitOps 实现云原生的持续交付","date":"2020-08-01","objectID":"/gitops-kubernetes/","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"GitOps GitOps是一种持续交付的方式。它的核心思想是将应用系统的声明性基础架构和应用程序存放在Git版本库中 Git 作为交付流水线的核心, 每个开发人员都可以提交拉取请求（Pull Request）并使用Git来加速和简化Kubernetes的应用程序部署和运维任务。通过使用像Git这样的简单工具, 开发人员可以更高效地将注意力集中在创建新功能而不是运维相关任务上（例如: 应用系统安装、配置、迁移等）。 ","date":"2020-08-01","objectID":"/gitops-kubernetes/:0:0","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"GitOps 优点 使用Git提交基础架构代码更改时, 自动化的交付流水线(Pipeline)会将这些更改应用到应用程序的实际基础架构上。 通过应用GitOps最佳实践, 应用系统的基础架构和应用程序代码都有\"真实来源\" —— 其实是将基础架构和应用程序代码都存放在gitlab、或者github等版本控制系统上。这使开发团队可以提高开发和部署速度并提高应用系统可靠性。 GitOps理论方法应用在持续交付流水线上, 有诸多优势和特点 安全的云原生CI/CD管道模型 更快的平均部署时间和平均恢复时间 稳定且可重现的回滚 (例如, 根据Git恢复/回滚/fork) 与监控和可视化工具相结合, 对已经部署的应用进行全方位的监控 ","date":"2020-08-01","objectID":"/gitops-kubernetes/:1:0","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"GitOps With Kubernetes GitOps 是在具体Kubernetes的应用实践中出现的, GitOps 需要依托于 “不可变基础架构” 才能发挥其作用。 在一定程度上说, “不可变基础架构” 为GitOps的出现创造了必要的条件, 反过来GitOps 应用Kubernetes的容器编排能力, 能够迅速的使用镜像搭建出应用系统所需的组件。 所谓不可变基础架构: 应用都需要运行在多台机器上，它们被组织成不同的环境，例如开发环境、测试环境和生产环境等等。需要将相同的应用部署到不同的机器上。通常需要系统管理员确保所有的机器都处于相同的状态。接着所有的修改、补丁、升级需要在所有的机器中进行。随着时间的推移，很难再确保所有的机器处于相同的状态，同时越来越容易出错。这就是传统的可变架构中经常出现的问题。这时我们有了不可变架构，它将整个机器环境打包成一个单一的不可变单元, 而不是传统方式仅仅打包应用。这个单元包含了之前所说的整个环境栈和应用所有的修改、补丁和升级, 这就解决了前面的问题。 所谓声明性容器编排: Kubermetes作为一个云原生的工具, 可以把它的\"声明性\"看作是\"代码\", 声明意味着配置由一组事实而不是一组指令组成。 GitOps充分利用了\"不可变基础设施\"和\"声明性容器编排\", 通过GitOps可以轻松地管理多个部署。为了最大限度地降低部署后的变更风险, 无论是有意还是偶然的\"配置偏差”, GitOps构建了一个可重复且可靠的部署过程, 在整个应用系统宕机或者损坏情况下, 为快速且完全恢复提供了所需条件。 ","date":"2020-08-01","objectID":"/gitops-kubernetes/:2:0","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"GitOps 基本原则 任何能够被描述的内容都必须存储在Git库中 通过使用Git作为存储声明性基础架构和应用程序代码的存储仓库，可以方便地监控集群，以及检查比较实际环境的状态与代码库上的状态是否一致。所以，我们的目标是描述系统相关的所有内容：策略，代码，配置，甚至监控事件和版本控制等，并且将这些内容全部存储在版本库中，在通过版本库中的内容构建系统的基础架构或者应用程序的时候，如果没有成功，则可以迅速的回滚，并且重新来过。 不应直接使用Kubectl 一般规则, 不提倡在命令行中直接使用kubectl命令操作执行部署基础架构或应用程序到集群中。 还有一些开发者使用CI工具驱动应用程序的部署, 但如果这样做可能会给生产环境带来潜在不可预测的风险。 调用Kubernetes的API的接口或者控制器应该遵循Operator模式。 调用 Kubernetes 的API的接口或者控制器应该遵循 Operator 模式, 集群的状态和Git库中的配置文件等要保持一致, 并且查看分析它们之间的状态差异。 所谓 Operator 模式 - 是 Kubernetes 的扩展软件, 它利用自定义资源管理应用及其组件。 Operator 遵循 Kubernetes 的理念, 特别是在控制回路方面。 Kubernetes为自动化而生。无需任何修改, 您即可以从Kubernetes核心中获得许多内置的自动化功能。 您可以使用Kubernetes自动化部署和运行工作负载, 甚至可以自动化 Kubernetes 自身。 ","date":"2020-08-01","objectID":"/gitops-kubernetes/:3:0","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"推送流水线 推送流水线 目前大多数CI / CD工具都使用基于推送的模型。基于推送的流水线意味着代码从CI系统开始, 通过一系列构建测试等最终生成镜像, 最后手动使用 kubectl 将任何更改推送到Kubernetes 集群。 ","date":"2020-08-01","objectID":"/gitops-kubernetes/:4:0","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"拉式流水线 拉式流水线 GitOps中, 镜像被拉出并且凭证保留在集群中. Git库是拉式流水线模式的核心, 它存储应用程序和配置文件集。开发人员将更新的代码推送到Git代码库, CI 工具获取更改并最终构建Docker镜像。GitOps 检测到有镜像, 从存储库中提取新镜像, 然后在Git配置仓库中更新其YAML。然后, GitOps 会检测到群集已过期, 并从配置库中提取已更改的清单, 并将新镜像部署到群集。 ","date":"2020-08-01","objectID":"/gitops-kubernetes/:5:0","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"拉式流水线 流程 开发人员将更新的代码推送到 Git 代码库. CI 工具获取最新代码并最终构建成Docker image. GitOps 的 Config Update 检测到有镜像, 从存储库中提取新镜像, 然后在 Git 配置仓库中更新其 YAML 。 GitOps 的 Deploy Operator 会检测到群集已过期, 并从配置库中提取已更改的清单, 并将新镜像部署到群集。 使用群集内部的 Deploy Operator, 群集凭据不会在生产环境之外公开。一旦将 Deploy Operator 安装到集群与 Git 仓库建立连接, 线上环境中的任何更改都将通过具有完全回滚的 Git pull 请求以及 Git 提供的方便审计日志完成。 部署和发布自动化是应用落实 GitOps , 并使交付流水线工作的基础。GitOps 不仅要保证, 当开发人员通过 Git 更新配置文件集的时候, GitOps 流水线要自动根据最新的配置文件状态更新线上环境, 而且 GitOps 还要能够实时比对 Git 库中配置文件集最新的状态与线上环境最新的状态保持一致。 Config Update 和 Deploy Operator, 根据 GitOps 的实践, Config Update 和 Deploy Operator 是需要进行设计开发的, 它们是实现 GitOps 流水线必须的关键组件。GitOps 赋予了它们神奇的魔法, 它们既是自动化容器升级和发布到线上环境的工具, 可能也要负责服务、部署、网络策略甚至路由规则等任务。因此, Config Update 和 Deploy Operator 是映射代码, 服务和运行集群之间所有关系的\"粘合剂”。 当然, 您可以根据具体的设计, 赋予各种其他的功能, 但是自动同步是一定需要的, 确保如果对存储库进行任何更改, 这些更改将自动部署到线上环境中。 ","date":"2020-08-01","objectID":"/gitops-kubernetes/:5:1","tags":["kubernetes","docker","devops"],"title":"GitOps 实现云原生的持续交付","uri":"/gitops-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"CI/CD with Drone Kubernetes and Gogs","date":"2020-07-29","objectID":"/drone-kubernetes/","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"Drone Drone is a Container-Native, Continuous Delivery Platform。 我们可以理解为 Drone 是基于 容器原生的一个 持续集成、持续交付的平台。 官方网站是 https://drone.io/ Drone 支持的Git仓库有 GitHub GitLab Gogs Gitea Bitbucket Cloud Bitbucket Server ","date":"2020-07-29","objectID":"/drone-kubernetes/:0:0","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"Mysql 对于 Drone 与 Gogs 都会使用到 Mysql 注: 无论如何并不建议在 容器化环境下 部署 数据层 部署 Mysql 的 Yaml 文件 Secret 使用 base64 加密 (# echo -n “mysql\"| base64 ) # mysql-namespaces.yaml apiVersion: v1 kind: Namespace metadata: name: database # mysql-secret.yaml apiVersion: v1 kind: Secret metadata: name: mysql-secret namespace: database data: root_pass: cmxkYkAxMjM= # mysql-statefulset.yaml --- apiVersion: apps/v1 kind: StatefulSet metadata: name: mysql namespace: database labels: app: mysql service: database spec: serviceName: mysql replicas: 1 selector: matchLabels: app: mysql service: database template: metadata: labels: app: mysql service: database spec: terminationGracePeriodSeconds: 10 containers: - name: mysql image: mysql:8 args: - --character-set-server=utf8mb4 - --collation-server=utf8mb4_unicode_ci ports: - name: http containerPort: 3306 env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-secret key: root_pass resources: requests: cpu: \"0.01\" volumeMounts: - name: mysql-data-storage mountPath: /var/lib/mysql - name: tz-config mountPath: /etc/localtime readOnly: true restartPolicy: Always volumes: - name: mysql-data-storage hostPath: path: /opt/mysql/data type: Directory - name: tz-config hostPath: path: /etc/localtime --- apiVersion: v1 kind: Service metadata: name: mysql namespace: database labels: app: mysql service: database spec: ports: - name: http protocol: TCP port: 3306 targetPort: 3306 selector: app: mysql service: database type: ClusterIP # 查看验证服务 [root@jicki mysql]# kubectl get pods,statefulset,svc -n database NAME READY STATUS RESTARTS AGE pod/mysql-0 1/1 Running 0 6m1s NAME READY AGE statefulset.apps/mysql 1/1 6m1s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/mysql ClusterIP 10.104.238.35 \u003cnone\u003e 3306/TCP 6m1s ","date":"2020-07-29","objectID":"/drone-kubernetes/:1:0","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"Gogs Gogs 是一款 Go 语言开发轻量级的 Git 仓库, 是全完开源的。 ","date":"2020-07-29","objectID":"/drone-kubernetes/:2:0","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"部署 Gogs Gogs with Mysql to Kubernetes Deployment 在 WebUI 下初始化配置, 可能会遇到 Error 1049: Unknown database 'gogs' 需要在数据库先创建 用户 以及 数据库 数据库连接使用 svc 地址 mysql.database.svc.cluster.local:3306 相关 yaml 文件 # gogs-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: git # gogs-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: gogs namespace: git labels: app: gogs spec: selector: matchLabels: app: gogs template: metadata: labels: app: gogs spec: containers: - name: gogs image: gogs/gogs imagePullPolicy: IfNotPresent ports: - containerPort: 3000 name: gogs - containerPort: 22 name: ssh resources: requests: cpu: \"0.01\" volumeMounts: - name: gogs-data-storage mountPath: /data - name: tz-config mountPath: /etc/localtime readOnly: true restartPolicy: Always volumes: - name: gogs-data-storage hostPath: path: /opt/gogs/data type: Directory - name: tz-config hostPath: path: /etc/localtime --- apiVersion: v1 kind: Service metadata: name: gogs namespace: git spec: ports: - name: gogs protocol: TCP port: 3000 targetPort: 3000 selector: app: gogs --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: gogs-ingress namespace: git spec: rules: - host: gogs.jicki.cn http: paths: - backend: serviceName: gogs servicePort: 3000 # 创建服务 [root@jicki gogs]# kubectl apply -f gogs-deployment.yaml namespace/git created deployment.apps/gogs created service/gogs created ingress.extensions/gogs-ingress created 初始化 gogs ","date":"2020-07-29","objectID":"/drone-kubernetes/:2:1","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"Drone-Server level=fatal msg=\"main: invalid configuration\" error=\"Invalid port configuration. See https://discourse.drone.io/t/drone-server-changing-ports-protocol/4144\" 这个问题是 k8s 与 drone 之间的命名问题, 官方竟然一直不解决或者明确说明。 解决方式一: 在创建 deployment、StatefulSet、service 不能创建名字为 drone-server 的服务。 解决方式二: 配置 DRONE_SERVER_PORT=:80 变量 # drone-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: drone drone_agent_secret 是用于 drone-agent 与 drone-server 进行通讯的 秘钥。 使用 openssl rand -hex 16 进行生成。 # drone-configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: drone namespace: drone data: drone_gogs_server: http://gogs.jicki.cn drone_agents_enabled: 'true' drone_rpc_secret: 'ff7848cbd12a26c133fb6136301371c0' drone_db_driver: mysql drone_db_datasource: root:123456@tcp(mysql.database.svc.cluster.local:3306)/drone?parseTime=true drone_server_host: drone.jicki.cn drone_server_proto: http # drone-server-statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: drone namespace: drone labels: app: drone service: cicd spec: serviceName: drone replicas: 1 selector: matchLabels: app: drone service: cicd template: metadata: labels: app: drone service: cicd spec: terminationGracePeriodSeconds: 10 containers: - name: drone image: drone/drone ports: - name: http containerPort: 80 - name: https containerPort: 443 #securityContext: # capabilities: {} # privileged: true env: # Drone-ConfigMap - name: DRONE_AGENTS_ENABLED valueFrom: configMapKeyRef: name: drone key: drone_agents_enabled - name: DRONE_GOGS_SERVER valueFrom: configMapKeyRef: name: drone key: drone_gogs_server - name: DRONE_RPC_SECRET valueFrom: configMapKeyRef: name: drone key: drone_rpc_secret - name: DRONE_SERVER_HOST valueFrom: configMapKeyRef: name: drone key: drone_server_host - name: DRONE_SERVER_PROTO valueFrom: configMapKeyRef: name: drone key: drone_server_proto - name: DRONE_DATABASE_DRIVER valueFrom: configMapKeyRef: name: drone key: drone_db_driver - name: DRONE_DATABASE_DATASOURCE valueFrom: configMapKeyRef: name: drone key: drone_db_datasource volumeMounts: - name: drone-data-storage mountPath: /var/lib/drone - name: tz-config mountPath: /etc/localtime readOnly: true restartPolicy: Always volumes: - name: drone-data-storage hostPath: path: /opt/drone/data type: Directory - name: tz-config hostPath: path: /etc/localtime --- apiVersion: v1 kind: Service metadata: name: drone namespace: drone labels: app: drone service: cicd spec: ports: - name: http protocol: TCP port: 80 targetPort: 80 - name: https protocol: TCP port: 443 targetPort: 443 selector: app: drone service: cicd --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: drone-ingress namespace: drone spec: rules: - host: drone.jicki.cn http: paths: - backend: serviceName: drone servicePort: 80 # 导入yaml 文件 [root@jicki drone]# kubectl apply -f . namespace/drone created configmap/drone created statefulset.apps/drone created service/drone created ingress.extensions/drone-ingress created # 查看相关服务 [root@jicki drone]# kubectl get pods,svc,ingress -n drone NAME READY STATUS RESTARTS AGE pod/drone-0 1/1 Running 0 5m21s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/drone ClusterIP 10.100.29.28 \u003cnone\u003e 80/TCP,443/TCP 12m NAME CLASS HOSTS ADDRESS PORTS AGE ingress.extensions/drone-ingress \u003cnone\u003e drone.jicki.cn 10.99.155.236 80 12m ","date":"2020-07-29","objectID":"/drone-kubernetes/:3:0","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"访问WebUI 这里特别注意, 因为我们关联的是 gogs 的 git , 所以这里登录 drone 的时候, 使用 gogs 的账号密码。 ","date":"2020-07-29","objectID":"/drone-kubernetes/:3:1","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"drone-agent 官方说 Kubernetes 部署 drone-agent 还在测试中。 这里配置 drone-agent 的 namespaces 都在 default 下。 否则会报 ServiceAccount 权限错误。 drone-agent 默认使用 ServiceAccount 名字为 default 的用户执行构建。如需使用其他需要在 DRONE_SERVICE_ACCOUNT_DEFAULT 指定。 # drone-agent-rbac.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: default name: drone rules: - apiGroups: - \"\" resources: - secrets verbs: - create - delete - apiGroups: - \"\" resources: - pods - pods/log verbs: - get - create - delete - list - watch - update --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: drone namespace: default subjects: - kind: ServiceAccount name: default namespace: default roleRef: kind: Role name: drone apiGroup: rbac.authorization.k8s.io # drone-agent-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: drone-agent labels: app.kubernetes.io/name: drone-agent spec: replicas: 1 selector: matchLabels: app.kubernetes.io/name: drone-agent template: metadata: labels: app.kubernetes.io/name: drone-agent spec: containers: - name: runner image: drone/drone-runner-kube:latest ports: - containerPort: 3000 env: #- name: DRONE_SERVICE_ACCOUNT_DEFAULT # value: drone - name: DRONE_RPC_HOST value: drone.jicki.cn - name: DRONE_RPC_PROTO value: http - name: DRONE_RPC_SECRET value: ff7848cbd12a26c133fb6136301371c0 volumeMounts: - name: dockersocket mountPath: /var/run/docker.sock - name: dockersocket-2 mountPath: /run/docker.sock - name: docker-client mountPath: /usr/bin/docker restartPolicy: Always volumes: - name: dockersocket hostPath: path: /var/run/docker.sock - name: dockersocket-2 hostPath: path: /run/docker.sock - name: docker-client hostPath: path: /usr/bin/docker # 查看服务 [root@jicki drone]# kubectl get pods NAME READY STATUS RESTARTS AGE drone-agent-6c88f5847d-9c8x5 1/1 Running 0 60s # 查看连接日志状态 [root@jicki drone]# kubectl logs pods/drone-agent-6c88f5847d-9c8x5 time=\"2020-07-30T07:29:48Z\" level=info msg=\"starting the server\" addr=\":3000\" time=\"2020-07-30T07:29:52Z\" level=info msg=\"successfully pinged the remote server\" time=\"2020-07-30T07:29:52Z\" level=info msg=\"polling the remote server\" capacity=100 endpoint=\"http://drone.jicki.cn\" kind=pipeline type=kubernetes ","date":"2020-07-29","objectID":"/drone-kubernetes/:4:0","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":"测试CI/CD ","date":"2020-07-29","objectID":"/drone-kubernetes/:5:0","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["kubernetes","docker","devops"],"content":".drone.yml drone 的操作都是通过 配置 .drone.yml 文件来定义。 .drone.yml 在 kubernetes 中使用, 必须指定 type: kubernetes。 .drone.yml 创建以后提交到对应的仓库中。 # .drone.yml kind: pipeline type: kubernetes name: default steps: - name: Job image: alpine commands: - echo \"Drone With Kubernetes Pipeline CI\" # 构建过程中的 pods, 构建完成会自动销毁 [root@jicki drone]# kubectl get pods NAME READY STATUS RESTARTS AGE drone-2qeray1ji25xjbavnjdh 0/2 ContainerCreating 0 3m28s ","date":"2020-07-29","objectID":"/drone-kubernetes/:5:1","tags":["kubernetes","docker","devops"],"title":"CI/CD with Drone Kubernetes and Gogs","uri":"/drone-kubernetes/"},{"categories":["golang"],"content":"Go Web 开发 脚手架","date":"2020-07-28","objectID":"/golang-web-app/","tags":["golang"],"title":"Go Web 开发 通用模板","uri":"/golang-web-app/"},{"categories":["golang"],"content":" Go Web 通用脚手架 脚手架说明 Viper 用于配置文件管理。 Zap 用于日志管理。 ","date":"2020-07-28","objectID":"/golang-web-app/:0:0","tags":["golang"],"title":"Go Web 开发 通用模板","uri":"/golang-web-app/"},{"categories":["jicki"],"content":"Github Actions 构建 Hugo","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":" Hugo Hugo Hugo 是基于 Go 语言开发的静态网站构建程序. ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:0:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"流程 ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:1:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"创建Github仓库 创建 hugo 仓库, 并设置为 Private 此仓库用于存放 hugo 源代码. 创建 jicki.github.io 仓库, 此仓库用于 Github Pages . ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:2:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"创建 SSH Key SSH Key 主要用于 hugo 仓库 自动构建 Github Actions 生成静态文件后认证自动推送到 jicki.github.io 仓库. ssh-keygen -t rsa -b 4096 -C \"jicki@qq.com\" -f key/id_rsa_hugo [root@jicki key]# ls -lt total 8 -rw------- 1 root root 3243 Jun 30 16:39 id_rsa_hugo -rw-r--r-- 1 root root 738 Jun 30 16:39 id_rsa_hugo.pub ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:3:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"配置 SSH Key 配置 jicki.github.io 仓库 Deploy keys . 使用 id_rsa_hugo.pub 文件 配置 hugo 仓库 Secrets 使用 id_rsa_hugo 文件 ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:4:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"初始化 hugo 这里需要初始化一次 hugo 所以需要有 hugo 客户端, 请自行解决. ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:5:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"clone 仓库 git clone https://github.com/jicki/hugo cd hugo # 生成 hugo 源码 hugo new site . ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:5:1","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"配置 hugo 此处请自行配置 hugo 的相关设定, 如安装 主题, 配置 config.toml 等. ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:5:2","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"push 代码到 仓库 git add -A git commit -m \"Add hugo code\" git push ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:5:3","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"配置 Github Actions main.yml 文件内容 name: Deploy Hugo Site to Github Pages on Master Branch on: push: branches: - master jobs: build-deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v1 # v2 does not have submodules option now # with: # submodules: true - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.73.0' # extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: deploy_key: ${{ secrets.ACTIONS_DEPLOY }} # 这里的 ACTIONS_DEPLOY 则是上面设置 Private Key 的变量名 external_repository: jicki/jicki.github.io # Pages 远程仓库 publish_dir: \"public\" # hugo 默认生成目录为 public 这里一定要配置 keep_files: false # remove existing files publish_branch: master # deploying branch commit_message: ${{ github.event.head_commit.message }} ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:6:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"Actions Workflows 每次推送代码以及更新文件到 hugo 仓库会自动触发 Github Actions ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:7:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jicki"],"content":"测试访问 浏览器访问 https://jicki.github.io/ 查看具体的效果. ","date":"2020-06-30","objectID":"/hugo-github-pages-github-actions/:8:0","tags":["jicki"],"title":"Github Actions 构建 Hugo","uri":"/hugo-github-pages-github-actions/"},{"categories":["jenkins"],"content":"jenkins gradle android","date":"2020-04-26","objectID":"/jenkins-gradle-android/","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"Jenkins 构建 Android 包 使用 Jenkins 构建 Android APK 包 使用 腾讯 乐固 进行加固 加固以后进行重新签名 重新签名的 APK 上传到 蒲公英平台 发送通知到 企业微信群中 ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:0:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"安装部署 Jenkins 这一部分就省略了 ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:1:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置 java 环境 部署 Java 环境 - 这一部分也省略了, 可选择 openjdk 或 oraclejdk java -version java version \"1.8.0_241\" Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:2:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置 gradle 部署 gradle 也省略了. gradle -version ------------------------------------------------------------ Gradle 5.6.4 ------------------------------------------------------------ Build time: 2019-11-01 20:42:00 UTC Revision: dd870424f9bd8e195d614dc14bb140f43c22da98 Kotlin: 1.3.41 Groovy: 2.5.4 Ant: Apache Ant(TM) version 1.9.14 compiled on March 12 2019 JVM: 1.8.0_241 (Oracle Corporation 25.241-b07) OS: Linux 4.4.215-1.el7.elrepo.x86_64 amd64 ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:3:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置 Android SDK # 创建目录 mkdir -p /opt/android cd /opt/android wget https://dl.google.com/android/repository/sdk-tools-linux-4333796.zip unzip sdk-tools-linux-4333796.zip # 配置 env vi /etc/profile # anddrid env export ANDROID_HOME=/opt/android export PATH=$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools:$PATH # 生效配置 source /etc/profile ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:4:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"更新 Android SDK # 首先执行更新操作 sdkmanager --update # 更新 SDK 版本 android update sdk # 更新一些指定 tools, 根据自己的版本 android update sdk --no-ui --filter build-tools-29.0.3,android-29,extra-android-m2repository # 最后更新 licenses sdkmanager --licenses ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:4:1","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置腾讯云乐固SDK # 下载 乐固 SDK mkdir -p /opt/android/ms-client cd /opt/android/ms-client wget https://leguimg.qcloud.com/ms-client/java-tool/1.0.3/ms-shield.jar # 升级版本 java –jar ms-shield.jar –update ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:4:2","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置 Jenkins ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:5:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置 全局变量 必须配置, 否则读取不到 sdk 文件 # 系统管理 --\u003e 系统设置 --\u003e 全局属性 --\u003e 环境变量 --\u003e 新增 --\u003e 键值对列表 --\u003e 键: ANDROID_HOME 值: /opt/android ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:5:1","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置全局工具 配置 JAVA - 配置 Gradle ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:5:2","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"创建 Jenkins 工程 创建一个 自定义风格的工程 配置 构建 –\u003e shell 主要配置 乐固、签名以及上传蒲公英 #!/bin/sh set -e # 蒲公英 KEY PGYKEY= # 蒲公英 APIKEY PGYAPIKEY= # 腾讯云乐固认证 SECRET= # 腾讯云乐固认证 KEY SECRETKEY= echo \"================加固开始=======================\" rm -rf /opt/legu mkdir -p /opt/legu java -Dfile.encoding=utf-8 -jar /opt/android/ms-client/ms-shield.jar -sid ${SECRET} -skey ${SECRETKEY} -uploadPath /apk/release/app-product-release.apk -downloadPath /opt/legu echo \"================加固结束=======================\" ls -lt /opt/legu/app-product-release_legu.apk echo \"================开始签名=======================\" /opt/android/build-tools/29.0.3/apksigner sign \\ --ks /opt/android/sign/xx.jks \\ --ks-key-alias xxx \\ --ks-pass pass:xxxxx \\ --key-pass pass:xxxxx \\ --out /opt/legu/app-product-release-sign_legu.apk /opt/legu/app-product-release_legu.apk echo \"================签名结束=======================\" ls -lt /opt/legu/app-product-release-sign_legu.apk echo \"================上传蒲公英=====================\" echo \"upload online apk to 蒲公英\" RESULT=$(curl -F \"file=@/opt/legu/app-product-release-sign_legu.apk\" -F \"uKey=${PGYKEY}\" -F \"_api_key=${PGYAPIKEY}\" https://qiniu-storage.pgyer.com/apiv1/app/upload) echo \"================上传蒲公英结束=====================\" echo \"================下载地址==========================\" # 返回的是 json 格式的 所以格式化一下 echo $RESULT|jq . 构建后的信息 ================下载地址========================== { \"code\": 0, \"message\": \"\", \"data\": { \"appKey\": \"xxxxxxx\", \"userKey\": \"xxxxxxxxx\", \"appType\": \"2\", \"appIsLastest\": \"1\", \"appFileSize\": \"xxxxxxx\", \"appName\": \"jicki\", \"appVersion\": \"1.8.0\", \"appVersionNo\": \"13\", \"appBuildVersion\": \"55\", \"appIdentifier\": \"xxx.xx.xxx\", \"appIcon\": \"d9b32400cd42dcc1\", \"appDescription\": \"\", \"appUpdateDescription\": \"\", \"appScreenshots\": \"\", \"appShortcutUrl\": \"jicki\", \"appCreated\": \"2020-04-24 18:20:08\", \"appUpdated\": \"2020-04-24 18:20:08\", \"appQRCodeURL\": \"https://www.pgyer.com/app/qrcodeHistory/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" } } Finished: SUCCESS ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:6:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["jenkins"],"content":"配置企业微信通知 企业微信中, 创建一个群聊 进入群 -\u003e 右击群标签, 添加群机器人 添加机器人名称 复制 Webhook 安装 jenkins 插件 插件名称为 Qy Wechat Notification , 在 jenkins 插件管理中心安装 配置机器人 Webhook 进入 jenkins 的 Job 选择 构建后的操作 –\u003e 选择 企业微信通知 企业微信 构建信息 ","date":"2020-04-26","objectID":"/jenkins-gradle-android/:7:0","tags":["jenkins"],"title":"jenkins gradle android","uri":"/jenkins-gradle-android/"},{"categories":["kubernetes"],"content":"Kubernetes Horizontal Pod Autoscaler","date":"2020-04-20","objectID":"/horizontal-pod-autoscale/","tags":["kubernetes"],"title":"Kubernetes Horizontal Pod Autoscaler","uri":"/horizontal-pod-autoscale/"},{"categories":["kubernetes"],"content":"Horizontal Pod Autoscaler Pod 水平自动伸缩（Horizontal Pod Autoscaler） 简称 HPA HPA 可以基于CPU利用率自动伸缩 replication controller、deployment和 replica set 中的 pod 数量,（除了 CPU 利用率）也可以 基于其他应程序提供的度量指标 custom metrics。 Pod 水平自动伸缩特性由 Kubernetes API 资源和控制器实现。资源决定了控制器的行为。 控制器会周期性的获取平均 CPU 利用率，并与目标值相比较后来调整 replication controller 或 deployment 中的副本数量。 ","date":"2020-04-20","objectID":"/horizontal-pod-autoscale/:0:0","tags":["kubernetes"],"title":"Kubernetes Horizontal Pod Autoscaler","uri":"/horizontal-pod-autoscale/"},{"categories":["kubernetes"],"content":"HPA 原理 Pod 水平自动伸缩的实现是一个控制循环, 由 controller manager 的 --horizontal-pod-autoscaler-sync-period 参数 指定周期（默认值为15秒）。 每个周期内，controller manager 根据每个 HorizontalPodAutoscaler 定义中指定的指标查询资源利用率。 controller manager 可以从 resource metrics API（每个pod 资源指标）和 custom metrics API（其他指标）获取指标。 对于每个 pod 的资源指标（如 CPU），控制器从资源指标 API 中获取每一个 HorizontalPodAutoscaler 指定 的 pod 的指标，然后，如果设置了目标使用率，控制器获取每个 pod 中的容器资源使用情况，并计算资源使用率。 如果使用原始值，将直接使用原始数据（不再计算百分比）。 然后，控制器根据平均的资源使用率或原始值计算出缩放的比例，进而计算出目标副本数。 注: 如果 pod 某些容器不支持资源采集，那么控制器将不会使用该 pod 的 CPU 使用率。 如果 pod 使用自定义指示，控制器机制与资源指标类似，区别在于自定义指标只使用原始值，而不是使用率。 如果 pod 使用对象指标和外部指标（每个指标描述一个对象信息）。 这个指标将直接跟据目标设定值相比较，并生成一个上面提到的缩放比例。在 autoscaling/v2beta2 版本API中， 这个指标也可以根据 pod 数量平分后再计算。 通常情况下，控制器将从一系列的聚合 API （metrics.k8s.io、custom.metrics.k8s.io和external.metrics.k8s.io） 中获取指标数据。 metrics.k8s.io API 通常由 metrics-server（需要额外启动）提供。 注: 自 Kubernetes 1.11 起, 从 Heapster 获取指标特性已废弃。 ","date":"2020-04-20","objectID":"/horizontal-pod-autoscale/:1:0","tags":["kubernetes"],"title":"Kubernetes Horizontal Pod Autoscaler","uri":"/horizontal-pod-autoscale/"},{"categories":["kubernetes"],"content":"HPA 算法细节 从最基本的角度来看，pod 水平自动缩放控制器跟据当前指标和期望指标来计算缩放比例。 期望副本数 = ceil[当前副本数 * ( 当前指标 / 期望指标 )] 如: 当前指标为200m, 目标设定值为100m, 那么由于200.0 / 100.0 == 2.0, 副本数量将会翻倍。 如: 当前指标为50m, 副本数量将会减半，因为50.0 / 100.0 == 0.5。 如: 计算出的缩放比例接近1.0（跟据--horizontal-pod-autoscaler-tolerance 参数全局配置的容忍值，默认为0.1）, 将会放弃本次缩放。 ","date":"2020-04-20","objectID":"/horizontal-pod-autoscale/:2:0","tags":["kubernetes"],"title":"Kubernetes Horizontal Pod Autoscaler","uri":"/horizontal-pod-autoscale/"},{"categories":["kubernetes"],"content":"创建 HPA 如下使用 deployment 进行 HPA 的测试 HPA API v1 版本只支持CPU , v2beta2 版本支持多 metrics(CPU，memory)以及自定义metrics。 # 创建 deployment 与 service [root@k8s-node-1 hpa]# cat nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx version: v1.0.0 spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http # 资源的限制 resources: limits: cpu: 1000m memory: 500Mi requests: # 1 cpu = 1000m cpu: 0.1 memory: 250Mi --- apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: app: nginx 创建 hpa 通过命令 kubectl autoscale deployment nginx --min=1 --max=5 --cpu-percent=50 默认使用 api v1 。 [root@k8s-node-1 hpa]# cat nginx-hpa.yaml apiVersion: autoscaling/v2beta2 kind: HorizontalPodAutoscaler metadata: name: nginx-hpa labels: app: nginx-hpa spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: nginx # 最少pod数量 minReplicas: 1 # 最多pods数量 maxReplicas: 5 metrics: - type: Resource resource: name: memory target: type: Utilization averageUtilization: 50 - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 50 查看 hpa 服务 [root@k8s-node-1 hpa]# kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE nginx-hpa Deployment/nginx 1%/50%, 0%/50% 1 5 1 81m 测试一下 # 使用 wrk 进行压测 # 因为需要在 docker中执行,所以就直接对 svc 的IP进行测试 docker run --rm williamyeh/wrk -c100 -t5 -d300s http://10.254.60.87 # 查看 hpa 的情况 (感觉时间有点长, 特别是缩容的时候) [root@k8s-node-1 hpa]# kubectl get hpa -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE nginx-hpa Deployment/nginx 1%/50%, 0%/50% 1 5 1 82m nginx-hpa Deployment/nginx 1%/50%, 999%/50% 1 5 1 83m nginx-hpa Deployment/nginx 1%/50%, 999%/50% 1 5 4 83m nginx-hpa Deployment/nginx 1%/50%, 999%/50% 1 5 5 83m nginx-hpa Deployment/nginx 1%/50%, 265%/50% 1 5 5 84m nginx-hpa Deployment/nginx 1%/50%, 231%/50% 1 5 5 85m nginx-hpa Deployment/nginx 1%/50%, 245%/50% 1 5 5 85m nginx-hpa Deployment/nginx 1%/50%, 255%/50% 1 5 5 86m nginx-hpa Deployment/nginx 1%/50%, 0%/50% 1 5 5 87m nginx-hpa Deployment/nginx 1%/50%, 0%/50% 1 5 5 90m nginx-hpa Deployment/nginx 1%/50%, 0%/50% 1 5 5 92m nginx-hpa Deployment/nginx 1%/50%, 0%/50% 1 5 1 93m ","date":"2020-04-20","objectID":"/horizontal-pod-autoscale/:3:0","tags":["kubernetes"],"title":"Kubernetes Horizontal Pod Autoscaler","uri":"/horizontal-pod-autoscale/"},{"categories":["istio"],"content":"Istio 1.5.x 持续更新...","date":"2020-04-13","objectID":"/istio-1.5/","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio 官方文档 https://istio.io/zh/docs ","date":"2020-04-13","objectID":"/istio-1.5/:0:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio 是什么？(衣撕朵) 云平台令使用它们的公司受益匪浅。但不可否认的是，上云会给 DevOps 团队带来压力。为了可移植性，开发人员必须使用微服务来构建应用，同时运维人员也正在管理着极端庞大的混合云和多云的部署环境。 Istio 允许您连接、保护、控制和观察服务。 从较高的层面来说，Istio 有助于降低这些部署的复杂性，并减轻开发团队的压力。它是一个完全开源的服务网格，作为透明的一层接入到现有的分布式应用程序里。它也是一个平台，拥有可以集成任何日志、遥测和策略系统的 API 接口。Istio 多样化的特性使您能够成功且高效地运行分布式微服务架构，并提供保护、连接和监控微服务的统一方法。 ","date":"2020-04-13","objectID":"/istio-1.5/:1:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"服务网格 又是什么？ 服务网格 - 用来描述组成这些应用程序的微服务网络以及它们之间的交互。随着服务网格的规模和复杂性不断的增长，它将会变得越来越难以理解和管理。它的需求包括服务发现、负载均衡、故障恢复、度量和监控等。服务网格通常还有更复杂的运维需求，比如 A/B 测试、金丝雀发布、速率限制、访问控制和端到端认证。 ","date":"2020-04-13","objectID":"/istio-1.5/:2:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio 架构 Istio 服务网格从逻辑上分为数据平面和控制平面。 数据平面 由一组智能代理（Envoy）组成，被部署为 sidecar。这些代理通过一个通用的策略和遥测中心（Mixer）传递和控制微服务之间的所有网络通信。 控制平面 管理并配置代理来进行流量路由。此外，控制平面配置 Mixer 来执行策略和收集遥测数据。 ","date":"2020-04-13","objectID":"/istio-1.5/:3:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio 组件 ","date":"2020-04-13","objectID":"/istio-1.5/:4:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Envoy Istio 使用 Envoy 代理的扩展版本。 Envoy 是用 C++ 开发的高性能代理，用于协调服务网格中所有服务的入站和出站流量。 Envoy 代理是唯一与数据平面流量交互的 Istio 组件。 Envoy 代理被部署为服务的 sidecar，在逻辑上为服务增加了 Envoy 的许多内置特性: 动态服务发现 负载均衡 TLS 终端 HTTP/2 与 gRPC 代理 熔断器 健康检查 基于百分比流量分割的分阶段发布 故障注入 丰富的指标 sidecar 代理模型 sidecar 允许 Istio 提取大量关于流量行为的信号作为属性。反之，Istio 可以在 Mixer 中使用这些属性来执行决策，并将它们发送到监控系统，以提供整个网格的行为信息。 sidecar 还允许您向现有的部署添加 Istio 功能，而不需要重新设计架构或重写代码。 Envoy 代理在 istio 中可以实现 流量控制功能： 通过丰富的 HTTP、gRPC、WebSocket 和 TCP 流量路由规则来执行细粒度的流量控制。 网络弹性特性： 重试设置、故障转移、熔断器和故障注入。 安全性和身份验证特性： 执行安全性策略以及通过配置 API 定义的访问控制和速率限制。 ","date":"2020-04-13","objectID":"/istio-1.5/:4:1","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Mixer Mixer 是一个平台无关的组件。Mixer 在整个服务网格中执行访问控制和策略使用，并从 Envoy 代理和其他服务收集遥测数据。代理提取请求级别属性，并将其发送到 Mixer 进行评估。您可以在我们的 Mixer 配置文档中找到更多关于属性提取和策略评估的信息。 Mixer 包括一个灵活的插件模型。该模型使 Istio 能够与各种主机环境和后端基础设施进行交互。因此，Istio 从这些细节中抽象出 Envoy 代理和 Istio 管理的服务。 ","date":"2020-04-13","objectID":"/istio-1.5/:4:2","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Pilot Pilot 主要是为 Envoy sidecar 提供服务发现、用于智能路由的流量管理功能（例如，A/B 测试、金丝雀发布等）以及弹性功能（超时、重试、熔断器等）。 Pilot 将控制流量行为的高级路由规则转换为特定于环境的配置，并在运行时将它们传播到 sidecar。 Pilot 将特定于平台的服务发现机制抽象出来，并将它们合成为任何符合 Envoy API 的 sidecar 都可以使用的标准格式。 平台适配器 与 Envoy 交互图 (平台 支持 kubernetes、Consul、gcp、Nomad等) 平台启动一个服务的新实例，该实例通知其平台适配器。 平台适配器使用 Pilot 抽象模型注册实例。 Pilot 将流量规则和配置派发给 Envoy 代理，来传达此次更改。 可以使用 Istio 的流量管理 API 通过 Pilot 优化 Envoy 配置，以便对服务网格中的流量进行更细粒度地控制。 ","date":"2020-04-13","objectID":"/istio-1.5/:4:3","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Citadel Citadel 通过内置的身份和证书管理，可以支持强大的服务到服务以及最终用户的身份验证。 Citadel 可以用来升级服务网格中的未加密流量。 Citadel、operator 结合使用可以执行基于服务身份的策略，而不是相对不稳定的 3 层或 4 层网络标识。 Citadel 使用 Istio 的授权特性来控制谁可以访问您的服务。 ","date":"2020-04-13","objectID":"/istio-1.5/:4:4","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Galley Galley 是 Istio 的配置验证、提取、处理和分发组件。它负责将其余的 Istio 组件与从底层平台（例如 Kubernetes）获取用户配置的细节隔离开来。 ","date":"2020-04-13","objectID":"/istio-1.5/:4:5","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio Install 我这里 Kubernetes 版本为1.18 因为我目前只有这么一个集群 安装 Istio 环境准备 搭建 Kubernetes 集群, ( 请按照官方提供的兼容测试版本安装 目前支持 1.14, 1.15, 1.16 ) 下载 Istio 项目包. 项目包内包括(安装文件、示例和 istioctl 命令行工具。) 安装 Istio. 通过 istioctl 客户端工具直接安装istio 到 Kubernetes 中. ","date":"2020-04-13","objectID":"/istio-1.5/:5:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"搭建 Kubernetes 这一部分这里就省略了, 如需这一部分 文档, 请参考其他的博文。 ","date":"2020-04-13","objectID":"/istio-1.5/:5:1","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"下载 Istio 项目包 在 macOS 或 Linux 系统中, 也可以通过以下命令下载最新版本的 Istio # 新建目录 mkdir -p /opt/istio \u0026\u0026 cd /opt/istio # 设置安装版本 export ISTIO_VERSION=1.5.1 # 下载文件 curl -L https://istio.io/downloadIstio | sh - 输出如下: Istio 1.5.1 Download Complete! Istio has been successfully downloaded into the istio-1.5.1 folder on your system. Next Steps: See https://istio.io/docs/setup/kubernetes/install/ to add Istio to your Kubernetes cluster. To configure the istioctl client tool for your workstation, add the /opt/istio/istio-1.5.1/bin directory to your environment path variable with: export PATH=\"$PATH:/opt/istio/istio-1.5.1/bin\" Begin the Istio pre-installation verification check by running: istioctl verify-install Need more information? Visit https://istio.io/docs/setup/kubernetes/install/ 配置环境变量 vi /etc/profile # 添加 # istio export PATH=\"$PATH:/opt/istio/istio-1.5.1/bin\" # 生效配置 source /etc/profile 验证安装 istioctl verify-install Checking the cluster to make sure it is ready for Istio installation... #1. Kubernetes-api ----------------------- Can initialize the Kubernetes client. Can query the Kubernetes API Server. #2. Kubernetes-version ----------------------- Istio is compatible with Kubernetes: v1.18.0. #3. Istio-existence ----------------------- Istio will be installed in the istio-system namespace. #4. Kubernetes-setup ----------------------- Can create necessary Kubernetes configurations: Namespace,ClusterRole,ClusterRoleBinding,CustomResourceDefinition,Role,ServiceAccount,Service,Deployments,ConfigMap. #5. SideCar-Injector ----------------------- This Kubernetes cluster supports automatic sidecar injection. To enable automatic sidecar injection see https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/#deploying-an-app ----------------------- Install Pre-Check passed! The cluster is ready for Istio installation. 配置 istioctl 命令自动补全 # 创建目录 mkdir -p /usr/share/istio # 拷贝补全脚本 cp tools/istioctl.bash /usr/share/istio/ # 导入自动补全 source /usr/share/istio/istioctl.bash # 添加到 bashrc 中 vi ~/.bashrc # 添加如下: # istio source /usr/share/istio/istioctl.bash # 测试tab补全 [root@k8s-node-1 istio-1.5.1]# istioctl analyze authz dashboard experimental manifest profile proxy-status upgrade verify-install authn convert-ingress deregister kube-inject operator proxy-config register validate version 目录结构说明 bin 目录包含 istioctl 的客户端文件。istioctl 工具用于手动注入 Envoy sidecar 代理。 manifest.yaml 文件的 sha码。 samples 目录包含 istio 的实例应用程序。 tools 目录包含 一些脚本 convert_RbacConfig_to_ClusterRbacConfig.sh dump_kubernetes.sh _istioctl istioctl.bash istio 命令tab自动补全的脚本 install 目录包含如下目录: (istio除了支持 kubernetes 之外还支持 consul 和 gcp) consul 目录 gcp 目录 kubernetes 目录包含 kuebrnetes 服务相关的 YAML 文件。 tools 目录 ","date":"2020-04-13","objectID":"/istio-1.5/:5:2","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"安装 istio istio 包含两种安装方式 通过 istioctl 客户端命令安装 (推荐) 通过 helm 进行安装 # 通过如下命令进行安装 (manifest 是资源清单, profile 指定类型的清单) istioctl manifest apply --set profile=demo 输出如下: - Applying manifest for component Base... ✔ Finished applying manifest for component Base. - Applying manifest for component Pilot... ✔ Finished applying manifest for component Pilot. - Applying manifest for component EgressGateways... - Applying manifest for component IngressGateways... - Applying manifest for component AddonComponents... ✔ Finished applying manifest for component EgressGateways. ✔ Finished applying manifest for component IngressGateways. ✔ Finished applying manifest for component AddonComponents. ✔ Installation complete 查看部署情况 如果集群运行在一个不支持外部负载均衡器的环境中（例如：minikube），istio-ingressgateway 的 EXTERNAL-IP 将显示为 \u003cpending\u003e 状态。请使用服务的 NodePort 或 端口转发来访问网关。 kubectl get pods -n istio-system NAME READY STATUS RESTARTS AGE grafana-5f6f8cbf75-lqnjg 1/1 Running 0 17m istio-egressgateway-74896c8487-mjlg8 1/1 Running 0 17m istio-ingressgateway-54d494869-7npql 1/1 Running 0 17m istio-tracing-9dd6c4f7c-x5kcp 1/1 Running 0 17m istiod-756bd84654-n2k6m 1/1 Running 0 18m kiali-869c6894c5-64vmw 1/1 Running 0 17m prometheus-c89875c74-rgzdx 2/2 Running 0 17m kubectl get svc -n istio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE grafana ClusterIP 10.254.35.48 \u003cnone\u003e 3000/TCP 158m istio-egressgateway ClusterIP 10.254.10.123 \u003cnone\u003e 80/TCP,443/TCP,15443/TCP 158m istio-ingressgateway LoadBalancer 10.254.26.46 \u003cpending\u003e 15020:32142/TCP,80:30000/TCP,443:32701/TCP,15029:30413/TCP,15030:30781/TCP,15031:31714/TCP,15032:32419/TCP,31400:30673/TCP,15443:31123/TCP 158m istio-pilot ClusterIP 10.254.54.205 \u003cnone\u003e 15010/TCP,15011/TCP,15012/TCP,8080/TCP,15014/TCP,443/TCP 158m istiod ClusterIP 10.254.3.7 \u003cnone\u003e 15012/TCP,443/TCP 158m jaeger-agent ClusterIP None \u003cnone\u003e 5775/UDP,6831/UDP,6832/UDP 158m jaeger-collector ClusterIP 10.254.2.80 \u003cnone\u003e 14267/TCP,14268/TCP,14250/TCP 158m jaeger-collector-headless ClusterIP None \u003cnone\u003e 14250/TCP 158m jaeger-query ClusterIP 10.254.61.2 \u003cnone\u003e 16686/TCP 158m kiali ClusterIP 10.254.7.135 \u003cnone\u003e 20001/TCP 158m prometheus ClusterIP 10.254.8.200 \u003cnone\u003e 9090/TCP 158m tracing ClusterIP 10.254.39.104 \u003cnone\u003e 80/TCP 158m zipkin ClusterIP 10.254.32.230 \u003cnone\u003e 9411/TCP 158m 组件说明 tracing 全链路监控。 istio-pilot 服务发现与服务配置。 kiali 可视化服务网格展示。 服务拓扑图 分布式跟踪 指标度量收集和图标 配置校验 健康检查和显示 服务发现 prometheus 大家都懂的监控。 grafana prometheus监控的展示webui。 istio-ingressgateway 出口网关。 istio-egressgateway 入口网关。 jaeger  jaeger-agent jaeger client的一个代理程序，client将收集到的调用链数据发给agent，然后由agent发给collector； jaeger-collector 负责接收jaeger client或者jaeger agent上报上来的调用链数据，然后做一些校验，比如时间范围是否合法等，最终会经过内部的处理存储到后端存储； jaeger-query 专门负责调用链查询的一个服务。 修改 istio-ingressgateway 网络类型 为 NodePort kubectl patch service istio-ingressgateway -n istio-system -p '{\"spec\":{\"type\":\"NodePort\"}}' kubectl get svc -n istio-system istio-ingressgateway NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway NodePort 10.254.26.46 \u003cnone\u003e 15020:32142/TCP,80:30000/TCP,443:32701/TCP,15029:30413/TCP,15030:30781/TCP,15031:31714/TCP,15032:32419/TCP,31400:30673/TCP,15443:31123/TCP 176m ","date":"2020-04-13","objectID":"/istio-1.5/:5:3","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"验证 istio 查看版本 [root@k8s-node-1 ~]# istioctl version client version: 1.5.1 control plane version: 1.5.1 data plane version: 1.5.1 (3 proxies) ","date":"2020-04-13","objectID":"/istio-1.5/:5:4","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Kiali 组件 Kiali 以 web ui 的方式可视化服务网格。 查看 kiali svc [root@k8s-node-1 kubeadm]# kubectl get svc -n istio-system kiali NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kiali ClusterIP 10.254.7.135 \u003cnone\u003e 20001/TCP 4h24m 配置访问(我这里的node节点为云主机,所以我配置了一个 ingress) [root@k8s-node-1 ~]# cat kiali-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kiali-ingress namespace: istio-system spec: rules: - host: kiali.jicki.cn http: paths: - backend: serviceName: kiali servicePort: 20001 创建ingress服务 [root@k8s-node-1 kubeadm]# kubectl apply -f kiali-ingress.yaml ingress.extensions/kiali-ingress created # 查看服务 [root@k8s-node-1 ~]# kubectl get ingress -n istio-system NAME CLASS HOSTS ADDRESS PORTS AGE kiali-ingress \u003cnone\u003e kiali.jicki.cn 10.254.8.81 80 2m31s ","date":"2020-04-13","objectID":"/istio-1.5/:5:5","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio Profile Profile 相关的介绍以及具体的区别 istioctl profile list 命令可查看当前版本的 profile [root@k8s-node-1 istio]# istioctl profile list Istio configuration profiles: minimal remote separate default demo empty profile 包含如下: remote 远程kubernetes部署, 以及多kubernetes集群 separatei 独立部署,不建议使用,后续可能删除 default 默认安装, 根据IstioControlPlaneAPI的默认设置启用组件, 建议用于生产部署 demo 演示实例,展示istio 所有功能且资源需求适中的配置 empty 不部署任何内容。用于导出空的配置文件。 minimal 最小化安装。 istio组件 default demo minimal remote empty istio-egressgateway ✔ istio-ingressgateway ✔ ✔ ✔ istio-pilot ✔ ✔ ✔ ✔ grafana ✔ istio-tracing ✔ Kiali ✔ prometheus ✔ ✔ ✔ jaeger ✔ zipkin ✔ istioctl profile dump profileName 可以打印或者导出profile配置 这里导出的文件就是kubernetes 的 YAML 编排文件。api 为 istio 的 api。 这里可以导出 default 然后根据自己的环境自定义适合自己的 profile。 [root@k8s-node-1 istio]# istioctl profile dump default \u003e default.yaml ","date":"2020-04-13","objectID":"/istio-1.5/:5:6","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"istio injection injection 注入后的变化 原生 pod –\u003e pods 包含 程序 项目。 注入以后 pod –\u003e pods 包含 程序 项目、istio-init、istio-proxy。 istio-init 用于初始化网络配置, iptables 路由配置。 istio-proxy 用于当前pod 与集群内部其他资源进行交互。 可以被 injection 的服务 Deployment - 注入后会添加 istio-init、istio-proxy 。 ReplicaSet - 注入后会添加 istio-init、istio-proxy 。 DeamonSet - 注入后会添加 istio-init、istio-proxy 。 Pod - 注入后会添加 istio-init、istio-proxy 。 Job - 注入后会添加 istio-init、istio-proxy 。 Service - 注入后不会添加任何组件。 Secrets - 注入后不会添加任何组件。 ConfigMap - 注入后不会添加任何组件。 deployment yaml 编排文件注入 istioctl kube-inject -f nginx-test.yaml 对 Deployment编排文件进行注入(会修改yaml文件内容) kubectl apply -f \u003c(istioctl kube-inject -f nginx-test.yaml) 注入并 创建 服务到kubernetes中, 这样操作不会改变原有的编排文件。 # 创建一个 namespaces [root@k8s-node-1 istio]# kubectl create ns jicki namespace/jicki created # 创建一个 deployment [root@k8s-node-1 istio]# cat nginx-test.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: jicki labels: app: nginx spec: replicas: 1 selector: matchLabels: app: nginx template: metadata: labels: app: nginx version: v1.0.0 spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http 导出注入后的编排文件 [root@k8s-node-1 istio]# istioctl kube-inject -f nginx-test.yaml \u003e nginx-test-inject.yaml # 注入后 yaml 发生变化, 会增加很多istio的配置 # 包含新增的两个容器分别为 istio-proxy 与 istio-init image: docker.io/istio/proxyv2:1.5.1 imagePullPolicy: IfNotPresent name: istio-proxy image: docker.io/istio/proxyv2:1.5.1 imagePullPolicy: IfNotPresent name: istio-init 创建服务 [root@k8s-node-1 istio]# kubectl apply -f \u003c(istioctl kube-inject -f nginx-test.yaml) deployment.apps/nginx-deployment created [root@k8s-node-1 istio]# kubectl describe pods/nginx-deployment-59897745f9-vlvn9 -n jicki Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u003cunknown\u003e default-scheduler Successfully assigned jicki/nginx-deployment-59897745f9-vlvn9 to k8s-node-1 Normal Pulled 2m19s kubelet, k8s-node-1 Container image \"docker.io/istio/proxyv2:1.5.1\" already present on machine Normal Created 2m19s kubelet, k8s-node-1 Created container istio-init Normal Started 2m18s kubelet, k8s-node-1 Started container istio-init Normal Pulled 2m18s kubelet, k8s-node-1 Container image \"nginx:alpine\" already present on machine Normal Created 2m18s kubelet, k8s-node-1 Created container nginx Normal Started 2m17s kubelet, k8s-node-1 Started container nginx Normal Pulled 2m17s kubelet, k8s-node-1 Container image \"docker.io/istio/proxyv2:1.5.1\" already present on machine Normal Created 2m17s kubelet, k8s-node-1 Created container istio-proxy Normal Started 2m17s kubelet, k8s-node-1 Started container istio-proxy # 查看 istio 对外服务的端口以及相关进程 # 可以发现除了80端口还有其他5个额外的端口 [root@k8s-node-1 istio]# kubectl exec -it nginx-deployment-59897745f9-vlvn9 -n jicki -c istio-proxy -- netstat -ntlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN 17/envoy tcp 0 0 0.0.0.0:15006 0.0.0.0:* LISTEN 17/envoy tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:15090 0.0.0.0:* LISTEN 17/envoy tcp 0 0 127.0.0.1:15000 0.0.0.0:* LISTEN 17/envoy tcp6 0 0 :::15020 :::* LISTEN 1/pilot-agent Address 端口 类型 进程 说明 127.0.0.1 15000 TCP envoy envoy 命令行管理端口,可以获取程序运行信息。 0.0.0.0 15001 TCP envoy 0.0.0.0 15006 TCP envoy 0.0.0.0 15020 HTTP pilot-agent 0.0.0.0 15090 HTTP envoy pilot-agent 生成 envoy 配置文件 启动 envoy 进程 监控和管理envoy进程的运行状态, 故障恢复, envoy配置变更以及重新加载。 envoy 轻量级的代理服务器。 service yaml 编排文件注入 上面已经提到过 service 注入是不会添加任何其他的服务到 service 中的。 # 创建一个基于 nginx 的 svc [root@k8s-node-1 istio]# cat nginx-test-svc.yaml apiVersion: v1 kind: Service metadata: name: nginx-svc namespace: jicki labels: app: nginx spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: app: nginx # 创建服务 [root@k8s-node-1 istio]# kubectl apply -f \u003c(istioctl kube-inject -f nginx-test-svc.yaml) service/nginx-svc created # 输出一个注入后的文件 [root@k8s-node-1 istio]# istioctl kube-inject -f nginx-test-svc.yaml -o","date":"2020-04-13","objectID":"/istio-1.5/:5:7","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio 流量管理 Istio 的流量管理 就是配置 HTTP/TCP 路由功能。 Istio 流量 data plane 数据面流量 是指 网格内服务之间业务互相调用所产生的流量。 control plane 控制面流量 是指 Istio 各组件之间配置和控制网格行为所产生的流量。 Istio 流量管理分为六个部分 Destination Rule Envoy Filter Gateway Service Entry Sidecar Virtual Service ","date":"2020-04-13","objectID":"/istio-1.5/:6:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio 流量模型 Istio 依赖与服务注入(inject)时所部署的 Envoy 代理。 网格内发送与接收的所有的流量, 都通过 Envoy 进行代理。 Envoy代理 可以轻松的引导和控制网格的流量, 而不需要对服务进行任何修改。 ","date":"2020-04-13","objectID":"/istio-1.5/:6:1","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Virtual Service 在Istio所提供的基本连接和发现基础上, 通过虚拟服务(vitrual service), 能够将请求路由到Istio网格中的特定服务。每个虚拟服务(vitrual service)由一组路由规则组成, 这些路由规则使Istio能够将虚拟服务(vitrual service)的每个给定请求匹配到网格内特定的目标地址。 虚拟服务(Virtual Service) 定义了一组寻址主机时要使用的流量路由规则，每个路由规则为特定协议的流量定义匹配了条件。如果流量匹配，则将其发送到注册表中定义的命名目标服务（或其子集/版本）。同时流量来源也可以在路由规则中匹配，从而能够允许针对特定的客户端上下文自定义路由。 服务（Service）: 应用绑定到服务注册表中的唯一名称。服务包含多个网络端点，这些端点由在Pod、容器和VM等上运行的工作负载实例进行实现。 服务版本（Service versions）或子集: 在持续部署的场景中，对于给定的服务，可能存在不同的应用程序实例。它们可能是对同一服务的迭代更改，这些版本部署在不同的环境（产品，阶段和开发等）中。场景包括A / B测试 和 金丝雀发布 等。可以根据各种标准（标头，网址等）和/或通过分配给每个版本的权重来确定特定版本的选择。每个服务都有一个包含其所有实例的默认版本。 来源（Source）: 调用服务的下游客户端。 主机（Host）: 下游客户端连接服务时所使用的地址。 访问模式（Access model）: 应用程序仅需要处理目标服务（主机），而无需了解各个服务版本（子集）。版本的实际选择由代理/sidecar决定，从而使应用程序代码能够与所依赖服务的脱钩。 一个例子 注: 所有服务(deployment)、(service)、包括 client 都必须被 istio 注入(injection)。 # 2 个 nginx deployment 3 个 service [root@k8s-node-1 istio]# cat nginx-test.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-1 labels: web: nginx-1 spec: replicas: 1 selector: matchLabels: web: nginx-1 template: metadata: labels: app: nginx web: nginx-1 version: v1.0.0 spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http command: [\"/bin/sh\", \"-c\", \"echo 'hello nginx-1' \u003e /usr/share/nginx/html/index.html; nginx -g 'daemon off;'\"] --- apiVersion: apps/v1 kind: Deployment metadata: name: nginx-2 labels: web: nginx-2 spec: replicas: 1 selector: matchLabels: web: nginx-2 template: metadata: labels: app: nginx web: nginx-2 version: v1.0.1 spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http command: [\"/bin/sh\", \"-c\", \"echo 'hello nginx-2' \u003e /usr/share/nginx/html/index.html; nginx -g 'daemon off;'\"] --- apiVersion: v1 kind: Service metadata: name: nginx-svc-1 labels: web: nginx-1 spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: web: nginx-1 --- apiVersion: v1 kind: Service metadata: name: nginx-svc-2 labels: web: nginx-2 spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: web: nginx-2 --- apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: app: nginx 查看配置的三个 service 的 endpoints [root@k8s-node-1 istio]# kubectl get ep NAME ENDPOINTS AGE kubernetes 172.18.186.159:6443 16d nginx-svc 10.254.64.163:80,10.254.64.164:80 6m23s nginx-svc-1 10.254.64.163:80 26s nginx-svc-2 10.254.64.164:80 26s 配置一个 Virtual Service [root@k8s-node-1 istio]# cat nginx-test-vs.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: nginx-svc-vs spec: # 客户端访问服务的地址 hosts: - nginx-svc.default.svc.cluster.local # http协议 http: # 如下为匹配条件 - match: # http 头 包含如下信息 - headers: # key = vs-user vs-user: # exact 完全匹配 value = jicki exact: jicki route: - destination: host: nginx-svc-3.default.svc.cluster.local # 如下为匹配路由规则 - route: - destination: # 匹配的服务版本或子集为 nginx-1 host: nginx-svc-1.default.svc.cluster.local weight: 80 - destination: # 匹配的服务版本或子集为 nginx-2 host: nginx-svc-2.default.svc.cluster.local weight: 20 VirtualService 服务并非是 kubernetes 中实际的 service 服务。 kubectl get virtualservices 使用这个命令查看 Virtual Service [root@k8s-node-1 istio]# kubectl get virtualservices NAME GATEWAYS HOSTS AGE nginx-svc-vs [nginx-svc.default.svc.cluster.local] 3m1s 注入所有的服务, 所有服务都必须通过 istio 注入 # 注入 [root@k8s-node-1 istio]# kubectl apply -f \u003c(istioctl kube-inject -f nginx-test.yaml) deployment.apps/nginx-1 configured deployment.apps/nginx-2 configured service/nginx-svc-1 unchanged service/nginx-svc-2 unchanged service/nginx-svc unchanged 测试以及 kiali 中查看 # 创建一个 busybox 的 deployment 作为客户端 [root@k8s-node-1 istio]# cat busybox.yaml apiVersion: apps/v1 kind: Deployment metadata: name: busybox labels: app: busybox spec: replicas: 1 selector: matchLabels: app: busybox template: metadata: labels: app: busybox spec: containers: - name: busybox image: busybox imagePullPolicy: IfNotPresent command: [\"/bin/sh\", \"-c\", \"sle","date":"2020-04-13","objectID":"/istio-1.5/:6:2","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio Upgrade ","date":"2020-04-13","objectID":"/istio-1.5/:7:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"注意事项 升级 Istio 之前, 请确认是否支持升级 istioctl manifest versions 查看支持版本。 升级过程中可能发生流量中断。为了缩短流量中断时间, 请确保每个组件（Citadel 除外）至少运行有两个副本。同时, 确保 PodDistruptionBudgets 配置最小可用性为 1。 确保 istio profile 与 需要升级的版本所配置的 profile 一致。 ","date":"2020-04-13","objectID":"/istio-1.5/:7:1","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"升级步骤 1. 下载需要升级的 istio 版本 curl -L https://istio.io/downloadIstio | ISTIO_VERSION=1.5.1 sh - 2. 替换 istioctl 二进制文件, 或者 更改 istio 的环境变量PATH路径到新版本的目录中。 3. 如果配置了 istioctl 自动补全,还需要替换为 新的 自动补全脚本。 4. 使用新的 istioctl 导出新版本的 profile 文件 istioctl profile dump demo \u003e demo.yaml 5. 修改 demo.yaml 文件, 将其中 jwtPolicy 身份验证机构修改为 first-party-jwt。Istio 将默认使用第三方令牌。 5.1 验证是否支持 第三方令牌。kubectl get --raw /api/v1 | jq '.resources[] | select(.name | index(\"serviceaccounts/token\"))' 5.1.1 jwtPolicy = third-party-jwt 使用第三方令牌 更安全 Istio 默认使用这个选项。 5.1.2 jwtPolicy = first-party-jwt 使用第一方令牌 属性比较不安全。 6. istioctl upgrade -f demo.yaml 命令进行升级。 7. 观察 kubernets 中 istio-system 的服务更新完成。 8. 重新注入环境中部署的服务, 用以更新注入数据。 8.1 对于自动注入的情况可使用如下命令: 8.1.1 kubectl rollout restart deployment 8.1.2 kubectl rollout restart statefulset 8.1.3 kubectl rollout restart daemonset 8.2 对于手动注入的情况( 需要重新 apply 一下服务) : 8.2.1 kubectl apply -f \u003c(istioctl kube-inject -f nginx-test.yaml) 9. 检查升级, 执行 istioctl version 检查 9.1 client version 版本是否为新版本。 9.2 control plane version 版本是否为新版本。 9.3 data plane version 中是否全部 proxies 都为新版本。 ","date":"2020-04-13","objectID":"/istio-1.5/:7:2","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["istio"],"content":"Istio UnInstall 卸载 Istio 的话其实就是讲 kubernetes 所安装到的组件 delete 掉就可以了 # 可以直接运行istioctl 命令加上 kubectl delete 命令组合删除 istioctl manifest generate --set profile=demo |kubectl delete -f - ","date":"2020-04-13","objectID":"/istio-1.5/:8:0","tags":["istio"],"title":"Istio 1.5.x 持续更新...","uri":"/istio-1.5/"},{"categories":["golang"],"content":"LeetCode golang 题目","date":"2020-04-09","objectID":"/leetcode-100/","tags":["golang"],"title":"LeetCode golang 题目","uri":"/leetcode-100/"},{"categories":["golang"],"content":"LeetCode 基于多写多练的原则, LeetCode 刷起来.. ","date":"2020-04-09","objectID":"/leetcode-100/:0:0","tags":["golang"],"title":"LeetCode golang 题目","uri":"/leetcode-100/"},{"categories":["kubernetes"],"content":"Kubernetes RollingUpdate","date":"2020-04-02","objectID":"/k8s-rollingupdate/","tags":["kubernetes"],"title":"Kubernetes RollingUpdate","uri":"/k8s-rollingupdate/"},{"categories":["kubernetes"],"content":"RollingUpdate kubernetes 应用升级操作 Deployment StatefulSet DaemonSet ","date":"2020-04-02","objectID":"/k8s-rollingupdate/:0:0","tags":["kubernetes"],"title":"Kubernetes RollingUpdate","uri":"/k8s-rollingupdate/"},{"categories":["kubernetes"],"content":"应用部署/升级 kubernetes 在deployment statefulset daemonset 应用yaml中, 配置 RollingUpdate kubernetes 会将 pod 分批次逐步替换掉，可用来实现服务热升级。 一般来讲我们部署/升级应用都会遇到如下问题: pod启动以后, 应用需要花费一定的时间初始化(加载配置), 在这段时间内,程序是无法对外提供服务。 对于这种情况我们可以使用 ReadinessProbe (就绪探针) 。 ReadinessProbe 能让 kubernetes 更准确地判断容器是否就绪，从而构建出更健壮的应用。kubernetes 保证只有 pod 中的所有容器全部通过了就绪探测，才允许 service 将流量发往该 pod。一旦就绪探测失败，kubernetes 会停止将流量发往该 pod 。 ReadinessProbe 探针的配置非常灵活，用户可以指定探针的探测频率、探测成功阈值、探测失败阈值等。 pod启动完成, 应用也启动成功, 但是并不代表应用此时就可以正常提供服务。 对于这种情况我们可以使用 LivenessProbe (活性探针)。 LivenessProbe 能让 kubernetes 更准确地判断容器是否正常运行。如果容器没有通过活性探测，kubelet 会将其停止，并根据重启策略决定下一步的动作。 LivenessProbe 探针的配置非常灵活，用户可以指定探针的探测频率、探测成功阈值、探测失败阈值等。 应用如果无法对外提供服务,此时应用不一定就能立刻退出。 对于这种情况我们可以使用 terminationGracePeriodSeconds。 terminationGracePeriodSeconds 当 kubernetes 准备删除一个 pod 时，会向该 pod 中的容器发送 TERM 信号并同时将 pod 从 service 的 endpoint 列表中移除。如果容器无法在规定时间（默认 30 秒）内终止，kubernetes 会向容器发送 SIGKILL 信号强制终止进程。 应用升级过程中需要保证即将下线的应用实例不会接收到新的请求且有足够时间处理完当前请求。 对于这种情况我们可以使用 maxSurge 和 maxUnavailable。 maxSurge 指定在滚动更新过程中最多可创建多少个额外的 pod, 可以是数字或百分比。 maxUnavailable 指定在滚动更新过程中最多允许多少个 pod 不可用, 可以是数字或百分比。 ","date":"2020-04-02","objectID":"/k8s-rollingupdate/:1:0","tags":["kubernetes"],"title":"Kubernetes RollingUpdate","uri":"/k8s-rollingupdate/"},{"categories":["kubernetes"],"content":"deployment 例子 基于 java 的应用能更好的体现大多数的问题 apiVersion: apps/v1 kind: Deployment metadata: name: spring-boot-probes labels: app: spring-boot-probes spec: replicas: 4 strategy: # 配置 滚动更新 策略 type: RollingUpdate rollingUpdate: # 更新时增加新的 pods 数量(4 + 2 = 6) maxSurge: 2 # 更新时允许最大 pods 数量不对外服务 maxUnavailable: 1 # 新的 pods 就绪状态观察时间。 # 必须保证处于就绪状态的 pod 能经历一个完整的活性探测周期。 minReadySeconds: 120 selector: matchLabels: app: spring-boot-probes template: metadata: labels: app: spring-boot-probes version: v1.0.0 spec: containers: - name: spring-boot-probes image: registry.cn-hangzhou.aliyuncs.com/log-service/spring-boot-probes:1.0.0 ports: - containerPort: 8080 name: http # resources 资源限制 resources: limits: cpu: 500m memory: 400Mi requests: cpu: 200m memory: 200Mi # 就绪探针 如果探针判断失败,则不会有流量发往到这个pod。 readinessProbe: # http Get请求 httpGet: path: /actuator/health port: 8080 # 初始化检测时间, 指启动后30s开始探针检测 initialDelaySeconds: 30 # 探针探测的时间间隔为10s periodSeconds: 10 # 探针探测失败后, 最少连续探测成功多少次才被认定为成功 successThreshold: 1 # 探测成功后, 最少连续探测失败多少次才被认定为失败 failureThreshold: 1 # periodSeconds = 10s failureThreshold = 1 大约 10 秒后就不会有流量发往它 # 活性探测 如果探针判断失败, 则会重启这个 pod。 livenessProbe: # http Get请求 httpGet: path: /actuator/health port: 8080 # 初始化检测时间, 指启动后40s开始探针检测 # 必须在 readinessProbe 探针检测之后 initialDelaySeconds: 40 # 探针探测的时间间隔为20s periodSeconds: 20 # 探针探测失败后, 最少连续探测成功多少次才被认定为成功 successThreshold: 1 # 探测成功后, 最少连续探测失败多少次才被认定为失败 failureThreshold: 3 # periodSeconds = 20s failureThreshold = 3 当容器异常时, 大约 60 秒后就不会被重启。 --- apiVersion: v1 kind: Service metadata: labels: app: spring-boot-probes name: spring-boot-probes-svc spec: ports: - port: 8080 name: http targetPort: 8080 protocol: TCP selector: app: spring-boot-probes ","date":"2020-04-02","objectID":"/k8s-rollingupdate/:2:0","tags":["kubernetes"],"title":"Kubernetes RollingUpdate","uri":"/k8s-rollingupdate/"},{"categories":["kubernetes"],"content":"测试 部署应用以后,我们可以通过修改 version 进行滚动更新 [root@localhost ~]# kubectl get pods NAME READY STATUS RESTARTS AGE spring-boot-probes-8d6868f4f-4jcb7 1/1 Running 0 3m11s spring-boot-probes-8d6868f4f-r8zdc 1/1 Running 0 3m11s spring-boot-probes-8d6868f4f-vz6md 1/1 Running 0 3m11s spring-boot-probes-8d6868f4f-wmxcw 1/1 Running 0 3m11s 执行更新以后 通过 kubectl get rs -w 查看滚动更新流程 [root@localhost ~]# kubectl get rs -w NAME DESIRED CURRENT READY AGE spring-boot-probes-74dbf69548 3 3 0 2s spring-boot-probes-8d6868f4f 3 3 3 4m34s spring-boot-probes-74dbf69548 3 3 1 32s spring-boot-probes-74dbf69548 3 3 2 38s spring-boot-probes-74dbf69548 3 3 3 40s spring-boot-probes-74dbf69548 3 3 3 2m33s spring-boot-probes-8d6868f4f 2 3 3 7m5s spring-boot-probes-74dbf69548 4 3 3 2m33s spring-boot-probes-8d6868f4f 2 3 3 7m5s spring-boot-probes-8d6868f4f 2 2 2 7m5s spring-boot-probes-74dbf69548 4 3 3 2m33s spring-boot-probes-74dbf69548 4 4 3 2m33s spring-boot-probes-74dbf69548 4 4 4 3m11s spring-boot-probes-8d6868f4f 0 2 2 7m43s spring-boot-probes-8d6868f4f 0 2 2 7m43s spring-boot-probes-8d6868f4f 0 0 0 7m43s ","date":"2020-04-02","objectID":"/k8s-rollingupdate/:3:0","tags":["kubernetes"],"title":"Kubernetes RollingUpdate","uri":"/k8s-rollingupdate/"},{"categories":["kubernetes"],"content":"KubeSphere 2.1.1 to kubernetes","date":"2020-04-01","objectID":"/kubesphere-2.1.1/","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"KubeSphere 以应用为中心的容器平台, Golang 语言开发并且完全开源。 ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:0:0","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"什么是 KubeSphere KubeSphere 是一款面向云原生设计的开源项目，在目前主流容器调度平台 Kubernetes 之上构建的分布式多租户容器管理平台，提供简单易用的操作界面以及向导式操作方式，在降低用户使用容器调度平台学习成本的同时，极大降低开发、测试、运维的日常工作的复杂度。 ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:1:0","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"KubeSphere 特点 极简体验，向导式 UI 面向开发、测试、运维友好的用户界面，向导式用户体验，降低 Kubernetes 学习成本的设计理念 用户基于应用模板可以一键部署一个完整应用的所有服务，UI 提供全生命周期管理 业务高可靠与高可用 自动弹性伸缩：部署 (Deployment) 支持根据访问量进行动态横向伸缩和容器资源的弹性扩缩容，保证集群和容器资源的高可用 提供健康检查：支持为容器设置健康检查探针来检查容器的健康状态，确保业务的可靠性 容器化 DevOps 持续交付 DevOps：基于 Jenkins 的可视化 CI/CD 流水线编辑，无需对 Jenkins 进行配置，同时内置丰富的 CI/CD 流水线模版 Source to Image (s2i): 从已有的代码仓库中获取代码，并通过 s2i 自动构建镜像完成应用部署并自动推送至镜像仓库，无需编写 Dockerfile 端到端的流水线设置: 支持从仓库 (GitHub / SVN / Git)、代码编译、镜像制作、镜像安全、推送仓库、版本发布、到定时构建的端到端流水线设置 安全管理: 支持代码静态分析扫描以对 DevOps 工程中代码质量进行安全管理 日志: 日志完整记录 CI / CD 流水线运行全过程 开箱即用的微服务治理 灵活的微服务框架: 基于 Istio 微服务框架提供可视化的微服务治理功能，将 Kubernetes 的服务进行更细粒度的拆分，支持无侵入的微服务治理 完善的治理功能: 支持灰度发布、熔断、流量监测、流量管控、限流、链路追踪、智能路由等完善的微服务治理功能 灵活的持久化存储方案 支持 GlusterFS、CephRBD、NFS 等开源存储方案，支持有状态存储 NeonSAN CSI 插件对接 QingStor NeonSAN，以更低时延、更加弹性、更高性能的存储，满足核心业务需求 QingCloud CSI 插件对接 QingCloud 云平台各种性能的块存储服务 灵活的网络方案支持 支持 Calico、Flannel 等开源网络方案 分别开发了 QingCloud 云平台负载均衡器插件 和适用于物理机部署 Kubernetes 的 负载均衡器插件 Porter 商业验证的 SDN 能力：可通过 QingCloud CNI 插件对接 QingCloud SDN，获得更安全、更高性能的网络支持 多维度监控日志告警 KubeSphere 全监控运维功能可通过可视化界面操作，同时，开放标准接口对接企业运维系统，以统一运维入口实现集中化运维 可视化秒级监控: 秒级频率、双重维度、十六项指标立体化监控；提供服务组件监控，快速定位组件故障 提供按节点、企业空间、项目等资源用量排行 支持基于多租户、多维度的监控指标告警，目前告警策略支持集群节点级别和工作负载级别等两个层级 提供多租户日志管理，在 KubeSphere 的日志查询系统中，不同的租户只能看到属于自己的日志信息 ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:2:0","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"KubeSphere 架构 组件 功能说明 ks-account 提供用户、权限管理相关的 API ks-apiserver 整个集群管理的 API 接口和集群内部各个模块之间通信的枢纽，以及集群安全控制 ks-apigateway 负责处理服务请求和处理 API 调用过程中的所有任务 ks-console 提供 KubeSphere 的控制台服务 ks-controller-manager 实现业务逻辑的，例如创建企业空间时，为其创建对应的权限；或创建服务策略时，生成对应的 Istio 配置等 Metrics-server Kubernetes 的监控组件，从每个节点的 Kubelet 采集指标信息 Prometheus 提供集群、节点、工作负载、API 对象等相关监控数据与服务 Elasticsearch 提供集群的日志索引、查询、数据管理等服务，在安装时也可对接您已有的 ES 减少资源消耗 Fluent Bit 提供日志接收与转发，可将采集到的⽇志信息发送到 ElasticSearch、Kafka Jenkins 提供 CI/CD 流水线服务 SonarQube 可选安装项，提供代码静态检查与质量分析 Source-to-Image 将源代码自动将编译并打包成 Docker 镜像，方便快速构建镜像 Istio 提供微服务治理与流量管控，如灰度发布、金丝雀发布、熔断、流量镜像等 Jaeger 收集 Sidecar 数据，提供分布式 Tracing 服务 OpenPitrix 提供应用模板、应用部署与管理的服务 Alert 提供集群、Workload、Pod、容器级别的自定义告警服务 Notification 通用的通知服务，目前支持邮件通知 redis 将 ks-console 与 ks-account 的数据存储在内存中的存储系统 MySQL 集群后端组件的数据库，监控、告警、DevOps、OpenPitrix 共用 MySQL 服务 PostgreSQL SonarQube 和 Harbor 的后端数据库 OpenLDAP 负责集中存储和管理用户账号信息与对接外部的 LDAP 存储 内置 CSI 插件对接云平台存储服务，可选安装开源的 NFS/Ceph/Gluster 的客户端 网络 可选安装 Calico/Flannel 等开源的网络插件，支持对接云平台 SDN ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:3:0","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"KubeSphere 部署 KubeSphere 支持独立部署与 Linux 主机中, 也支持直接部署于 Kubernetes 集群中。 本文中将KubeSphere 部署于 Kubernetes 集群中。 集群需求: Kubernetes 版本: 1.15.x、1.16.x、1.17.x； Helm版本： 2.10.0 ≤ Helm Version ＜ 3.0.0； 关于 helm 的tiller init 安装, 使用国内的镜像 helm init --upgrade -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.5 --stable-repo-url https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts 集群已有默认的存储类型（StorageClass）。 必须要有默认的 StorageClass, 可以安装 nfs,gfs,cephfs, openobs 等。 CSR signing 功能在 kube-apiserver 中被激活。 集群能够访问外网。 ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:4:0","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"Kubernetes 集群信息 HostName IP Docker Kernel kuberneres 作用 k8s-node-1 10.18.77.61 19.03.6-ce 4.14.173 1.18.0 Master节点 k8s-node-2 10.18.77.117 19.03.6-ce 4.14.173 1.18.0 Master节点 or Node节点 k8s-node-3 10.18.77.218 19.03.6-ce 4.14.173 1.18.0 Master节点 or Node节点 or nfs Server ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:4:1","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"NFS StorageClass NFS Server 节点安装服务 [root@k8s-node-3 opt]# yum -y install nfs-utils rpcbind 所有 K8S 节点都需要安装 nfs-utils 依赖, 否则无法挂载 yum -y install nfs-utils 配置 NFS 目录与权限 # 添加 nfs 存储目录 [root@k8s-node-3 opt]# mkdir -p /opt/nfsdata # 增加权限以及目录到 nfs [root@k8s-node-3 opt]# vi /etc/exports 增加 /opt/nfsdata 10.18.77.0/24(rw,sync,no_root_squash) 启动 NFS 服务 systemctl enable rpcbind.service systemctl enable nfs-server.service systemctl start rpcbind.service systemctl start nfs-server.service # 查看信息 [root@k8s-node-3 opt]# showmount -e 10.18.77.218 Export list for 10.18.77.218: /opt/nfsdata 10.18.77.0/24 部署 StorageClass 此版本为古老的版本, k8s 新版的存储驱动必须符合 CSI 接口实现。 官方有提供一个基于 CSI 接口的 https://github.com/kubernetes-csi/csi-driver-nfs 但是目前并不支持 StorageClass # 下载 yaml 文件 for file in class.yaml deployment.yaml rbac.yaml; do wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/$file; done 修改 deployment.yaml 文件 # vi deployment.yaml # 修改如下部分 volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 10.18.77.218 - name: NFS_PATH value: /opt/nfsdata volumes: - name: nfs-client-root nfs: server: 10.18.77.218 path: /opt/nfsdata 创建服务 [root@k8s-node-1 nfs]# kubectl apply -f . storageclass.storage.k8s.io/managed-nfs-storage created deployment.apps/nfs-client-provisioner created serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created 查看服务 [root@k8s-node-1 nfs]# kubectl get pods NAME READY STATUS RESTARTS AGE nfs-client-provisioner-576d645cbc-6546h 1/1 Running 0 59s [root@k8s-node-1 nfs]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-nfs-storage fuseim.pri/ifs Delete Immediate false 74s 标记一个默认的StorageClass 集群内只能有一个 StorageClass 能够标记为默认。 # 注意 managed-nfs-storage 为 storageclass的名称 kubectl patch storageclass managed-nfs-storage -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}' # 输出: storageclass.storage.k8s.io/managed-nfs-storage patched # 再次查看 storageclass [root@k8s-node-1 nfs]# kubectl get storageclass NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE managed-nfs-storage (default) fuseim.pri/ifs Delete Immediate false 6m19s ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:4:2","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"部署 KubeSphere 官方提供 helm 与 yaml 文件导入. 集群可用的资源符合 CPU \u003e 1 Core，可用内存 \u003e 2 G 。使用 minimal 版本 minimal 版本只开启了最基本的KubeSphere组件。 kubectl apply -f https://raw.githubusercontent.com/kubesphere/ks-installer/master/kubesphere-minimal.yaml 集群可用的资源符合 CPU ≥ 8 Core，可用内存 ≥ 16 G。使用完整版本 完整的版本, 开启了所有的 KubeSphere组件。 kubectl apply -f https://raw.githubusercontent.com/kubesphere/ks-installer/master/kubesphere-complete-setup.yaml 查看安装过程 ks-installer 可以看到是利用 ansible 安装的。 kubectl logs -n kubesphere-system $(kubectl get pod -n kubesphere-system -l app=ks-install -o jsonpath='{.items[0].metadata.name}') -f 查看服务 [root@localhost kubesphere]# kubectl get pods,svc -n kubesphere-system NAME READY STATUS RESTARTS AGE pod/ks-account-596657f8c6-dbsm4 1/1 Running 0 31m pod/ks-apigateway-78bcdc8ffc-jbghc 1/1 Running 0 31m pod/ks-apiserver-5b548d7c5c-jzxnt 1/1 Running 0 31m pod/ks-console-78bcf96dbf-2cdtx 1/1 Running 0 31m pod/ks-controller-manager-696986f8d9-v5lzp 1/1 Running 0 31m pod/ks-installer-75b8d89dff-f2hln 1/1 Running 0 32m pod/openldap-0 1/1 Running 0 31m pod/redis-6fd6c6d6f9-mprdb 1/1 Running 0 31m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ks-account ClusterIP 10.254.43.12 \u003cnone\u003e 80/TCP 31m service/ks-apigateway ClusterIP 10.254.56.201 \u003cnone\u003e 80/TCP 31m service/ks-apiserver ClusterIP 10.254.25.243 \u003cnone\u003e 80/TCP 31m service/ks-console ClusterIP 10.254.58.101 \u003cnone\u003e 80/TCP 31m service/openldap ClusterIP None \u003cnone\u003e 389/TCP 31m service/redis ClusterIP 10.254.38.34 \u003cnone\u003e 6379/TCP 31m 修改默认访问端口 默认配置, 使用 nodeport 映射30880 端口。 修改为 ingress 用域名访问 # 编辑 ks-console service kubectl edit svc/ks-console -n kubesphere-system # 去掉 nodePort: 30880 type: NodePort 配置域名访问 配置ssl (可选) # 使用自签的ssl证书 kubectl -n kubesphere-system create \\ secret tls tls-kubesphere-ingress \\ --cert=3258931_kubesphere.jicki.cn.pem \\ --key=3258931_kubesphere.jicki.cn.key # 编写 kubesphere-ingress.yaml 文件 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: ks-console-ingress namespace: kubesphere-system annotations: nginx.ingress.kubernetes.io/proxy-connect-timeout: \"30\" nginx.ingress.kubernetes.io/proxy-read-timeout: \"1800\" nginx.ingress.kubernetes.io/proxy-send-timeout: \"1800\" spec: rules: - host: kubesphere.jicki.cn http: paths: - backend: serviceName: ks-console servicePort: 80 tls: - hosts: - kubesphere.jicki.cn secretName: tls-kubesphere-ingress 查看服务 [root@localhost kubesphere]# kubectl get ingress -n kubesphere-system NAME HOSTS ADDRESS PORTS AGE ks-console-ingress kubesphere.jicki.cn 80, 443 14m ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:4:3","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"访问 KubeSphere 默认账号密码 用户名: admin 密码: P@88w0rd ","date":"2020-04-01","objectID":"/kubesphere-2.1.1/:4:4","tags":["kubernetes","kubeSphere","docker"],"title":"KubeSphere 2.1.1 to kubernetes","uri":"/kubesphere-2.1.1/"},{"categories":["kubernetes"],"content":"kubeadm v1.18.0 HA","date":"2020-03-30","objectID":"/kubeadm-1.18.0/","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":" kubeadm v1.18.0 kubernetes 1.18.0 本文基于 kubeadm 方式部署，kubeadm 在1.13 版本以后正式进入 GA. 目前国内各大厂商都有 kubeadm 的镜像源，对于部署 kubernetes 来说是大大的便利. 从官方对 kubeadm 的更新频繁度来看，kubeadm 应该是后面的趋势，毕竟二进制部署确实麻烦了点. 1. 环境说明 系统 IP Containerd Kernel hostname 备注 Aws Linux 10.18.77.61 19.03.6-ce 4.14.171 k8s-node-3 Master Aws Linux 10.18.77.117 19.03.6-ce 4.14.171 k8s-node-1 Master or node Aws Linux 10.18.77.218 19.03.6-ce 4.14.171 k8s-node-2 Master or node ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:0:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"1.1 初始化环境 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:1:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"1.1.1 配置 hosts hostnamectl --static set-hostname hostname hostnamectl --transient set-hostname hostname k8s-node-1 10.18.77.61 k8s-node-2 10.18.77.117 k8s-node-3 10.18.77.218 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 10.18.77.61 k8s-node-1 10.18.77.117 k8s-node-2 10.18.77.218 k8s-node-3 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:1:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"1.1.2 关闭防火墙 sed -ri 's#(SELINUX=).*#\\1disabled#' /etc/selinux/config setenforce 0 systemctl disable firewalld systemctl stop firewalld ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:1:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"1.1.3 关闭虚拟内存 # 临时关闭 swapoff -a # 永久关闭 vi /etc/fstab 注释掉关于 swap 的一段 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:1:3","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"1.1.4 添加内核配置 # 开启内核 namespace 支持 grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" # 修改内核参数 cat\u003c\u003cEOF \u003e /etc/sysctl.d/docker.conf net.ipv4.ip_forward=1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-arptables = 1 vm.swappiness=0 EOF # 生效配置 sysctl --system # 重启系统 reboot # 添加 kubernetes 内核优化 cat\u003c\u003cEOF \u003e /etc/sysctl.d/kubernetes.conf # conntrack 连接跟踪数最大数量 net.netfilter.nf_conntrack_max = 10485760 # 允许送到队列的数据包的最大数目 net.core.netdev_max_backlog = 10000 # ARP 高速缓存中的最少层数 net.ipv4.neigh.default.gc_thresh1 = 80000 # ARP 高速缓存中的最多的记录软限制 net.ipv4.neigh.default.gc_thresh2 = 90000 # ARP 高速缓存中的最多记录的硬限制 net.ipv4.neigh.default.gc_thresh3 = 100000 EOF # 生效配置 sysctl --system ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:1:4","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"1.1.5 配置IPVS模块 kube-proxy 使用 ipvs 方式负载 ，所以需要内核加载 ipvs 模块, 否则只会使用 iptables 方式 cat \u003e /etc/sysconfig/modules/ipvs.modules \u003c\u003cEOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF # 授权 chmod 755 /etc/sysconfig/modules/ipvs.modules # 加载模块 bash /etc/sysconfig/modules/ipvs.modules # 查看加载 lsmod | grep -e ip_vs -e nf_conntrack_ipv4 # 输出如下: ----------------------------------------------------------------------- nf_conntrack_ipv4 20480 0 nf_defrag_ipv4 16384 1 nf_conntrack_ipv4 ip_vs_sh 16384 0 ip_vs_wrr 16384 0 ip_vs_rr 16384 0 ip_vs 147456 6 ip_vs_rr,ip_vs_sh,ip_vs_wrr nf_conntrack 110592 2 ip_vs,nf_conntrack_ipv4 libcrc32c 16384 2 xfs,ip_vs ----------------------------------------------------------------------- ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:1:5","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"1.1.6 配置yum源 使用 阿里 的 yum 源 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 更新 yum yum makecache 2. 安装 docker ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:1:6","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"2.1 检查系统 curl -s https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:2:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"2.2 安装 docker # 清除缓存 yum makecache yum -y install docker 因为 aws linux 不支持如下安装: 如下支持 ubuntu, debain, centos, rhel # 指定安装,并指定安装源 export VERSION=19.03 curl -fsSL \"https://get.docker.com/\" | bash -s -- --mirror Aliyun ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:3:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"2.3 配置 docker mkdir -p /etc/docker/ cat\u003e/etc/docker/daemon.json\u003c\u003cEOF { \"bip\": \"172.17.0.1/16\", \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://9jwx2023.mirror.aliyuncs.com\"], \"data-root\": \"/opt/docker\", \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\", \"max-file\": \"5\" }, \"dns-search\": [\"default.svc.cluster.local\", \"svc.cluster.local\", \"localdomain\"], \"dns-opts\": [\"ndots:2\", \"timeout:2\", \"attempts:2\"] } EOF ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:4:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"2.4 启动docker systemctl enable docker systemctl start docker systemctl status docker docker info 3. 部署 kubernetes ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:5:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.1 安装相关软件 所有软件安装都通过 yum 安装 # kubernetes 相关 (Master) yum -y install tc kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0 # kubernetes 相关 (Node) yum -y install tc kubelet-1.18.0 kubeadm-1.18.0 # ipvs 相关 yum -y install ipvsadm ipset # 配置 kubelet 自动启动 (暂时不需要启动) systemctl enable kubelet.service 配置 kubectl 命令补全 # 安装 bash-completion yum -y install bash-completion # Linux 默认脚本路径为 /usr/share/bash-completion/bash_completion # 配置 bashrc vi ~/.bashrc # 添加如下: # kubectl source /usr/share/bash-completion/bash_completion source \u003c(kubectl completion bash) # 生效配置 source ~/.bashrc ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:6:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.2 修改证书期限 默认基本证书的有效期为1年 # 下载源码 git clone https://github.com/kubernetes/kubernetes Cloning into 'kubernetes'... remote: Enumerating objects: 219, done. remote: Counting objects: 100% (219/219), done. remote: Compressing objects: 100% (128/128), done. remote: Total 1087208 (delta 112), reused 91 (delta 91), pack-reused 1086989 Receiving objects: 100% (1087208/1087208), 668.66 MiB | 486.00 KiB/s, done. Resolving deltas: 100% (777513/777513), done. # 查看分支 cd kubernetes git branch -a 查看当前的分支 git branch # 切换到相关的分支 git checkout remotes/origin/release-1.18 修改 cert.go 文件 # 打开文件 vi staging/src/k8s.io/client-go/util/cert/cert.go # 如下 默认已经是10年,可不修改,也可以修改99年,但是不能超过100年 NotAfter: now.Add(duration365d * 10).UTC(), 修改 constants.go 文件 # 打开文件 vi cmd/kubeadm/app/constants/constants.go # 如下 默认是 1年, 修改为 10 年 CertificateValidity = time.Hour * 24 * 365 # 修改为 CertificateValidity = time.Hour * 24 * 365 * 10 重新编译 kubeadm make all WHAT=cmd/kubeadm GOFLAGS=-v 拷贝 覆盖 kubeadm 拷贝到所有的 master 中 # 编译后生成目录为 _output/local/bin/linux/amd64 cp _output/local/bin/linux/amd64/kubeadm /usr/bin/kubeadm cp: overwrite ‘/usr/bin/kubeadm’? y ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:7:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.3 修改 kubeadm 配置信息 打印 kubeadm init 的 yaml 配置 kubeadm config print init-defaults kubeadm config print init-defaults --component-configs KubeletConfiguration kubeadm config print init-defaults --component-configs KubeProxyConfiguration # 导出 配置 信息 kubeadm config print init-defaults \u003e kubeadm-init.yaml 文中配置的 127.0.0.1 均为后续配置的 Nginx Api 代理ip advertiseAddress: 10.18.77.218 与 bindPort: 5443 为程序绑定的地址与端口 controlPlaneEndpoint: \"127.0.0.1:6443\" 为实际访问 ApiServer 的地址 这里这样配置是为了维持 Apiserver 的HA, 所以每个机器上部署一个 Nginx 做4层代理 ApiServer # 修改相关配置，本文配置信息如下 apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: # ApiServer 程序绑定的 ip, 填写网卡实际ip advertiseAddress: 10.18.77.61 # ApiServer 程序绑定的端口,修改为5443是为怕跟下面不冲突 bindPort: 5443 nodeRegistration: criSocket: /var/run/dockershim.sock name: k8s-node-1 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: # apiserver相关配置 extraArgs: # 审计日志相关配置 audit-log-maxage: \"20\" audit-log-maxbackup: \"10\" audit-log-maxsize: \"100\" audit-log-path: \"/var/log/kube-audit/audit.log\" audit-policy-file: \"/etc/kubernetes/audit-policy.yaml\" audit-log-format: json # 开启审计日志配置, 所以需要将宿主机上的审计配置 extraVolumes: - name: \"audit-config\" hostPath: \"/etc/kubernetes/audit-policy.yaml\" mountPath: \"/etc/kubernetes/audit-policy.yaml\" readOnly: true pathType: \"File\" - name: \"audit-log\" hostPath: \"/var/log/kube-audit\" mountPath: \"/var/log/kube-audit\" pathType: \"DirectoryOrCreate\" timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes # Api Server 实际访问地址 controlPlaneEndpoint: \"127.0.0.1:6443\" controllerManager: {} dns: type: CoreDNS etcd: local: # Etcd Volume 本地路径,最好修改为独立的磁盘 dataDir: /var/lib/etcd imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers kind: ClusterConfiguration kubernetesVersion: v1.18.0 networking: dnsDomain: cluster.local # K8s Pod ip地址的取值范围 podSubnet: 10.253.0.0/16 # K8s Svc ip地址的取值范围 serviceSubnet: 10.254.0.0/16 scheduler: {} --- # kubelet 相关配置 apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration clusterDNS: # coredns 默认ip地址 - 10.96.0.10 # 如下为 NodeLocal DNSCache 默认主机地址 #- 169.254.20.10 clusterDomain: cluster.local --- # kube-proxy 相关配置 apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \"ipvs\" ipvs: minSyncPeriod: 5s syncPeriod: 5s # 加权轮询调度 scheduler: \"wrr\" 创建审计策略文件 vi /etc/kubernetes/audit-policy.yaml apiVersion: audit.k8s.io/v1 # This is required. kind: Policy omitStages: - \"RequestReceived\" rules: - level: RequestResponse resources: - group: \"\" resources: [\"pods\"] - level: Metadata resources: - group: \"\" resources: [\"pods/log\", \"pods/status\"] - level: None resources: - group: \"\" resources: [\"configmaps\"] resourceNames: [\"controller-leader\"] - level: None users: [\"system:kube-proxy\"] verbs: [\"watch\"] resources: - group: \"\" # core API group resources: [\"endpoints\", \"services\"] - level: None userGroups: [\"system:authenticated\"] nonResourceURLs: - \"/api*\" # Wildcard matching. - \"/version\" - level: Request resources: - group: \"\" # core API group resources: [\"configmaps\"] namespaces: [\"kube-system\"] - level: Metadata resources: - group: \"\" # core API group resources: [\"secrets\", \"configmaps\"] - level: Request resources: - group: \"\" # core API group - group: \"extensions\" # Version of group should NOT be included. - level: Metadata omitStages: - \"RequestReceived\" ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:8:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.4 配置 Nginx Proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 10.18.77.61:5443; server 10.18.77.117:5443; server 10.18.77.218:5443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF 授权 # 更新权限 chmod +r /etc/nginx/nginx.conf 创建系统 systemd.service 文件 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF 启动 Nginx Proxy # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:9:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.5 初始化集群 --upload-certs 会在加入 master 节点的时候自动拷贝证书 kubeadm init --config kubeadm-init.yaml --upload-certs # 输出如下: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of the control-plane node running the following command on each as root: kubeadm join 127.0.0.1:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:ed09a75d84bfbb751462262757310d0cf3d015eaa45680130be1d383245354f8 \\ --control-plane --certificate-key 93cb0d7b46ba4ac64c6ffd2e9f022cc5f22bea81acd264fb4e1f6150489cd07a Please note that the certificate-key gives access to cluster sensitive data, keep it secret! As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use \"kubeadm init phase upload-certs --upload-certs\" to reload certs afterward. Then you can join any number of worker nodes by running the following on each as root: kubeadm join 127.0.0.1:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:ed09a75d84bfbb751462262757310d0cf3d015eaa45680130be1d383245354f8 # 拷贝权限文件 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config # 查看集群状态 [root@k8s-node-1 kubeadm]# kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\":\"true\"} ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:10:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.6 加入 kubernetes 集群 如上有 kubeadm init 后有两条 kubeadm join 命令, –control-plane 为 加入 Master 另外token 有时效性，如果提示 token 失效，请自行创建一个新的 token. kubeadm token create –print-join-command 创建新的 join token ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:11:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.6.1 加入 其他 Master 节点 我这里三个服务器都是 Master 节点,所有都加入 –control-plane 的选项 创建审计策略文件 # 其他两台服务器创建 ssh k8s-node-2 \"mkdir -p /etc/kubernetes/\" ssh k8s-node-3 \"mkdir -p /etc/kubernetes/\" 拷贝策略文件 # k8s-node-2 节点 scp /etc/kubernetes/audit-policy.yaml k8s-node-2:/etc/kubernetes/ # k8s-node-3 节点 scp /etc/kubernetes/audit-policy.yaml k8s-node-3:/etc/kubernetes/ 分别 join master # 先测试 api server 连通性 curl -k https://127.0.0.1:6443 # 返回如下信息: { \"kind\": \"Status\", \"apiVersion\": \"v1\", \"metadata\": { }, \"status\": \"Failure\", \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\", \"reason\": \"Forbidden\", \"details\": { }, \"code\": 403 增加额外的配置,用于区分不用的 master 中的 apiserver-advertise-address 与 apiserver-bind-port # k8s-node-2 kubeadm join 127.0.0.1:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:ed09a75d84bfbb751462262757310d0cf3d015eaa45680130be1d383245354f8 \\ --control-plane --certificate-key 93cb0d7b46ba4ac64c6ffd2e9f022cc5f22bea81acd264fb4e1f6150489cd07a \\ --apiserver-advertise-address 10.18.77.117 \\ --apiserver-bind-port 5443 # k8s-node-3 kubeadm join 127.0.0.1:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:ed09a75d84bfbb751462262757310d0cf3d015eaa45680130be1d383245354f8 \\ --control-plane --certificate-key 93cb0d7b46ba4ac64c6ffd2e9f022cc5f22bea81acd264fb4e1f6150489cd07a \\ --apiserver-advertise-address 10.18.77.218 \\ --apiserver-bind-port 5443 拷贝 config 配置文件 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:11:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.6.2 验证 Master 节点 这里 STATUS 显示 NotReady 是因为 没有安装网络组件 # 查看 node [root@k8s-node-1 kubeadm]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 NotReady master 106m v1.18.0 k8s-node-2 NotReady master 2m18s v1.18.0 k8s-node-3 NotReady master 63s v1.18.0 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:11:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.6.3 配置 Master to node 这里主要是让 master 直接可以运行 pods 执行命令: kubectl taint node node-name node-role.kubernetes.io/master- 禁止 master 运行pod kubectl taint nodes node-name node-role.kubernetes.io/master=:NoSchedule 增加 ROLES 标签: kubectl label nodes localhost node-role.kubernetes.io/node= 删除 ROLES 标签: kubectl label nodes localhost node-role.kubernetes.io/node- ROLES 标签可以添加任意的值, 如: kubectl label nodes localhost node-role.kubernetes.io/jicki= ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:11:3","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.7 部署 Node 节点 node 节点, 直接 join 就可以 kubeadm join 127.0.0.1:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:ed09a75d84bfbb751462262757310d0cf3d015eaa45680130be1d383245354f8 # 输出如下: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:12:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.7.1 验证 所有 节点 这里 STATUS 显示 NotReady 是因为 没有安装网络组件 [root@k8s-node-1 yaml]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 NotReady master 106m v1.18.0 k8s-node-2 NotReady master 2m18s v1.18.0 k8s-node-3 NotReady master 63s v1.18.0 k8s-node-4 NotReady \u003cnone\u003e 2m46s v1.18.0 k8s-node-5 NotReady \u003cnone\u003e 2m46s v1.18.0 k8s-node-6 NotReady \u003cnone\u003e 2m46s v1.18.0 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:12:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.7.2 查看验证证书 这里如果后续替换的话, 所有 master 节点都需要执行如下更新命令 # 更新证书 kubeadm alpha certs renew all # 查看证书时间 kubeadm alpha certs check-expiration [root@k8s-node-1 kubeadm]# kubeadm alpha certs check-expiration CERTIFICATE EXPIRES RESIDUAL TIME CERTIFICATE AUTHORITY EXTERNALLY MANAGED admin.conf Mar 07, 2119 06:22 UTC 98y no apiserver Mar 07, 2119 06:22 UTC 98y ca no apiserver-etcd-client Mar 07, 2119 06:22 UTC 98y etcd-ca no apiserver-kubelet-client Mar 07, 2119 06:22 UTC 98y ca no controller-manager.conf Mar 07, 2119 06:22 UTC 98y no etcd-healthcheck-client Mar 07, 2119 06:22 UTC 98y etcd-ca no etcd-peer Mar 07, 2119 06:22 UTC 98y etcd-ca no etcd-server Mar 07, 2119 06:22 UTC 98y etcd-ca no front-proxy-client Mar 07, 2119 06:22 UTC 98y front-proxy-ca no scheduler.conf Mar 07, 2119 06:22 UTC 98y no CERTIFICATE AUTHORITY EXPIRES RESIDUAL TIME EXTERNALLY MANAGED ca Mar 28, 2030 04:30 UTC 9y no etcd-ca Mar 28, 2030 04:30 UTC 9y no front-proxy-ca Mar 28, 2030 04:30 UTC 9y no ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:12:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.8 安装网络组件 Flannel 网络组件 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:13:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.8.1 下载 Flannel yaml # 下载 yaml 文件 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:13:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.8.2 修改 Flannel 配置 这里只需要修改 分配的 CIDR 就可以 vi kube-flannel.yml # 修改 pods 分配的 IP 段, 与模式 vxlan # \"Type\": \"vxlan\" , 云上一般都不支持 host-gw 模式,一般只用于 2层网络。 # 主要是如下部分 data: cni-conf.json: | { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } net-conf.json: | { \"Network\": \"10.253.0.0/16\", \"Backend\": { \"Type\": \"vxlan\" } } --- # 导入 yaml 文件 [root@k8s-node-1 flannel]# kubectl apply -f kube-flannel.yml podsecuritypolicy.policy/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.apps/kube-flannel-ds-amd64 created daemonset.apps/kube-flannel-ds-arm64 created daemonset.apps/kube-flannel-ds-arm created daemonset.apps/kube-flannel-ds-ppc64le created daemonset.apps/kube-flannel-ds-s390x created # 查看服务 [root@k8s-node-1 flannel]# kubectl get pods -n kube-system -o wide |grep kube-flannel kube-flannel-ds-amd64-2tw6q 1/1 Running 0 88s 10.18.77.61 k8s-node-1 \u003cnone\u003e \u003cnone\u003e kube-flannel-ds-amd64-8nrtd 1/1 Running 0 88s 10.18.77.218 k8s-node-3 \u003cnone\u003e \u003cnone\u003e kube-flannel-ds-amd64-frmk9 1/1 Running 0 88s 10.18.77.117 k8s-node-2 \u003cnone\u003e \u003cnone\u003e 优化 Coredns 配置 根据 node 情况增加 replicas 数量 最好可以 约束 coredns 的 pod 调度到不同的 node 中。kubectl edit deploy coredns -n kube-system kubectl scale deploy/coredns --replicas=3 -n kube-system 使用 NodeLocal DNSCache 官方文档 https://kubernetes.io/zh/docs/tasks/administer-cluster/nodelocaldns/ NodeLocal DNSCache - 通过在集群节点上作为 DaemonSet 运行 dns 缓存代理来提高集群 DNS 性能。 NodeLocal DNSCache - 集群中的 Pods 将可以访问在同一节点上运行的 dns 缓存代理，从而避免了iptables DNAT 规则和连接跟踪。 本地缓存代理将查询 kube-dns 服务以获取集群主机名的缓存缺失（默认为 cluster.local 后缀）。 NodeLocal DNSCache 架构图 部署 NodeLocal DNSCache 建议在 kubeadm init 阶段以后就配置整体 dns 如果在旧的集群部署 NodeLocal DNSCache 原来的所有应用组件建议重新部署,包括网络组建, 否则会遇到很多莫名其妙问题。 如果使用 istio 的话, 会出现一些问题, 暂时还不兼容 istio , 或者是我配置上有问题。 # 下载 YAML wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml # 修改配置 sed -i 's/k8s\\.gcr\\.io/jicki/g' nodelocaldns.yaml sed -i 's/__PILLAR__LOCAL__DNS__/10\\.254\\.0\\.10/g' nodelocaldns.yaml sed -i 's/__PILLAR__DNS__SERVER__/169\\.254\\.20\\.10/g' nodelocaldns.yaml sed -i 's/__PILLAR__DNS__DOMAIN__/cluster\\.local/g' nodelocaldns.yaml # __PILLAR__DNS__SERVER__ -设置为 coredns svc 的 IP。 # __PILLAR__LOCAL__DNS__ -设置为本地链接IP（默认为169.254.20.10）。 # __PILLAR__DNS__DOMAIN__ -设置为群集域（默认为cluster.local）。 # 创建服务 [root@k8s-node-1 kubeadm]# kubectl apply -f nodelocaldns.yaml # 查看服务 [root@k8s-node-1 kubeadm]# kubectl get pods -n kube-system |grep node-local-dns node-local-dns-mfxdk 1/1 Running 0 3m12s [root@k8s-node-1 kubeadm]# kubectl get svc -n kube-system kube-dns-upstream NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns-upstream ClusterIP 10.254.45.66 \u003cnone\u003e 53/UDP,53/TCP 23m # 查看本地开放端口 [root@k8s-node-1 kubeadm]# netstat -lan|grep 169.254.20.10 tcp 0 0 169.254.20.10:53 0.0.0.0:* LISTEN udp 0 0 169.254.20.10:53 0.0.0.0:* 修改 kubelet 使用 NodeLocal DNSCache kubeadm 部署的集群, kubelet 的配置在 /var/lib/kubelet/config.yaml 中 vi /var/lib/kubelet/config.yaml # 修 改 clusterDNS: - 10.254.0.10 # 修改为 本机 ip clusterDNS: - 169.254.20.10 重启 kubelet 这里也可以在 kubeadm init 的阶段就配置好 NodeLocal 的ip # 重启 kubelet 应用dns systemctl daemon-reload \u0026\u0026 systemctl restart kubelet ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:13:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.9 检验整体集群 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:14:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.9.1 查看 状态 所有的 STATUS 都为 Ready [root@k8s-node-1 flannel]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 Ready master 131m v1.18.0 k8s-node-2 Ready master 27m v1.18.0 k8s-node-3 Ready master 26m v1.18.0 查看 etcd 状态 # 这里目前只有一个 etcd 节点,多个节点 就写多个就可以 export ETCDCTL_API=3 # 1 etcdctl -w table \\ --endpoints=https://k8s-node-1:2379,https://k8s-node-2:2379,https://k8s-node-3:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint status +-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS | +-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ | https://k8s-node-1:2379 | 930e2b9d17050efd | 3.4.3 | 2.4 MB | true | false | 8 | 23258 | 23258 | | | https://k8s-node-2:2379 | 94853f1a64b6f05 | 3.4.3 | 2.4 MB | false | false | 8 | 23258 | 23258 | | | https://k8s-node-3:2379 | c4a2be5275d5ce12 | 3.4.3 | 2.4 MB | false | false | 8 | 23258 | 23258 | | +-------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+ # 2 etcdctl -w table \\ --endpoints=https://k8s-node-1:2379,https://k8s-node-2:2379,https://k8s-node-3:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint health +-------------------------+--------+-------------+-------+ | ENDPOINT | HEALTH | TOOK | ERROR | +-------------------------+--------+-------------+-------+ | https://k8s-node-1:2379 | true | 13.300955ms | | | https://k8s-node-3:2379 | true | 14.65399ms | | | https://k8s-node-2:2379 | true | 17.387096ms | | +-------------------------+--------+-------------+-------+ # 3 etcdctl -w table \\ --endpoints=https://k8s-node-1:2379,https://k8s-node-2:2379,https://k8s-node-3:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ member list +------------------+---------+------------+---------------------------+---------------------------+------------+ | ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER | +------------------+---------+------------+---------------------------+---------------------------+------------+ | 94853f1a64b6f05 | started | k8s-node-2 | https://10.18.77.117:2380 | https://10.18.77.117:2379 | false | | 930e2b9d17050efd | started | k8s-node-1 | https://10.18.77.61:2380 | https://10.18.77.61:2379 | false | | c4a2be5275d5ce12 | started | k8s-node-3 | https://10.18.77.218:2380 | https://10.18.77.218:2379 | false | +------------------+---------+------------+---------------------------+---------------------------+------------+ ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:14:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.9.2 查看 pods 状态 [root@k8s-node-1 flannel]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-546565776c-9zbqz 1/1 Running 0 137m kube-system coredns-546565776c-lz5fs 1/1 Running 0 137m kube-system etcd-k8s-node-1 1/1 Running 0 138m kube-system etcd-k8s-node-2 1/1 Running 0 34m kube-system etcd-k8s-node-3 1/1 Running 0 33m kube-system kube-apiserver-k8s-node-1 1/1 Running 0 138m kube-system kube-apiserver-k8s-node-2 1/1 Running 0 34m kube-system kube-apiserver-k8s-node-3 1/1 Running 0 33m kube-system kube-controller-manager-k8s-node-1 1/1 Running 1 138m kube-system kube-controller-manager-k8s-node-2 1/1 Running 0 34m kube-system kube-controller-manager-k8s-node-3 1/1 Running 0 33m kube-system kube-flannel-ds-amd64-2tw6q 1/1 Running 0 9m11s kube-system kube-flannel-ds-amd64-8nrtd 1/1 Running 0 9m11s kube-system kube-flannel-ds-amd64-frmk9 1/1 Running 0 9m11s kube-system kube-proxy-9qv4l 1/1 Running 0 34m kube-system kube-proxy-f29dk 1/1 Running 0 137m kube-system kube-proxy-zgjnf 1/1 Running 0 33m kube-system kube-scheduler-k8s-node-1 1/1 Running 1 138m kube-system kube-scheduler-k8s-node-2 1/1 Running 0 34m kube-system kube-scheduler-k8s-node-3 1/1 Running 0 33m ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:14:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.9.3 查看 svc 的状态 [root@k8s-node-1 flannel]# kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 138m kube-system kube-dns ClusterIP 10.254.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 138m ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:14:3","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"3.9.3 查看 IPVS 的状态 [root@k8s-node-1 flannel]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 wrr -\u003e 10.18.77.61:5443 Masq 1 2 0 -\u003e 10.18.77.117:5443 Masq 1 0 0 -\u003e 10.18.77.218:5443 Masq 1 0 0 TCP 10.254.0.10:53 wrr -\u003e 10.254.64.3:53 Masq 1 0 0 -\u003e 10.254.65.4:53 Masq 1 0 0 TCP 10.254.0.10:9153 wrr -\u003e 10.254.64.3:9153 Masq 1 0 0 -\u003e 10.254.65.4:9153 Masq 1 0 0 TCP 10.254.28.93:80 wrr -\u003e 10.254.65.5:80 Masq 1 0 1 -\u003e 10.254.66.3:80 Masq 1 0 2 UDP 10.254.0.10:53 wrr -\u003e 10.254.64.3:53 Masq 1 0 0 -\u003e 10.254.65.4:53 Masq 1 0 0 4. 测试集群 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:14:4","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"4.1 创建一个 nginx deployment apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm labels: app: nginx spec: replicas: 3 strategy: # 配置滚动升级策略 type: RollingUpdate rollingUpdate: # 生成1个新的pod完成后再删除1个旧的pod maxSurge: 1 # 设置最多容忍2个pods处于无法提供服务的状态 maxUnavailable: 2 # 控制 pod 处于就绪状态的观察时间 # pod 在这段时间内都正常运行, 才认为新 pod 可用, 将老的 pod 删除掉。 minReadySeconds: 120 selector: matchLabels: app: nginx template: metadata: labels: app: nginx version: v1.0.0 spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http # 资源的限制 resources: limits: cpu: 1000m memory: 500Mi requests: # 1 cpu = 1000m cpu: 0.5 memory: 250Mi volumeMounts: - name: tz-config mountPath: /etc/localtime readOnly: true # readinessProbe - 检测pod 的 Ready 是否为 true # 就绪探针 如果探针判断失败,则不会有流量发往到这个pod。 readinessProbe: tcpSocket: port: 80 # 启动后5s 开始检测 initialDelaySeconds: 5 # 检测 间隔为 10s periodSeconds: 10 # 探针探测失败后, 最少连续探测成功多少次才被认定为成功 successThreshold: 1 # 探测成功后, 最少连续探测失败多少次才被认定为失败 failureThreshold: 1 # livenessProbe - 检测 pod 的 State 是否为 Running # 活性探测 如果探针判断失败, 则会重启这个 pod。 livenessProbe: httpGet: path: / port: 80 # 启动后 15s 开始检测 # 检测时间必须在 readinessProbe 之后 initialDelaySeconds: 15 # 检测 间隔为 20s periodSeconds: 20 # 探针探测失败后, 最少连续探测成功多少次才被认定为成功 successThreshold: 1 # 探测成功后, 最少连续探测失败多少次才被认定为失败 failureThreshold: 3 volumes: - name: tz-config hostPath: path: /etc/localtime --- apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: app: nginx # 导入文件 [root@k8s-node-1 kubeadm]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-dm created service/nginx-svc created # 查看服务 [root@k8s-node-1 kubeadm]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-8665b6b679-lf72f 1/1 Running 0 37s nginx-dm-8665b6b679-mqn5f 1/1 Running 0 37s # 查看 svc [root@k8s-node-1 kubeadm]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 146m \u003cnone\u003e nginx-svc ClusterIP 10.254.23.158 \u003cnone\u003e 80/TCP 54s name=nginx 访问 svc 与 # node-1 访问 svc [root@k8s-node-1 yaml]# curl 10.254.28.93 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # node-2 访问 svc [root@k8s-node-2 ~]# curl 10.254.28.93 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@k8s-node-1 yaml]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 wrr -\u003e 10.18.77.61:5443 Masq 1 2 0 -\u003e 10.18.77.117:5443 Masq 1 0 0 -\u003e 10.18.77.218:5443 Masq 1 0 0 TCP 10.254.0.10:53 wrr -\u003e 10.254.64.3:53 Masq 1 0 0 -\u003e 10.254.65.4:53 Masq 1 0 0 TCP 10.254.0.10:9153 wrr -\u003e 10.254.64.3:9153 Masq 1 0 0 -\u003e 10.254.65.4:9153 Masq 1 0 0 TCP 10.254.28.93:80 wrr -\u003e 10.254.65.5:80 Masq 1 0 10 -\u003e 10.254.66.3:80 Masq 1 0 10 UDP 10.254.0.10:53 wrr -\u003e 10.254.64.3:53 Masq 1 0 0 -\u003e 10.254.65.4:53 Masq 1 0 0 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:15:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"4.2 验证 dns 的服务 # 测试 [root@k8s-node-1 kubeadm]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-8665b6b679-28zbw 1/1 Running 0 7m54s nginx-dm-8665b6b679-h5rhn 1/1 Running 0 7m54s # kubernetes 服务 [root@k8s-node-1 kubeadm]# kubectl exec -it nginx-dm-8665b6b679-28zbw -- nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local # nginx-svc 服务 [root@k8s-node-1 kubeadm]# kubectl exec -it nginx-dm-8665b6b679-28zbw -- nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.27.199 nginx-svc.default.svc.cluster.local 5. 部署 Metrics-Server 官方 https://github.com/kubernetes-incubator/metrics-server ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:16:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"5.1 Metrics-Server 说明 v1.11 以后不再支持通过 heaspter 采集监控数据，支持新的监控数据采集组件metrics-server，比heaspter轻量很多，也不做数据的持久化存储，提供实时的监控数据查询。 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:17:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"5.1.1 创建 Metrics-Server 文件 # vi metrics-server.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:aggregated-metrics-reader labels: rbac.authorization.k8s.io/aggregate-to-view: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rules: - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1beta1.metrics.k8s.io spec: service: name: metrics-server namespace: kube-system group: metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:metrics-server rules: - apiGroups: - \"\" resources: - pods - nodes - nodes/stats - namespaces verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 imagePullPolicy: IfNotPresent args: - --cert-dir=/tmp - --secure-port=4443 ports: - name: main-port containerPort: 4443 protocol: TCP securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 imagePullPolicy: Always volumeMounts: - name: tmp-dir mountPath: /tmp command: - /metrics-server - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP nodeSelector: beta.kubernetes.io/os: linux kubernetes.io/arch: \"amd64\" --- apiVersion: v1 kind: Service metadata: name: metrics-server namespace: kube-system labels: kubernetes.io/name: \"Metrics-server\" kubernetes.io/cluster-service: \"true\" spec: selector: k8s-app: metrics-server ports: - port: 443 protocol: TCP targetPort: main-port # 导入服务 [root@k8s-node-1 metrics]# kubectl apply -f metrics-server.yaml clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/system:metrics-server created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created serviceaccount/metrics-server created serviceaccount/metrics-server unchanged deployment.apps/metrics-server created service/metrics-server created ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:17:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"5.1.2 查看服务 [root@k8s-node-1 metrics]# kubectl get pods -n kube-system |grep metrics metrics-server-7b5b7fd65-v8sqc 1/1 Running 0 11s ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:17:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"5.1.3 测试采集 提示 error: metrics not available yet , 请等待一会采集后再查询 查看 pods 的信息 [root@k8s-node-1 metrics]# kubectl top pods -n kube-system NAME CPU(cores) MEMORY(bytes) coredns-546565776c-9zbqz 2m 5Mi coredns-546565776c-lz5fs 2m 5Mi etcd-k8s-node-1 27m 75Mi etcd-k8s-node-2 25m 76Mi etcd-k8s-node-3 23m 75Mi kube-apiserver-k8s-node-1 21m 272Mi kube-apiserver-k8s-node-2 19m 277Mi kube-apiserver-k8s-node-3 23m 279Mi kube-controller-manager-k8s-node-1 12m 37Mi kube-controller-manager-k8s-node-2 2m 12Mi kube-controller-manager-k8s-node-3 2m 12Mi kube-flannel-ds-amd64-f2ck7 2m 8Mi kube-flannel-ds-amd64-g6tp6 2m 8Mi kube-flannel-ds-amd64-z2cvb 2m 9Mi kube-proxy-9qv4l 12m 9Mi kube-proxy-f29dk 11m 9Mi kube-proxy-zgjnf 10m 9Mi kube-scheduler-k8s-node-1 3m 9Mi kube-scheduler-k8s-node-2 2m 8Mi kube-scheduler-k8s-node-3 2m 10Mi metrics-server-7ff8dccd5b-jsjkk 2m 13Mi 查看 node 信息 [root@k8s-node-1 metrics]# kubectl top nodes NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-node-1 110m 5% 1100Mi 28% k8s-node-2 97m 4% 1042Mi 27% k8s-node-3 94m 4% 1028Mi 26% 6. Nginx Ingress (更新 2020-07-29) 真是不想吐槽这个 ingress 每次更新都是大的变动而且文档说明也没有, 更加不可能兼容前面的。 官方地址 https://kubernetes.github.io/ingress-nginx/ ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:17:3","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.1 Nginx Ingress 介绍 基于 Nginx 使用 Kubernetes ConfigMap 来存储 Nginx 配置文件 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:18:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.2 部署 Nginx ingress ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:19:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.2.1 下载 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.34.1/deploy/static/provider/cloud/deploy.yaml ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:19:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.2.2 修改 yaml 文件 # 镜像下载地址 image: us.gcr.io/k8s-artifacts-prod/ingress-nginx/controller:v0.34.1@sha256:0e072dddd1f7f8fc8909a2ca6f65e76c5f0d2fcfb8be47935ae3457e8bbceb20 # 替换为 image: jicki/controller:v0.34.1 # 修改如下部分: # Source: ingress-nginx/templates/controller-service.yaml apiVersion: v1 kind: Service # 如下部分: type: LoadBalancer externalTrafficPolicy: Local # 修改为 type: ClusterIP externalTrafficPolicy: Local # Deployment 部分 # Source: ingress-nginx/templates/controller-deployment.yaml apiVersion: apps/v1 kind: Deployment # 配置 node affinity # 配置 hostNetwork # 配置 dnsPolicy: ClusterFirstWithHostNet # 在 如下之间添加 spec: dnsPolicy: ClusterFirst # 修改为如下: spec: hostNetwork: true dnsPolicy: ClusterFirstWithHostNet ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:19:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.2.3 apply 导入 文件 [root@k8s-node-1 ingress]# kubectl apply -f deploy.yaml namespace/ingress-nginx created serviceaccount/ingress-nginx created configmap/ingress-nginx-controller created clusterrole.rbac.authorization.k8s.io/ingress-nginx created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created role.rbac.authorization.k8s.io/ingress-nginx created rolebinding.rbac.authorization.k8s.io/ingress-nginx created service/ingress-nginx-controller-admission created service/ingress-nginx-controller created deployment.apps/ingress-nginx-controller created validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created job.batch/ingress-nginx-admission-create created job.batch/ingress-nginx-admission-patch created role.rbac.authorization.k8s.io/ingress-nginx-admission created rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created serviceaccount/ingress-nginx-admission created ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:19:3","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.2.4 查看服务状态 [root@k8s-node-1 ingress]# kubectl get pods,svc -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/ingress-nginx-admission-create-dgg76 0/1 Completed 0 114s pod/ingress-nginx-admission-patch-f65qj 0/1 Completed 1 114s pod/ingress-nginx-controller-5f4cb6d6f4-pft6x 1/1 Running 0 2m4s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ingress-nginx-controller ClusterIP 10.99.155.236 \u003cnone\u003e 80/TCP,443/TCP 2m4s service/ingress-nginx-controller-admission ClusterIP 10.102.95.56 \u003cnone\u003e 443/TCP 2m4s ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:19:4","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.2.5 测试 ingress # 查看之前创建的 Nginx [root@k8s-node-1 ingress]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 74m nginx-svc ClusterIP 10.254.52.255 \u003cnone\u003e 80/TCP 19m # 创建一个 nginx-svc 的 ingress vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 # 导入 yaml [root@k8s-node-1 kubeadm]# kubectl apply -f nginx-ingress.yaml ingress.extensions/nginx-ingress created # 查看 ingress [root@k8s-node-1 kubeadm]# kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE nginx-ingress \u003cnone\u003e nginx.jicki.cn 80 34s ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:19:5","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"6.2.6 测试访问 [root@k8s-node-1 kubeadm]# curl -I nginx.jicki.cn HTTP/1.1 200 OK Server: nginx/1.17.8 Date: Mon, 30 Mar 2020 08:54:56 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Vary: Accept-Encoding Last-Modified: Tue, 03 Mar 2020 17:36:53 GMT ETag: \"5e5e95b5-264\" Accept-Ranges: bytes 7. Dashboard 官方 https://github.com/kubernetes/dashboard ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:19:6","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.1 Dashboard 介绍 Dashboard 是 Kubernetes 集群的 通用 WEB UI 它允许用户管理集群中运行的应用程序并对其进行故障排除，以及管理集群本身。 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:20:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2 部署 Dashboard 注意 dashboard 1.10.x 版本 不支持 kubernetes 1.16.x 以上的必须使用 2.0 版本否则报错 404 the server could not find the requested resource 目前 Dashboard 已经进入 rc6 阶段 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2.1 下载 yaml 文件 # 下载 yaml 文件 https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2.2 apply 导入文件 [root@k8s-node-1 dashboard]# kubectl apply -f recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:2","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2.3 查看服务状态 [root@k8s-node-1 dashboard]# kubectl get pods -n kubernetes-dashboard |grep dashboard dashboard-metrics-scraper-779f5454cb-8m5p5 1/1 Running 0 19s kubernetes-dashboard-64686c4bf9-bwvvj 1/1 Running 0 19s # svc 服务 [root@k8s-node-1 dashboard]# kubectl get svc -n kubernetes-dashboard |grep dashboard dashboard-metrics-scraper ClusterIP 10.254.39.66 \u003cnone\u003e 8000/TCP 43s kubernetes-dashboard ClusterIP 10.254.53.202 \u003cnone\u003e 443/TCP 44s ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:3","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2.4 暴露公网 访问 kubernetes 服务，既暴露 kubernetes 内的端口到 外网，有很多种方案 LoadBlancer ( 支持的公有云服务的负载均衡 ) NodePort (映射所有 node 中的某个端口，暴露到公网中) Ingress ( 支持反向代理软件的对外服务, 如: Nginx , HAproxy 等) # 由于我们已经部署了 Nginx-ingress 所以这里使用 ingress 来暴露出去 部署好 dashboard 以后会生成一个 自签的证书 kubernetes-dashboard-certs 后面 ingress 会使用到这个证书 [root@k8s-node-1 dashboard]# kubectl get secret -n kubernetes-dashboard NAME TYPE DATA AGE default-token-nnn5x kubernetes.io/service-account-token 3 6m32s kubernetes-dashboard-certs Opaque 0 6m32s kubernetes-dashboard-csrf Opaque 1 6m32s kubernetes-dashboard-key-holder Opaque 2 6m32s kubernetes-dashboard-token-7plmf kubernetes.io/service-account-token 3 6m32s # 创建 dashboard ingress # 这里面 annotations 中的 backend 声明,从 v0.21.0 版本开始变更, 一定注意 # nginx-ingress \u003c v0.21.0 使用 nginx.ingress.kubernetes.io/secure-backends: \"true\" # nginx-ingress \u003e v0.21.0 使用 nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" # 创建 ingress 文件 vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard namespace: kubernetes-dashboard annotations: ingress.kubernetes.io/ssl-passthrough: \"true\" nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec: tls: - hosts: - dashboard.jicki.cn secretName: kubernetes-dashboard-certs rules: - host: dashboard.jicki.cn http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443 # 导入 yaml [root@k8s-node-1 dashboard]# kubectl apply -f dashboard-ingress.yaml ingress.extensions/kubernetes-dashboard created # 查看 ingress [root@k8s-node-1 dashboard]# kubectl get ingress -n kubernetes-dashboard NAME CLASS HOSTS ADDRESS PORTS AGE kubernetes-dashboard \u003cnone\u003e dashboard.jicki.cn 80, 443 2m53s ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:4","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2.6 测试访问 [root@k8s-node-1 dashboard]# curl -I -k https://dashboard.jicki.cn HTTP/2 200 server: nginx/1.17.8 date: Mon, 30 Mar 2020 09:41:02 GMT content-type: text/html; charset=utf-8 content-length: 1287 vary: Accept-Encoding accept-ranges: bytes cache-control: no-store last-modified: Fri, 13 Mar 2020 13:43:54 GMT strict-transport-security: max-age=15724800; includeSubDomains ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:5","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2.7 令牌 登录认证 # 创建一个 dashboard rbac 超级用户 vi dashboard-admin-rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kubernetes-dashboard # 导入文件 [root@k8s-node-1 dashboard]# kubectl apply -f dashboard-admin-rbac.yaml serviceaccount/kubernetes-dashboard-admin created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin created # 查看 secret [root@k8s-node-1 dashboard]# kubectl get secret -n kubernetes-dashboard | grep kubernetes-dashboard-admin kubernetes-dashboard-admin-token-9dkg4 kubernetes.io/service-account-token 3 38s # 查看 token 部分 [root@k8s-node-1 dashboard]# kubectl describe -n kubernetes-dashboard secret/kubernetes-dashboard-admin-token-9dkg4 Name: kubernetes-dashboard-admin-token-9dkg4 Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: kubernetes-dashboard-admin kubernetes.io/service-account.uid: aee23b33-43a4-4fb4-b498-6c2fb029d63c Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlI4UlpGcTcwR2hkdWZfZWk1X0RUcVI5dkdraXFnNW8yYUV1VVRPQlJYMEkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi05ZGtnNCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImFlZTIzYjMzLTQzYTQtNGZiNC1iNDk4LTZjMmZiMDI5ZDYzYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.oyvo_bIM0Ukbs3ov8XbmJffpdK1nec7oKJBxu8V4vesPY_keQhNS9xiAw6zdF2Db2tiEzcpmN3SAgwGjfid5rlSQxGpNK3mkp1r60WSAhyU5e7RqwA9xRO-EtCZ2akrqFKzEn4j_7FGwbKbNsdRurDdOLtKU5KvFsFh5eRxvB6PECT2mgSugfHorrI1cYOw0jcQKE_hjVa94xUseYX12PyGQfoUyC6ZhwIBkRnCSNdbcb0VcGwTerwysR0HFvozAJALh_iOBTDYDUNh94XIRh2AHCib-KVoJt-e2jUaGH-Z6yniLmNr15q5xLfNBd1qPpZHCgoJ1JYz4TeF6udNxIA # 复制 token 如下部分: token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlI4UlpGcTcwR2hkdWZfZWk1X0RUcVI5dkdraXFnNW8yYUV1VVRPQlJYMEkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi05ZGtnNCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImFlZTIzYjMzLTQzYTQtNGZiNC1iNDk4LTZjMmZiMDI5ZDYzYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.oyvo_bIM0Ukbs3ov8XbmJffpdK1nec7oKJBxu8V4vesPY_keQhNS9xiAw6zdF2Db2tiEzcpmN3SAgwGjfid5rlSQxGpNK3mkp1r60WSAhyU5e7RqwA9xRO-EtCZ2akrqFKzEn4j_7FGwbKbNsdRurDdOLtKU5KvFsFh5eRxvB6PECT2mgSugfHorrI1cYOw0jcQKE_hjVa94xUseYX12PyGQfoUyC6ZhwIBkRnCSNdbcb0VcGwTerwysR0HFvozAJALh_iOBTDYDUNh94XIRh2AHCib-KVoJt-e2jUaGH-Z6yniLmNr15q5xLfNBd1qPpZHCgoJ1JYz4TeF6udNxIA ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:6","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"7.2.8 浏览器访问 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:21:7","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"FAQ Failed to get system container stats for \"/system.slice/docker.service\": failed to get cgroup stats 错误 推测是由于 kubernetes 版本与 docker 版本不兼容导致的问题 # 打开10-kuberadm.conf 文件 vi /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf # 添加如下: Environment=\"KUBELET_CGROUP_ARGS=--cgroup-driver=systemd --runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice\" # 加载配置 systemctl daemon-reload # 重启 kubelet systemctl restart kubelet ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:22:0","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["kubernetes"],"content":"修改 node 名称 vi /var/lib/kubelet/kubeadm-flags.env # 修改其中的 --hostname-override= 变量 # 重启 kubelet systemctl daemon-reload systemctl restart kubelet # 删除旧的 node kubectl delete no nod-name # 查看 csr [root@k8s-node-1 kubeadm]# kubectl get csr NAME AGE SIGNERNAME REQUESTOR CONDITION csr-nzhlq 17s kubernetes.io/kube-apiserver-client-kubelet system:node:localhost Pending # 通过 csr [root@k8s-node-1 kubeadm]# kubectl certificate approve csr-nzhlq # 通过以后再查看 node [root@k8s-node-1 kubeadm]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 NotReady \u003cnone\u003e 8s v1.18.0 # 需要等待一段时间等待状态 [root@k8s-node-1 kubeadm]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 Ready \u003cnone\u003e 63s v1.18.0 ","date":"2020-03-30","objectID":"/kubeadm-1.18.0/:22:1","tags":["kubernetes","docker"],"title":"kubeadm v1.18.0 HA","uri":"/kubeadm-1.18.0/"},{"categories":["rancher"],"content":"Rancher 2.x to kubernetes","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"Rancher Rancher 官方文档 https://docs.rancher.cn/ ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:0:0","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"什么是 Rancher Rancher是一套容器管理平台，它可以帮助组织在生产环境中轻松快捷的部署和管理容器。 Rancher可以轻松地管理各种环境的Kubernetes，满足IT需求并为DevOps团队提供支持。 Kubernetes不仅已经成为的容器编排标准，它也正在迅速成为各类云和虚拟化厂商提供的标准基础架构。Rancher用户可以选择使用Rancher Kubernetes Engine(RKE)创建Kubernetes集群，也可以使用GKE，AKS和EKS等云Kubernetes服务。 Rancher用户还可以导入和管理现有的Kubernetes集群。 Rancher支持各类集中式身份验证系统来管理Kubernetes集群。例如，大型企业的员工可以使用其公司Active Directory凭证访问GKE中的Kubernetes集群。IT管理员可以在用户，组，项目，集群和云中设置访问控制和安全策略。 IT管理员可以在单个页面对所有Kubernetes集群的健康状况和容量进行监控。 Rancher为DevOps工程师提供了一个直观的用户界面来管理他们的服务容器，用户不需要深入了解Kubernetes概念就可以开始使用Rancher。 Rancher包含应用商店，支持一键式部署Helm和Compose模板。Rancher通过各种云、本地生态系统产品认证，其中包括安全工具，监控系统，容器仓库以及存储和网络驱动程序。下图说明了Rancher在IT和DevOps组织中扮演的角色。每个团队都会在他们选择的公共云或私有云上部署应用程序。 ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:1:0","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"Rancher 架构 大多数Rancher2.0软件运行在Rancher Server节点上,Rancher Server包括用于管理整个Rancher部署的所有组件。 下图说明了Rancher2.0 的运行架构。该图描绘了管理两个Kubernetes集群的Rancher server安装: 一个由RKE创建。 一个由GKE创建。 ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:2:0","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"Rancher 组件介绍 Rancher API服务器 Rancher API server 建立在嵌入式Kubernetes API服务器和etcd数据库之上。 Rancher API server: 管理与外部身份验证提供程序(如Active Directory或GitHub)对应的用户身份 认证授权: Rancher API server管理访问控制和安全策略 项目: 项目是集群中的一组多个命名空间和访问控制策略的集合 节点: Rancher API server跟踪所有集群中所有节点的标识 集群控制 和 Agent 集群控制器和集群代理实现管理Kubernetes集群所需的业务逻辑 集群控制器实现Rancher安装所需的全局逻辑。 ( 1. 为集群和项目配置访问控制策略 2. 通过调用以下方式配置集群 2.1 Docker machine驱动程序。 2.2 RKE和GKE这样的Kubernetes引擎 ) 单独的集群代理实例实现相应集群所需的逻辑。 ( 1. 工作负载管理，例如每个集群中的pod创建和部署 2. 绑定并应用每个集群全局策略中定义的角色 3. 集群与Rancher Server之间的通信:事件，统计信息，节点信息和运行状况 ） 认证代理 该认证代理转发所有Kubernetes API调用。它集成了身份验证服务，如本地身份验证，Active Directory和GitHub。在每个Kubernetes API调用中，身份验证代理会对调用方进行身份验证，并在将调用转发给Kubernetes主服务器之前设置正确的Kubernetes模拟标头。 Rancher使用服务帐户与Kubernetes集群通信。 ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:3:0","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"Rancher 页面模块 全局层 – 全局层主要对Rancher server 自身的基础配置, 比如Rancher Server URL、登录认证等。 集群 – 全局层的集群菜单，用于列出集群中所有的K8S集群。 主机驱动 – 用于与三方云平台API对接的中间件程序。 应用商店-全局 – 全局层的应用商店，负责应用商店的开关与添加。 用户 – 添加或者删除用户，或者修改用户的权限。 系统设置 – 全局下系统的基础配置，比如系统默认镜像仓库。 安全 角色 – 一组权限的集合 Pod安全策略 – Pod安全设置 登录认证 – 用户的登录访问认证 集群层 – 集群相关的配置 集群 – 集群仪表盘，显示当前集群的资源状态 主机 – 当前集群中添加的所有主机 存储 – 存储卷, 持久卷。 项目与命名空间 – 此集群拥有的项目和命名空间 ( deployment 与 namespaces ) 集群成员 – 集群用户 工具 – 告警、通知、日志、CI/CD 项目层 工作负载 工作负载服务 负载均衡 服务发现 数据卷 CI/CD 应用商店 – 项目 资源 告警 证书 配置映射 日志收集 镜像仓库 密文 (secret) 命名空间 项目成员 其他 API Keys 主机模板 喜好设定 ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:4:0","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"安装 Rancher 目前 Rancher 版本为 v2.3.4 ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:5:0","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"系统需求 系统 版本 docker型号 CentOS 7.5, 7.6, 7.7 Docker 17.03.2, 18.06.2, 18.09.x, 19.03.x Oracle Linux 7.6 Docker 19.03.x RancherOS 1.5.4 Docker 17.03.2, 18.06.2, 18.09.x (up to 18.09.8), 19.03.x RHEL 7.5, 7.6, 7.7 RHEL Docker 1.13.x Docker 17.03.2, 18.06.2, 18.09.x, 19.03.x Ubuntu 16.04, 18.04 Docker 17.03.2, 18.06.2, 18.09.x, 19.03.x Windows Server 1809, 1903 Docker 19.03.x EE ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:5:1","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"Kubernetes 版本 系统 版本 组件 kubernetes v1.17.0+ etcd: v3.4.3 flannel: v0.11.0 canal: v3.10.2 nginx-ingress-controller: 0.25.1 kubernetes v1.16.3+ etcd: v3.3.15 flannel: v0.11.0 canal: v3.8.1 nginx-ingress-controller: 0.25.1 kubernetes v1.15.6+ etcd: v3.3.10 flannel: v0.11.0 canal: v3.7.4 nginx-ingress-controller: 0.25.1 ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:5:2","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"单节点 Rancher Rancher 使用 docker 启动运行 # 启动 docker run -d \\ --name=rancher \\ --restart=unless-stopped \\ -v /opt/rancher/auditlog:/var/log/auditlog \\ -e AUDIT_LEVEL=3 \\ -e AUDIT_LOG_PATH=/var/log/auditlog/rancher-api-audit.log \\ -e AUDIT_LOG_MAXAGE=20 \\ -e AUDIT_LOG_MAXBACKUP=20 \\ -e AUDIT_LOG_MAXSIZE=100 \\ -p 80:80 -p 443:443 \\ rancher/rancher ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:5:3","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"多节点 Rancher 多节点是基于 Kubernetes 集群部署 Rancher 由于我只有一台服务器所以k8s 只有一个节点,既是 Master 又是 node kubectl taint node localhost node-role.kubernetes.io/master- master 调度 pods [root@localhost rancher]# kubectl get nodes NAME STATUS ROLES AGE VERSION localhost Ready master 17h v1.17.4 部署 Helm v3 官方推荐使用 Helm 来安装 Rancher 所以这里先安装一个 Helm v3 下载二进制文件 https://github.com/helm/helm/releases mkdir /opt/helm cd /opt/helm wget https://get.helm.sh/helm-v3.1.2-linux-amd64.tar.gz tar zxvf helm-v3.1.2-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/README.md linux-amd64/LICENSE mv linux-amd64/ bin # 测试 [root@localhost ~]# /opt/helm/bin/helm version version.BuildInfo{Version:\"v3.1.2\", GitCommit:\"d878d4d45863e42fd5cff6743294a11d28a9abce\", GitTreeState:\"clean\", GoVersion:\"go1.13.8\"} 添加Chart 仓库地址 # 添加 rancher 的源 /opt/helm/bin/helm repo add rancher-stable \\ https://releases.rancher.com/server-charts/stable # 输出如下: \"rancher-stable\" has been added to your repositories 部署 cert-manager 部署 cert-manager 用于 rancher 的ssl 证书 kubectl apply --validate=false -f https://github.com/jetstack/cert-manager/releases/download/v0.14.0/cert-manager.yaml 验证cert-manager安装 [root@localhost rancher]# kubectl get pods -n cert-manager NAME READY STATUS RESTARTS AGE cert-manager-75f6cdcb64-ng7xp 1/1 Running 0 7m47s cert-manager-cainjector-79788689f9-6gq8m 1/1 Running 0 7m47s cert-manager-webhook-5b6c798c9-r6bkz 1/1 Running 0 7m47s 导出 rancher YAML 文件 安装 rancher 渲染 yaml 文件 这里由于我不太使用 helm 所以我现在重新渲染成完整的 yaml 。 使用 cert-manager # 创建 namespaces , 这里 cattle-system 会存放后续 rancher 的 agent 等项目 kubectl create namespace cattle-system # 安装 rancher 使用 cert-manager 做为证书 /opt/helm/bin/helm install rancher rancher-stable/rancher \\ --namespace cattle-system \\ --set hostname=rancher.jicki.cn \\ --debug --dry-run clusterRoleBinding.yaml 文件 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: rancher labels: app: rancher chart: rancher-2.3.5 heritage: Helm release: rancher subjects: - kind: ServiceAccount name: rancher namespace: cattle-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io serviceAccount.yaml 文件 kind: ServiceAccount apiVersion: v1 metadata: name: rancher namespace: cattle-system labels: app: rancher chart: rancher-2.3.5 heritage: Helm release: rancher deployment.yaml 文件 kind: Deployment apiVersion: apps/v1 metadata: name: rancher namespace: cattle-system labels: app: rancher chart: rancher-2.3.5 heritage: Helm release: rancher spec: replicas: 3 selector: matchLabels: app: rancher strategy: rollingUpdate: maxSurge: 1 maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: rancher release: rancher spec: serviceAccountName: rancher affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - rancher topologyKey: kubernetes.io/hostname containers: - image: rancher/rancher:v2.3.5 imagePullPolicy: IfNotPresent name: rancher ports: - containerPort: 80 protocol: TCP args: # Public trusted CA - clear ca certs - \"--http-listen-port=80\" - \"--https-listen-port=443\" - \"--add-local=auto\" env: - name: CATTLE_NAMESPACE value: cattle-system - name: CATTLE_PEER_SERVICE value: rancher livenessProbe: httpGet: path: /healthz port: 80 initialDelaySeconds: 60 periodSeconds: 30 readinessProbe: httpGet: path: /healthz port: 80 initialDelaySeconds: 5 periodSeconds: 30 resources: {} volumeMounts: volumes: service.yaml 文件 apiVersion: v1 kind: Service metadata: name: rancher namespace: cattle-system labels: app: rancher chart: rancher-2.3.5 heritage: Helm release: rancher spec: ports: - port: 80 targetPort: 80 protocol: TCP name: http selector: app: rancher ingress.yaml 文件 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: rancher namespace: cattle-system labels: app: rancher chart: rancher-2.3.5 heritage: Helm release: rancher annotations: cert-manager.io/issuer: rancher nginx.ingress.kubernetes.io/proxy-connect-ti","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:5:4","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"访问 Rancher 如上配置, 已经添加 ingress 并且配置了域名为 rancher.jicki.cn 通过 浏览器访问 https://rancher.jicki.cn 添加一个集群 k8s 集群服务的配置 我这里只有一台服务器, 所以都是基于rancher部署的那个导入的k8s集群。 ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:5:5","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["rancher"],"content":"清除 rancher # 删除 yaml 文件 kubectl delete -f . # 删除命名空间 # cattle-system kubectl patch namespace cattle-system -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' -n cattle-system kubectl delete namespace cattle-system --grace-period=0 --force # cattle-global-data kubectl patch namespace cattle-global-data -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' -n cattle-system kubectl delete namespace cattle-global-data --grace-period=0 --force # cattle-global-nt kubectl patch namespace cattle-global-nt -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' -n cattle-system kubectl delete namespace cattle-global-nt --grace-period=0 --force # local kubectl patch namespace local -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' -n cattle-system for resource in `kubectl api-resources --verbs=list --namespaced -o name | xargs -n 1 kubectl get -o name -n local`; do kubectl patch $resource -p '{\"metadata\": {\"finalizers\": []}}' --type='merge' -n local; done kubectl delete namespace local --grace-period=0 --force # 其他相关 namespace, 具体名字根据自己环境 get namespace 查询 p-nw6f6 Active 46m p-s8vtd Active 46m user-6jntr Active 43m kubectl delete namespace p-nw6f6 --grace-period=0 --force kubectl delete namespace p-s8vtd --grace-period=0 --force kubectl delete namespace user-6jntr --grace-period=0 --force kubectl patch namespace p-nw6f6 -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' -n cattle-system kubectl patch namespace p-s8vtd -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' -n cattle-system kubectl patch namespace user-6jntr -p '{\"metadata\":{\"finalizers\":[]}}' --type='merge' -n cattle-system ","date":"2020-03-24","objectID":"/rancher-2.x-kubernetes/:5:6","tags":["kubernetes","rancher","docker"],"title":"Rancher 2.x to kubernetes","uri":"/rancher-2.x-kubernetes/"},{"categories":["fabric"],"content":"hyperledger-fabric v2.0","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":" fabric v2.0 , 单机 多节点 RAFT 手动部署, 所有服务均 开启 SSL 认证。 hyperledger fabric ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:0:0","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"Hyperleger fabric 是什么? Linux基金会于2015年创建了Hyperledger项目，以推进跨行业的区块链技术。Hyperledger Fabric是Hyperledger中的区块链项目之一。像其他区块链技术一样，它具有分类帐，使用智能合约，并且是参与者用来管理其交易的系统。Hyperledger Fabric与其他区块链系统最大的不同体现在私有和许可。 Hyperledger Fabric 是一个（平台），提供分布式账本解决方案的平台，Hyperledger Fabric 由模块化架构支撑，并具备极佳的保密性、可伸缩性、灵活性、可扩展性，Hyperledger Fabric 被设计成（模块直接插拔，适用多种场景），支持不同的模块组件直接拔插启用，并能适应在经济生态系统中错综复杂的各种场景。 ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:1:0","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"Hyperledger Fabric 交易流程 ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:2:0","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"部署 Hyperledger Fabric v2.0 官网 github — https://github.com/hyperledger/fabric ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:0","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"准备阶段 docker-ce docker-compose Hyperledger Fabric images ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:1","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"安装 docker 升级内核 # 导入 Key rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 安装 Yum 源 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm # 更新 kernel yum --enablerepo=elrepo-kernel install -y kernel-lt kernel-lt-devel # 配置 内核优先 grub2-set-default 0 开启内核 namespace 支持 # 执行如下 grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" # 必须重启系统 reboot 修改内核参数 cat\u003c\u003cEOF \u003e /etc/sysctl.d/docker.conf # 要求iptables不对bridge的数据进行处理 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-arptables = 1 EOF # 生效配置 sysctl --system 检查环境状态 curl -s https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash 安装 docker # 指定安装,并指定安装源 export VERSION=19.03 curl -fsSL \"https://get.docker.com/\" | bash -s -- --mirror Aliyun docker 额外配置 mkdir -p /etc/docker/ cat\u003e/etc/docker/daemon.json\u003c\u003cEOF { \"bip\": \"172.17.0.1/16\", \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\",\"https://gcr.azk8s.cn\",\"https://quay.azk8s.cn\"], \"data-root\": \"/opt/docker\", \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\", \"max-file\": \"5\" } } EOF 启动 docker # 配置开机启动 systemctl enable docker # 启动 docker systemctl start docker ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:2","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"安装 docker-compose # 国内下载点 curl -L https://get.daocloud.io/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` \u003e /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose # github 下载 curl -L \"https://github.com/docker/compose/releases/download/1.25.4/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:3","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"Hyperledger Fabric images 由于 image 镜像较大,下载过程会比较久, 所以这里提前将 images 下载到本地。 #!/bin/bash FABRIC_TAG=2.0 function pullImages() { docker pull hyperledger/fabric-couchdb:0.4.20 docker pull hyperledger/fabric-ca:1.4.6 for image in peer orderer ccenv tools; do echo \"Pull image hyperledger/fabric-$image:$FABRIC_TAG\" docker pull hyperledger/fabric-\"$image\":\"$FABRIC_TAG\" sleep 1 done } echo \"Pull images for hyperledger fabric network.\" pullImages [root@localhost jicki]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE hyperledger/fabric-tools 2.0 5c9a03790913 3 weeks ago 512MB hyperledger/fabric-peer 2.0 5c7e5946f3dc 3 weeks ago 57.2MB hyperledger/fabric-orderer 2.0 92bd220edcdd 3 weeks ago 39.7MB hyperledger/fabric-ccenv 2.0 800087268d9b 3 weeks ago 529MB hyperledger/fabric-ca 1.4.6 3b96a893c1e4 3 weeks ago 150MB ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:4","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"创建 crypto-config.yaml 文件 Fabric 网络成员加密配置 如下配置 3个 orderer 节点, 2个Org组织, 每个组织下有 2个Peer节点。 OrdererOrgs: - Name: Orderer Domain: jicki.cn CA: Country: CN Province: GuangDong Locality: ShenZhen Specs: - Hostname: orderer0 - Hostname: orderer1 - Hostname: orderer2 PeerOrgs: - Name: Org1 Domain: org1.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 - Name: Org2 Domain: org2.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:5","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"创建 configtxgen.yaml 文件 网络初始化以及通道配置 --- Organizations: - \u0026OrdererOrg Name: OrdererOrg ID: OrdererMSP MSPDir: crypto-config/ordererOrganizations/jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Writers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Admins: Type: Signature Rule: \"OR('OrdererMSP.admin')\" OrdererEndpoints: - orderer0.jicki.cn:7050 - \u0026Org1 Name: Org1MSP ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.peer', 'Org1MSP.client')\" Writers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.client')\" Admins: Type: Signature Rule: \"OR('Org1MSP.admin')\" Endorsement: Type: Signature Rule: \"OR('Org1MSP.peer')\" AnchorPeers: - Host: peer0.org1.jicki.cn Port: 7051 - \u0026Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.peer', 'Org2MSP.client')\" Writers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.client')\" Admins: Type: Signature Rule: \"OR('Org2MSP.admin')\" Endorsement: Type: Signature Rule: \"OR('Org2MSP.peer')\" AnchorPeers: - Host: peer0.org2.jicki.cn Port: 7051 Capabilities: Channel: \u0026ChannelCapabilities V2_0: true Orderer: \u0026OrdererCapabilities V2_0: true Application: \u0026ApplicationCapabilities V2_0: true Application: \u0026ApplicationDefaults Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" LifecycleEndorsement: Type: ImplicitMeta Rule: \"MAJORITY Endorsement\" Endorsement: Type: ImplicitMeta Rule: \"MAJORITY Endorsement\" Capabilities: \u003c\u003c: *ApplicationCapabilities Orderer: \u0026OrdererDefaults OrdererType: etcdraft BatchTimeout: 2s BatchSize: MaxMessageCount: 10 AbsoluteMaxBytes: 99 MB PreferredMaxBytes: 512 KB Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" BlockValidation: Type: ImplicitMeta Rule: \"ANY Writers\" Channel: \u0026ChannelDefaults Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ChannelCapabilities Profiles: TwoOrgsChannel: Consortium: SampleConsortium \u003c\u003c: *ChannelDefaults Application: \u003c\u003c: *ApplicationDefaults Organizations: - *Org1 - *Org2 Capabilities: \u003c\u003c: *ApplicationCapabilities SampleMultiNodeEtcdRaft: \u003c\u003c: *ChannelDefaults Capabilities: \u003c\u003c: *ChannelCapabilities Orderer: \u003c\u003c: *OrdererDefaults OrdererType: etcdraft EtcdRaft: Consenters: - Host: orderer0.jicki.cn Port: 7050 ClientTLSCert: crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/server.crt ServerTLSCert: crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/server.crt - Host: orderer1.jicki.cn Port: 7050 ClientTLSCert: crypto-config/ordererOrganizations/jicki.cn/orderers/orderer1.jicki.cn/tls/server.crt ServerTLSCert: crypto-config/ordererOrganizations/jicki.cn/orderers/orderer1.jicki.cn/tls/server.crt - Host: orderer2.jicki.cn Port: 7050 ClientTLSCert: crypto-config/ordererOrganizations/jicki.cn/orderers/orderer2.jicki.cn/tls/server.crt ServerTLSCert: crypto-config/ordererOrganizations/jicki.cn/orderers/orderer2.jicki.cn/tls/server.crt Addresses: - orderer0.jicki.cn:7050 - orderer1.jicki.cn:7050 - orderer2.jicki.cn:7050 Organizations: - *OrdererOrg Capabilities: \u003c\u003c: *OrdererCapabilities Application: \u003c\u003c: *ApplicationDefaults Organizations: - \u003c\u003c: *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:6","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"生成证书和创世区块 # 下载证书生成工具 cd /opt/jicki wget https://github.com/hyperledger/fabric/releases/download/v2.0.0/hyperledger-fabric-linux-amd64-2.0.0.tar.gz [root@localhost jicki]# tar zxvf hyperledger-fabric-linux-amd64-2.0.0.tar.gz bin/ bin/configtxgen bin/orderer bin/peer bin/discover bin/idemixgen bin/configtxlator bin/cryptogen config/ config/configtx.yaml config/core.yaml config/orderer.yaml # 为方便使用 我们配置一个 环境变量 vi /etc/profile # fabric env export PATH=$PATH:/opt/jicki/bin # 使文件生效 source /etc/profile # 首先需要创建一个文件夹 mkdir -p /opt/jicki/channel-artifacts 生成证书 # 然后这里使用 cryptogen 软件来生成相应的证书了 [root@localhost jicki]# cryptogen generate --config=./crypto-config.yaml org1.jicki.cn org2.jicki.cn 生成区块 -profile 在 configtx.yaml 文件中查找 Profiles: 下面字段 # generate genesis block for orderer # 生成启动 orderer 所需要的创世区块文件 # 这里特别注意 orderer 的channelID 与 下面创建的 channelID 千万不能相同。 configtxgen -profile SampleMultiNodeEtcdRaft \\ -outputBlock ./channel-artifacts/genesis.block \\ -channelID orderer-channel # 输出如下: 2020-03-23 19:01:00.548 CST [common.tools.configtxgen] main -\u003e INFO 001 Loading configuration 2020-03-23 19:01:00.576 CST [common.tools.configtxgen.localconfig] completeInitialization -\u003e INFO 002 orderer type: etcdraft 2020-03-23 19:01:00.576 CST [common.tools.configtxgen.localconfig] completeInitialization -\u003e INFO 003 Orderer.EtcdRaft.Options unset, setting to tick_interval:\"500ms\" election_tick:10 heartbeat_tick:1 max_inflight_blocks:5 snapshot_interval_size:16777216 2020-03-23 19:01:00.576 CST [common.tools.configtxgen.localconfig] Load -\u003e INFO 004 Loaded configuration: /opt/jicki/configtx.yaml 2020-03-23 19:01:00.578 CST [common.tools.configtxgen] doOutputBlock -\u003e INFO 005 Generating genesis block 2020-03-23 19:01:00.578 CST [common.tools.configtxgen] doOutputBlock -\u003e INFO 006 Writing genesis block # generate channel configuration transaction # 生成用于配置自定义 channel 的交易文件 # 这里特别注意 configuration 的channelID 不能与上面 Orderer的channelID相同. configtxgen \\ -profile TwoOrgsChannel \\ -outputCreateChannelTx ./channel-artifacts/channel.tx \\ -channelID mychannel # 输出如下: 2020-03-23 19:02:33.902 CST [common.tools.configtxgen] main -\u003e INFO 001 Loading configuration 2020-03-23 19:02:33.929 CST [common.tools.configtxgen.localconfig] Load -\u003e INFO 002 Loaded configuration: /opt/jicki/configtx.yaml 2020-03-23 19:02:33.929 CST [common.tools.configtxgen] doOutputChannelCreateTx -\u003e INFO 003 Generating new channel configtx 2020-03-23 19:02:33.932 CST [common.tools.configtxgen] doOutputChannelCreateTx -\u003e INFO 004 Writing new channel tx # 生成用于配置 anchor peer 的交易文件 # Org1 configtxgen \\ -profile TwoOrgsChannel \\ -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx \\ -channelID mychannel \\ -asOrg Org1MSP # Org2 configtxgen \\ -profile TwoOrgsChannel \\ -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx \\ -channelID mychannel \\ -asOrg Org2MSP # 输出如下: 2020-03-23 19:04:41.859 CST [common.tools.configtxgen] main -\u003e INFO 001 Loading configuration 2020-03-23 19:04:41.888 CST [common.tools.configtxgen.localconfig] Load -\u003e INFO 002 Loaded configuration: /opt/jicki/configtx.yaml 2020-03-23 19:04:41.889 CST [common.tools.configtxgen] doOutputAnchorPeersUpdate -\u003e INFO 003 Generating anchor peer update 2020-03-23 19:04:41.890 CST [common.tools.configtxgen] doOutputAnchorPeersUpdate -\u003e INFO 004 Writing anchor peer update ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:7","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"启动 fabric docker-compose.yaml 文件 version: '2' services: orderer0.jicki.cn: container_name: orderer0.jicki.cn image: hyperledger/fabric-orderer:2.0 environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt, /etc/hyperledger/crypto/peerOrg1/tls/ca.crt, /etc/hyperledger/crypto/peer1rg1/tls/ca.crt, /etc/hyperledger/crypto/peerOrg2/tls/ca.crt, /etc/hyperledger/crypto/peer1rg2/tls/ca.crt] - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: # 数据持久化,以及存储 - ./data/orderer0:/var/hyperledger/production - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/:/var/hyperledger/orderer/tls - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/:/etc/hyperledger/crypto/peerOrg1 - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer1.org1.jicki.cn/:/etc/hyperledger/crypto/peer1rg1 - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/:/etc/hyperledger/crypto/peerOrg2 - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer1.org2.jicki.cn/:/etc/hyperledger/crypto/peer1rg2 networks: default: aliases: - jicki ports: - 7050:7050 orderer1.jicki.cn: container_name: orderer1.jicki.cn image: hyperledger/fabric-orderer:2.0 environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt, /etc/hyperledger/crypto/peerOrg1/tls/ca.crt, /etc/hyperledger/crypto/peer1rg1/tls/ca.crt, /etc/hyperledger/crypto/peerOrg2/tls/ca.crt, /etc/hyperledger/crypto/peer1rg2/tls/ca.crt] - ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_CLUSTER_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: # 数据持久化,以及存储 - ./data/orderer1:/var/hyperledger/production - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer1.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer1.jicki.cn/tls/:/var/hyperledger/orderer/tls - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/:/etc/hyperledger/crypto/peerOrg1 - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer1.org1.jicki.cn/:/etc/hyperledger/crypto/peer1rg1 - ./crypto-config/p","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:8","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"创建 Channel #!/bin/bash CLI_NAME=cli ORDERER=orderer0.jicki.cn:7050 CORE_PEER_TLS_ENABLED=true CAFILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # Channel 通道ID 不能与 之前 Orderer 之前创建的通道相同,否则报错 # config update for existing channel did not pass initial checks: implicit policy evaluation failed - 0 sub-policies were satisfied, but this policy requires 1 of the 'Writers' sub-policies to be satisfied: permission denied CHANNEL=mychannel docker exec -it -e \"ORDERER=$ORDERER\" -e \"CORE_PEER_TLS_ENABLED=$CORE_PEER_TLS_ENABLED\" -e \"CAFILE=$CAFILE\" -e \"CHANNEL=$CHANNEL\" \"$(docker ps -q -f status=running --filter name=$CLI_NAME)\" sh -c 'peer channel create -o $ORDERER -c $CHANNEL -f ./channel-artifacts/channel.tx --tls $CORE_PEER_TLS_ENABLED --cafile $CAFILE' ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:9","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"加入 Channel (Join) 每个 peer 节点都需要单独加入到 Channel 中，这里有4个peer节点。 首先是 org1 组织 # org1 下的 peer0 节点 [root@localhost jicki]# docker exec -it org1.cli bash export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp export CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt peer channel join \\ -b mychannel.block \\ -o orderer0.jicki.cn:7050 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA # org1 下的 peer1 节点 export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp export CORE_PEER_ADDRESS=peer1.org1.jicki.cn:7051 export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer1.org1.jicki.cn/tls/ca.crt peer channel join \\ -b mychannel.block \\ -o orderer0.jicki.cn:7050 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA 接下来是 org2 组织 # org2 组织下的 peer0 节点 export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp export CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt peer channel join \\ -b mychannel.block \\ -o orderer0.jicki.cn:7050 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA # org2 组织下的 peer1 节点 export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp export CORE_PEER_ADDRESS=peer1.org2.jicki.cn:7051 export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer1.org2.jicki.cn/tls/ca.crt peer channel join \\ -b mychannel.block \\ -o orderer0.jicki.cn:7050 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA 输出日志 # 加入成功都会输出如下信息 2020-03-24 04:14:54.947 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2020-03-24 04:14:54.950 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2020-03-24 04:14:54.954 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2020-03-24 04:14:55.092 UTC [channelCmd] executeJoin -\u003e INFO 004 Successfully submitted proposal to join channel ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:10","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"更新组织的 anchor peer org1 组织 docker exec -it org1.cli bash peer channel update \\ -o orderer0.jicki.cn:7050 \\ -c mychannel \\ -f ./channel-artifacts/Org1MSPanchors.tx \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA org2 组织 docker exec -it org2.cli bash peer channel update \\ -o orderer0.jicki.cn:7050 \\ -c mychannel \\ -f ./channel-artifacts/Org2MSPanchors.tx \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA 输出如下信息 2020-03-24 04:19:46.008 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2020-03-24 04:19:46.011 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2020-03-24 04:19:46.011 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2020-03-24 04:19:46.027 UTC [channelCmd] update -\u003e INFO 004 Successfully submitted channel update ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:11","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"安装 ChainCode # git clone 下载一个 fabric-samples git clone https://github.com/hyperledger/fabric-samples Cloning into 'fabric-samples'... remote: Enumerating objects: 4891, done. remote: Total 4891 (delta 0), reused 0 (delta 0), pack-reused 4891 Receiving objects: 100% (4891/4891), 1.72 MiB | 369.00 KiB/s, done. Resolving deltas: 100% (2478/2478), done. 拷贝 chaincode 到 cli 的目录下 # 创建一个目录 mkdir -p ./data/cli/chaincode/go/mycc # 拷贝到目录下 cp fabric-samples/chaincode/abstore/go/* ./data/cli/chaincode/go/mycc/ 打包 chaincode # 打包 chaincode docker exec -it org1.cli bash peer lifecycle chaincode package mycc.tar.gz \\ --path /opt/gopath/src/github.com/hyperledger/fabric/jicki/chaincode/go/mycc \\ --lang golang \\ --label mycc # 打包完以后会在当前目录生成 mycc.tar.gz 文件 安装 chaincode 每个组织在任意peer节点安装一次既可。 在 peer0.org1 上安装 chaincode docker exec -it org1.cli bash # 安装 peer lifecycle chaincode install mycc.tar.gz # 查看安装情况 peer lifecycle chaincode queryinstalled 在 peer0.org2 上安装 chaincode docker exec -it org2.cli bash # 安装 peer lifecycle chaincode install mycc.tar.gz # 查看安装情况 peer lifecycle chaincode queryinstalled ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:12","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"chaincode 投票 每次安装新的 chaincode 组织都需要对其进行投票。 org1 投票 peer lifecycle chaincode queryinstalled 查看 Package ID: 部分为 CC_PACKAGE_ID docker exec -it org1.cli bash # 获取 CC_PACKAGE_ID bash-5.0# peer lifecycle chaincode queryinstalled Installed chaincodes on peer: Package ID: mycc:14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be, Label: mycc # 设置变量 export CC_PACKAGE_ID=mycc:14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be # 进行投票 peer lifecycle chaincode approveformyorg \\ --channelID mychannel \\ --name mycc \\ --version 1 \\ --init-required \\ --package-id $CC_PACKAGE_ID \\ --sequence 1 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA # 输出如下: 2020-03-24 06:24:19.615 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2020-03-24 06:24:19.619 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2020-03-24 06:24:19.634 UTC [cli.lifecycle.chaincode] setOrdererClient -\u003e INFO 003 Retrieved channel (mychannel) orderer endpoint: orderer0.jicki.cn:7050 2020-03-24 06:24:21.880 UTC [chaincodeCmd] ClientWait -\u003e INFO 004 txid [fb01bb2497d00ea1b4e63ef1f9afad125f892dd84717c0712e649716f3092493] committed with status (VALID) at # 查看投票情况 peer lifecycle chaincode checkcommitreadiness \\ --channelID mychannel \\ --name mycc \\ --version 1 \\ --init-required \\ --sequence 1 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA \\ --output json org2 投票 docker exec -it org2.cli bash # 获取 CC_PACKAGE_ID bash-5.0# peer lifecycle chaincode queryinstalled Installed chaincodes on peer: Package ID: mycc:14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be, Label: mycc # 设置变量 export CC_PACKAGE_ID=mycc:14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be # 进行投票 peer lifecycle chaincode approveformyorg \\ --channelID mychannel \\ --name mycc \\ --version 1 \\ --init-required \\ --package-id $CC_PACKAGE_ID \\ --sequence 1 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA # 输出如下: 2020-03-24 06:26:25.523 UTC [cli.lifecycle.chaincode] setOrdererClient -\u003e INFO 003 Retrieved channel (mychannel) orderer endpoint: orderer0.jicki.cn:7050 2020-03-24 06:26:27.675 UTC [chaincodeCmd] ClientWait -\u003e INFO 004 txid [067cce8f1c3ca9fa8f9239ad78542e30fee5b83d68ae054a8e9b76a01393bb08] committed with status (VALID) at # 查看投票情况 peer lifecycle chaincode checkcommitreadiness \\ --channelID mychannel \\ --name mycc \\ --version 1 \\ --init-required \\ --sequence 1 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA \\ --output json # 投票情况 { \"approvals\": { \"Org1MSP\": true, \"Org2MSP\": true } } ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:13","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"提交确认 ( submit ) docker exec -it org1.cli bash # 提交 commit peer lifecycle chaincode commit \\ -o orderer0.jicki.cn:7050 \\ --channelID mychannel \\ --name mycc \\ --version 1 \\ --sequence 1 \\ --init-required \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA \\ --peerAddresses peer0.org1.jicki.cn:7051 \\ --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt \\ --peerAddresses peer0.org2.jicki.cn:7051 \\ --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt 输出如下信息 2020-03-24 07:14:12.187 UTC [chaincodeCmd] ClientWait -\u003e INFO 003 txid [131d247fb9035ccb832b15ab34b64fd03fa74d1d5db762388a837b31c4c29e93] committed with status (VALID) at peer0.org1.jicki.cn:7051 2020-03-24 07:14:12.199 UTC [chaincodeCmd] ClientWait -\u003e INFO 004 txid [131d247fb9035ccb832b15ab34b64fd03fa74d1d5db762388a837b31c4c29e93] committed with status (VALID) at peer0.org2.jicki.cn:7051 分别检查是否成功提交 (commit) org1 # org1 docker exec -it org1.cli bash peer lifecycle chaincode querycommitted \\ --channelID mychannel \\ --name mycc # 输出如下: Committed chaincode definition for chaincode 'mycc' on channel 'mychannel': Version: 1, Sequence: 1, Endorsement Plugin: escc, Validation Plugin: vscc, Approvals: [Org1MSP: true, Org2MSP: true] org2 # org2 docker exec -it org2.cli bash peer lifecycle chaincode querycommitted \\ --channelID mychannel \\ --name mycc # 输出如下: Committed chaincode definition for chaincode 'mycc' on channel 'mychannel': Version: 1, Sequence: 1, Endorsement Plugin: escc, Validation Plugin: vscc, Approvals: [Org1MSP: true, Org2MSP: true] ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:14","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"操作 chaincode Invoke 操作 docker exec -it org1.cli bash # Invoke 操作 ( 初始化 a, b 的值为100 ) peer chaincode invoke \\ -o orderer0.jicki.cn:7050 \\ --tls $CORE_PEER_TLS_ENABLED \\ --cafile $ORDERER_CA \\ -C mychannel \\ -n mycc \\ --peerAddresses peer0.org1.jicki.cn:7051 \\ --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt \\ --peerAddresses peer0.org2.jicki.cn:7051 \\ --tlsRootCertFiles /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt \\ --isInit \\ -c '{\"Args\":[\"Init\",\"a\",\"100\",\"b\",\"100\"]}' # 输出: 2020-03-24 07:20:59.683 UTC [chaincodeCmd] chaincodeInvokeOrQuery -\u003e INFO 003 Chaincode invoke successful. result: status:200 Query 操作 peer chaincode query \\ -C mychannel \\ -n mycc \\ -c '{\"Args\":[\"query\",\"a\"]}' # 输出: 100 ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:15","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["fabric"],"content":"查看 docker container 如下为安装 chaincode 以后,生成的容器 [root@localhost jicki]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 439369210b83 jicki-peer0.org2.jicki.cn-mycc-14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be-4b4600a44f861b0308c82385aacc3c53c711443e78b5bbfda58137dc13822a5a \"chaincode -peer.add…\" 9 minutes ago Up 9 minutes jicki-peer0.org2.jicki.cn-mycc-14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be 471657d67a7a jicki-peer0.org1.jicki.cn-mycc-14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be-80a4542fb02360e50a162b4d72841d190b64ed34b9b4a06234a09ed2a8c4d392 \"chaincode -peer.add…\" 9 minutes ago Up 9 minutes jicki-peer0.org1.jicki.cn-mycc-14332eb253d2983dce11dc45734ea312538a70db1b61d79439fe8a42fb7572be ","date":"2020-03-23","objectID":"/hyperledger-fabric-2.0/:3:16","tags":["fabric","docker"],"title":"hyperledger-fabric v2.0","uri":"/hyperledger-fabric-2.0/"},{"categories":["kubernetes"],"content":"Flannel 网络模型","date":"2020-02-01","objectID":"/kubernetes-flannel/","tags":["kubernetes","network"],"title":"Flannel 网络模型","uri":"/kubernetes-flannel/"},{"categories":["kubernetes"],"content":"Flannel Flannel 是 CoreOS 公司针对 Kubernetes 设计的一个基于CNI标准的网络工具, 其目在于帮助 Kuberentes 实现一个简单的3层网络。 常用网络术语 2层网络 OSI 网络模型中的数据链路层. 处理网络上两个相邻节点之间的帧传递. 以太网就工作在第2层, MAC地址表示为子层. 3层网络 OSI 网络模型中的网络层. 处理主机之间路由数据包. IPv4、IPv6、ICMP 工作在第3层. VXLAN VXLAN代表虚拟可扩展的LAN. VXLAN是一种封装和覆盖协议, 可在现有网络上运行. VXLAN虚拟化与VLAN类似, 但提供更大的灵活性和功能, VLAN只有4096个网络ID. VXLAN用于通过在UDP数据报中封装第2层以太网帧来实现大型网络部署. Overlay Overlay网络是建立在现有网络之上的虚拟逻辑网络. Overlay网络通常用于在现有网络之上提供有用的抽象, 并分离和保护不同的逻辑网络. 数据封装 数据封装是指在附加层中封装网络数据包以提供其他上下文和信息的过程. 在Overlay网络中, 数据封装被用于从虚拟网络转换到底层地址空间, 从而能路由到不同的位置. 网状网络(Mesh NetWork) 网状网络是指每个节点连接到许多其他节点以协作路由、并实现更大连接的网络. 网状网络允许通过多个路径进行路由, 从而提供更可靠的网络. 网状网络的缺点是每个附加节点都会增加大量开销. BGP BGP代表\"边界网关协议”, 用于管理边缘路由器之间数据包的路由方式. BGP通过考虑可用路径, 路由规则和特定网络策略, 帮助弄清楚如何将数据包从一个网络发送到另一个网络. BGP有时被用作CNI插件中的路由机制, 而不是封装的覆盖网络. ","date":"2020-02-01","objectID":"/kubernetes-flannel/:0:0","tags":["kubernetes","network"],"title":"Flannel 网络模型","uri":"/kubernetes-flannel/"},{"categories":["kubernetes"],"content":"Flannel 工作原理 Flannel网络模型 udp: 使用用户态udp封装,默认使用8285端口, 由于是在用户态 封包与解包, 性能上有较大的损耗. vxlan: vxlan封装, 需要配置VNI, Port ( 默认端口8247 ) 和 BGP. host-gw: 直接路由模式, 将容器网络的路由信息直接更新到主机的路由表中. 仅适用于二层直接可达的网络中, 推荐在网络自由的环境中使用, 效率较高. aws-vpc: 使用 Amazon VPC route table 自动创建路由, 仅适用于 AWS 中的 EC2 使用. gce: 使用Google Compute Engine Network创建路由, 所有instance需要开启IP forwarding, 仅适用于 GCE 上运行. Ali-vpc: 使用阿里云 VPC route table 创建路由, 仅适用于阿里云ECS上运行. ","date":"2020-02-01","objectID":"/kubernetes-flannel/:1:0","tags":["kubernetes","network"],"title":"Flannel 网络模型","uri":"/kubernetes-flannel/"},{"categories":["kubernetes"],"content":"UDP 模式 UDP 模式相对来说比较简单, 采用UDP模式时, 需要在flannel 的配置文件中指定Backend.Type为UDP, 也可通过直接修改flannel的ConfigMap的方式实现。 UDP模式中, flannel 进程在启动时会通过打开/dev/net/tun 的方式生产一个tun设备, tun设备可简单理解为Linux提供的一种内核网络与用户空间（应用程序）通信的机制, 及应用可以通过直接读写tun设备的方式收发RAW IP包。 flannel 启动后会生成一个 flannel0 的网络接口(网口), 通过 ip addr 可查看. 通过 ip -d link show flannel0 可查看此网络接口为 tun 设备. flannel0 网络接口的 MTU 值为 1472 , 相对于物理网口 eth0 等少了28个字节. udp 模式默认监听端口为 8285。 主机通信 同主机通信 容器A发送到同一个subnet的容器B时, 容器都处于同一个子网中, 此时容器A与容器B中的网络都桥接在cni0网络接口中, 这样就可以实现同主机的直接通信. 跨主机通信 容器A发送 ICMP 到 cni0 接口. cni0 匹配相应的路由规则, 将 RAW 包发送到 flannel0接口. 因为flannel0为 tun设备, 所以flannel 会将 RAW 进行 UDP封包. UDP报文 包含 UDP 头(8字节), IP信息头(20字节). 所以 MUT 值会比 物理接口 eth0 的MUT 值少 28 字节. 为了封包后添加的这28字节. 将 UDP 封包后的 UDP报文 通过 物理接口 eth0 转发出去. 容器B 从 物理接口 eth0 接收到 容器A 转发过来的 UDP报文. 容器B 中的 flannel 将 UDP 报文进行解包, 获得 RAW 包. 通过解包后的 RAW 包, 获取路由匹配规则, 通过路由规则将 RAW转发给 cni0 网桥. cni0 网桥 将数据包 发送给桥接到 cni0 接口的容器B . ","date":"2020-02-01","objectID":"/kubernetes-flannel/:1:1","tags":["kubernetes","network"],"title":"Flannel 网络模型","uri":"/kubernetes-flannel/"},{"categories":["kubernetes"],"content":"Host-gw 模式 host-gw模式下, 各节点之间的跨主机网络通信要通过节点上的路由表实现, 因此必须要通信双方所在的宿主机能够直接路由。这就要求flannel host-gw模式下集群中的所有节点必须在一个二层的直接网络内, 这限制使得host-gw模式无法适用于集群规模较大且需要对节点进行网段划分的场景。 host-gw模式下, 另外一个限制则是随着集群中节点规模的增大, flannel维护主机上成千上万条路由表的动态更新也是一个不小的压力, 因此在路由方式下, 路由表规则的数量是限制网络规模的一个重要因素. host-gw模式下, flannel的唯一作用就是负责主机上路由表的动态更新。 ","date":"2020-02-01","objectID":"/kubernetes-flannel/:1:2","tags":["kubernetes","network"],"title":"Flannel 网络模型","uri":"/kubernetes-flannel/"},{"categories":["kubernetes"],"content":"VXLAN 模式 VXLAN 模式使用比较简单, flannel 会在各节点生成一个 flannel.1 的 VXLAN 网卡(VTEP设备). VXLAN 模式下封包与解包的工作是由内核进行的. flannel不转发数据, 仅动态设置ARP和FDB表项. VXLAN 模式下通信如下(跨主机的情况): 容器A 中的数据包先通过容器A 的路由表发送到 cni0. cni0 通过匹配主机A中的路由表信息,将数据包发送到 flannel.1 接口. flannel.1 是一个 VTEP 设备, 收到报文后按照 VTEP的配置进行封包. 根据flannel.1设备创建时设置的参数 VNI、local IP、Port 进行VXLAN封包。 主机A通过物理网卡 eth0 发送封包到 主机B的物理网卡 eth0中. 主机B的物理网卡eth0 再通过VXLAN默认端口8472 转发到 VTEP 设备flannel.1 进行解包. 解包以后根据IP头匹配路由规则, 内核将包发送到cni0. cni0 发送到桥接到此接口的容器B中. Flannel VXLAN工作模式 flannel 主动给子网添加远端主机路由的方式。同时, VTEP和网桥各自分配三层IP地址。当数据包到达目的主机后, 在内部进行三层寻址, 路由数目与主机数（而不是容器数）线性相关。官方声称同一个VXLAN子网下每个主机对应一个路由表项, 一个ARP表项和一个FDB表项。 Flannel 创建VXLAN设备, 不再监听L2Miss和L3Miss事件. L2Miss, 通过获取VTEP上的对外IP地址实现. L3Miss, 通过查找ARP表Mac地址完成. Flannel 为远端主机创建静态ARP表项. Flannel 创建FDB转发表项, 包含VTEP、 Mac地址和远端flannel的对外IP. ","date":"2020-02-01","objectID":"/kubernetes-flannel/:1:3","tags":["kubernetes","network"],"title":"Flannel 网络模型","uri":"/kubernetes-flannel/"},{"categories":["golang"],"content":"Golang 轻量级TCP框架 Zinx","date":"2020-01-07","objectID":"/golang-zinx-tcp-framework/","tags":["golang"],"title":"Golang 轻量级TCP框架 Zinx","uri":"/golang-zinx-tcp-framework/"},{"categories":["golang"],"content":"Zinx Zinx 是一个基于Golang的轻量级并发服务器框架. [Github] https://github.com/aceld/zinx ","date":"2020-01-07","objectID":"/golang-zinx-tcp-framework/:0:0","tags":["golang"],"title":"Golang 轻量级TCP框架 Zinx","uri":"/golang-zinx-tcp-framework/"},{"categories":["golang"],"content":"Zinx 架构图 ","date":"2020-01-07","objectID":"/golang-zinx-tcp-framework/:1:0","tags":["golang"],"title":"Golang 轻量级TCP框架 Zinx","uri":"/golang-zinx-tcp-framework/"},{"categories":["golang"],"content":"Zinx 使用 创建Server句柄 配置自定义Router路由以及定义业务功能 启动服务 ","date":"2020-01-07","objectID":"/golang-zinx-tcp-framework/:2:0","tags":["golang"],"title":"Golang 轻量级TCP框架 Zinx","uri":"/golang-zinx-tcp-framework/"},{"categories":["golang"],"content":"Server 端 例子: package main import ( \"fmt\" \"zinx/ziface\" \"zinx/znet\" ) // 自定义路由 // 1. 创建一个结构体 type PingRouter struct { // 要使用路由 首先继承 BaseRouter 这个结构体 znet.BaseRouter } /* 2. 重写 Router 的方法 (1. PreHandle 2. Handle 3. PostHandle) 初始PreHandle, Handle, PostHandle 方法没有实现任何业务。 可重写任何一个方法, 完成指定的业务。 */ // 3. 重写 Handle 方法 func (p *PingRouter) Handle(request ziface.IRequest) { // 读取客户端的数据 fmt.Println(\"recv from client: msgId=\", request.GetMsgID(), \", data=\", string(request.GetData())) // 回写数据到服务端 err := request.GetConnection().SendBuffMsg(0, []byte(\"ping...ping...ping\")) if err != nil { fmt.Println(err) } } // 启动服务 func main() { // 1. 创建一个Server 句柄 s := znet.NewServer() // 2. 配置自定义的 Router 路由 s.AddRouter(0, \u0026PingRouter{}) // 3. 开启服务 s.Serve() } ","date":"2020-01-07","objectID":"/golang-zinx-tcp-framework/:2:1","tags":["golang"],"title":"Golang 轻量级TCP框架 Zinx","uri":"/golang-zinx-tcp-framework/"},{"categories":["golang"],"content":"Client Zinx 的消息处理采用 [MsgLength] 、 [MsgID] 、 [Data] 的封包格式。 package main import ( \"fmt\" \"io\" \"net\" \"time\" \"zinx/znet\" ) /* 模拟客户端 */ func main() { fmt.Println(\"Client Start ...\") // 等待 3 秒之后发起测试请求, 给服务端开启服务的机会 time.Sleep(3 * time.Second) // 初始化连接 服务端的 conn 句柄 conn,err := net.Dial(\"tcp\", \"127.0.0.1:8888\") if err != nil { fmt.Println(\"Client Start err, exit!\") return } for i := 0; i \u003c 3; i++ { // 发封包message消息 dp := znet.NewDataPack() msg, _ := dp.Pack(znet.NewMsgPackage(0, []byte(\"Zinx Client Test Message\"))) _, err := conn.Write(msg) if err !=nil { fmt.Println(\"write error err \", err) return } // 读取流中的 head 部分 headData := make([]byte, dp.GetHeadLen()) // ReadFull 会把msg填充满为止 _, err = io.ReadFull(conn, headData) if err != nil { fmt.Println(\"read head error\") break } //将headData字节流 拆包到msg中 msgHead, err := dp.Unpack(headData) if err != nil { fmt.Println(\"server unpack err:\", err) return } if msgHead.GetDataLen() \u003e 0 { //msg 是有data数据的，需要再次读取data数据 msg := msgHead.(*znet.Message) msg.Data = make([]byte, msg.GetDataLen()) //根据dataLen从io中读取字节流 _, err := io.ReadFull(conn, msg.Data) if err != nil { fmt.Println(\"server unpack data err:\", err) return } fmt.Println(\"==\u003e Recv Msg: ID=\", msg.Id, \", len=\", msg.DataLen, \", data=\", string(msg.Data)) } time.Sleep(time.Second) } } ","date":"2020-01-07","objectID":"/golang-zinx-tcp-framework/:2:2","tags":["golang"],"title":"Golang 轻量级TCP框架 Zinx","uri":"/golang-zinx-tcp-framework/"},{"categories":["golang"],"content":"Zinx 配置文件 { \"Name\":\"zinx demoApp\", \"Host\":\"127.0.0.1\", \"TcpPort\":8888, \"MaxConn\":3, \"WorkerPoolSize\":10, \"LogDir\": \"./mylog\", \"LogFile\":\"zinx.log\" } Name: 服务器应用名称 Host: 服务器IP TcpPort: 服务器监听端口 MaxConn: 允许的客户端链接最大数量 WorkerPoolSize: 工作任务池最大工作Goroutine数量 LogDir: 日志文件夹 LogFile: 日志文件名称(如果不提供, 则日志信息打印到Stderr) ","date":"2020-01-07","objectID":"/golang-zinx-tcp-framework/:2:3","tags":["golang"],"title":"Golang 轻量级TCP框架 Zinx","uri":"/golang-zinx-tcp-framework/"},{"categories":["flannel"],"content":"Container Network Interface - CNI","date":"2020-01-02","objectID":"/contaienr-network-interface/","tags":["kubernetes","network"],"title":"Container Network Interface - CNI","uri":"/contaienr-network-interface/"},{"categories":["flannel"],"content":"CNI CNI 既 Container Network Interface 的简写. CNI 是 Kubernetes 网络的一个标准 CNI 提供 POD 的网络连接. CNI 提供分配IP地址的能力, 为POD 提供 ipv4 与 ipv6 的 DHCP 功能. CNI 还提供了 NetWrok Policy 的能力, 定义 POD 与 POD 之间的网络规则. CNI运行原理 根据CNI标准, 网络组件需要在 CNI 下创建二进制执行文件, 用于配置网络环境. 当 kubernetes 创建/删除 pod 时, 调用二进制执行文件进行相应的网络配置. CNI 下的二进制执行文件会包含如下参数 Container ID 容器ID号, 需要用于指定此容器配置网络, 也有一些CNI组件需要利用容器ID进行数据索引. Network namespace path 当前 container 真正的 network namespace 路径 . 每个 container 都会拥有至少一个 network namespace. namespace 是通过 Linux kernel 操作的, 因此大部分 CNI 组件都会根据这个路径, 通过 network namespace 进行相关的网络配置. Network configuration 用于定义CNI组件的相关配置. 包含两部分组成 CNI标准 和 网络组件额外定义. Name of the interface inside the container 在当前container 中创建网卡名称, 如: eth0 等. network namespace 如上面所说每个 container 都至少拥有一个 network namespace, 但两个 container 可以共用一个相同的 network namespace . # 创建一个容器 n1 [root@k8s-node-1 ~]# docker run -d --name n1 alpine sleep 3600s # 查看容器相关网络 [root@k8s-node-1 ~]# docker exec -it n1 ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) # 创建另一个容器 n2 并配置网络 --net=container:{容器id} # 容器id 是 n1 的容器id [root@k8s-node-1 ~]# docker run -d --net=container:b27fa0d5db21 --name n2 alpine sleep 3600s # 查看容器相关网络 [root@k8s-node-1 ~]# docker exec -it n2 ifconfig eth0 Link encap:Ethernet HWaddr 02:42:AC:11:00:02 inet addr:172.17.0.2 Bcast:172.17.255.255 Mask:255.255.0.0 UP BROADCAST RUNNING MULTICAST MTU:1500 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:0 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) lo Link encap:Local Loopback inet addr:127.0.0.1 Mask:255.0.0.0 UP LOOPBACK RUNNING MTU:65536 Metric:1 RX packets:0 errors:0 dropped:0 overruns:0 frame:0 TX packets:0 errors:0 dropped:0 overruns:0 carrier:0 collisions:0 txqueuelen:1 RX bytes:0 (0.0 B) TX bytes:0 (0.0 B) 可以看到如上两个容器的网络都是同一个, 这里就可以想到 kubernetes 的 pod 的网络. 实际 pod 中有更灵活的设置, 详见如下架构图 Network Configuration Network Configuration 就是一个 json 格式的配置文件. 如上我们讲过 这个配置文件包含两部分 CNI 标准的配置 - 根据官方 CNI Spec 包含如下配置项 cniVersion 定义CNI的版本, CNI 组件必须相同, 否则报错. name 唯一的名称标识. type CNI组件定义的名称, 上层应用会根据这个名称去找对应的CNI组件. 然后调用CNI组件的二进制文件进行网络配置. 这个配置非常重要. args 额外的配置, 主要用于上层应用调用CNI时的一些额外参数. ipMasq 标注当前CNI组件是否支持 SNAT. 只做标识, 实际SNAT是在CNI组件中实现. dns CNI组件的 DNS 配置, 如: nameserver、search、domain、options 等. 注: 在kubernetes中, 会忽略这里的 DNS 配置, 因为 kubernetes 会在自身设置 DNS 配置. ipam 既 IP Address Management Plugin, 负责IP地址的分配. 一般为host-local 和 dhcp . 一个配置文件 { \"cniVersion\": \"0.4.0\", \"name\": \"dbnet\", \"type\": \"bridge\", // type (plugin) specific \"bridge\": \"cni0\", \"ipam\": { \"type\": \"host-local\", // ipam specific \"subnet\": \"10.1.0.0/16\", \"gateway\": \"10.1.0.1\" }, \"dns\": { \"nameservers\": [ \"10.1.0.1\" ] } } ","date":"2020-01-02","objectID":"/contaienr-network-interface/:0:0","tags":["kubernetes","network"],"title":"Container Network Interface - CNI","uri":"/contaienr-network-interface/"},{"categories":["flannel"],"content":"Kubernetes CNI Kubernetes 目前网络有两种配置, 通过 kubelet 中的 --network-plugin 来指定选择哪一种配置. cni 目前kubernetes 部署方式基本都使用 cni 配置网络. 如上所述, cni包含两个部分 cni组件的二进制文件 --cni-bin-dir 默认存放于 /opt/cni/bin 目录下. cni组件的 网络配置(Network Configuration) 或 网络配置列表(Network Configuration List) --cni-conf-dir 默认存放于 etc/cni/net.d/ 目录中. [root@k8s-node-1 ~]# ls -lt /opt/cni/bin/ total 36132 -rwxr-xr-x 1 root root 2973336 Mar 26 2019 bridge -rwxr-xr-x 1 root root 7598064 Mar 26 2019 dhcp -rwxr-xr-x 1 root root 2110208 Mar 26 2019 flannel -rwxr-xr-x 1 root root 2288536 Mar 26 2019 host-device -rwxr-xr-x 1 root root 2238208 Mar 26 2019 host-local -rwxr-xr-x 1 root root 2621472 Mar 26 2019 ipvlan -rwxr-xr-x 1 root root 2257808 Mar 26 2019 loopback -rwxr-xr-x 1 root root 2650160 Mar 26 2019 macvlan -rwxr-xr-x 1 root root 2613864 Mar 26 2019 portmap -rwxr-xr-x 1 root root 2946664 Mar 26 2019 ptp -rwxr-xr-x 1 root root 1951880 Mar 26 2019 sample -rwxr-xr-x 1 root root 2103456 Mar 26 2019 tuning -rwxr-xr-x 1 root root 2617328 Mar 26 2019 vlan [root@k8s-node-1 ~]# ls -lt /etc/cni/net.d/ total 4 -rw-r--r-- 1 root root 292 Apr 26 14:05 10-flannel.conflist [root@k8s-node-1 ~]# cat /etc/cni/net.d/10-flannel.conflist { \"name\": \"cbr0\", \"cniVersion\": \"0.3.1\", \"plugins\": [ { \"type\": \"flannel\", \"delegate\": { \"hairpinMode\": true, \"isDefaultGateway\": true } }, { \"type\": \"portmap\", \"capabilities\": { \"portMappings\": true } } ] } CNI工作流程 Client 发送创建 Pod的请求. Pod的创建中间流程忽略, 最后到 kubelet . kubelet 通过dockershim + docker-containerd 的方式创建 container . Pod 创建完毕以后, kubelet 通过 CNI 相关流程创建 Pod 网络. CNI组件搜索 --cni-conf-dir 目录下的配置文件, 按照数字顺序查找合法的网络配置. Flannel 的配置文件为 10-flannel.conflist 读取配置文件, 通过 type 或者 cni组件. 通过 type 调用--cni-bin-dir 目录下的 cni组件的二进制文件. 通过配置文件参数配置容器网络. 配置完网络以后, 调用 --cni-bin-dir 目录下的 portmap 二进制文件, 将容器的IP和端口通过iptables映射到宿主机的端口上. Kubenet工作流程 kubenet 是个非常简单的 L2 Bridge, 其实也是通过 CNI 建立一个简单的网络, 由 kubenet 配置的网络只能用于单节点的网络通讯. 所以 kubenet 是用于测试. ","date":"2020-01-02","objectID":"/contaienr-network-interface/:1:0","tags":["kubernetes","network"],"title":"Container Network Interface - CNI","uri":"/contaienr-network-interface/"},{"categories":["etcd"],"content":"ETCD 原理剖析","date":"2020-01-01","objectID":"/etcd-working-principle/","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"ETCD etcd 官方地址 https://etcd.io/ etcd 是 CoreOS 公司, 最初用于解决集群管理系统中 OS 升级时的分布式并发控制、配置文件的存储于分发等问题。 etcd 被设计为提供高可用、强一致性的小型 key/value 数据存储服务。 etcd 目前已经隶属于CNCF (Cloud Native Computing Foundation) 基金会, 被包含 AWS 、Google、Microsoft、Alibaba 等大型互联网公司广泛使用。 etcd 最早在 2013年6月份 于 github 中开源。 etcd 于 2014年6月份 正式被 kubernetes 使用, 用于存储 kubernetes 集群中的元数据。 etcd 于 2015年2月份 发布 2.0 版本, 正式支持分布式协议 Raft, 并支持 1000/s 的并发 writes 。 etcd 于 2017年1月份 发布 3.1 版本, 全面优化了 etcd , 新的API、重写了强一致性(read)读的方法,并提供了 gRPC Proxy 接口, 以及大量 gc 的优化。 etcd 于 2018年11月 加入 CNCF 基金会。 etcd 于 2019年 发布 etcd v3.4 该版本又 Google、AWS、Alibaba 等大公司联合打造的一个版本。将进一步优化稳定性以及性能。 ","date":"2020-01-01","objectID":"/etcd-working-principle/:0:0","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"Etcd 架构与内部机制 ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:0","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"Etcd 术语简介 名称 含义 Raft etcd 使用的一致性算法 WAL 预写Log, 持久化数据, 防止节点重启等导致数据丢失 Snapshot 快照, 数据更新超过阈值时, 会通过快照的方式来压缩 WAL 文件大小 MVCC 多版本并发控制 DB boltdb/bboltdb, 实际存储 etcd v3 的数据 Revision 版本号, 作为 etcd 数据的逻辑时钟 Auth revision 鉴权操作所用的版本号, 为了避免 TOCTOU 问题引入 Propose 发起一次 Raft 请求提案 Committed 一半以上的节点同意这次请求后的状态, 此时数据可以被应用层 apply Apply 应用层实际将 Committed 的数据应用到 DB Compact 压缩历史版本数据 Defrag 碎片整理, 压缩 etcd 的 db 存储大小 Endpoint 客户端指定的 etcd 访问地址 Node 组成 etcd 集群的节点 Term Leader 任期, 每进行一次 leader 选举 Term 会增加 1 Index 单调递增, 每次经过 Raft 模块发起变更操作时由 leader 增加 CommittedIndex 经过 Raft 协议同意提交的数据 Index AppliedIndex 已经被应用层应用的 Index ConsistentIndex 为保证不重复 Apply 同一条数据引入, 保证 Apply 操作的幂等性 ReadIndex 通过 Raft 模块获取 leader 当前的 committedIndex ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:1","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 基础概念 etcd 是一个 分布式、强一致性可靠、key/value 存储系统。 它主要用于存储分布式系统中的 关键数据。 etcd key/value存储是按照 有序 key 排列的, 可以顺序遍历。 因为 key 有序, 所以 etcd 支持按目录结构高效遍历。 支持复杂事务, 提供类似 if ... then ... else ... 的事务能力。 基于租约机制实现 key 的 TTL 过期。 etcd 包含二种状态：Leader Follower etcd 集群通常由 奇数(最低3)个 etcd 组成。 集群中多个 etcd 通过 Raft consensus algorithm 算法进行协同, 多个 etcd 会通过 Raft 算法选举出一个 Leader, 由 Leader 节点进行数据同步, 以及数据分发。 etcd 通过 boltdb 持久化存储数据。 当 Leader 出现故障时, 集群中的 etcd 会投票选举出另一个 Leader , 并重新进行数据同步以及数据分发。 客户端从任何一个 etcd 都可以进行 读/写 操作。 在 etcd 集群中 有一个关键概念 quorum、 quorum = ( n + 1 ) / 2, 也就是说超过集群中半数节点组成的一个团体。 集群中可以容忍故障的数量, 如 (3 + 1) / 2 = 1 可以容忍的故障数为1台。 在 etcd 集群中 任意两个 quorum 的成员之间一定会有交集, 只要有任意一个quorum存活, 其中一定存在某一个节点它包含 etcd 集群中最新的数据, 基于这种假设, Raft 一致性算法就可以在一个 quorum 之间采用这份最新的数据去完成数据的同步。 quorum: 在Raft 中超过一半以上的人数就是法定人数。 etcd 提供了如下 API Put(key, value) 增加 Delete(key) 删除 Get(key) / Get(keyFrom, keyEnd) 查询 Watch(key) / Watch(key前缀) 事件监听, 监听key的变化 Transactions(if / then / else ops).Commit() 事务操作, 指定某些条件, 为true时执行其他操作。 Leasesi Grant / Revoke / KeepAlive ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:2","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 写操作流程 1. etcd 任一节点的 etcd Server 模块收到 Client 写请求 1.1. 如果是 follower 节点, 会先通过 Raft 模块将请求转发至 leader 节点处理。 2. etcd Server 将请求封装为 Raft 请求, 然后提交给 Raft 模块处理。 3. leader 通过 Raft 协议与集群中 follower 节点进行交互, 将消息复制到 follower 节点, 于此同时, 并行将日志持久化到 WAL。 4. follower 节点对该请求进行响应, 回复自己是否同意该请求。 5. 当集群中超过半数节点（(n/2)+1 members ）同意接收这条日志数据时, 表示该请求可以被Commit, Raft 模块通知 etcd Server 该日志数据已经 Commit, 可以进行 Apply。 6. 各个节点的 etcd Server 的 applierV3 模块异步进行 Apply 操作, 并通过 MVCC 模块写入后端存储 BoltDB。 7. 当 client 所连接的节点数据 apply 成功后, 会返回给客户端 apply 的结果。 ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:3","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 读操作流程 1. etcd 任一节点的 etcd Server 模块收到客户端读请求(Range 请求)。 2. 判断读请求类型, 如果是串行化读(serializable)则直接进入 Apply 流程。 3. 如果是线性一致性读(linearizable), 则进入 Raft 模块。 4. Raft 模块向 leader 发出 ReadIndex 请求, 获取当前集群已经提交的最新数据 Index 。 5. 等待本地 AppliedIndex 大于或等于 ReadIndex 获取的 CommittedIndex 时, 进入 Apply 流程。 6. Apply 流程: 通过 Key 名从 KV Index 模块获取 Key 最新的 Revision, 再通过 Revision 从 BoltDB 获取对应的 Key 和 Value。 ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:4","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 数据版本号机制 etcd 的数据版本号机制非常重要 term: 全局单调递增 (64bits), term 表示整个集群中 leader 的任期, 当集群中发生 leader 切换, 如: leader节点故障、leader网络故障、整个集群重启都会发生 leader 切换, 这个时候 term = term + 1。 revision: 全局单调递增 (64bits), revision 表示在整个集群中数据变更版本号, 当集群中数据发生变更, 包括 创建、修改、删除 的时候, revision = revision + 1。 Key/Vaule: Create_revision: 表示在当前 Key/Value 中在整个集群数据中创建时(revision)的版本号。每个 Key/Value 都有一个 Create_revision。 mod_revision: 表示当前 Key/Value 等于当前修改时的全局的版本数 (revision) 既 mod_version = 当前 revision。 version: 表示当前 Key/Value 被修改了多少次。 实际例子操作 查看 key 的相关版本信息 [root@k8s-node-1 opt]# etcdctl -w json get key0 | jq { # header 下显示的是 etcd 全局中的信息 \"header\": { \"cluster_id\": 12826157174689708000, \"member_id\": 15154619590888327000, # revision 全局数据变更版本号 \"revision\": 4462858, # term 是全局 leader 任期 \"raft_term\": 4 }, # kvs 表示当前 key/value 的信息 \"kvs\": [ { # key 在这里显示是 base64 编码后的二进制数 \"key\": \"a2V5MA==\", # create_revision 与 mod_revision 相同 # 因为没有对 key0 进行任何的修改 \"create_revision\": 4462566, # 如果对 key0 进行修改 # mod_revision 等于 当前 revision 版本数 \"mod_revision\": 4462566, # version 为 1 如果对 key0 进行修改 # version 会递增修改的次数 \"version\": 1, # value 在这里显示是 base64 编码后的二进制数 \"value\": \"dmFsdWUw\" } ], # 返回的数据条数 \"count\": 1 } ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:5","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd leader 选举机制 集群选举 Leader 需要半数以上节点参与 节点 revision 版本最大的允许选举为 Leader 节点中 revision 相同, 则 term 越大的允许选举为 Leader ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:6","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd mvcc \u0026\u0026 watch mvcc: 全称 Multi-Version Concurrency Control 即多版本并发控制。 mvcc: 是一种并发控制的方法, 一般在数据库管理系统中, 实现对数据库的并发访问。 在 etcd 中, 支持对同一个 Key 发起多次数据修改。因为已经知道每次数据修改都对应一个版本号(mod_revision), 多次修改就意味着一个 key 中存在多个版本, 在查询数据的时候可以通过不指定版本号查询Get key, 这时 etcd 会返回该数据的最新版本。当我们指定一个版本号查询数据后Get --rev=1 Key, 可以获取到一个 Key 的历史版本。 在 watch 的时候指定数据的版本, 创建一个 watcher, 并通过这个 watcher 提供的一个数据管道, 能够获取到指定的 revision 之后所有的数据变更。如果指定的 revision 是一个旧版本, 可以立即拿到从旧版本到当前版本所有的数据更新。并且, watch 的机制会保证 etcd 中, 该 Key 的数据发生后续的修改后, 依然可以从这个数据管道中拿到数据增量的更新。 在 etcd 中 所有的数据都存储在一个 b + tree 中。 b + tree 是保存在磁盘中, 并通过 mmap 的方式映射到内存用来查询操作。 mmap 是一种内存映射文件的方法, 即将一个文件或者其对象映射到进程的内存地址空间, 实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。 b + tree 维护着 revision 到 value 的映射关系。也就是说当指定 revision 查询数据的时候, 就可以通过该 b + tree 直接返回数据。当我们通过 watch 来订阅数据的时候, 也可以通过这个 b + tree 维护的 revision 到 value 映射关系, 从而通过指定的 revision 开始遍历这个b + tree ,拿到所有的数据更新。 在 etcd 内部还维护着另外一个 b + tree 。它管理着 key 到 revision 的映射关系。当需要查询 Key 对应数据的时候, 会通过 etcd 内部的 b + tree, 将 key 翻译成 revision。再通过磁盘中的 b + tree 的 revision 获取到对应的 value。 在 etcd 中 一个数据是存在多个版本的。 在 etcd 持续运行过程中会不断的发生修改, 意味着 etcd 中内存及磁盘的数据都会持续增长。这对资源有限的场景来说是无法接受的。因此在 etcd 中会周期性的运行一个 Compaction 的机制来清理历史数据。 对于一个 Key 的历史版本数据, 可以选择清理掉。 ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:7","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd mini-transactions etcd 的 transaction 机制比较简单, 基本可以理解为一段 if else 程序, 在 if 中可以提供多个操作。 # 进入 事务操作 [root@k8s-node-1 opt]# etcdctl txn -i compares: # 执行条件 value(\"key0\") = \"value0\" # 成功时执行的命令 success requests (get, put, del): get key1 # 失败时执行的命令 failure requests (get, put, del): put key0 value0 get key2 # 结果判定 SUCCESS # 返回执行后的命令 key1 value1 在 etcd 内部会保证整个事务操作的原子性。也就是说 If 操作所有的比较条件, 其看到的视图, 一定是一致的。同时它能够确保在争执条件中, 多个操作的原子性不会出现 etc 仅执行了一半的情况。 通过 etcd 提供的事务操作, 我们可以在多个竞争中去保证数据读写的一致性, 比如 Kubernetes , 它正是利用了 etcd 的事务机制, 来实现多个 Kubernetes API server 对同样一个数据修改的一致性。 Kubernetes 在使用 etcd 做为元数据存储后 元数据实现高可用, 无单点故障 系统无状态, 故障修复相对容易 系统可水平扩展, 横向提升性能以及容量 简化整体架构, 降低维护的复杂度 ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:8","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd lease lease 是分布式系统中一个常见的概念, 用于代表一个租约。通常情况下, 在分布式系统中需要去检测一个节点是否存活的时候, 就需要租约机制。 etcd 通过 CreateLease(时间) 来创建一个租约。如: lease = CreateLease(10s) 创建一个 10s 过期的一个租约。 通过 Put(key1, value1, lease) 可以将之前创建的 租约绑定到 key1 中(同一个租约可以绑定多个key)。当租约过期时, etcd会自动清理 key1 对应的 value1 值。 KeepAlive 方法: 可以续约租期。 比如说需要检测分布式系统中一个进程是否存活, 那么就会在这个分布式进程中去访问 etcd 并且创建一个租约, 同时在该进程中去调用 KeepAlive 的方法, 与 etcd 保持一个租约不断的续约。当进程挂掉了, 租约在进程挂掉的一段时间就会被 etcd 自动清理掉。所以可以通过这个机制来判定节点是否存活。 ","date":"2020-01-01","objectID":"/etcd-working-principle/:1:9","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 性能优化 ","date":"2020-01-01","objectID":"/etcd-working-principle/:2:0","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 性能分析 ","date":"2020-01-01","objectID":"/etcd-working-principle/:2:1","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"Etcd Server 硬件需求 etcd 硬件需求 (如下为官方提供的参考数据) 小型集群 少于100个客户端, 每秒少于200个请求, 存储数据少于100MB。如: 少于50节点的Kubernetes集群。 CPU 内存 最大并发 磁盘吞吐量 2核 4G 1500IOPS 50MB/S 中型集群 少于500个客户端, 每秒少于1000个请求, 存储数据少于500MB。如: 少于250节点的Kubernetes集群。 CPU 内存 最大并发 磁盘吞吐量 4核 16G 5000IOPS 100MB/S 大型集群 少于1500个客户端, 每秒少于10000个请求, 存储数据少于1GB。 如: 少于1000节点的Kubernetes集群。 CPU 内存 最大并发 磁盘吞吐量 8核 32G 8000IOPS 200MB/S 超大型集群 大于1500个客户端, 每秒处理大于10000个请求, 存储数据大于1GB。如: 少于3000个节点的Kubernetes集群 CPU 内存 最大并发 磁盘吞吐量 16核 64G 15000IOPS 300MB/S ","date":"2020-01-01","objectID":"/etcd-working-principle/:2:2","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 运维 ","date":"2020-01-01","objectID":"/etcd-working-principle/:3:0","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 集群数据备份 准备一些测试数据 #!/bin/bash export ETCDCTL_API=3 export IPS=\"https://10.0.0.1:2379,https://10.0.0.2:2379,https://10.0.0.3:2379\" for((i=1;i\u003c=10;i++));do etcdctl --endpoints=${IPS} \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ put key$i value$i done for((i=1;i\u003c=10;i++));do etcdctl --endpoints=${IPS} \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ get key$i done 备份 Etcd 快照 snapshot save 命令备份。 在集群状态正常的情况下对任意一个节点进行数据备份都可以。 etcd v3.4 只能一个节点进行数据备份。Snapshot can only be requested from one etcd node 在使用 ETCD API 3 的情况下,只会备份 3 的数据。 #!/bin/bash ETCD_BACK=/opt/etcd_bak mkdir -p ${ETCD_BACK} export ETCDCTL_API=3 etcdctl --endpoints=https://10.0.0.1 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot save ${ETCD_BACK}/snap-$(date +%Y%m%d%H%M).db 删除创建的数据 #!/bin/bash export ETCDCTL_API=3 export IPS=\"https://10.0.0.1:2379,https://10.0.0.2:2379,https://10.0.0.3:2379\" for((i=1;i\u003c=10;i++));do etcdctl --endpoints=${IPS} \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ del key$i done for((i=1;i\u003c=10;i++));do etcdctl --endpoints=${IPS} \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ get key$i done ","date":"2020-01-01","objectID":"/etcd-working-principle/:3:1","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["etcd"],"content":"etcd 集群数据恢复 恢复集群数据 etcdctl snapshot restore 命令恢复 恢复数据, 需要将快照覆盖到所有ETCD集群节点。 节点-1 --data-dir 指定恢复数据的目录, 如果数据目录存在会报错, 可以选择删除原来的, 或者备份为新的目录。 --name 每个节点的 name 都不相同 initial-cluster 配置为集群所有节点的 2380 内部通讯端口 initial-cluster-token 必须配置与原来相同 initial-advertise-peer-urls 配置为恢复节点的 IP:2380 #!/bin/bash export ETCDCTL_API=3 export ETCD_IPS=\"etcd1=https://10.0.0.1:2380,etcd2=https://10.0.0.2:2380,etcd3=https://10.0.0.3:2380\" etcdctl \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot restore /opt/etcd_bak/snapshot.db \\ --data-dir=/var/lib/etcd-new \\ --name etcd1 \\ --initial-cluster ${ETCD_IPS} \\ --initial-cluster-token etcd-cluster \\ --initial-advertise-peer-urls https://10.0.0.1:2380 节点-2 --data-dir 指定恢复数据的目录, 如果数据目录存在会报错, 可以选择删除原来的, 或者备份为新的目录。 --name 每个节点的 name 都不相同 initial-cluster 配置为集群所有节点的 2380 内部通讯端口 initial-cluster-token 必须配置与原来相同 initial-advertise-peer-urls 配置为恢复节点的 IP:2380 #!/bin/bash export ETCDCTL_API=3 export ETCD_IPS=\"etcd1=https://10.0.0.1:2380,etcd2=https://10.0.0.2:2380,etcd3=https://10.0.0.3:2380\" etcdctl \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot restore /opt/etcd_bak/snapshot.db \\ --data-dir=/var/lib/etcd-new \\ --name etcd2 \\ --initial-cluster ${ETCD_IPS} \\ --initial-cluster-token etcd-cluster \\ --initial-advertise-peer-urls https://10.0.0.2:2380 节点-3 --data-dir 指定恢复数据的目录, 如果数据目录存在会报错, 可以选择删除原来的, 或者备份为新的目录。 --name 每个节点的 name 都不相同 initial-cluster 配置为集群所有节点的 2380 内部通讯端口 initial-cluster-token 必须配置与原来相同 initial-advertise-peer-urls 配置为恢复节点的 IP:2380 #!/bin/bash export ETCDCTL_API=3 export ETCD_IPS=\"etcd1=https://10.0.0.1:2380,etcd2=https://10.0.0.2:2380,etcd3=https://10.0.0.3:2380\" etcdctl \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ snapshot restore /opt/etcd_bak/snapshot.db \\ --data-dir=/var/lib/etcd-new \\ --name etcd3 \\ --initial-cluster ${ETCD_IPS} \\ --initial-cluster-token etcd-cluster \\ --initial-advertise-peer-urls https://10.0.0.3:2380 重启所有 etcd 服务 如上恢复数据到新的数据目录 --data-dir # 停止所有节点的 etcd 服务 # 系统部署的 etcd systemctl stop etcd # 容器化的 docker stop etcd 备份原来的数据目录 mv /var/lib/etcd /var/lib/etcd-old 使用新的数据 mv /var/lib/etcd-new /var/lib/etcd # 重新启动所有节点的 etcd 服务 systemctl start etcd # 容器化的 docker start etcd 查看节点的情况 # status etcdctl \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint status -w table 查询恢复数据 #!/bin/bash export ETCDCTL_API=3 export IPS=\"https://10.0.0.1:2379,https://10.0.0.2:2379,https://10.0.0.3:2379\" for((i=1;i\u003c=10;i++));do etcdctl --endpoints=${IPS} \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ get key$i done ","date":"2020-01-01","objectID":"/etcd-working-principle/:3:2","tags":["etcd"],"title":"ETCD 原理剖析","uri":"/etcd-working-principle/"},{"categories":["docker","jenkins"],"content":"Jenkins Pipeline 语法","date":"2020-01-01","objectID":"/jenkins-pipeline/","tags":["jenkins","docker"],"title":"Jenkins Pipeline 语法","uri":"/jenkins-pipeline/"},{"categories":["docker","jenkins"],"content":"Jenkins Pipeline 的核心概念 Stage 阶段 一个Pipeline可以划分成若干个Stage，每个Stage代表一组操作，例如：“Build”，“Test”，“Deploy”。 Node 节点 一个Node就是一个Jenkins节点，或者是Master，或者是Agent，是执行Step的具体运行环境。 Step 步骤 Step是最基本的操作单元，小到创建一个目录，大到构建一个Docker镜像，由各类Jenklins Plugin提供，例如：sh ‘make’ 。 ","date":"2020-01-01","objectID":"/jenkins-pipeline/:0:0","tags":["jenkins","docker"],"title":"Jenkins Pipeline 语法","uri":"/jenkins-pipeline/"},{"categories":["docker","jenkins"],"content":"Pipeline 变量传递 变量的传递非常重要, 不同的变量定义 必须用在不同阶段中, 否则报错. 自定义变量(局部变量) # 注意 变量的引用 一定要用双引号, 单引号识别为字符串。 def username = 'Jenkins' echo \"Hello Mr.${username}\" def username = 'Jenkins' pipeline { agent none stages { stage('Stage1') { agent { label \"jnlp-dev-android\" } steps { timestamps { echo '这是 jnlp-dev-android 第一个被执行的 stage.' echo \"Hello Mr.${username}\" } } } stage('Stage2') { agent { label \"jnlp-test-java\" } steps { timestamps { echo \"在 agent jnlp-test-java 上执行的并行任务 1.\" echo \"Hello Mr.${username}\" echo \"在 agent jnlp-test-java 上执行的并行任务 1 结束.\" } } } } } 环境变量(局部) withEnv(['MY_HOME=/opt']){ sh 'echo $MY_HOME' } pipeline { agent none stages { stage('Stage1') { agent { label \"jnlp-dev-android\" } steps { timestamps { echo '这是 jnlp-dev-android 第一个被执行的 stage.' withEnv(['MY_HOME=/opt']){ sh 'echo $MY_HOME' } } } } stage('Stage2') { agent { label \"jnlp-test-java\" } steps { timestamps { echo \"在 agent jnlp-test-java 上执行的并行任务 1.\" echo \"在 agent jnlp-test-java 上执行的并行任务 1 结束.\" } } } } } 环境变量(全局) environment {MY_HOME='/opt'} echo \"MY_HOME is ${env.MY_HOME}\" pipeline { environment {MY_HOME='/opt'} agent none stages { stage('Stage1') { agent { label \"jnlp-dev-android\" } steps { timestamps { echo \"这是 jnlp-dev-android 第一个被执行 开始.\" echo \"MY_HOME is ${env.MY_HOME}\" echo \"这是 jnlp-dev-android 第一个被执行 结束.\" } } } stage('Stage2') { agent { label \"jnlp-test-java\" } steps { timestamps { echo \"在 agent jnlp-test-java 第二个被执行 开始.\" echo \"MY_HOME is ${env.MY_HOME}\" echo \"在 agent jnlp-test-java 第二个被执行 结束.\" } } } } } 参数化构建(全局) 参数化构建需用安装额外的插件 parameters {string(name:'Jenkins',defaultValue:'Hello',description:'How should I greet the world')} ehco \"${params.Greeting} World!\" ","date":"2020-01-01","objectID":"/jenkins-pipeline/:1:0","tags":["jenkins","docker"],"title":"Jenkins Pipeline 语法","uri":"/jenkins-pipeline/"},{"categories":["docker","jenkins"],"content":"Pipeline 一些例子 ","date":"2020-01-01","objectID":"/jenkins-pipeline/:2:0","tags":["jenkins","docker"],"title":"Jenkins Pipeline 语法","uri":"/jenkins-pipeline/"},{"categories":["docker","jenkins"],"content":"stage 间变量传递 pipeline { agent none stages { stage('Stage1') { agent { label \"jnlp-dev-android\" } steps { script { echo \"这是 jnlp-dev-android 第一个被执行 开始.\" FOO = \"test2\" env.BAR = \"bar2\" echo \"env.BAR = ${BAR}\" // print BAR = bar2 echo \"FOO = ${FOO}\" // print FOO = test2 echo \"这是 jnlp-dev-android 第一个被执行 结束.\" } } } stage('Stage2') { agent { label \"jnlp-test-java\" } steps { script { echo \"在 agent jnlp-test-java 第二个被执行 开始.\" echo \"env.BAR = ${BAR}\" // print BAR = bar2 echo \"FOO = ${FOO}\" // print FOO = test2 echo \"在 agent jnlp-test-java 第二个被执行 结束.\" } } } } } ","date":"2020-01-01","objectID":"/jenkins-pipeline/:2:1","tags":["jenkins","docker"],"title":"Jenkins Pipeline 语法","uri":"/jenkins-pipeline/"},{"categories":["docker","jenkins"],"content":"stage 中修改 全局环境变量 修改后只作用于 stages 这一阶段 pipeline { agent none environment { FOO = \"bar\" } stages { stage('Stage1') { agent { label \"jnlp-dev-android\" } steps { script { echo \"这是 jnlp-dev-android 第一个被执行 开始.\" withEnv([\"FOO=newbar\"]) { echo \"FOO = ${env.FOO}\" // print FOO = newbar } echo \"这是 jnlp-dev-android 第一个被执行 结束.\" } } } stage('Stage2') { agent { label \"jnlp-test-java\" } steps { script { echo \"在 agent jnlp-test-java 第二个被执行 开始.\" echo \"FOO = ${env.FOO}\" // print FOO = bar echo \"在 agent jnlp-test-java 第二个被执行 结束.\" } } } } } ","date":"2020-01-01","objectID":"/jenkins-pipeline/:2:2","tags":["jenkins","docker"],"title":"Jenkins Pipeline 语法","uri":"/jenkins-pipeline/"},{"categories":["jicki"],"content":"怎么样才算是一个充实的人生？","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"What is a full life ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:0:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"1. You don’t have to have a dream 你不一定要拥有梦想 人们总是在谈梦想, 如果你一直都想去做某件事, 打从心底想要去完成, Go for it。就看你想把时间花在什么地方，去追逐梦想或者其他。 如果是一个远大的梦想, 可能会花上你一辈子的时间才可能去完成它, 当你最终达成时, 回过头来看自己那毫无意义的所谓的“成就”, 生命即将中止,这些“成就”对你来说都不重要了。 我们应该热情、全心全意的去追逐短程的目标, 在小目标上保持雄心壮志, 埋头苦干并以此为荣, 专注眼前的事物, 你永远不知道你的人生终点是在哪里, 留意你下一个值得追逐的目标, 也许就在你身边。这就是为什么应该小心看待远程的梦想, 当你把眼光放的太远, 你可能会错过眼前耀眼的机会。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:1:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"2. Don’t seek happiness 别刻意找寻快乐 用心做人,给别人带来快乐,你可能会因而获得更多的快乐, 安逸的现状并没有让我们变得永远安逸, 安逸的原始人在改变安逸之前可能早就被吃掉了。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:2:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"3. It’s all luck 一切一切的事情都跟运气有关 今天你很幸运能有目前的生活, 你超级幸运才能来到这个世界上。 当你了解到你的成就并不完全是你的功劳, 而失败也不能把过失完全归咎于他人, 这样就会让你更加谦虚, 让你更加有同情心, 同情心是种直觉, 但你也可以用你的智慧去栽培它。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:3:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"4. Exercise 运动、运动、运动 你们这些颓废抽烟的人, 当你皱眉头, 就像笛卡尔曲线一样, 当你看着其他人为了生存, 在拥挤的人生道路上力争上游, 你就意识到错了, 他们才是对的。当然你思故你在, 休养生息, 你才不会被生存主义式的焦虑所淹没。 去运动, 跑步、瑜伽、无论如何动起来, 总之照顾好自己的身体, ‘迟些’ 你会需要到他, 这样大多数人会活的更久。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:4:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"5. Be hard on your opinions 严格审视自己的意见 我们需要明辨思考, 不只是对于他人的意见, 更要严格审视自己所相信的事物, 严格审视自己的知识,认清你的成见、歧视还有特权, 社会上多数的争论起源都是在细微差别下无法被获得认同,我们经常会产生假象二元对立, 我们应该用两种不同的假设去争论同一个问题。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:5:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"6. Be a teacher Please please please 当一个老师 老师是个令人崇敬和重要的职业, 你不需要一辈子当老师, 加入你不知道自己想做什么的时候那就当个好老师吧。 就算你不是老师, 也请你做一个老师, 分享你的想法, 别把自己所学的当成理所当然, 把你所学的传播出去, 回馈给社会。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:6:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"7. Define yourself by what you love 以喜好来定义自己 我们倾向于用讨厌和反对的事物来定义自己, 但你可以去试着对你所爱的事情表达你的热情, 对于你欣赏的事大方的给予赞美并给予鼓掌, 对事情给予正面的赞许而不是只懂得反对。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:7:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"8. Respect people with less power than you 尊重位阶比你低的人 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:8:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"9. Don’t rush 别急、别急、别急 你不必现在就知道你整个人生的全盘计划, 别慌, 人生迟早有走完得一天。 人生有时候看起来漫长而又痛苦, 有时候总是会想 为什么生活那么累? 活的那么辛苦？ 有开心的时候必然就会有难过的时候, 然后你终将老去,然后死去, 对于这些虚无的存在感你能做的一个明智的决定就是去充实它 Fill it ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:9:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["jicki"],"content":"10. Life is best filled 充实的人生 尽量去学习, 能学多少就学多少, 无论做什么事情都要引以为荣, 保持同情心, 无私分享, 运动, 保持热情, 当然还有 爱情、旅行、酒、艺术、孩子等等… 这些都令人无比的振奋, 这些你看起来毫无意义的人生。 ","date":"2020-01-01","objectID":"/what-is-a-fulfilling-life/:10:0","tags":["jicki"],"title":"怎么样才算是一个充实的人生？","uri":"/what-is-a-fulfilling-life/"},{"categories":["kubernetes"],"content":"kubeadm cri to 1.16.3","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":" kubeadm cri to 1.16.3 kubernetes 1.16.3 本文基于 kubeadm 方式部署，kubeadm 在1.13 版本以后正式进入 GA. 目前国内各大厂商都有 kubeadm 的镜像源，对于部署 kubernetes 来说是大大的便利. 从官方对 kubeadm 的更新频繁度来看，kubeadm 应该是后面的趋势，毕竟二进制部署确实麻烦了点. 1. 环境说明 系统 IP Containerd Kernel 作用 CentOS 7 x64 172.16.0.3 18.09.6 4.4.205 K8s-Master CentOS 7 x64 172.16.0.10 18.09.6 4.4.205 K8s-Node ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:0:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"1.1 初始化环境 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:1:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"1.1.1 配置 hosts hostnamectl --static set-hostname hostname k8s-node-1 172.16.0.3 k8s-node-2 172.16.0.10 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.0.3 k8s-node-1 172.16.0.10 k8s-node-2 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:1:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"1.1.2 关闭防火墙 sed -ri 's#(SELINUX=).*#\\1disabled#' /etc/selinux/config setenforce 0 systemctl disable firewalld systemctl stop firewalld ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:1:2","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"1.1.3 关闭虚拟内存 # 临时关闭 swapoff -a # 永久关闭 vi /etc/fstab 注释掉关于 swap 的一段 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:1:3","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"1.1.4 添加内核配置 vi /etc/sysctl.conf net.ipv4.ip_forward=1 net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 vm.swappiness=0 # 生效配置 sysctl -p ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:1:4","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"1.1.5 配置IPVS模块 kube-proxy 使用 ipvs 方式负载 ，所以需要内核加载 ipvs 模块, 否则只会使用 iptables 方式 cat \u003e /etc/sysconfig/modules/ipvs.modules \u003c\u003cEOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF # 授权 chmod 755 /etc/sysconfig/modules/ipvs.modules # 加载模块 bash /etc/sysconfig/modules/ipvs.modules # 查看加载 lsmod | grep -e ip_vs -e nf_conntrack_ipv4 # 输出如下: ----------------------------------------------------------------------- nf_conntrack_ipv4 20480 0 nf_defrag_ipv4 16384 1 nf_conntrack_ipv4 ip_vs_sh 16384 0 ip_vs_wrr 16384 0 ip_vs_rr 16384 0 ip_vs 147456 6 ip_vs_rr,ip_vs_sh,ip_vs_wrr nf_conntrack 110592 2 ip_vs,nf_conntrack_ipv4 libcrc32c 16384 2 xfs,ip_vs ----------------------------------------------------------------------- ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:1:5","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"1.1.6 配置yum源 使用 阿里 的 yum 源 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 更新 yum yum makecache 2. 安装 crictl和containerd ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:1:6","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"安装 containerd # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装 containerd.io yum update \u0026\u0026 yum install containerd.io # 导出完整配置文件 mv /etc/containerd/config.toml /etc/containerd/config.toml-bak containerd config default \u003e /etc/containerd/config.toml # 修改配置文件 vi /etc/containerd/config.toml # 按需修改如下配置 root = \"/opt/containerd\" sandbox_image = \"registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.1\" [plugins.cri.registry] [plugins.cri.registry.mirrors] [plugins.cri.registry.mirrors.\"docker.io\"] endpoint = [\"https://registry-1.docker.io\"] [plugins.cri.registry.mirrors.\"daocloud.io\"] endpoint = [\"http://b438f72b.m.daocloud.io\"] # 修改 systemctl 文件，配置 modprobe 加载 vi /usr/lib/systemd/system/containerd.service 在 ExecStartPre=-/sbin/modprobe overlay 下面添加: ExecStartPre=-/sbin/modprobe br_netfilter systemctl enable containerd systemctl start containerd systemctl status containerd ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:2:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"配置 crictl # 配置文件 cat \u003c\u003cEOF \u003e /etc/crictl.yaml runtime-endpoint: unix:///run/containerd/containerd.sock image-endpoint: unix:///run/containerd/containerd.sock timeout: 10 debug: false EOF ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:3:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"测试安装 # 测试拉取镜像 crictl pull nginx:alpine Image is up to date for sha256:a624d888d69ffdc185ed3b9c9c0645e8eaaac843ce59e89f1fbe45b0581e4ef6 # 查看 crictl images IMAGE TAG IMAGE ID SIZE docker.io/library/nginx alpine a624d888d69ff 8.78MB 3. 部署 kubernetes ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:3:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.1 安装相关软件 所有软件安装都通过 yum 安装 # kubernetes 相关 (Master) yum -y install kubelet-1.16.3 kubeadm-1.16.3 kubectl-1.16.3 # kubernetes 相关 (Node) yum -y install kubelet-1.16.3 kubeadm-1.16.3 # ipvs 相关 yum -y install ipvsadm ipset # 配置 kubelet 自动启动 (暂时不需要启动) systemctl enable kubelet.service 修改源码, 增加证书 10年期限 # 下载源码 git clone https://github.com/kubernetes/kubernetes Cloning into 'kubernetes'... remote: Enumerating objects: 219, done. remote: Counting objects: 100% (219/219), done. remote: Compressing objects: 100% (128/128), done. remote: Total 1087208 (delta 112), reused 91 (delta 91), pack-reused 1086989 Receiving objects: 100% (1087208/1087208), 668.66 MiB | 486.00 KiB/s, done. Resolving deltas: 100% (777513/777513), done. # 查看分支 cd kubernetes git branch -a 查看当前的分支 git branch # 切换到相关的分支 git checkout remotes/origin/release-1.16 修改 cert.go 文件 # 打开文件 vi staging/src/k8s.io/client-go/util/cert/cert.go # 如下 默认已经是10年,可不修改,也可以修改99年,但是不能超过100年 NotAfter: now.Add(duration365d * 10).UTC(), 修改 constants.go 文件 # 打开文件 vi cmd/kubeadm/app/constants/constants.go # 如下 默认是 1年, 修改为 10 年 CertificateValidity = time.Hour * 24 * 365 # 修改为 CertificateValidity = time.Hour * 24 * 365 * 10 重新编译 kubeadm make all WHAT=cmd/kubeadm GOFLAGS=-v 拷贝 覆盖 kubeadm # 编译后生成目录为 _output/local/bin/linux/amd64 cp _output/local/bin/linux/amd64/kubeadm /usr/bin/kubeadm cp: overwrite ‘/usr/bin/kubeadm’? y ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:4:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.3 修改 kubeadm 配置信息 # 导出 配置 信息 kubeadm config print init-defaults \u003e kubeadm-init.yaml # 修改相关配置，本文配置信息如下 # 这里特别要说明 criSocket: /run/containerd/containerd.sock apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 172.16.0.3 bindPort: 6443 nodeRegistration: criSocket: /run/containerd/containerd.sock name: k8s-node-1 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: \"172.16.0.3:6443\" controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers kind: ClusterConfiguration kubernetesVersion: v1.16.3 networking: dnsDomain: cluster.local podSubnet: 10.254.64.0/18 serviceSubnet: 10.254.0.0/18 scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \"ipvs\" ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:5:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.4 初始化集群 kubeadm init --config kubeadm-init.yaml # 输出如下: Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 172.16.0.3:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:02260b45ddd43ab04ee54b3d77f9677a6eb7166b5501acf3ee61f069eceb626e \\ --control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 172.16.0.3:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:02260b45ddd43ab04ee54b3d77f9677a6eb7166b5501acf3ee61f069eceb626e # 拷贝权限文件 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config # 查看集群状态(1.16 版本 AGE 都显示 unknown ) [root@k8s-node-1 yaml]# kubectl get cs NAME AGE scheduler \u003cunknown\u003e controller-manager \u003cunknown\u003e etcd-0 \u003cunknown\u003e 至此，集群初始化完成, Master 部署完成。 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:6:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.5 加入 kubernetes 集群 如上有 kubeadm init 后有两条 kubeadm join 命令, –experimental-control-plane 为 加入 Master 另外token 有时效性，如果提示 token 失效，请自行创建一个新的 token. kubeadm token create –print-join-command 创建新的 join token ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:7:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.5.1 验证 Master 节点 这里 STATUS 显示 NotReady 是因为 没有安装网络组件 # 查看 node [root@k8s-node-1 yaml]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 NotReady master 4m41s v1.16.3 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:7:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.6 部署 Node 节点 kubeadm join 172.16.0.3:6443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:02260b45ddd43ab04ee54b3d77f9677a6eb7166b5501acf3ee61f069eceb626e # 输出如下: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:8:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.6.1 验证 所有 节点 这里 STATUS 显示 NotReady 是因为 没有安装网络组件 [root@k8s-node-1 yaml]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 NotReady master 4m41s v1.16.3 k8s-node-2 NotReady \u003cnone\u003e 2m46s v1.16.3 查看证书 # 更新证书 # kubeadm alpha certs renew all # 查看证书时间 kubeadm alpha certs check-expiration ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:8:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.7 安装网络组件 Calico 网络 官方文档 https://docs.projectcalico.org/v3.10/introduction ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:9:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.7.1 下载 Calico yaml # 下载 yaml 文件 wget https://docs.projectcalico.org/v3.10/manifests/calico.yaml ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:9:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.7.2 修改 Calico 配置 这里只需要修改 分配的 CIDR 就可以 vi calico.yaml # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: \"10.254.64.0/18\" # 导入 yaml 文件 [root@k8s-node-1 calico]# kubectl apply -f calico.yaml configmap/calico-config unchanged customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org unchanged customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org unchanged clusterrole.rbac.authorization.k8s.io/calico-kube-controllers unchanged clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers unchanged clusterrole.rbac.authorization.k8s.io/calico-node unchanged clusterrolebinding.rbac.authorization.k8s.io/calico-node unchanged daemonset.apps/calico-node configured serviceaccount/calico-node unchanged deployment.apps/calico-kube-controllers unchanged serviceaccount/calico-kube-controllers unchanged # 查看服务 [root@k8s-node-1 calico]# kubectl get pods -n kube-system |grep calico calico-kube-controllers-6b64bcd855-r2llv 1/1 Running 0 18m calico-node-2jgz4 1/1 Running 0 18m calico-node-sblfx 1/1 Running 0 18m [root@k8s-node-1 calico]# crictl ps |grep calico 73dd289b349f6 8f87d09ab8119 18 minutes ago Running calico-kube-controllers 0 fd39d95666bd1 502550c1b5d0f 4a88ba569c297 18 minutes ago Running calico-node 0 e9801794bb446 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:9:2","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.8 检验整体集群 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:10:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.8.1 查看 状态 所有的 STATUS 都为 Ready [root@k8s-node-1 calico]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-node-1 Ready master 51m v1.16.3 k8s-node-2 Ready \u003cnone\u003e 49m v1.16.3 查看 etcd 状态 # 这里目前只有一个 etcd 节点,多个节点 就写多个就可以 export ETCDCTL_API=3 # 1 etcdctl -w table \\ --endpoints=https://172.16.0.3:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint status # 2 etcdctl -w table \\ --endpoints=https://172.16.0.3:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint health # 3 etcdctl -w table \\ --endpoints=https://172.16.0.3:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ member list ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:10:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.8.2 查看 pods 状态 [root@k8s-node-1 calico]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-6b64bcd855-r2llv 1/1 Running 0 21m kube-system calico-node-2jgz4 1/1 Running 0 21m kube-system calico-node-sblfx 1/1 Running 0 21m kube-system coredns-67c766df46-6zk8j 1/1 Running 0 52m kube-system coredns-67c766df46-rpwd5 1/1 Running 0 52m kube-system etcd-k8s-node-1 1/1 Running 1 51m kube-system kube-apiserver-k8s-node-1 1/1 Running 1 51m kube-system kube-controller-manager-k8s-node-1 1/1 Running 0 51m kube-system kube-proxy-tshbd 1/1 Running 0 52m kube-system kube-proxy-vsn2l 1/1 Running 0 50m kube-system kube-scheduler-k8s-node-1 1/1 Running 1 51m ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:10:2","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.8.3 查看 svc 的状态 [root@k8s-node-1 calico]# kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 53m kube-system kube-dns ClusterIP 10.254.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 53m ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:10:3","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"3.8.3 查看 IPVS 的状态 [root@k8s-node-1 calico]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr -\u003e 172.16.0.3:6443 Masq 1 5 0 TCP 10.254.0.10:53 rr -\u003e 10.254.69.130:53 Masq 1 0 0 -\u003e 10.254.69.131:53 Masq 1 0 0 TCP 10.254.0.10:9153 rr -\u003e 10.254.69.130:9153 Masq 1 0 0 -\u003e 10.254.69.131:9153 Masq 1 0 0 UDP 10.254.0.10:53 rr -\u003e 10.254.69.130:53 Masq 1 0 0 -\u003e 10.254.69.131:53 Masq 1 0 0 4. 测试集群 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:10:4","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"4.1 创建一个 nginx deployment apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http volumeMounts: - name: tz-config mountPath: /etc/localtime readOnly: true # readinessProbe - 检测pod 的 Ready 是否为 true readinessProbe: tcpSocket: port: 80 # 启动后5s 开始检测 initialDelaySeconds: 5 # 检测 间隔为 10s periodSeconds: 10 # livenessProbe - 检测 pod 的 State 是否为 Running livenessProbe: httpGet: path: / port: 80 # 启动后 15s 开始检测 # 检测时间必须在 readinessProbe 之后 initialDelaySeconds: 15 # 检测 间隔为 20s periodSeconds: 20 volumes: - name: tz-config hostPath: path: /etc/localtime --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: name: nginx # 导入文件 [root@k8s-node-1 yaml]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-dm created service/nginx-svc created # 查看服务 [root@k8s-node-1 yaml]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-dm-9947b45c5-7wvlq 1/1 Running 0 6m25s 10.254.76.66 k8s-node-2 \u003cnone\u003e \u003cnone\u003e nginx-dm-9947b45c5-dlbv8 1/1 Running 0 6m25s 10.254.76.65 k8s-node-2 \u003cnone\u003e \u003cnone\u003e # 查看 svc [root@k8s-node-1 yaml]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 61m \u003cnone\u003e nginx-svc ClusterIP 10.254.52.255 \u003cnone\u003e 80/TCP 6m42s name=nginx # 查看node-2 服务 [root@k8s-node-2 ~]# crictl ps |grep nginx 0209fb1b4d88b a624d888d69ff About a minute ago Running nginx 0 735222980d8a8 f6016fae8ced0 a624d888d69ff About a minute ago Running nginx 0 500132de59fb2 # node-1 访问 svc [root@k8s-node-1 yaml]# curl 10.254.52.255 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # node-2 访问 svc [root@k8s-node-2 ~]# curl 10.254.52.255 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@k8s-node-1 yaml]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr -\u003e 172.16.0.3:6443 Masq 1 5 0 TCP 10.254.0.10:53 rr -\u003e 10.254.69.130:53 Masq 1 0 0 -\u003e 10.254.69.131:53 Masq 1 0 0 TCP 10.254.0.10:9153 rr -\u003e 10.254.69.130:9153 Masq 1 0 0 -\u003e 10.254.69.131:9153 Masq 1 0 0 TCP 10.254.52.255:80 rr -\u003e 10.254.76.65:80 Masq 1 0 1 -\u003e 10.254.76.66:80 Masq 1 0 0 UDP 10.254.0.10:53 rr -\u003e 10.254.69.130:53 Masq 1 0 0 -\u003e 10.254.69.131:53 Masq 1 0 0 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:11:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"4.2 验证 dns 的服务 # 创建一个 pod apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sleep - \"3600\" # 查看 创建的服务 [root@k8s-node-1 yaml]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 47s nginx-dm-9947b45c5-7wvlq 1/1 Running 0 10m nginx-dm-9947b45c5-dlbv8 1/1 Running 0 10m # 测试 # kubernetes 服务 [root@k8s-node-1 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local # nginx-svc 服务 [root@k8s-node-1 yaml]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.52.255 nginx-svc.default.svc.cluster.local 5. 部署 Metrics-Server 官方 https://github.com/kubernetes-incubator/metrics-server ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:12:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"5.1 Metrics-Server 说明 v1.11 以后不再支持通过 heaspter 采集监控数据，支持新的监控数据采集组件metrics-server，比heaspter轻量很多，也不做数据的持久化存储，提供实时的监控数据查询。 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:13:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"5.1.1 创建 Metrics-Server 文件 # vi metrics-server.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:aggregated-metrics-reader labels: rbac.authorization.k8s.io/aggregate-to-view: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rules: - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1beta1.metrics.k8s.io spec: service: name: metrics-server namespace: kube-system group: metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:metrics-server rules: - apiGroups: - \"\" resources: - pods - nodes - nodes/stats - namespaces verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 args: - --cert-dir=/tmp - --secure-port=4443 ports: - name: main-port containerPort: 4443 protocol: TCP securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 imagePullPolicy: Always volumeMounts: - name: tmp-dir mountPath: /tmp command: - /metrics-server - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP nodeSelector: beta.kubernetes.io/os: linux --- apiVersion: v1 kind: Service metadata: name: metrics-server namespace: kube-system labels: kubernetes.io/name: \"Metrics-server\" kubernetes.io/cluster-service: \"true\" spec: selector: k8s-app: metrics-server ports: - port: 443 protocol: TCP targetPort: main-port # 导入服务 [root@k8s-node-1 metrics]# kubectl apply -f metrics-server.yaml clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/system:metrics-server created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created serviceaccount/metrics-server created deployment.apps/metrics-server created service/metrics-server created ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:13:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"5.1.2 查看服务 [root@k8s-node-1 metrics]# kubectl get pods -n kube-system |grep metrics metrics-server-7b5b7fd65-v8sqc 1/1 Running 0 11s ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:13:2","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"5.1.3 测试采集 提示 error: metrics not available yet , 请等待一会采集后再查询 [root@k8s-node-1 metrics]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-node-1 139m 6% 1647Mi 44% k8s-node-2 71m 3% 810Mi 21% 6. Nginx Ingress 官方地址 https://kubernetes.github.io/ingress-nginx/ ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:13:3","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.1 Nginx Ingress 介绍 基于 Nginx 使用 Kubernetes ConfigMap 来存储 Nginx 配置文件 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:14:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.2 部署 Nginx ingress ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:15:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.2.1 下载 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:15:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.2.2 修改 yaml 文件 # 替换 阿里 镜像下载地址 sed -i 's/quay\\.io\\/kubernetes-ingress-controller/registry\\.cn-hangzhou\\.aliyuncs\\.com\\/google_containers/g' mandatory.yaml # 配置 node affinity 与 hostNetwork # 在 如下之间添加 spec: serviceAccountName: nginx-ingress-serviceaccount # 添加完如下: spec: hostNetwork: true affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - k8s-node-1 - k8s-node-2 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - ingress-nginx topologyKey: \"kubernetes.io/hostname\" tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule serviceAccountName: nginx-ingress-serviceaccount # 如上 affinity 说明 affinity: # 声明 亲和性设置 nodeAffinity: # 声明 为 Node 亲和性设置 requiredDuringSchedulingIgnoredDuringExecution: # 必须满足下面条件 nodeSelectorTerms: # 声明 为 Node 调度选择标签 - matchExpressions: # 设置node拥有的标签 - key: kubernetes.io/hostname # kubernetes内置标签 operator: In # 操作符 values: # 值,既集群 node 名称 - k8s-node-1 - k8s-node-2 podAntiAffinity: # 声明 为 Pod 亲和性设置 requiredDuringSchedulingIgnoredDuringExecution: # 必须满足下面条件 - labelSelector: # 与哪个pod有亲和性，在此设置此pod具有的标签 matchExpressions: # 要匹配如下的pod的,标签定义 - key: app.kubernetes.io/name # 标签定义为 空间名称(namespaces) operator: In values: - ingress-nginx topologyKey: \"kubernetes.io/hostname\" # 节点所属拓朴域 tolerations: # 声明 为 可容忍 的选项 - key: node-role.kubernetes.io/master # 声明 标签为 node-role 选项 effect: NoSchedule # 声明 node-role 为 NoSchedule 也可容忍 serviceAccountName: nginx-ingress-serviceaccount ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:15:2","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.2.3 apply 导入 文件 [root@k8s-node-1 ingress]# kubectl apply -f mandatory.yaml namespace/ingress-nginx created configmap/nginx-configuration created configmap/tcp-services created configmap/udp-services created serviceaccount/nginx-ingress-serviceaccount created clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created role.rbac.authorization.k8s.io/nginx-ingress-role created rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created deployment.apps/nginx-ingress-controller created ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:15:3","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.2.4 查看服务状态 [root@k8s-node-1 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ingress-controller-7ff768bcfd-5q9ns 1/1 Running 0 95s 172.16.0.3 k8s-node-1 \u003cnone\u003e \u003cnone\u003e nginx-ingress-controller-7ff768bcfd-zxmv2 1/1 Running 0 96s 172.16.0.10 k8s-node-2 \u003cnone\u003e \u003cnone\u003e ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:15:4","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.2.5 测试 ingress # 查看之前创建的 Nginx [root@k8s-node-1 ingress]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 74m nginx-svc ClusterIP 10.254.52.255 \u003cnone\u003e 80/TCP 19m # 创建一个 nginx-svc 的 ingress vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 # 导入 yaml [root@k8s-node-1 yaml]# kubectl apply -f nginx-ingress.yaml ingress.extensions/nginx-ingress created # 查看 ingress [root@k8s-node-1 yaml]# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 17s ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:15:5","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"6.2.6 测试访问 [root@k8s-node-1 yaml]# curl -I nginx.jicki.cn HTTP/1.1 200 OK Server: openresty/1.15.8.2 Date: Thu, 05 Dec 2019 05:41:07 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Vary: Accept-Encoding Last-Modified: Tue, 19 Nov 2019 15:14:41 GMT ETag: \"5dd406e1-264\" Accept-Ranges: bytes 7. Dashboard 官方 https://github.com/kubernetes/dashboard ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:15:6","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.1 Dashboard 介绍 Dashboard 是 Kubernetes 集群的 通用 WEB UI 它允许用户管理集群中运行的应用程序并对其进行故障排除，以及管理集群本身。 ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:16:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.2 部署 Dashboard 注意 dashboard 1.10.x 版本 不支持 kubernetes 1.16.x 必须使用 2.0 版本否则报错 404 the server could not find the requested resource ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:17:0","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.2.1 下载 yaml 文件 # 下载 yaml 文件 https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta6/aio/deploy/recommended.yaml ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:17:1","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.2.2 apply 导入文件 [root@k8s-node-1 dashboard]# kubectl apply -f recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:17:2","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.2.3 查看服务状态 [root@k8s-node-1 yaml]# kubectl get pods -n kubernetes-dashboard |grep dashboard dashboard-metrics-scraper-76585494d8-vgfmz 1/1 Running 0 9m22s kubernetes-dashboard-b65488c4-cqnc8 1/1 Running 0 9m22s # svc 服务 [root@k8s-node-1 yaml]# kubectl get svc -n kubernetes-dashboard |grep dashboard dashboard-metrics-scraper ClusterIP 10.254.17.210 \u003cnone\u003e 8000/TCP 9m44s kubernetes-dashboard ClusterIP 10.254.7.84 \u003cnone\u003e 443/TCP 9m44s ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:17:3","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.2.4 暴露公网 访问 kubernetes 服务，既暴露 kubernetes 内的端口到 外网，有很多种方案 LoadBlancer ( 支持的公有云服务的负载均衡 ) NodePort (映射所有 node 中的某个端口，暴露到公网中) Ingress ( 支持反向代理软件的对外服务, 如: Nginx , HAproxy 等) # 由于我们已经部署了 Nginx-ingress 所以这里使用 ingress 来暴露出去 # Dashboard 这边 从 svc 上看只 暴露了 443 端口，所以这边需要生成一个证书 # 注: 这里由于测试，所以使用 openssl 生成临时的证书 # 生成证书 # 创建一个 基于 自身域名的 证书 openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout dashboard.jicki.cn-key.key -out dashboard.jicki.cn.pem -subj \"/CN=dashboard.jicki.cn\" # 导入 域名的证书 到 集群 的 secret 中 kubectl create secret tls dashboard-secret --namespace=kubernetes-dashboard --cert dashboard.jicki.cn.pem --key dashboard.jicki.cn-key.key # 查看 secret [root@k8s-node-1 ssl]# kubectl get secret -n kubernetes-dashboard |grep dashboard dashboard-secret kubernetes.io/tls 2 22s [root@k8s-node-1 ssl]# kubectl describe secret/dashboard-secret -n kubernetes-dashboard Name: dashboard-secret Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: kubernetes.io/tls Data ==== tls.crt: 1119 bytes tls.key: 1704 bytes # 创建 dashboard ingress # 这里面 annotations 中的 backend 声明,从 v0.21.0 版本开始变更, 一定注意 # nginx-ingress \u003c v0.21.0 使用 nginx.ingress.kubernetes.io/secure-backends: \"true\" # nginx-ingress \u003e v0.21.0 使用 nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" # 创建 ingress 文件 vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard namespace: kubernetes-dashboard annotations: ingress.kubernetes.io/ssl-passthrough: \"true\" nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec: tls: - hosts: - dashboard.jicki.cn secretName: dashboard-secret rules: - host: dashboard.jicki.cn http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443 # 导入 yaml [root@k8s-node-1 dashboard]# kubectl apply -f dashboard-ingress.yaml ingress.extensions/kubernetes-dashboard created # 查看 ingress [root@k8s-node-1 dashboard]# kubectl get ingress -n kubernetes-dashboard NAME HOSTS ADDRESS PORTS AGE kubernetes-dashboard dashboard.jicki.cn 80, 443 34s ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:17:4","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.2.6 测试访问 [root@k8s-node-1 dashboard]# curl -I -k https://dashboard.jicki.cn HTTP/2 200 server: openresty/1.15.8.2 date: Thu, 05 Dec 2019 06:39:03 GMT content-type: text/html; charset=utf-8 content-length: 1262 vary: Accept-Encoding strict-transport-security: max-age=15724800; includeSubDomains accept-ranges: bytes cache-control: no-store last-modified: Thu, 14 Nov 2019 13:39:35 GMT ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:17:5","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["kubernetes"],"content":"7.2.7 令牌 登录认证 # 创建一个 dashboard rbac 超级用户 vi dashboard-admin-rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kubernetes-dashboard # 导入文件 [root@k8s-node-1 dashboard]# kubectl apply -f dashboard-admin-rbac.yaml serviceaccount/kubernetes-dashboard-admin created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin created # 查看 secret [root@k8s-node-1 dashboard]# kubectl get secret -n kubernetes-dashboard | grep kubernetes-dashboard-admin kubernetes-dashboard-admin-token-9dkg4 kubernetes.io/service-account-token 3 38s # 查看 token 部分 [root@k8s-node-1 dashboard]# kubectl describe -n kubernetes-dashboard secret/kubernetes-dashboard-admin-token-9dkg4 Name: kubernetes-dashboard-admin-token-9dkg4 Namespace: kubernetes-dashboard Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: kubernetes-dashboard-admin kubernetes.io/service-account.uid: aee23b33-43a4-4fb4-b498-6c2fb029d63c Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlI4UlpGcTcwR2hkdWZfZWk1X0RUcVI5dkdraXFnNW8yYUV1VVRPQlJYMEkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi05ZGtnNCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImFlZTIzYjMzLTQzYTQtNGZiNC1iNDk4LTZjMmZiMDI5ZDYzYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.oyvo_bIM0Ukbs3ov8XbmJffpdK1nec7oKJBxu8V4vesPY_keQhNS9xiAw6zdF2Db2tiEzcpmN3SAgwGjfid5rlSQxGpNK3mkp1r60WSAhyU5e7RqwA9xRO-EtCZ2akrqFKzEn4j_7FGwbKbNsdRurDdOLtKU5KvFsFh5eRxvB6PECT2mgSugfHorrI1cYOw0jcQKE_hjVa94xUseYX12PyGQfoUyC6ZhwIBkRnCSNdbcb0VcGwTerwysR0HFvozAJALh_iOBTDYDUNh94XIRh2AHCib-KVoJt-e2jUaGH-Z6yniLmNr15q5xLfNBd1qPpZHCgoJ1JYz4TeF6udNxIA # 复制 token 如下部分: token: eyJhbGciOiJSUzI1NiIsImtpZCI6IlI4UlpGcTcwR2hkdWZfZWk1X0RUcVI5dkdraXFnNW8yYUV1VVRPQlJYMEkifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi05ZGtnNCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImFlZTIzYjMzLTQzYTQtNGZiNC1iNDk4LTZjMmZiMDI5ZDYzYyIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.oyvo_bIM0Ukbs3ov8XbmJffpdK1nec7oKJBxu8V4vesPY_keQhNS9xiAw6zdF2Db2tiEzcpmN3SAgwGjfid5rlSQxGpNK3mkp1r60WSAhyU5e7RqwA9xRO-EtCZ2akrqFKzEn4j_7FGwbKbNsdRurDdOLtKU5KvFsFh5eRxvB6PECT2mgSugfHorrI1cYOw0jcQKE_hjVa94xUseYX12PyGQfoUyC6ZhwIBkRnCSNdbcb0VcGwTerwysR0HFvozAJALh_iOBTDYDUNh94XIRh2AHCib-KVoJt-e2jUaGH-Z6yniLmNr15q5xLfNBd1qPpZHCgoJ1JYz4TeF6udNxIA ","date":"2019-12-05","objectID":"/kubeadm-cri-1.16.3/:17:6","tags":["kubernetes","docker"],"title":"kubeadm cri to 1.16.3","uri":"/kubeadm-cri-1.16.3/"},{"categories":["fabric"],"content":"hyperledger-fabric v 1.4","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":" fabric v1.4 , 单机 多节点 kafka 手动部署, 所有服务均 开启 SSL 认证。 部署 hyperledger-fabric v1.4 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:0:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"环境规划 相关hostname 必须配置 dns 关于 orderer 集群 当orderer 向peer节点提交Transaction的时候，peer节点会得到或返回一个读写集结果，该结果会发送给orderer节点进行共识和排序，此时如果orderer节点突然down掉，就会使请求服务失效而引发的数据丢失等问题，且目前的sdk对orderer发送的Transaction的回调会占用极长的时间，当大批量数据导入的时候该回调可认为不可用。 节点标识 hostname IP 开放端口 系统 orderer0节点 orderer0.jicki.cn 192.168.100.100 7050 CentOS 7 x64 peer0节点 peer0.org1.jicki.cn 192.168.100.100 7051, 7052, 7053 CentOS 7 x64 peer0节点 peer0.org2.jicki.cn 192.168.100.100 7051, 7052, 7053 CentOS 7 x64 zk0节点 zookeeper0 192.168.100.100 2181 CentOS 7 x64 zk1节点 zookeeper1 192.168.100.100 2181 CentOS 7 x64 zk2节点 zookeeper2 192.168.100.100 2181 CentOS 7 x64 kafka0节点 kafka0 192.168.100.100 9092 CentOS 7 x64 kafka1节点 kafka1 192.168.100.100 9092 CentOS 7 x64 kafka2节点 kafka2 192.168.100.100 9092 CentOS 7 x64 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:1:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"官方地址 文档以官方文档为主 http://hyperledger-fabric.readthedocs.io/en/release-1.4/prereqs.html # 官网 github https://github.com/hyperledger/fabric ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:2:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"环境准备 所有机器 安装 Docker (用于 fabric 服务启动) # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装 docker yum -y install docker-ce # 启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 查看 docker 版本 docker version Client: Docker Engine - Community Version: 19.03.4 API version: 1.40 Go version: go1.12.10 Git commit: 9013bf583a Built: Fri Oct 18 15:52:22 2019 OS/Arch: linux/amd64 Experimental: false Server: Docker Engine - Community Engine: Version: 19.03.4 API version: 1.40 (minimum version 1.12) Go version: go1.12.10 Git commit: 9013bf583a Built: Fri Oct 18 15:50:54 2019 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.2.10 GitCommit: b34a5c8af56e510852c35414db4c1f4fa6172339 runc: Version: 1.0.0-rc8+dev GitCommit: 3e425f80a8c931f88e6d94a8c831b9d5aa481657 docker-init: Version: 0.18.0 GitCommit: fec3683 安装 Docker-compose (用于 docker 容器服务统一管理 编排) # 安装 pip curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py # 安装 docker-compose pip install docker-compose --ignore-installed requests curl -L \"https://github.com/docker/compose/releases/download/1.25.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose version docker-compose version 1.24.1, build 4667896 docker-py version: 3.7.3 CPython version: 2.7.5 OpenSSL version: OpenSSL 1.0.2k-fips 26 Jan 2017 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:3:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"Hyperledger Fabric 源码 fabric 源码用于 cli 智能合约安装时的依赖, 这里只用于第一个节点 # 下载 Fabric 源码, 源码中 import 的路径为github.com/hyperledger/fabric ,所以我们要按照这个路径 mkdir -p /opt/gopath/src/github.com/hyperledger cd /opt/gopath/src/github.com/hyperledger git clone https://github.com/hyperledger/fabric # 查看分支 git branch -a # 查看本地分支 git branch # 切换分支 git checkout -b release-1.4 remotes/origin/release-1.4 # 文件如下: [root@localhost fabric]# ls -lt total 1420 drwxr-xr-x 3 root root 4096 Oct 29 11:19 sampleconfig drwxr-xr-x 9 root root 4096 Oct 29 09:48 vendor drwxr-xr-x 8 root root 4096 Oct 29 09:48 token -rw-r--r-- 1 root root 495 Oct 29 09:48 tox.ini drwxr-xr-x 2 root root 4096 Oct 29 09:48 unit-test -rw-r--r-- 1 root root 3816 Oct 29 09:48 testingInfo.rst -rw-r--r-- 1 root root 438053 Oct 29 09:48 test-pyramid.png drwxr-xr-x 2 root root 4096 Oct 29 09:48 scripts -rw-r--r-- 1 root root 316 Oct 29 09:48 settings.gradle drwxr-xr-x 3 root root 4096 Oct 29 09:48 release drwxr-xr-x 2 root root 4096 Oct 29 09:48 release_notes drwxr-xr-x 14 root root 4096 Oct 29 09:48 protos drwxr-xr-x 11 root root 4096 Oct 29 09:48 peer drwxr-xr-x 6 root root 4096 Oct 29 09:48 orderer drwxr-xr-x 6 root root 4096 Oct 29 09:48 msp drwxr-xr-x 13 root root 4096 Oct 29 09:48 integration drwxr-xr-x 2 root root 4096 Oct 29 09:48 idemix drwxr-xr-x 8 root root 4096 Oct 29 09:48 images -rw-r--r-- 1 root root 2999 Oct 29 09:48 gotools.mk drwxr-xr-x 17 root root 4096 Oct 29 09:48 gossip drwxr-xr-x 7 root root 4096 Oct 29 09:48 examples drwxr-xr-x 5 root root 4096 Oct 29 09:48 docs drwxr-xr-x 7 root root 4096 Oct 29 09:48 discovery -rw-r--r-- 1 root root 3355 Oct 29 09:48 docker-env.mk drwxr-xr-x 4 root root 4096 Oct 29 09:48 devenv drwxr-xr-x 24 root root 4096 Oct 29 09:48 core drwxr-xr-x 28 root root 4096 Oct 29 09:48 common drwxr-xr-x 4 root root 4096 Oct 29 09:48 cmd -rw-r--r-- 1 root root 14 Oct 29 09:48 ci.properties drwxr-xr-x 9 root root 4096 Oct 29 09:48 bccsp -rw-r--r-- 1 root root 29937 Oct 29 09:48 Gopkg.lock -rw-r--r-- 1 root root 3849 Oct 29 09:48 Gopkg.toml -rw-r--r-- 1 root root 11358 Oct 29 09:48 LICENSE -rwxr-xr-x 1 root root 17474 Oct 29 09:48 Makefile -rw-r--r-- 1 root root 7391 Oct 29 09:48 README.md -rw-r--r-- 1 root root 1035 Oct 29 09:48 SECURITY.md -rw-r--r-- 1 root root 804634 Oct 29 09:48 CHANGELOG.md -rw-r--r-- 1 root root 597 Oct 29 09:48 CODE_OF_CONDUCT.md -rw-r--r-- 1 root root 661 Oct 29 09:48 CONTRIBUTING.md ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:4:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"生成 Hyperledger Fabric 证书 证书生成只需要生成一次，这里只在第一个节点配置 # 下载官方证书生成软件(均为二进制文件) # 官方离线下载地址为 https://github.com/hyperledger/fabric/releases # 选择相应版本 CentOS 选择 linux-amd64-1.4.0 Mac 选择 darwin-amd64-1.4.0 # 下载地址为: https://github.com/hyperledger/fabric/releases/download/v1.4.0/hyperledger-fabric-linux-amd64-1.4.0.tar.gz mkdir /opt/jicki/ cd /opt/jicki wget https://github.com/hyperledger/fabric/releases/download/v1.4.0/hyperledger-fabric-linux-amd64-1.4.0.tar.gz tar zxvf hyperledger-fabric-linux-amd64-1.4.0.tar.gz # 解压后是 一个 bin 与 一个 config 目录 [root@localhost jicki]# tree . └── bin ├── configtxgen ├── configtxlator ├── cryptogen ├── discover ├── get-docker-images.sh ├── idemixgen ├── orderer └── peer 1 directory, 8 files # 为方便使用 我们配置一个 环境变量 vi /etc/profile # fabric env export PATH=$PATH:/opt/jicki/bin # 使文件生效 source /etc/profile # 创建 cryptogen.yaml 文件 OrdererOrgs: - Name: Orderer Domain: jicki.cn CA: Country: CN Province: GuangDong Locality: ShenZhen Specs: - Hostname: orderer0 PeerOrgs: - Name: Org1 Domain: org1.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 - Name: Org2 Domain: org2.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 # 然后这里使用 cryptogen 软件来生成相应的证书了 [root@localhost jicki]# cryptogen generate --config=./cryptogen.yaml org1.jicki.cn org2.jicki.cn # 生成一个 crypto-config 证书目录 [root@payment jicki]# tree crypto-config crypto-config ├── ordererOrganizations │ └── jicki.cn │ ├── ca │ │ ├── 87fcad73e61dbc5e267d0b56e991e2ef445407ddf89924debc299cf42dde53aa_sk │ │ └── ca.jicki.cn-cert.pem │ ├── msp │ │ ├── admincerts │ │ │ └── Admin@jicki.cn-cert.pem │ │ ├── cacerts │ │ │ └── ca.jicki.cn-cert.pem │ │ └── tlscacerts │ │ └── tlsca.jicki.cn-cert.pem │ ├── orderers │ │ └── orderer0.jicki.cn │ │ ├── msp │ │ │ ├── admincerts │ │ │ │ └── Admin@jicki.cn-cert.pem │ │ │ ├── cacerts │ │ │ │ └── ca.jicki.cn-cert.pem │ │ │ ├── keystore │ │ │ │ └── 8b8f847af6be4f902f4a85236155a0b9ee37d17edee74ee56212d84cb4b52219_sk │ │ │ ├── signcerts │ │ │ │ └── orderer0.jicki.cn-cert.pem │ │ │ └── tlscacerts │ │ │ └── tlsca.jicki.cn-cert.pem │ │ └── tls │ │ ├── ca.crt │ │ ├── server.crt │ │ └── server.key │ ├── tlsca │ │ ├── d13f753996d547371bcecc387472e88b95cc790dbcbb59a914f6aa05531e8a18_sk │ │ └── tlsca.jicki.cn-cert.pem │ └── users │ └── Admin@jicki.cn │ ├── msp │ │ ├── admincerts │ │ │ └── Admin@jicki.cn-cert.pem │ │ ├── cacerts │ │ │ └── ca.jicki.cn-cert.pem │ │ ├── keystore │ │ │ └── 7dfa64d80276527ed1c4ffd030c8b1f4fda213c85396ac3e06794d3957d825bc_sk │ │ ├── signcerts │ │ │ └── Admin@jicki.cn-cert.pem │ │ └── tlscacerts │ │ └── tlsca.jicki.cn-cert.pem │ └── tls │ ├── ca.crt │ ├── client.crt │ └── client.key └── peerOrganizations ├── org1.jicki.cn │ ├── ca │ │ ├── 3a233ccbd6706cee4c66a320e43bedad3e72b6f68e831ce121f35760eb1ac275_sk │ │ └── ca.org1.jicki.cn-cert.pem │ ├── msp │ │ ├── admincerts │ │ │ └── Admin@org1.jicki.cn-cert.pem │ │ ├── cacerts │ │ │ └── ca.org1.jicki.cn-cert.pem │ │ ├── config.yaml │ │ └── tlscacerts │ │ └── tlsca.org1.jicki.cn-cert.pem │ ├── peers │ │ ├── peer0.org1.jicki.cn │ │ │ ├── msp │ │ │ │ ├── admincerts │ │ │ │ │ └── Admin@org1.jicki.cn-cert.pem │ │ │ │ ├── cacerts │ │ │ │ │ └── ca.org1.jicki.cn-cert.pem │ │ │ │ ├── config.yaml │ │ │ │ ├── keystore │ │ │ │ │ └── eecd3dec5e6ba609a931d94bf0a1fb9defe047e68e1437fd1fec5fcfbe7dea23_sk │ │ │ │ ├── signcerts │ │ │ │ │ └── peer0.org1.jicki.cn-cert.pem │ │ │ │ └── tlscacerts │ │ │ │ └── tlsca.org1.jicki.cn-cert.pem │ │ │ └── tls │ │ │ ├── ca.crt │ │ │ ├── server.crt │ │ │ └── server.key │ │ └── peer1.org1.jicki.cn │ │ ├── msp │ │ │ ├── admincerts │ │ │ │ └── Admin@org1.jicki.cn-cert.pem │ │ │ ├── cacerts │ │ │ │ └── ca.org1.jicki.cn-cert.pem │ │ │ ├── config.yaml │ │ │ ├── keystore │ │ │ │ └── d5a32a61b164604e6352a783f843e00c46ca2dfcefbc1b78c3ea14536483169b_sk │ │ │ ├── signcerts │ │ │ │ └── peer1.org1.jicki.cn-cert.pem │ │ │ └── tlscacerts │ │ │ └── tlsca.","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:5:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"生成 Hyperledger Fabric 创世区块 # 这里使用 configtxgen 来创建 创世区块 # 首先需要创建一个文件夹 mkdir -p /opt/jicki/channel-artifacts # 完整 configtx.yaml 如下: # configtx.yaml 文件格式 请千万注意 空格 与 tab 键 里的缩进，否则会报错。 Organizations: - \u0026OrdererOrg Name: OrdererMSP ID: OrdererMSP MSPDir: crypto-config/ordererOrganizations/jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Writers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Admins: Type: Signature Rule: \"OR('OrdererMSP.admin')\" - \u0026Org1 Name: Org1MSP ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.peer', 'Org1MSP.client')\" Writers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.client')\" Admins: Type: Signature Rule: \"OR('Org1MSP.admin')\" AnchorPeers: - Host: peer0.org1.jicki.cn Port: 7051 - \u0026Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.peer', 'Org2MSP.client')\" Writers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.client')\" Admins: Type: Signature Rule: \"OR('Org2MSP.admin')\" AnchorPeers: - Host: peer0.org2.jicki.cn Port: 7051 Capabilities: Channel: \u0026ChannelCapabilities V1_3: true Orderer: \u0026OrdererCapabilities V1_1: true Application: \u0026ApplicationCapabilities V1_3: true V1_2: false V1_1: false Application: \u0026ApplicationDefaults Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ApplicationCapabilities Orderer: \u0026OrdererDefaults OrdererType: kafka Addresses: - orderer0.jicki.cn:7050 BatchTimeout: 2s BatchSize: MaxMessageCount: 10 AbsoluteMaxBytes: 99 MB PreferredMaxBytes: 512 KB Kafka: Brokers: - kafka0:9092 - kafka1:9092 - kafka2:9092 Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" BlockValidation: Type: ImplicitMeta Rule: \"ANY Writers\" Channel: \u0026ChannelDefaults Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ChannelCapabilities Profiles: TwoOrgsOrdererGenesis: \u003c\u003c: *ChannelDefaults Orderer: \u003c\u003c: *OrdererDefaults Organizations: - *OrdererOrg Capabilities: \u003c\u003c: *OrdererCapabilities Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: \u003c\u003c: *ApplicationDefaults Organizations: - *Org1 - *Org2 Capabilities: \u003c\u003c: *ApplicationCapabilities SampleDevModeKafka: \u003c\u003c: *ChannelDefaults Capabilities: \u003c\u003c: *ChannelCapabilities Orderer: \u003c\u003c: *OrdererDefaults OrdererType: kafka Kafka: Brokers: - kafka0:9092 - kafka1:9092 - kafka2:9092 Organizations: - *OrdererOrg Capabilities: \u003c\u003c: *OrdererCapabilities Application: \u003c\u003c: *ApplicationDefaults Organizations: - \u003c\u003c: *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 # 首先需要创建一个文件夹 mkdir -p /opt/jicki/channel-artifacts # 创建 创世区块 TwoOrgsOrdererGenesis 名称为 configtx.yaml 中 Profiles 字段下的 [root@localhost jicki]# configtxgen -profile TwoOrgsOrdererGenesis \\ -outputBlock ./channel-artifacts/genesis.block 2019-10-29 12:22:06.715 CST [common.tools.configtxgen] main -\u003e WARN 001 Omitting the channel ID for configtxgen for output operations is deprecated. Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'. 2019-10-29 12:22:06.715 CST [common.tools.configtxgen] main -\u003e INFO 002 Loading configuration 2019-10-29 12:22:06.743 CST [common.tools.configtxgen.localconfig] completeInitialization -\u003e INFO 003 orderer type: kafka 2019-10-29 12:22:06.743 CST [common.tools.configtxgen.localconfig] Load -\u003e INFO 004 Loaded configuration: /opt/jicki/configtx.yaml 2019-10-29 12:22:06.772 CST [common.tools.configtxgen.localconfig] completeInitialization -\u003e INFO 005 order","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:6:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"配置 fabric docker-compose.yaml version: '2' services: zookeeper1: container_name: zookeeper1 hostname: zookeeper1 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=1 - ZOO_SERVERS=server.1=zookeeper1:2888:3888 server.2=zookeeper2:2888:3888 server.3=zookeeper3:2888:3888 volumes: # 存储数据与日志 - ./data/zookeeper1/data:/data - ./data/zookeeper1/datalog:/datalog networks: default: aliases: - jicki zookeeper2: container_name: zookeeper2 hostname: zookeeper2 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=2 - ZOO_SERVERS=server.1=zookeeper1:2888:3888 server.2=zookeeper2:2888:3888 server.3=zookeeper3:2888:3888 volumes: # 存储数据与日志 - ./data/zookeeper2/data:/data - ./data/zookeeper2/datalog:/datalog networks: default: aliases: - jicki zookeeper3: container_name: zookeeper3 hostname: zookeeper3 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=3 - ZOO_SERVERS=server.1=zookeeper1:2888:3888 server.2=zookeeper2:2888:3888 server.3=zookeeper3:2888:3888 volumes: # 存储数据与日志 - ./data/zookeeper3/data:/data - ./data/zookeeper3/datalog:/datalog networks: default: aliases: - jicki kafka0: container_name: kafka0 hostname: kafka0 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=1 # 设置一个M值,数据提交时会写入至少M个副本(这里M=2)（这些数据会被同步并且归属到in-sync 副本集合或ISR）M 必须小于 如下 N 值,并且大于1,既最小为2。 - KAFKA_MIN_INSYNC_REPLICAS=2 # 设置一个N值, N代表着每个channel都保存N个副本的数据到Kafka的代理上。N 必须大于如上 M 值, 既 N 值最小值为 3。 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181 # 如下99为configtx.yaml中会设置最大的区块大小(参考configtx.yaml中AbsoluteMaxBytes参数) # 每个区块最大有Orderer.AbsoluteMaxBytes个字节 # 99 * 1024 * 1024 B - KAFKA_MESSAGE_MAX_BYTES=103809024 # 每个通道获取的消息的字节数 如上一样 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 # 数据一致性在区块链环境中是至关重要的, 我们不能从in-sync 副本（ISR）集合之外选取channel leader , 否则我们将会面临对于之前的leader产生的offsets覆盖的风险 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false # 关闭基于时间的日志保留方式并且避免分段到期。 - KAFKA_LOG_RETENTION_MS=-1 - GODEBUG=netdns=go volumes: # 存储数据与日志. - ./data/kafka1/data:/data - ./data/kafka1/data:/logs networks: default: aliases: - jicki depends_on: - zookeeper1 - zookeeper2 - zookeeper3 kafka1: container_name: kafka1 hostname: kafka1 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=2 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181 - KAFKA_MESSAGE_MAX_BYTES=103809024 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false - KAFKA_LOG_RETENTION_MS=-1 - GODEBUG=netdns=go volumes: # 存储数据与日志. - ./data/kafka1/data:/data - ./data/kafka1/data:/logs networks: default: aliases: - jicki depends_on: - zookeeper1 - zookeeper2 - zookeeper3 kafka2: container_name: kafka2 hostname: kafka2 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=3 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181 - KAFKA_MESSAGE_MAX_BYTES=103809024 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false # 关闭基于时间的日志保留方式并且避免分段到期。 - KAFKA_LOG_RETENTION_MS=-1 - GODEBUG=netdns=go volumes: # 存储数据与日志. - ./data/kafka2/data:/data - ./data/kafka2/data:/logs networks: default: aliases: - jicki depends_on: - zookeeper1 - zookeeper2 - zookeeper3 orderer0.jicki.cn: container_name: orderer0.jicki.cn image: hyperledger/fabric-orderer:1.4.0 environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default # - ORDERER_GENERAL_LOGLEVEL=error - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 #- ORDERER_GENERAL_GENESISPROFILE=AntiMothOrdererGenesis - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #- ORDERER_GENE","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:7:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"启动服务 docker-compose up -d ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:7:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"Hyperledger Fabric 创建 Channel # 上面我们创建了 cli 容器，我们可以直接进入 容器里操作 [root@localhost jicki]# docker exec -it cli bash root@0b55c64a9853:/opt/gopath/src/github.com/hyperledger/fabric/peer# # 执行 创建命令 (未启动 认证) peer channel create -c mychannel -f ./channel-artifacts/channel.tx --orderer orderer0.jicki.cn:7050 # 提示如下表示认证不通过 Error: failed to create deliver client: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: \u003cnil\u003e # 以下为启用认证 peer channel create -o orderer0.jicki.cn:7050 -c mychannel -f ./channel-artifacts/channel.tx --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输出如下: 2019-10-29 05:47:10.407 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 05:47:10.423 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 05:47:10.426 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2019-10-29 05:47:10.479 UTC [cli.common] readBlock -\u003e INFO 004 Got status: \u0026{SERVICE_UNAVAILABLE} 2019-10-29 05:47:10.483 UTC [channelCmd] InitCmdFactory -\u003e INFO 005 Endorser and orderer connections initialized 2019-10-29 05:47:10.685 UTC [cli.common] readBlock -\u003e INFO 006 Got status: \u0026{SERVICE_UNAVAILABLE} 2019-10-29 05:47:10.687 UTC [channelCmd] InitCmdFactory -\u003e INFO 007 Endorser and orderer connections initialized 2019-10-29 05:47:10.890 UTC [cli.common] readBlock -\u003e INFO 008 Received block: 0 # 创建以后生成文件 mychannel.block total 24 -rw-r--r-- 1 root root 15632 Oct 29 05:47 mychannel.block drwxr-xr-x 2 root root 4096 Oct 29 05:42 channel-artifacts drwxr-xr-x 4 root root 4096 Oct 29 05:25 crypto ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:8:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"Hyperledger Fabric 加入 Channel 我们这边有2个 peer 所以需要分别加入, 后续有多少个 peer 都需要加入到 Channel 中 # peer0.org1.jicki.cn 加入 此 channel 中，首先需要查看如下 环境变量 echo $CORE_PEER_LOCALMSPID echo $CORE_PEER_ADDRESS echo $CORE_PEER_MSPCONFIGPATH echo $CORE_PEER_TLS_ROOTCERT_FILE # 加入 channel (未开启认证) peer channel join -b mychannel.block # 加入 channel (开启认证) peer channel join -b mychannel.block -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输出如下: 2019-10-29 06:35:49.083 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:35:49.097 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:35:49.101 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2019-10-29 06:35:49.293 UTC [channelCmd] executeJoin -\u003e INFO 004 Successfully submitted proposal to join channel # peer1.org2.jicki.cn 加入 此 channel 中，这里配置一下环境变量 export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp # 加入 channel (未开启认证) peer channel join -b mychannel.block # 加入 channel (开启认证) peer channel join -b mychannel.block -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输入如下: 2019-10-29 06:36:34.577 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:36:34.595 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:36:34.599 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2019-10-29 06:36:34.717 UTC [channelCmd] executeJoin -\u003e INFO 004 Successfully submitted proposal to join channel ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:9:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"Hyperledger Fabric 实例化测试 在上面我们已经拷贝了官方的例子，在 chaincode 下, 下面我们来测试一下 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:10:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"安装智能合约 # cli 部分 ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/jicki/chaincode/go # 为 智能合约的目录 我们约定为这个目录 需要预先创建 mkdir -p /opt/jicki/data/cli/chaincode/go/ cd /opt/jicki/data/cli/chaincode/go/ # 创建以后~我们拷贝官方的 例子进来，方便后面进行合约测试 cp -r /opt/jicki/fabric/examples/chaincode/go/example0* /opt/jicki/data/cli/chaincode/go/ # 官方这里有5个例子 [root@localhost jicki]# ls -lt chaincode/go/ total 20 drwxr-xr-x 3 root root 4096 Oct 29 14:37 example01 drwxr-xr-x 3 root root 4096 Oct 29 14:37 example02 drwxr-xr-x 3 root root 4096 Oct 29 14:37 example03 drwxr-xr-x 3 root root 4096 Oct 29 14:37 example04 drwxr-xr-x 3 root root 4096 Oct 29 14:37 example05 # 如上我们挂载的地址为 github.com/hyperledger/fabric/jicki/chaincode/go # 注: 这里面的 example02 的 package 为 example02 会报错 Error: could not assemble transaction, err Proposal response was not successful, error code 500, msg failed to execute transaction 819b581ce88604e9b6651764324876f2ca7a47d7aeb7ee307f273af867a4a134: error starting container: error starting container: API error (404): oci runtime error: container_linux.go:247: starting container process caused \"exec: \\\"chaincode\\\": executable file not found in $PATH\" # 将 chaincode.go chaincode_test.go 中 package 修改成 main 然后在最下面增加 main()函数 package example02 修改为 package main # 在最后添加如下: func main() { err := shim.Start(new(SimpleChaincode)) if err != nil { fmt.Printf(\"Error starting Simple chaincode: %s\", err) } } # 安装指定合约到 所有的 peer 节点中，每个节点都必须安装一次 # 同样需要先配置变量 export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp # 安装 合约 peer chaincode install -n example2 -p github.com/hyperledger/fabric/jicki/chaincode/go/example02 -v 1.0 # 输出如下: 2019-10-29 06:43:32.933 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:43:32.947 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:43:32.955 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 003 Using default escc 2019-10-29 06:43:32.955 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 004 Using default vscc 2019-10-29 06:43:33.665 UTC [chaincodeCmd] install -\u003e INFO 005 Installed remotely response:\u003cstatus:200 payload:\"OK\" \u003e # 安装指定合约到 所有的 peer 节点中，每个节点都必须安装一次 # 同样需要先配置变量 export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp # 安装 合约 peer chaincode install -n example2 -p github.com/hyperledger/fabric/jicki/chaincode/go/example02 -v 1.0 # 输出如下: 2019-10-29 06:44:06.573 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:44:06.588 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:44:06.596 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 003 Using default escc 2019-10-29 06:44:06.596 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 004 Using default vscc 2019-10-29 06:44:06.826 UTC [chaincodeCmd] install -\u003e INFO 005 Installed remotely response:\u003cstatus:200 payload:\"OK\" \u003e ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:10:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"实例化 Chaincode 这里无论多少个 peer 节点, 实例化只需要实例化一次，就可以。 # 实例化合约 (未认证) peer chaincode instantiate -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"200\",\"B\",\"500\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.0 # 实例化合约 (已认证) peer chaincode instantiate -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"200\",\"B\",\"500\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.0 # 输出如下: 2019-10-29 06:48:30.883 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:48:30.897 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:48:30.908 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 003 Using default escc 2019-10-29 06:48:30.908 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 004 Using default vscc ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:10:2","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"操作智能合约 # query 查询方法 # 查询 A 账户里的余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"A\"]}' # 输出如下: 2019-10-29 06:49:30.885 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:49:30.901 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 200 # 可以看到 返回 200 # 查询 B 账户里的余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"B\"]}' # 输出如下: 2019-10-29 06:50:27.353 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:50:27.368 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 500 # 可以看到 返回 500 # invoke 转账方法 # 从A账户 转账 100 个币 到 B 账户 (未认证) peer chaincode invoke -C mychannel -n example2 -c '{\"Args\":[\"invoke\", \"A\", \"B\", \"100\"]}' # 从A账户 转账 100 个币 到 B 账户 (开启认证) peer chaincode invoke -C mychannel -n example2 -c '{\"Args\":[\"invoke\", \"A\", \"B\", \"100\"]}' -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输出如下: 2019-10-29 06:51:12.681 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:51:12.695 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:51:12.721 UTC [chaincodeCmd] chaincodeInvokeOrQuery -\u003e INFO 003 Chaincode invoke successful. result: status:200 # 可以看到返回 invoke successful. result: status:200 成功 # 这里再查询 A 与 B 的账户 # A 账户余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"A\"]}' # 输出如下: 2019-10-29 06:51:43.325 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:51:43.340 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 100 # B 账户余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"B\"]}' # 输出如下: 2019-10-29 06:51:55.761 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-10-29 06:51:55.775 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 600 # 查看 peer0.org1.jicki.cn 节点里 生成的容器 [root@localhost jicki]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1e3b763e3034 jicki-peer0.org2.jicki.cn-example2-1.0-3b619b4d039726a6e131bc22c779eab46f4f1325e0299f289761f6372e0fc252 \"chaincode -peer.add…\" 3 minutes ago Up 3 minutes jicki-peer0.org2.jicki.cn-example2-1.0 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:10:3","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"Hyperledger Fabric 操作命令 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:11:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"peer 命令 peer chaincode # 对链进行操作 peer channel # channel相关操作 peer logging # 设置日志级别 peer node # 启动、管理节点 peer version # 查看版本信息 upgrade 更新合约 更新合约相当于将合约重新实例化，并带有一个新的版本号。 更新合约之前，需要在所有的 peer节点 上安装(install)最新的合约，并使用新的版本号。 # 更新合约 # 首先安装(install)新的合约, 以本文为例, chaincode_example02, 初次安装版本号为 1.0 peer chaincode install -n example2 -p github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02 -v 1.1 # 更新版本为 1.1 的合约 (未开启认证) peer chaincode upgrade -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"100\",\"B\",\"50\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.1 # 更新版本为 1.1 的合约 (开启认证) peer chaincode upgrade -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"100\",\"B\",\"50\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.1 -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 旧版本的合约, 目前，fabric不支持合约的启动与暂停。要暂停或删除合约，只能到peer上手动删除容器。 # 查看 已经创建的 通道 (channel) peer channel list # 查看通道(channel) 的状态 -c(小写) 加 通道名称 peer channel getinfo -c mychannel # 查看已经 安装的 智能合约(chincode) peer chaincode list --installed # 查看已经 实例化的 智能合约(chincode) 需要使用 -C(大写) 加通道名称 peer chaincode -C mychannel list --instantiated 配置 Hyperledger Fabric balance-transfer ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:11:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"安装 node.js # 安装NodeJS curl --silent --location https://rpm.nodesource.com/setup_8.x | sudo bash - yum install -y nodejs # 验证 node -v v8.11.4 npm -v 5.6.0 # 更改 npm 源为 taobao 源 npm install node-gyp --registry=https://registry.npm.taobao.org npm install node-pre-gyp --registry=https://registry.npm.taobao.org npm install grpc --registry=https://registry.npm.taobao.org npm install --registry=https://registry.npm.taobao.org npm rebuild # 安装一下 jq yum install jq -y ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:12:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"下载源码 cd /opt/jicki git clone https://github.com/hyperledger/fabric-samples.git mv fabric-samples/balance-transfer . ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:13:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"修改配置文件 mv network-config.yaml network-config.yaml-bak # 增加 network-config 文件 vi network-config.yaml --- name: \"balance-transfer\" x-type: \"hlfv1\" description: \"Balance Transfer Network\" version: \"1.0\" channels: mychannel: orderers: - orderer0.jicki.cn peers: peer0.org1.jicki.cn: endorsingPeer: true chaincodeQuery: true ledgerQuery: true eventSource: true #peer1.org1.jicki.cn: # endorsingPeer: false # chaincodeQuery: true # ledgerQuery: true # eventSource: false peer0.org2.jicki.cn: endorsingPeer: true chaincodeQuery: true ledgerQuery: true eventSource: true #peer1.org2.jicki.cn: # endorsingPeer: false # chaincodeQuery: true # ledgerQuery: true # eventSource: false chaincodes: - mycc:v0 organizations: Org1: mspid: Org1MSP peers: - peer0.org1.jicki.cn #- peer1.org1.jicki.cn certificateAuthorities: - ca-org1 adminPrivateKey: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp/keystore/a9666f561d211e7b7cc170bfe854721431a1038b7914a67555d82dcf6b9eaaf8_sk signedCert: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp/signcerts/Admin@org1.jicki.cn-cert.pem Org2: mspid: Org2MSP peers: - peer0.org2.jicki.cn #- peer1.org2.jicki.cn certificateAuthorities: - ca-org2 adminPrivateKey: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp/keystore/d402b684b807080653511978a51fcd2326668156cd8a10fb612b80bc49c9b354_sk signedCert: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp/signcerts/Admin@org2.jicki.cn-cert.pem orderers: orderer0.jicki.cn: url: grpcs://localhost:7050 grpcOptions: ssl-target-name-override: orderer0.jicki.cn tlsCACerts: path: artifacts/channel/crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/ca.crt peers: peer0.org1.jicki.cn: # this URL is used to send endorsement and query requests url: grpcs://localhost:7051 grpcOptions: ssl-target-name-override: peer0.org1.jicki.cn tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt #peer1.org1.jicki.cn: # url: grpcs://localhost:7056 # grpcOptions: # ssl-target-name-override: peer1.org1.jicki.cn # tlsCACerts: # path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/peers/peer1.org1.jicki.cn/tls/ca.crt peer0.org2.jicki.cn: url: grpcs://localhost:8051 grpcOptions: ssl-target-name-override: peer0.org2.jicki.cn tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt #peer1.org2.jicki.cn: # url: grpcs://localhost:8056 # eventUrl: grpcs://localhost:8058 # grpcOptions: # ssl-target-name-override: peer1.org2.jicki.cn # tlsCACerts: # path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/peers/peer1.org2.jicki.cn/tls/ca.crt certificateAuthorities: ca-org1: url: https://localhost:7054 httpOptions: verify: false tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/ca/ca.org1.jicki.cn-cert.pem registrar: - enrollId: admin enrollSecret: adminpw caName: ca-org1 ca-org2: url: https://localhost:8054 httpOptions: verify: false tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/ca/ca.org2.jicki.cn-cert.pem registrar: - enrollId: admin enrollSecret: adminpw caName: ca-org2 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:14:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"拷贝证书文件 cd /opt/jicki/balance-transfer/artifacts/ mv channel channel-bak mkdir channel # 拷贝自行生成的证书文件以及tx，等文件 [root@localhost channel]# ls -lt 总用量 56 -rw-r--r-- 1 root root 281 11月 5 16:40 Org1MSPanchors.tx -rw-r--r-- 1 root root 281 11月 5 16:40 Org2MSPanchors.tx -rw-r--r-- 1 root root 346 11月 5 16:40 channel.tx -rw-r--r-- 1 root root 12479 11月 5 16:40 genesis.block -rw-r--r-- 1 root root 15407 11月 5 16:39 mychannel.block -rw-r--r-- 1 root root 645 11月 5 16:39 crypto-config.yaml -rw-r--r-- 1 root root 3859 11月 5 16:39 configtx.yaml drwxr-xr-x 4 root root 69 11月 5 16:38 crypto-config ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:15:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"启动 balance-transfer runApp.sh 脚本里包含了 启动 ca 节点 以及 peer 节点的docker-compose 如上我们已经启动过了，只需要直接运行 node app 既可，所以不需要用到此脚本 # 安装依赖 npm install # 导入环境变量 PORT=4000 HOST=192.168.168.100 # 启动 node app ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:16:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"测试 调用 API # 运行 testAPIs.sh 可全部API跑一次 # 本文中只运行了 2个 peer # 所以需要 编辑 testAPIs.sh 修改文件中 删除 peer1.org1.jicki.cn 以及 peer1.org2.jicki.cn # 修改 \"channelConfigPath\":\"../artifacts/channel/mychannel.tx\" 中 mychannel.tx 为 channel.tx ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"创建用户 # 创建用户 curl -s -X POST http://localhost:4000/users -H \"content-type: application/x-www-form-urlencoded\" -d 'username=jicki\u0026orgName=Org1' {\"success\":true,\"secret\":\"\",\"message\":\"jicki enrolled Successfully\",\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDE2MTIwNzAsInVzZXJuYW1lIjoiSmltIiwib3JnTmFtZSI6Ik9yZzEiLCJpYXQiOjE1NDE1NzYwNzB9.pQZAqkeRW7vtwvNSrSTy4SKsVt5yFGafzlWTQjjONWE\"} # 注册成功以后~~取 token 部分,配置成环境变量, 如下操作需用使用此 token ORG1_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDE2MTIwNzAsInVzZXJuYW1lIjoiSmltIiwib3JnTmFtZSI6Ik9yZzEiLCJpYXQiOjE1NDE1NzYwNzB9.pQZAqkeRW7vtwvNSrSTy4SKsVt5yFGafzlWTQjjONWE ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"创建 channel # 创建 channel # 此处 channelName 如果存在会失败~ 报如下错误 response ::{\"status\":\"BAD_REQUEST\",\"info\":\"error authorizing update: error validating ReadSet: readset expected key [Group] /Channel/Application at version 0, but got version 1\"} curl -s -X POST \\ http://localhost:4000/channels \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"channelName\":\"mychannel\", \"channelConfigPath\":\"../artifacts/channel/channel.tx\" }' # 关于 channelName 需要跟 创建 channel.tx 时的 channelID 对应，否则报如下错误: [2018-11-09 12:03:28.276] [DEBUG] Create-Channel - response ::{\"status\":\"BAD_REQUEST\",\"info\":\"Failing initial channel config creation: mismatched channel IDs: 'mychannel' != 'youchannel'\"} [2018-11-09 12:03:28.277] [ERROR] Create-Channel - !!!!!!!!! Failed to create the channel 'youchannel' !!!!!!!!! [2018-11-09 12:03:28.277] [ERROR] Create-Channel - Error: Failed to create the channel 'youchannel' ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:2","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"加入 channel # 加入 ORG1 加入 channel # 如果已加入 channel 报如下错误: [DEBUG] Join-Channel - Join Channel R E S P O N S E : [null,[{\"status\":500,\"payload\":{\"type\":\"Buffer\",\"data\":[]},\"isProposalResponse\":true}]] [ERROR] Join-Channel - Failed to join peer to the channel mychannel [ERROR] Join-Channel - Failed to join all peers to channel. cause:Failed to join peer to the channel mychannel curl -s -X POST \\ http://localhost:4000/channels/mychannel/peers \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\"] }' ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:3","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"安装 chaincode # 为 ORG1 安装 chaincode # 已存在报如下错误: [2018-11-07 16:10:44.011] [ERROR] install-chaincode - TypeError: proposalResponses.toJSON is not a function at Object.installChaincode (/opt/jicki/balance-transfer/app/install-chaincode.js:58:66) at \u003canonymous\u003e [2018-11-07 16:10:44.011] [ERROR] install-chaincode - Failed to install due to:TypeError: proposalResponses.toJSON is not a function curl -s -X POST \\ http://localhost:4000/chaincodes \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\"], \"chaincodeName\":\"mycc\", \"chaincodePath\":\"github.com/example_cc/go\", \"chaincodeType\": \"golang\", \"chaincodeVersion\":\"v0\" }' ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:4","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"实例化chaincode # 如果已经实例化 报如下错误: [2018-11-07 16:12:10.040] [ERROR] instantiate-chaincode - instantiate proposal was bad [2018-11-07 16:12:10.040] [DEBUG] instantiate-chaincode - Failed to send Proposal and receive all good ProposalResponse [2018-11-07 16:12:10.040] [ERROR] instantiate-chaincode - Failed to instantiate. cause:Failed to send Proposal and receive all good ProposalResponse curl -s -X POST \\ http://localhost:4000/channels/mychannel/chaincodes \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\"], \"chaincodeName\":\"mycc\", \"chaincodeVersion\":\"v0\", \"chaincodeType\": \"golang\", \"args\":[\"a\",\"100\",\"b\",\"200\"] }' ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:5","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"查询已创建的 channel curl -s -X GET \\ \"http://localhost:4000/channels?peer=peer0.org1.jicki.cn\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:19:29.592] [DEBUG] Query - \u003c\u003c\u003c channels \u003e\u003e\u003e [2018-11-07 16:19:29.592] [DEBUG] Query - [ 'channel id: mychannel' ] ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:6","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"查询 chaincode # %5B%22a%22%5D Escape 加密以后 等于 [\"a\"] # %5B%22b%22%5D Escape 加密以后 等于 [\"b\"] # 查询 a 的值 curl -s -X GET \\ \"http://localhost:4000/channels/mychannel/chaincodes/mycc?peer=peer0.org1.jicki.cn\u0026fcn=query\u0026args=%5B%22a%22%5D\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 输出如下: 2018-11-07 16:33:29.266] [INFO] Query - a now has 100 after the move # 查询 b 的值 curl -s -X GET \\ \"http://localhost:4000/channels/mychannel/chaincodes/mycc?peer=peer0.org1.jicki.cn\u0026fcn=query\u0026args=%5B%22b%22%5D\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 输出如下: [2018-11-07 16:34:12.378] [INFO] Query - b now has 200 after the move ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:7","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"操作交易 # 从 A账号 转账 10 到 B账号中 # 必须配置 org1 与 org2 节点的 peers 否则报错: [2018-11-07 17:17:53.267] [ERROR] invoke-chaincode - The invoke chaincode transaction was invalid, code:ENDORSEMENT_POLICY_FAILURE [2018-11-07 17:17:53.268] [ERROR] invoke-chaincode - Error: The invoke chaincode transaction was invalid, code:ENDORSEMENT_POLICY_FAILURE [2018-11-07 17:17:53.268] [ERROR] invoke-chaincode - Failed to invoke chaincode. cause:Error: The invoke chaincode transaction was invalid, code:ENDORSEMENT_POLICY_FAILURE curl -s -X POST \\ http://localhost:4000/channels/mychannel/chaincodes/mycc \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\", \"peer0.org2.jicki.cn\"], \"fcn\":\"move\", \"args\":[\"a\",\"b\",\"10\"] }' # 输出如下: POST invoke chaincode on peers of Org1 and Org2 Transaction ID is 70a5157704b950cca09a6a46f5be7fca61355b43ed83f3d9a5b633f3e38b3619 ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:8","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"查询 chainInfo curl -s -X GET \\ \"http://localhost:4000/channels/mychannel?peer=peer0.org1.jicki.cn\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:14:55.648] [DEBUG] Query - { height: Long { low: 5, high: 0, unsigned: true }, currentBlockHash: ByteBuffer { buffer: \u003cBuffer 08 05 12 20 c0 c4 f3 65 a3 8a 66 d4 1e ff a4 45 3e 1c e6 2b 90 d3 38 4f 18 3f d5 b3 38 76 0e 26 30 32 e0 f9 1a 20 da 35 eb d4 1b 11 b2 7f 1a 07 c5 30 ... \u003e, offset: 4, markedOffset: -1, limit: 36, littleEndian: true, noAssert: false }, previousBlockHash: ByteBuffer { buffer: \u003cBuffer 08 05 12 20 c0 c4 f3 65 a3 8a 66 d4 1e ff a4 45 3e 1c e6 2b 90 d3 38 4f 18 3f d5 b3 38 76 0e 26 30 32 e0 f9 1a 20 da 35 eb d4 1b 11 b2 7f 1a 07 c5 30 ... \u003e, offset: 38, markedOffset: -1, limit: 70, littleEndian: true, noAssert: false } } ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:9","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"查询已安装 chaincode curl -s -X GET \\ \"http://localhost:4000/chaincodes?peer=peer0.org1.jicki.cn\u0026type=installed\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:17:01.758] [DEBUG] Query - \u003c\u003c\u003c Installed Chaincodes \u003e\u003e\u003e [2018-11-07 16:17:01.758] [DEBUG] Query - name: mycc, version: v0, path: github.com/example_cc/go ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:10","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["fabric"],"content":"查询实例化 chaincode curl -s -X GET \\ \"http://localhost:4000/chaincodes?peer=peer0.org1.jicki.cn\u0026type=instantiated\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:18:06.494] [DEBUG] Query - \u003c\u003c\u003c Installed Chaincodes \u003e\u003e\u003e [2018-11-07 16:18:06.494] [DEBUG] Query - name: mycc, version: v0, path: github.com/example_cc/go ","date":"2019-10-29","objectID":"/hyperledger-fabric-1.4/:17:11","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.4","uri":"/hyperledger-fabric-1.4/"},{"categories":["kubernetes"],"content":"Kata to docker 集成","date":"2019-09-27","objectID":"/kata-to-docker/","tags":["kubernetes","docker"],"title":"Kata to docker 集成","uri":"/kata-to-docker/"},{"categories":["kubernetes"],"content":"Kata 简介 Katacontainer 是 OpenStack 基金会于 2017 KubeCon 峰会上正式发布，在2018年5月份 OpenStack 温哥华峰会上对外发布1.0版本，并且在那届峰会上还有好几个关于 katacontainer 的演讲。我对 KataContainers 的具体实现原理不清楚，只知道它是一个轻量虚拟机实现，可以无缝地与容器生态系统(实现 OCI 接口)进行集成。 katacontainer(包括 Google 的 gVisor)的主要优势是能够对接遗留系统以及提供比传统容器更好的安全性。我在本文后面的实验也可以证明，katacontainer 可以与传统的 runc 并存，为不同性质的应用负载提供了强大的灵活性。 安装 Docker # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 安装 docker yum -y install docker-ce ","date":"2019-09-27","objectID":"/kata-to-docker/:0:0","tags":["kubernetes","docker"],"title":"Kata to docker 集成","uri":"/kata-to-docker/"},{"categories":["kubernetes"],"content":"配置 docker # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min # restart the docker process if it exits prematurely Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target # 创建额外配置目录 mkdir -p /etc/systemd/system/docker.service.d/ # 添加额外配置 vi /etc/systemd/system/docker.service.d/docker-options.conf [Service] Environment=\"DOCKER_OPTS=\\ --registry-mirror=http://b438f72b.m.daocloud.io \\ --exec-opt native.cgroupdriver=systemd \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" # 重新读取配置，启动 docker, 并设置自动启动 systemctl daemon-reload systemctl start docker systemctl enable docker # 验证 安装 docker info Client: Debug Mode: false Server: Containers: 0 Running: 0 Paused: 0 Stopped: 0 Images: 0 Server Version: 19.03.2 Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true Logging Driver: json-file Cgroup Driver: systemd Plugins: Volume: local Network: bridge host ipvlan macvlan null overlay Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 894b81a4b802e4eb2a91d1ce216b8817763c29fb runc version: 425e105d5a03fabd737a126ad93d62a9eeede87f init version: fec3683 Security Options: seccomp Profile: default Kernel Version: 4.4.194-1.el7.elrepo.x86_64 Operating System: CentOS Linux 7 (Core) OSType: linux Architecture: x86_64 CPUs: 2 Total Memory: 3.859GiB Name: localhost ID: 4JQ2:WFL6:YSBJ:TFS4:E5H4:3TGD:2CY4:FRYC:7DCC:5RX2:E5SO:MJQW Docker Root Dir: /opt/docker Debug Mode: false Registry: https://index.docker.io/v1/ Labels: Experimental: false Insecure Registries: 127.0.0.0/8 Registry Mirrors: http://b438f72b.m.daocloud.io/ Live Restore Enabled: false 安装 Kata # 下载 yum 源 curl http://download.opensuse.org/repositories/home:/katacontainers:/releases:/x86_64:/master/CentOS_7/home:katacontainers:releases:x86_64:master.repo -o /etc/yum.repos.d/katacontainers.repo # 更新yum 缓存 yum makecache # 安装 kata yum -y install kata-runtime kata-proxy kata-shim # 检查安装,并确认硬件支持 kata-runtime kata-check # 输出 最后一句 System can currently create Kata Containers 证明可以正常支持。 ","date":"2019-09-27","objectID":"/kata-to-docker/:1:0","tags":["kubernetes","docker"],"title":"Kata to docker 集成","uri":"/kata-to-docker/"},{"categories":["kubernetes"],"content":"配置 kata # 拷贝 kata 配置文件 cp /usr/share/defaults/kata-containers/configuration.toml /etc/kata-containers/configuration.toml # 添加一个 config 配置 docker 运行 kata # 这里配置 默认还是使用 docker 的 runc 作为容器 也可以将 --default-runtime 配置为 kata # 运行容器的时候使用 --runtime kata-runtime 指定 使用 kata-runntime # 如: docker run -d --name kata-nginx -p 80:80 --runtime kata-runtime nginx:alpine vi /etc/systemd/system/docker.service.d/kata-containers.conf [Service] Type=simple ExecStart= ExecStart=/usr/bin/dockerd -D --default-runtime runc --add-runtime kata-runtime=/usr/bin/kata-runtime # 生效配置,重启 docker systemctl daemon-reload systemctl restart docker ","date":"2019-09-27","objectID":"/kata-to-docker/:2:0","tags":["kubernetes","docker"],"title":"Kata to docker 集成","uri":"/kata-to-docker/"},{"categories":["kubernetes"],"content":"测试 kata # 启动容器 docker run -d --name kata-nginx -p 80:80 --runtime kata-runtime nginx:alpine # 查看运行的 kata-runntime kata-runtime list ","date":"2019-09-27","objectID":"/kata-to-docker/:3:0","tags":["kubernetes","docker"],"title":"Kata to docker 集成","uri":"/kata-to-docker/"},{"categories":["docker","kubernetes"],"content":"ChatOps RocketChat Hubot","date":"2019-07-24","objectID":"/chatops-rocket-hubot/","tags":["kubernetes","docker"],"title":"ChatOps RocketChat Hubot","uri":"/chatops-rocket-hubot/"},{"categories":["docker","kubernetes"],"content":"ChatOps 什么是ChatOps ChatOps 以 IM（沟通平台）为中心，通过一系列的机器人去对接后台的各种服务，我们只需在聊天窗口中与机器人对话，即可与后台服务进行交互，整个工作的展开就像是使唤一个智能助手那样简单自然。 ChatOps的好处 方便简单友好。只需要在IM前台与预设好的机器人对话即可完成与后台工具、系统的交互，在移动环境下无须再与众多复杂的工具直接对接，大大提升移动办公的可行性。 DevOps文化。用与机器人对话这种简单的方式降低 DevOps 的接受门槛，让这种自动化办公的理念更容易地扩展到团队的每一个角落。 公开透明。所有的工作消息都在同一个聊天平台中沉淀并公开给所有相关成员，可以消除沟通壁垒，工作历史有迹可循，团队合作更加顺畅。 上下文共享。减少因工作台切换等对消息的截断，保证消息的完整性，让工作承接有序，各角色，各工具都成为完成工作流中的一环，打造真正流畅的工作体验。 ChatOps 的实践经验 ChatOps 主要由四个部分组成：自动化的理念、一个沟通承载平台、一系列连接人与工具的机器人，以及一些后台工具和服务（基础设施）。它不仅可以应用在技术团队中，还可以发展为适应不同种类团队的方法模型，这也是 ChatOps 这个概念提出的背景之一。随着全行业的发展和人力成本的攀升，ChatOps 也可以说是应用于全行业的 DevOps。 GitHub 工作 Github 的员工 60% 都是在家里远程办公的， 新员工入职培训就是查看老员工跟其他人的聊天信息，感受别人是如何进行工作。 Github 使用了 机器人执行大部分的运维脚本，将各种重复、以及繁琐的工作交给机器人去处理。 在聊天工具(IM)中和同事聊天、查看运维信息、查看监控等, 所有相关人员都可以看到具体的情况。 聊天工具(IM)中保留了相关员工的操作记录，执行了那些操作，做过什么事情，都会记录下来。 ","date":"2019-07-24","objectID":"/chatops-rocket-hubot/:0:0","tags":["kubernetes","docker"],"title":"ChatOps RocketChat Hubot","uri":"/chatops-rocket-hubot/"},{"categories":["docker","kubernetes"],"content":"hubot hubot 是github 开源的一款 机器人 https://hubot.github.com/ docker 部署 docker 略 .. RocketChat RocketChat 是一款开源的 聊天室 官方 github https://github.com/RocketChat/Rocket.Chat 官方主页 https://rocket.chat/ vi docker-compose.yaml version: '2' services: mongo: image: mongo:4 hostname: mongo container_name: mongo restart: always volumes: - ./data/rocket/mongo/data:/data/db:z command: mongod --smallfiles --oplogSize 1024 --replSet rs1 --storageEngine=mmapv1 rocket: image: rocketchat/rocket.chat hostname: rocket container_name: rocket restart: always environment: - DEPLOY_METHOD=docker - NODE_ENV=production - MONGO_URL=mongodb://mongo:27017/rocketchat - MONGO_OPLOG_URL=mongodb://mongo:27017/local?replSet=rs01 - HOME=/tmp - PORT=3000 - ROOT_URL=http://localhost:3000 - Accounts_AvatarStorePath=/app/uploads volumes: - ./data/rocket/uploads:/app/uploads ports: - \"3000:3000\" depends_on: - mongo # 这里先启动一下 mongo docker-compose up -d mongo # 这里mongo 还需要执行一些设置 docker exec -d mongo bash -c 'echo -e \"replication:\\n replSetName: \\\"rs01\\\"\" | tee -a /etc/mongod.conf \u0026\u0026 mongo --eval \"printjson(rs.initiate())\"' # 启动 rocketchat docker-compose up -d rocket # 查看 rocketchat 的 logs 确保没报错 docker logs rocket ➔ System ➔ startup ➔ +----------------------------------------------+ ➔ | SERVER RUNNING | ➔ +----------------------------------------------+ ➔ | | ➔ | Rocket.Chat Version: 1.2.1 | ➔ | NodeJS Version: 8.11.4 - x64 | ➔ | MongoDB Version: 4.0.10 | ➔ | MongoDB Engine: mmapv1 | ➔ | Platform: linux | ➔ | Process Port: 3000 | ➔ | Site URL: http://localhost:3000 | ➔ | ReplicaSet OpLog: Enabled | ➔ | Commit Hash: 7475d7628a | ➔ | Commit Branch: HEAD | ➔ | | ➔ +----------------------------------------------+ # 配置 rocket ，增加 hubot 机器人 # 增加 hubot 机器人 # 在 rocket-chat 中默认已经有一个 rocket.cat 的机器人账号 # 只需要修改一下密码就可以 # 创建 hubot-rocketchat vi docker-compose.yaml hubot: image: rocketchat/hubot-rocketchat hostname: hubot container_name: hubot restart: always environment: - ROCKETCHAT_URL=http://192.168.168.11:3000 - ROCKETCHAT_ROOM='' - LISTEN_ON_ALL_PUBLIC=true - ROCKETCHAT_USER=rocket.cat - ROCKETCHAT_PASSWORD=123456 #- ROCKETCHAT_AUTH=password - BOT_NAME=rocket.cat # Scrpits 可到这里查看 https://www.npmjs.com/search?q=keywords:hubot-scripts - EXTERNAL_SCRIPTS=hubot-pugme,hubot-help volumes: # Scrpits 目录，自己编写的脚本放于此处既可 - ./data/rocket/scripts:/home/hubot/scripts # 启动机器人 docker-compose up -d hubot # 编辑信息 cd ./data/rocket/scripts # 里面有太多例子了，清除掉只留下如下: vi example.coffee module.exports = (robot) -\u003e robot.catchAll (res) -\u003e res.send \"对不起，我不认识这条命令，请查看 help\" robot.hear /hi/i, (res) -\u003e res.reply \"hello\" robot.hear /吃饭了吗/, (res) -\u003e res.send '猫只吃鱼' # 修改了任何脚本都需要重启机器人 docker restart hubot 简单测试: ","date":"2019-07-24","objectID":"/chatops-rocket-hubot/:1:0","tags":["kubernetes","docker"],"title":"ChatOps RocketChat Hubot","uri":"/chatops-rocket-hubot/"},{"categories":["docker","kubernetes"],"content":"添加 hubot 插件 hubot-script-shellcmd 是执行 shell 命令的一个插件 插件官方 github https://github.com/coderofsalvation/hubot-script-shellcmd # 在 docker-compose.yaml 的 EXTERNAL_SCRIPTS 中增加 hubot-script-shellcmd # 将这个路径挂载到我们本地 方便修改 environment: - EXTERNAL_SCRIPTS=hubot-pugme,hubot-help,hubot-script-shellcmd volumes: - ./data/rocket/bash:/home/hubot/bash # 创建一下目录 mkdir -p ./data/rocket/bash/handlers # 增加完毕以后重启 hubot 机器人 docker restart hubot # 重启以后，将生成的 bash 文件拷贝到我们的目录 # 这里一定要拷贝，否则执行会卡住 cd ./data/rocket/bash docker cp hubot:/home/hubot/node_modules/hubot-script-shellcmd/bash/handler . cd ./data/rocket/bash/handlers docker cp hubot:/home/hubot/node_modules/hubot-script-shellcmd/bash/handlers/helloworld . docker cp hubot:/home/hubot/node_modules/hubot-script-shellcmd/bash/handlers/update . 测试shellcmd 创建 bash cd ./data/rocket/bash/handlers # 新建一个 bash ,并且授权必须有执行权限 vi date #!/bin/bash time=$(date \"+%Y-%m-%d %H:%M:%S\") echo \"${time}\" exit 0 # 创建脚本不需要重启 hubot 机器人 操作docker 基于 shellcmd 插件，我们可以操作一下 docker # 添加如下挂载, 并且这里镜像中运行用户必须使用 root user: root volumes: - /usr/bin/docker:/usr/bin/docker - /var/run/docker.sock:/var/run/docker.sock:z # 添加 一个 bash cd ./data/rocket/bash/handlers vi dockerversion #!/bin/bash docker version exit 0 ","date":"2019-07-24","objectID":"/chatops-rocket-hubot/:2:0","tags":["kubernetes","docker"],"title":"ChatOps RocketChat Hubot","uri":"/chatops-rocket-hubot/"},{"categories":["docker","kubernetes"],"content":"Coreos kube-prometheus 监控","date":"2019-07-22","objectID":"/kube-prometheus/","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"kube-prometheus kube-prometheus 既 Prometheus Operator kube-prometheus creates/configures/manages Prometheus clusters atop Kubernetes Components included in this package: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter Prometheus Adapter for Kubernetes Metrics APIs kube-state-metrics Grafana # 要求 Kubernetes cluster of version \u003e=1.8.0 # 说明 我们一般监控系统基本上来说都是读取系统上面的 /proc 和 /sys 的信息 prometheus 的 node-exporter 服务配置选项 --path.procfs 和 --path.sysfs 指定从这俩选项的值的proc和sys读取,容器跑node-exporter 只需要挂载宿主机的 /proc 和 /sys 到容器fs的某个路径挂载属性设置为readonly 后用这两个选项指定即可读取到. Prometheus 与 Prometheus-operator 的区别 Prometheus - 是主动去拉取数据的, 所以在k8s里pod因为调度的原因导致pod的ip会发生变化,人工不可能去维持. Prometheus-operator - 是自主定义的CRD资源以及Controller的实现，Prometheus Operator这个controller有RBAC权限下去负责监听 这些自定义资源的变化，并且根据这些资源的定义自动化的完成如Prometheus Server自身以及配置的自动化管理工作 在Kubernetes中我们使用Deployment、DamenSet，StatefulSet来管理应用Workload，使用Service，Ingress来管理应用的访问方式，使用ConfigMap和Secret来管理应用配置。我们在集群中对这些资源的创建，更新，删除的动作都会被转换为事件(Event)，Kubernetes的Controller Manager负责监听这些事件并触发相应的任务来满足用户的期望。这种方式我们成为声明式，用户只需要关心应用程序的最终状态，其它的都通过Kubernetes来帮助我们完成，通过这种方式可以大大简化应用的配置管理复杂度。 Prometheus Operator 架构图如下: ","date":"2019-07-22","objectID":"/kube-prometheus/:0:0","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"CustomResourceDefinitions Prometheus 定义了所需要的Prometheus部署, Kind: Prometheus 类型中, 所描述部署的 Prometheus Server 集群. ServiceMonitor 以声明方式指定应如何监控服务组数据, yaml中通过Selector来依据 Labels 选取对应的Service的 endpoints，并让 Prometheus Server 通过 Service 进行拉取（拉）指标资料(也就是metrics信息), metrics信息要在http的url输出符合metrics格式的信息. PrometheusRule 它定义了一个所需的Prometheus规则文件，该文件可以由包含Prometheus警报和记录规则的Prometheus实例加载。 Alertmanager 通过 kind: Alertmanager 自定义资源来描述信息，再由 Operator 依据描述内容部署 Alertmanager 集群。 ","date":"2019-07-22","objectID":"/kube-prometheus/:1:0","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"部署 kube-Prometheus 官方 github https://github.com/coreos/kube-prometheus 下载官方的yaml 按照以下顺序导入: namespaces wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/00namespace-namespace.yaml prometheus-operator wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-serviceAccount.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-0alertmanagerCustomResourceDefinition.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-0podmonitorCustomResourceDefinition.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-0prometheusCustomResourceDefinition.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-0prometheusruleCustomResourceDefinition.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-0servicemonitorCustomResourceDefinition.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-clusterRole.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-clusterRoleBinding.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-deployment.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-service.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/0prometheus-operator-serviceMonitor.yaml prometheus-adapter wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-serviceAccount.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-roleBindingAuthReader.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-apiService.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-clusterRole.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-clusterRoleAggregatedMetricsReader.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-clusterRoleBinding.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-clusterRoleBindingDelegator.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-clusterRoleServerResources.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-configMap.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-deployment.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/prometheus-adapter-service.yaml node-exporter wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/node-exporter-serviceAccount.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/node-exporter-clusterRole.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/node-exporter-clusterRoleBinding.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/node-exporter-daemonset.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/node-exporter-service.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/node-exporter-serviceMonitor.yaml kube-state-metrics wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/kube-state-metrics-serviceAccount.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/manifests/kube-state-metrics-role.yaml wget https://raw.githubusercontent.com/coreos/kube-prometheus/master/ma","date":"2019-07-22","objectID":"/kube-prometheus/:2:0","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"导入服务 # 这里有一个 k8s.gcr.io 的镜像,修改一下 sed -i 's/k8s\\.gcr\\.io/registry\\.cn-hangzhou\\.aliyuncs\\.com\\/google_containers/g' kube-state-metrics-deployment.yaml kubectl apply -f . ","date":"2019-07-22","objectID":"/kube-prometheus/:2:1","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"查看服务 [root@kubernetes-1 prometheus]# kubectl get all -n monitoring NAME READY STATUS RESTARTS AGE pod/alertmanager-main-0 2/2 Running 0 26m pod/alertmanager-main-1 2/2 Running 0 25m pod/alertmanager-main-2 2/2 Running 0 16m pod/grafana-558647b59-k88zr 1/1 Running 0 26m pod/kube-state-metrics-b96d568bf-2tdds 4/4 Running 0 10m pod/node-exporter-fqn64 2/2 Running 0 26m pod/node-exporter-gp5wr 2/2 Running 0 26m pod/node-exporter-m7kkx 2/2 Running 0 26m pod/node-exporter-mlzp2 2/2 Running 0 26m pod/node-exporter-pfnfz 2/2 Running 0 26m pod/prometheus-adapter-74fc6495d7-m5fqr 1/1 Running 0 26m pod/prometheus-k8s-0 3/3 Running 1 27m pod/prometheus-k8s-1 3/3 Running 1 27m pod/prometheus-operator-69bd579bf9-94qw4 1/1 Running 0 28m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/alertmanager-main ClusterIP 10.254.26.190 \u003cnone\u003e 9093/TCP 26m service/alertmanager-operated ClusterIP None \u003cnone\u003e 9093/TCP,6783/TCP 26m service/grafana ClusterIP 10.254.63.91 \u003cnone\u003e 3000/TCP 26m service/kube-state-metrics ClusterIP None \u003cnone\u003e 8443/TCP,9443/TCP 26m service/node-exporter ClusterIP None \u003cnone\u003e 9100/TCP 26m service/prometheus-adapter ClusterIP 10.254.40.87 \u003cnone\u003e 443/TCP 26m service/prometheus-k8s ClusterIP 10.254.60.94 \u003cnone\u003e 9090/TCP 27m service/prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 27m service/prometheus-operator ClusterIP None \u003cnone\u003e 8080/TCP 28m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/node-exporter 5 5 5 5 5 kubernetes.io/os=linux 26m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/grafana 1/1 1 1 26m deployment.apps/kube-state-metrics 1/1 1 1 26m deployment.apps/prometheus-adapter 1/1 1 1 26m deployment.apps/prometheus-operator 1/1 1 1 28m NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-558647b59 1 1 1 26m replicaset.apps/kube-state-metrics-54c8c98cb5 0 0 0 11m replicaset.apps/kube-state-metrics-b96d568bf 1 1 1 10m replicaset.apps/kube-state-metrics-f7bd7b6cd 0 0 0 26m replicaset.apps/prometheus-adapter-74fc6495d7 1 1 1 26m replicaset.apps/prometheus-operator-69bd579bf9 1 1 1 28m NAME READY AGE statefulset.apps/alertmanager-main 3/3 26m statefulset.apps/prometheus-k8s 2/2 27m ","date":"2019-07-22","objectID":"/kube-prometheus/:2:2","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"配置web入口 # 查看 svc [root@kubernetes-1 prometheus]# kubectl get svc -n monitoring NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE alertmanager-main ClusterIP 10.254.26.190 \u003cnone\u003e 9093/TCP 44m alertmanager-operated ClusterIP None \u003cnone\u003e 9093/TCP,6783/TCP 44m grafana ClusterIP 10.254.63.91 \u003cnone\u003e 3000/TCP 44m kube-state-metrics ClusterIP None \u003cnone\u003e 8443/TCP,9443/TCP 45m node-exporter ClusterIP None \u003cnone\u003e 9100/TCP 45m prometheus-adapter ClusterIP 10.254.40.87 \u003cnone\u003e 443/TCP 45m prometheus-k8s ClusterIP 10.254.60.94 \u003cnone\u003e 9090/TCP 45m prometheus-operated ClusterIP None \u003cnone\u003e 9090/TCP 45m prometheus-operator ClusterIP None \u003cnone\u003e 8080/TCP 47m # 首先配置 prometheus 的 webui 配置一个 ingress vi prometheus-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: prometheus-ingress namespace: monitoring spec: rules: - host: prometheus.jicki.cn http: paths: - backend: serviceName: prometheus-k8s servicePort: 9090 # 配置 grafana 的 ingress vi grafana-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: grafana-ingress namespace: monitoring spec: rules: - host: grafana.jicki.cn http: paths: - backend: serviceName: grafana servicePort: 3000 # 配置 alertmanager 的 ingress vi alertmanager-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: alertmanager-ingress namespace: monitoring spec: rules: - host: alertmanager.jicki.cn http: paths: - backend: serviceName: alertmanager-main servicePort: 9093 ","date":"2019-07-22","objectID":"/kube-prometheus/:2:3","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"查看 Prometheus ","date":"2019-07-22","objectID":"/kube-prometheus/:2:4","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"查看 grafana 初始账号密码都是 admin , 初始登录以后会让你修改密码 ","date":"2019-07-22","objectID":"/kube-prometheus/:2:5","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"查看 alertmanager ","date":"2019-07-22","objectID":"/kube-prometheus/:2:6","tags":["kubernetes","docker"],"title":"Coreos kube-prometheus 监控","uri":"/kube-prometheus/"},{"categories":["docker","kubernetes"],"content":"Elasticsearch Log-pilot Kibana","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"Kubernetes ELK 旧 ELK = Elasticsearch + Logstash + Kibana 新 ELK = Elasticsearch + Log-pilot + Kibana ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:0:0","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"Log-pilot 介绍 Log-Pilot 是阿里开源的一个智能容器日志采集工具，它不仅能够高效便捷地将容器日志采集输出到多种存储日志后端，同时还能够动态地发现和采集容器内部的日志文件。 Github : https://github.com/AliyunContainerService/log-pilot 在 kubernetes 下, Log-Pilot 可以依据环境变量 aliyun_logs_$name = $path 动态地生成日志采集配置文件, 其中包含两个变量: $name 是我们自定义的一个字符串, 它在不同的场景下指代不同的含义, 在本场景中, 将日志采集到 ElasticSearch 的时候, 这个 $name 表示的是 Index。 $path 支持两种输入形式, stdout 和容器内部日志文件的路径, 对应日志标准输出和容器内的日志文件。 第一种 约定关键字 stdout 表示的是采集容器的标准输出日志, 如本例中我们要采集 Nginx 容器日志, 那么我们通过配置标签 aliyun_logs_nginx-logs=stdout 来采集 nginx 标准输出日志。 第二种 容器内部日志文件的路径, 也支持通配符的方式, 通过配置环境变量 aliyun_logs_access=/var/log/nginx/*.log 来采集 Nginx 容器内部的日志。当然如果你不想使用 aliyun 这个关键字, Log-Pilot 也提供了环境变量 PILOT_LOG_PREFIX 可以指定自己的声明式日志配置前缀, 比如 PILOT_LOG_PREFIX: \"aliyun,custom\"。 Log-Pilot 还支持多种日志解析格式, 通过 aliyun_logs_$name_format=\u003cformat\u003e 标签就可以告诉 Log-Pilot 在采集日志的时候, 同时以什么样的格式来解析日志记录, 支持的格式包括：none、json、csv、nginx、apache2 和 regxp。 Log-Pilot 同时支持自定义 tag 我们可以在环境变量里配置 aliyun_logs_$name_tags=\"K1=V1,K2=V2\" 那么在采集日志的时候也会将 K1=V1 和 K2=V2 采集到容器的日志输出中。自定义 tag 可帮助您给日志产生的环境打上 tag 方便进行日志统计、日志路由和日志过滤。 ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:1:0","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"部署 ELK 环境介绍: CentOS 7 x64 Docker-ce 18.09.6 Kubernetes v1.14.3 Kubernetes 5 * Node ( 3 Master 2 Node ) ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:2:0","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"部署 Elasticsearch 使用谷歌官方的 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch # 先下载 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/fluentd-elasticsearch/es-service.yaml # 这里修改的东西有点 # 创建 一个 es-statefulset.yaml 文件 : vi es-statefulset.yaml # RBAC authn and authz apiVersion: v1 kind: ServiceAccount metadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: elasticsearch-logging labels: k8s-app: elasticsearch-logging addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: - \"\" resources: - \"services\" - \"namespaces\" - \"endpoints\" verbs: - \"get\" --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: kube-system name: elasticsearch-logging labels: k8s-app: elasticsearch-logging addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: elasticsearch-logging namespace: kube-system apiGroup: \"\" roleRef: kind: ClusterRole name: elasticsearch-logging apiGroup: \"\" --- # Elasticsearch deployment itself apiVersion: apps/v1 kind: StatefulSet metadata: name: elasticsearch-logging namespace: kube-system labels: k8s-app: elasticsearch-logging version: v6.6.1 addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/cluster-service: \"true\" spec: serviceName: elasticsearch-logging replicas: 3 selector: matchLabels: k8s-app: elasticsearch-logging version: v6.7.2 template: metadata: labels: k8s-app: elasticsearch-logging version: v6.7.2 spec: tolerations: - effect: NoSchedule key: node-role.kubernetes.io/master serviceAccountName: elasticsearch-logging containers: - image: jicki/elasticsearch:v6.6.1 name: elasticsearch-logging resources: # need more cpu upon initialization, therefore burstable class limits: cpu: 1000m requests: cpu: 100m ports: - containerPort: 9200 name: db protocol: TCP - containerPort: 9300 name: transport protocol: TCP securityContext: capabilities: add: - IPC_LOCK - SYS_RESOURCE volumeMounts: - name: elasticsearch-logging mountPath: /data env: - name: \"http.host\" value: \"0.0.0.0\" - name: \"network.host\" value: \"_eth0_\" - name: \"bootstrap.memory_lock\" value: \"false\" - name: \"cluster.name\" value: \"docker-cluster\" - name: \"NAMESPACE\" valueFrom: fieldRef: fieldPath: metadata.namespace volumes: - name: elasticsearch-logging emptyDir: {} # Elasticsearch requires vm.max_map_count to be at least 262144. # If your OS already sets up this number to a higher value, feel free # to remove this init container. initContainers: - image: alpine:3.6 command: [\"/sbin/sysctl\", \"-w\", \"vm.max_map_count=262144\"] name: elasticsearch-logging-init securityContext: privileged: true # 创建服务 [root@kubernetes-1 elk]# kubectl apply -f . service/elasticsearch-logging created serviceaccount/elasticsearch-logging created clusterrole.rbac.authorization.k8s.io/elasticsearch-logging created clusterrolebinding.rbac.authorization.k8s.io/elasticsearch-logging created statefulset.apps/elasticsearch-logging created # 查看服务 [root@kubernetes-1 elk]# kubectl get statefulset,svc,pods -n kube-system |grep elasticsearch statefulset.apps/elasticsearch-logging 3/3 119s service/elasticsearch-logging ClusterIP 10.254.25.18 \u003cnone\u003e 9200/TCP 17h pod/elasticsearch-logging-0 1/1 Running 0 119s pod/elasticsearch-logging-1 1/1 Running 0 116s pod/elasticsearch-logging-2 1/1 Running 0 114s ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:2:1","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"部署 Log-pilot log-pilot 会在每个 node 里创建一个 服务，用于接收 docker 的日志。 # 创建一个 log-pilot.yaml 文件 vi log-pilot.yaml apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: log-pilot namespace: kube-system labels: k8s-app: log-pilot spec: updateStrategy: type: RollingUpdate template: metadata: labels: k8s-app: log-pilot spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule serviceAccountName: elasticsearch-logging containers: - name: log-pilot image: registry.cn-hangzhou.aliyuncs.com/acs/log-pilot:0.9.7-filebeat env: - name: \"LOGGING_OUTPUT\" value: \"elasticsearch\" - name: \"ELASTICSEARCH_HOSTS\" value: \"elasticsearch-logging:9200\" - name: \"NODE_NAME\" valueFrom: fieldRef: fieldPath: spec.nodeName volumeMounts: - name: sock mountPath: /var/run/docker.sock - name: logs mountPath: /var/log/filebeat - name: state mountPath: /var/lib/filebeat - name: root mountPath: /host readOnly: true - name: localtime mountPath: /etc/localtime securityContext: capabilities: add: - SYS_ADMIN terminationGracePeriodSeconds: 30 volumes: - name: sock hostPath: path: /var/run/docker.sock - name: logs hostPath: path: /var/log/filebeat - name: state hostPath: path: /var/lib/filebeat - name: root hostPath: path: / - name: localtime hostPath: path: /etc/localtime # 创建 log-pilot 服务 [root@kubernetes-1 elk]# kubectl apply -f log-pilot.yaml daemonset.extensions/log-pilot created # 查看服务 [root@kubernetes-1 elk]# kubectl get pods -n kube-system -o wide |grep log-pilot log-pilot-59f22 1/1 Running 0 30s 10.254.91.122 kubernetes-4 \u003cnone\u003e \u003cnone\u003e log-pilot-9hqxj 1/1 Running 0 30s 10.254.105.92 kubernetes-1 \u003cnone\u003e \u003cnone\u003e log-pilot-mvjcr 1/1 Running 0 30s 10.254.90.151 kubernetes-2 \u003cnone\u003e \u003cnone\u003e log-pilot-nml7n 1/1 Running 0 30s 10.254.96.72 kubernetes-5 \u003cnone\u003e \u003cnone\u003e log-pilot-wr766 1/1 Running 0 30s 10.254.101.27 kubernetes-3 \u003cnone\u003e \u003cnone\u003e ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:2:2","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"部署 Kibana 使用谷歌官方的 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch # 下载 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/fluentd-elasticsearch/kibana-deployment.yaml wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/fluentd-elasticsearch/kibana-service.yaml # 编辑 kibana-deployment.yaml # 这里我们是使用 ingress 暴露端口 # SERVER_BASEPATH 这个变量 注释掉 # 如果我们使用 kubectl proxy 访问，就不要注释 # 并且我们添加一个 CLUSTER_NAME - name: CLUSTER_NAME value: docker-cluster # - name: SERVER_BASEPATH # value: /api/v1/namespaces/kube-system/services/kibana-logging/proxy # 配置一个 ingress vi kibana-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana-ingress namespace: kube-system spec: rules: - host: kibana.jicki.cn http: paths: - backend: serviceName: kibana-logging servicePort: 5601 # 创建 Kibana 服务 [root@kubernetes-1 elk]# kubectl apply -f kibana-deployment.yaml deployment.apps/kibana-logging created [root@kubernetes-1 elk]# kubectl apply -f kibana-service.yaml service/kibana-logging created [root@kubernetes-1 elk]# kubectl apply -f kibana-ingress.yaml ingress.extensions/kibana-ingress created # 查看服务 [root@kubernetes-1 elk]# kubectl get pods,svc,ingress -n kube-system|grep kibana pod/kibana-logging-5bf6bbccf9-jv4d2 1/1 Running 0 4m31s service/kibana-logging ClusterIP 10.254.31.15 \u003cnone\u003e 5601/TCP 4m26s ingress.extensions/kibana-ingress kibana.jicki.cn 80 47s ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:2:3","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"创建一个 Nginx 服务 创建一个标准的 Nginx 服务, 主要是 env 添加了两种,上面我们介绍过 一 是 基于 docker stdout 输出日志 (aliyun_logs_catalina) 二 是 基于 程序指定输出到指定的目录中 (aliyun_logs_access) ( elasticsearch )环境变量中的 $name 表示 Index，这里 $name 即是 catalina 和 access, 这里用于 Kibana 查询日志 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: tolerations: - key: \"node-role.kubernetes.io/master\" effect: \"NoSchedule\" containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent env: - name: aliyun_logs_nginx-log value: \"stdout\" # - name: aliyun_logs_access # value: \"/var/log/nginx/*.log\" ports: - containerPort: 80 name: http --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: name: nginx # 创建 nginx 服务 [root@kubernetes-1 nginx]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-dm created service/nginx-svc created # 查看服务 [root@kubernetes-1 nginx]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-64c8d4bb84-5fj62 1/1 Running 0 3s nginx-dm-64c8d4bb84-79hwc 1/1 Running 0 3s nginx-dm-64c8d4bb84-zhp49 1/1 Running 0 3s # 查看 elasticsearch 里面的索引, 既 index # 查看 elasticsearch svc [root@kubernetes-1 elk]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch-logging ClusterIP 10.254.25.18 \u003cnone\u003e 9200/TCP 17h # 查看索引 [root@kubernetes-1 elk]# curl 10.254.25.18:9200/_cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open .kibana_1 Q8rg1kKsTxSuApXL2SJSqg 1 1 1 0 7.4kb 3.7kb green open nginx-log-2019.07.02 TuXuOOONTL-aIZt2wq_hHQ 5 1 20 0 319kb 166.2kb ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:2:4","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"配置 Kibana 查询 上面已经部署 Kibana 服务, 并且通过 ingress 暴露了 http 服务. # 访问 kibana 配置 Java 日志多行匹配 ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:2:5","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"filebeat 配置 # 修改镜像内的模板文件 添加 multiline # 源码中路径 log-pilot/assets/filebeat/filebeat.tpl # 镜像中路径 /pilot/filebeat.tpl {{range .configList}} - type: log enabled: true paths: - {{ .HostDir }}/{{ .File }} multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}' multiline.negate: true multiline.match: after multiline.timeout: 10s multiline.max_lines: 10000 ","date":"2019-07-02","objectID":"/kuebrnetes-new-elk/:3:0","tags":["kubernetes","docker"],"title":"Elasticsearch Log-pilot Kibana","uri":"/kuebrnetes-new-elk/"},{"categories":["docker","kubernetes"],"content":"Jenkins Kubernetes Pipeline","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"1. Jenkins Kubernetes Jenkins作为最为流行的持续集成工具，有着丰富的用户群、强大的扩展能力、丰富的插件，是目前最为常见的CI/CD工具。在Jenkins 加强其Pipeline功能后，更是可以通过丰富的step库，实现各种复杂的流程。同时随着Docker的流行，Jenkins内也增加了对Docker的支持，可以实现容器内流程的执行, 随着 kubernetes 集群的流行，Jenkins 也有对于 Kubernetes 的插件，实现 pods 动态资源伸缩，构建时创建 pods, 完成时删除 pods, 既用既销。 2. Jenkins 这里 默认是已经部署好了 kubernetes 集群. ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:0:0","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.1 环境说明 节点标识 hostname 系统 IP K8S-Master-1 kubernetes-1 CentOS 7x64 192.168.168.11 K8S-Master-2 kubernetes-2 CentOS 7x64 192.168.168.12 K8S-Master-3 kubernetes-3 CentOS 7x64 192.168.168.13 K8S-Node-1 kubernetes-4 CentOS 7x64 192.168.168.14 K8S-Node-2 kubernetes-5 CentOS 7x64 192.168.168.15 ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:1:0","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.2 配置 NFS 共享存储 在部署 Jenkins 之前，我们需要先创建一个 共享存储，用于存放 Jenkins 生成的数据文件，以及配置文件。 这里面我们选择 K8S-Node-2 这台服务器部署 NFS 服务。 ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:2:0","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.2.1 安装NFS软件 # 安装 NFS 服务的软件 [root@kubernetes-5 ~]# yum -y install nfs-utils rpcbind 已安装: nfs-utils.x86_64 1:1.3.0-0.61.el7 rpcbind.x86_64 0:0.2.0-47.el7 作为依赖被安装: gssproxy.x86_64 0:0.7.0-21.el7 keyutils.x86_64 0:1.5.8-3.el7 libbasicobjects.x86_64 0:0.1.1-32.el7 libcollection.x86_64 0:0.7.0-32.el7 libevent.x86_64 0:2.0.21-4.el7 libini_config.x86_64 0:1.3.1-32.el7 libnfsidmap.x86_64 0:0.25-19.el7 libpath_utils.x86_64 0:0.2.1-32.el7 libref_array.x86_64 0:0.1.5-32.el7 libtirpc.x86_64 0:0.2.4-0.15.el7 libverto-libevent.x86_64 0:0.2.5-4.el7 quota.x86_64 1:4.01-17.el7 quota-nls.noarch 1:4.01-17.el7 tcp_wrappers.x86_64 0:7.6-77.el7 完毕！ # 其他所有 k8s 用于调度的服务器都必须安装 nfs 客户端 [root@kubernetes-1 ~]# yum -y install nfs-utils [root@kubernetes-2 ~]# yum -y install nfs-utils [root@kubernetes-3 ~]# yum -y install nfs-utils [root@kubernetes-4 ~]# yum -y install nfs-utils ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:2:1","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.2.2 配置 NFS 在 NFS 服务器（K8S-Node-2）中创建 目录 /opt/nfs/jenkins [root@kubernetes-5 ~]# mkdir -p /opt/nfs/jenkins # 修改配置文件 [root@kubernetes-5 ~]# vi /etc/exports 增加 /opt/nfs/jenkins 192.168.168.0/24(rw,sync,no_root_squash) # 启动 NFS 服务 [root@kubernetes-5 ~]# systemctl enable rpcbind.service [root@kubernetes-5 ~]# systemctl enable nfs-server.service [root@kubernetes-5 ~]# systemctl start rpcbind.service [root@kubernetes-5 ~]# systemctl start nfs-server.service [root@kubernetes-5 ~]# systemctl status rpcbind.service [root@kubernetes-5 ~]# systemctl status nfs-server.service # 查看服务 [root@kubernetes-5 ~]# showmount -e 192.168.168.15 Export list for 192.168.168.15: /opt/nfs/jenkins 192.168.168.0/24 ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:2:2","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.2.3 配置 NFS Client Provisioner NFS Client Provisioner 是一个自动配置NFS的服务, 它使现有和已经配置的NFS服务器，通过持久卷声明支持Kubernetes持久卷的动态配置。 持久卷命名规则是 ${namespace}-${pvcName}-${pvName} # 下载官方 yaml 文件 wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/rbac.yaml wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/deployment.yaml wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/class.yaml # 修改 deployment.yaml 文件，配置对应 NFS IP 与 path apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.168.15 - name: NFS_PATH value: /opt/nfs/jenkins volumes: - name: nfs-client-root nfs: server: 192.168.168.15 path: /opt/nfs/jenkins # 导入 服务 文件 [root@kubernetes-1 nfs-client]# kubectl apply -f . storageclass.storage.k8s.io/managed-nfs-storage created serviceaccount/nfs-client-provisioner created deployment.extensions/nfs-client-provisioner created serviceaccount/nfs-client-provisioner unchanged clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created # 查看 storageclass [root@kubernetes-1 nfs-client]# kubectl get storageclass NAME PROVISIONER AGE managed-nfs-storage fuseim.pri/ifs 39s # 查看 pods [root@kubernetes-1 nfs-client]# kubectl get pods NAME READY STATUS RESTARTS AGE nfs-client-provisioner-cc5f6f9f6-d8njv 1/1 Running 0 53s ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:2:3","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.3 部署Jenkins 官方 github https://github.com/jenkinsci/kubernetes-plugin # 下载 Jenkins 官方的 yml 文件 wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/jenkins.yml wget https://raw.githubusercontent.com/jenkinsci/kubernetes-plugin/master/src/main/kubernetes/service-account.yml # 修改 jenkins.yml 文件, 修改后文件如下: # 主要是修改 volumeClaimTemplates: # Ingress 部分的域名 # jenkins --- apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: jenkins labels: name: jenkins spec: serviceName: jenkins replicas: 1 updateStrategy: type: RollingUpdate template: metadata: name: jenkins labels: name: jenkins spec: terminationGracePeriodSeconds: 10 serviceAccountName: jenkins containers: - name: jenkins image: jenkins/jenkins:lts-alpine imagePullPolicy: Always ports: - containerPort: 8080 - containerPort: 50000 resources: limits: cpu: 1 memory: 1Gi requests: cpu: 0.5 memory: 500Mi env: - name: LIMITS_MEMORY valueFrom: resourceFieldRef: resource: limits.memory divisor: 1Mi - name: JAVA_OPTS # value: -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=1 -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 value: -Xmx$(LIMITS_MEMORY)m -XshowSettings:vm -Dhudson.slaves.NodeProvisioner.initialDelay=0 -Dhudson.slaves.NodeProvisioner.MARGIN=50 -Dhudson.slaves.NodeProvisioner.MARGIN0=0.85 -Dorg.csanchez.jenkins.plugins.kubernetes.clients.cacheExpiration=60 volumeMounts: - name: jenkins-home mountPath: /var/jenkins_home livenessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes readinessProbe: httpGet: path: /login port: 8080 initialDelaySeconds: 60 timeoutSeconds: 5 failureThreshold: 12 # ~2 minutes securityContext: fsGroup: 1000 volumeClaimTemplates: - metadata: name: jenkins-home annotations: volume.beta.kubernetes.io/storage-class: \"managed-nfs-storage\" spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 10Gi --- apiVersion: v1 kind: Service metadata: name: jenkins spec: # type: LoadBalancer selector: name: jenkins # ensure the client ip is propagated to avoid the invalid crumb issue when using LoadBalancer (k8s \u003e=1.7) #externalTrafficPolicy: Local ports: - name: http port: 80 targetPort: 8080 protocol: TCP - name: agent port: 50000 protocol: TCP --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: jenkins annotations: nginx.ingress.kubernetes.io/ssl-redirect: \"true\" kubernetes.io/tls-acme: \"true\" # \"413 Request Entity Too Large\" uploading plugins, increase client_max_body_size nginx.ingress.kubernetes.io/proxy-body-size: 50m nginx.ingress.kubernetes.io/proxy-request-buffering: \"off\" # For nginx-ingress controller \u003c 0.9.0.beta-18 ingress.kubernetes.io/ssl-redirect: \"true\" # \"413 Request Entity Too Large\" uploading plugins, increase client_max_body_size ingress.kubernetes.io/proxy-body-size: 50m ingress.kubernetes.io/proxy-request-buffering: \"off\" spec: rules: - http: paths: - path: / backend: serviceName: jenkins servicePort: 80 host: jenkins.jicki.cn tls: - hosts: - jenkins.jicki.cn secretName: tls-jenkins # 导入文件 [root@kubernetes-1 jenkins]# kubectl apply -f . statefulset.apps/jenkins created service/jenkins created ingress.extensions/jenkins created serviceaccount/jenkins created role.rbac.authorization.k8s.io/jenkins created rolebinding.rbac.authorization.k8s.io/jenkins created # 查看 pods , svc, ingress [root@kubernetes-1 jenkins]# kubectl get pods NAME READY STATUS RESTARTS AGE jenkins-0 1/1 Running 0 2m42s [root@kubernetes-1 jenkins]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE jenkins ClusterIP 10.254.28.67 \u003cnone\u003e 80/TCP,50000/TCP 2m52s [root@kubernetes-1 jenkins]# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE jenkins jenkins.jicki.cn 80, 443 3m2s # 测试访问 https://jenkins.jicki.cn # 查询密码 [root@kubernetes-1 jenkins]# kubectl logs pods/jenkins-0 ..... ******************************","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:3:0","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.4 配置 Jenkins Jenkins 支持 kubernetes 需要 安装 kubernetes plugin 这个插件 ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:4:0","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.4.1 安装kubernetes 插件 系统管理 –\u003e 插件管理 –\u003e 可选插件 –\u003e kubernetes plugin ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:4:1","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.4.2 配置 kubernetes 插件 系统管理 –\u003e 系统设置 –\u003e 云 –\u003e 新增一个云 –\u003e kubernetes # 配置说明: 1. Name 处默认为 kubernetes，也可以修改为其他名称，如果这里修改了，下边在执行 Job 时指定 podTemplate() 参数 cloud 为其对应名称，否则会找不到，cloud 默认值取：kubernetes 2. Kubernetes URL 处我填写了 https://kubernetes.default 这里我填写了 Kubernetes Service 对应的 DNS 记录，通过该 DNS 记录可以解析成该 Service 的 Cluster IP，注意：也可以填写 https://kubernetes.default.svc.cluster.local , 或者直接填写外部 Kubernetes 的地址 https://\u003cClusterIP\u003e:\u003cPorts\u003e。 3. Jenkins URL 处我填写了 http://jenkins.default，跟上边类似，也是使用 Jenkins Service 对应的 DNS 记录. 连接测试 ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:4:2","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.5 Pipeline pipeline 是一套jenkins官方提供的插件，它可以用来在jenkins 中实现和集成连续交付。 如上图所示 pipeline 定义了一个流程，流程中完成过一个 CI/CD 流程的步骤. Pipeline 我们可以分为 step , node , stage node: 是 pipleline里groovy的一个概念，node可以给定参数用来选择 agent，node里的steps将会运行在node选择的agent上, job里可以有多个node，将job的steps按照需求运行在不同的 agent 中. stage: 是 pipeline里groovy里引入的一个虚拟的概念，是一些step的集合，通过stage我们可以将job的所有steps划分为不同的stage，使得整个job像管道一样更容易维护. step: 理解为 job, 是 pipeline 的一个任务,是最小的单位. ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:5:0","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.5.1 创建 Pipeline Pipeline 也是 Jenkins 的一个插件，需要先安装 pipeline 插件, 安装 jenkins 的时候选择默认插件安装，会默认安装这个插件. 新建任务 –\u003e 输入任务名称 –\u003e 选择 流水线(pipeline) # 在 流水线 中选择 Pipeline script def label = \"mypod-${UUID.randomUUID().toString()}\" podTemplate(label: 'label', cloud: 'kubernetes') { node('label') { stage('Run shell') { sh 'echo hello world' } } } # 添加多个 stage def label = \"mypod-${UUID.randomUUID().toString()}\" podTemplate(label: 'label', cloud: 'kubernetes') { node('label') { stage('Git Pull') { sh 'echo Git Pull Job' } stage('Build Job') { sh 'echo Gradle Job Build' } stage('Docker Build') { sh 'echo Dokcer Build Image' } stage('Push Docker Imges') { sh 'echo Docker Push Imges To Registry' } stage('Update Deployment') { sh 'echo Update Deployment Image' } stage('Send Message') { sh 'echo Send Message to Wechat' } } } ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:6:0","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.5.2 基于自己的 slave 镜像构建 在使用 pipeline 的时候默认会启动 官方的 image jenkins/jnlp-slave:alpine 去运行我们的流程 当然我们也可以构建自己的 slave 镜像，指定运行. # 首先需要下载官方的 jenkins-slave 文件 wget https://raw.githubusercontent.com/jenkinsci/docker-jnlp-slave/master/jenkins-slave # 这里使用我个人的镜像 # 镜像基于 alpine 软件包含 oracle 1.8 与 gradle 4.5 构建 # 镜像名称为 jicki/slave:3.16 # 创建 dockerfile vi dockerfile FROM jicki/slave:3.16 COPY jenkins-slave /usr/local/bin/jenkins-slave USER root RUN chmod +x /usr/local/bin/jenkins-slave USER jenkins ENTRYPOINT [\"jenkins-slave\"] # 构建镜像 docker build -t=\"jicki/jenkins-jnlp\" . # 配置 Pipeline def label = \"mypod-${UUID.randomUUID().toString()}\" podTemplate(label: 'label', cloud: 'kubernetes', containers: [ containerTemplate( name: 'jnlp', image: 'jicki/jenkins-jnlp', alwaysPullImage: true, args: '${computer.jnlpmac} ${computer.name}'), ], volumes: [ hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock'), ],) { node('label') { stage('Task-1') { stage('show Java version') { sh 'java -version' } stage('show Gradle version') { sh 'gradle -version' } } } } ","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:6:1","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"2.5.3 完整的 java 项目 通过 Pipeline Scm 插件， CheckOut git 代码，然后执行 Gradle 打包 # 以下为配置的 Pipeline def label = \"mypod-${UUID.randomUUID().toString()}\" podTemplate(label: 'label', cloud: 'kubernetes', containers: [ containerTemplate( name: 'jnlp', image: 'jicki/jenkins-jnlp', alwaysPullImage: true, args: '${computer.jnlpmac} ${computer.name}'), ], volumes: [ hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock'), ],) { node('label') { stage('Task-1') { stage('Git CheckOut') { checkout([$class: 'GitSCM', branches: [[name: '*/master']], doGenerateSubmoduleConfigurations: false, extensions: [[$class: 'CleanBeforeCheckout']], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '4f419afa-8f63-45d8-85af-66381f17b924', url: 'http://192.168.168.14:3000/jicki/demo.git']]]) echo 'Checkout' } stage('show Java version') { sh 'java -version' } stage('show Gradle version') { sh 'gradle -version' } stage('Gradle Build') { echo 'Gradle Building' sh 'gradle clean build jar --stacktrace --debug' echo 'Show Build jar' sh 'ls -lt /home/jenkins/workspace/${JOB_NAME}/build/libs/*.jar' } } } } # 这里面备注一下 Pipeline SCM 插件 checkout([$class: 'GitSCM', branches: [[name: '*/master']], doGenerateSubmoduleConfigurations: false, extensions: [[$class: 'CleanBeforeCheckout']], submoduleCfg: [], userRemoteConfigs: [[credentialsId: '4f419afa-8f63-45d8-85af-66381f17b924', url: 'http://192.168.168.14:3000/jicki/demo.git']]]) # credentialsId 标签 这里是 Jenkins 里创建的一个 凭据, 这个凭据 的用户名密码为 Git 代码库的用户密码. # 创建完毕以后, 编辑里可以查看到 Id 下面增加一个 docker build 流程 # 编辑以上 jicki/jenkins-jnlp 镜像, 增加 docker # dockerfile FROM frolvlad/alpine-java ENV GRADLE_VERSION=5.4.1 \\ GRADLE_HOME=/opt/gradle \\ GRADLE_FOLDER=/root/.gradle \\ DOCKER_CHANNEL=stable \\ DOCKER_VERSION=18.09.6 \\ HOME=/home/jenkins USER root # Change to tmp folder WORKDIR /tmp # Download and extract gradle to opt folder RUN wget https://downloads.gradle.org/distributions/gradle-${GRADLE_VERSION}-bin.zip \\ \u0026\u0026 unzip gradle-${GRADLE_VERSION}-bin.zip -d /opt \\ \u0026\u0026 ln -s /opt/gradle-${GRADLE_VERSION} /opt/gradle \\ \u0026\u0026 rm -f gradle-${GRADLE_VERSION}-bin.zip \\ \u0026\u0026 ln -s /opt/gradle/bin/gradle /usr/bin/gradle \\ \u0026\u0026 apk add --no-cache libstdc++ git libltdl wget \\ ca-certificates \\ \u0026\u0026 echo 'hosts: files dns' \u003e /etc/nsswitch.conf \\ \u0026\u0026 mkdir -p $GRADLE_FOLDER \\ \u0026\u0026 addgroup -S -g 10000 jenkins \\ \u0026\u0026 adduser -D -S -G jenkins -u 10000 jenkins LABEL Description=\"This is a base image, which provides the Jenkins agent executable (slave.jar)\" Vendor=\"Jenkins project\" Version=\"3.19\" ARG VERSION=3.19 ARG AGENT_WORKDIR=/home/jenkins/agent RUN apk add --no-cache --virtual .build-deps \\ curl RUN curl --create-dirs -sSLo /usr/share/jenkins/slave.jar https://repo.jenkins-ci.org/public/org/jenkins-ci/main/remoting/${VERSION}/remoting-${VERSION}.jar \\ \u0026\u0026 chmod 755 /usr/share/jenkins \\ \u0026\u0026 chmod 644 /usr/share/jenkins/slave.jar \\ \u0026\u0026 apk del .build-deps ENV AGENT_WORKDIR=${AGENT_WORKDIR} RUN mkdir /home/jenkins/.jenkins \u0026\u0026 mkdir -p ${AGENT_WORKDIR} # Mark as volume VOLUME $GRADLE_FOLDER VOLUME /home/jenkins/.jenkins VOLUME ${AGENT_WORKDIR} RUN set -eux; \\ \\ # this \"case\" statement is generated via \"update.sh\" apkArch=\"$(apk --print-arch)\"; \\ case \"$apkArch\" in \\ # amd64 x86_64) dockerArch='x86_64' ;; \\ # arm32v6 armhf) dockerArch='armel' ;; \\ # arm32v7 armv7) dockerArch='armhf' ;; \\ # arm64v8 aarch64) dockerArch='aarch64' ;; \\ *) echo \u003e\u00262 \"error: unsupported architecture ($apkArch)\"; exit 1 ;;\\ esac; \\ \\ if ! wget -O docker.tgz \"https://download.docker.com/linux/static/${DOCKER_CHANNEL}/${dockerArch}/docker-${DOCKER_VERSION}.tgz\"; then \\ echo \u003e\u00262 \"error: failed to download 'docker-${DOCKER_VERSION}' from '${DOCKER_CHANNEL}' for '${dockerArch}'\"; \\ exit 1; \\ fi; \\ \\ tar --extract \\ --file docker.tgz \\ --strip-components 1 \\ --directory /usr/local/bin/ \\ ; \\ rm docker.tgz; \\ \\ dockerd --version; \\ docker --version # Down Docker File RUN wget https://raw.githubusercontent.com/docker-library/docker/cdcff675cecbae122cbae49ed6e17fa78bb6116a/18.09/modprobe.sh","date":"2019-06-26","objectID":"/kubernetes-jenkins-pipeline/:6:2","tags":["kubernetes","docker"],"title":"Jenkins Kubernetes Pipeline","uri":"/kubernetes-jenkins-pipeline/"},{"categories":["docker","kubernetes"],"content":"GitLab CI/CD Docker","date":"2019-06-20","objectID":"/gitlab-ci/","tags":["kubernetes","docker"],"title":"GitLab CI/CD Docker","uri":"/gitlab-ci/"},{"categories":["docker","kubernetes"],"content":"Gitlab CI 介绍 GitLab CI 是 GitLab 为了提升其在软件开发工程中作用，完善 DevOps 理念所加入的 CI/CD 基础功能。可以便捷的融入软件开发环节中。通过 GitLab CI 可以定义完善的 CI/CD Pipeline。 GitLab CI 是默认包含在 GitLab 中的，如果代码在 GitLab 进行托管，可以很容易的进行集成 GitLab CI 的前端界面比较美观，容易被人接受 构建日志相对完整，容易追踪错误 使用 YAML 进行配置，任何人都可以很方便的使用 Gitlab 名词解释 gitlab 中的名词，我们在 .gitlab-ci.yaml 中会经常使用到 1. Pipeline Pipeline 相当于一个构建任务，里面可以包含多个流程，如依赖安装、编译、测试、部署等, 任何提交或者 Merge Request 的合并都可以触发 Pipeline. 2. Stages Stage 表示构建的阶段，即 Pipeline 中的包含的流程. 所有 Stages 按顺序执行，即当一个 Stage 完成后，下一个 Stage 才会开始. 任一 Stage 失败，后面的 Stages 将永不会执行，Pipeline 整个过程失败. 只有当所有 Stages 完成后，Pipeline 才会成功. 3. Jobs Job 是 Stage 中的任务. 相同 Stage 中的 Jobs 会并行执行. 任一 Job 失败，那么 Stage 失败，Pipeline 失败. 相同 Stage 中的 Jobs 都执行成功时，该 Stage 成功. Gitlab-ce 搭建 Gitlab 支持本地部署, 支持 docker 部署, 支持 kubernetes 部署. 为了方便还是使用 docker 部署最简单, 使用 docker-compose 直接启动. # 创建 gitlab 的 docker-compose.yaml 文件 version: '2' services: gitlab: image: gitlab/gitlab-ce restart: always container_name: gitlab hostname: gitlab environment: GITLAB_OMNIBUS_CONFIG: | external_url 'http://gitlab.jicki.cn' ports: - '80:80' - '443:443' - '8022:22' volumes: - './data/gitlab/config:/etc/gitlab' - './data/gitlab/logs:/var/log/gitlab' - './data/gitlab/data:/var/opt/gitlab' # 使用 docker-compose up -d 启动服务 # 查看启动的服务 [root@localhost compose]# docker-compose ps Name Command State Ports -------------------------------------------------------------------------------------------------------- gitlab /assets/wrapper Up (healthy) 0.0.0.0:8022-\u003e22/tcp, 0.0.0.0:443-\u003e443/tcp, 0.0.0.0:80-\u003e80/tcp 浏览器 http://gitlab.jicki.cn 访问 gitlab 首次访问会提示配置 初始密码，最少8位 # 创建一个 用户，作为后续 gitlab-ci 的项目关联 用户. # 创建一个 Groups, 作为后续 整体团队关联 Project 项目. # 添加 上面创建的用户到此 Groups 中. # 创建一个 Project 项目. 生成java项目 使用 https://start.spring.io/ 生成一个基于 Gradle 的java 项目 # 将生成的项目 上传到 Gitlab 的 TestProject 项目中 # 生成的项目 [root@localhost testproject]# ll 总用量 28 -rw-r--r-- 1 root root 446 6月 19 06:32 build.gradle drwxr-xr-x 3 root root 21 6月 19 06:32 gradle -rwxr-xr-x 1 root root 5305 6月 19 06:32 gradlew -rw-r--r-- 1 root root 2269 6月 19 06:32 gradlew.bat -rw-r--r-- 1 root root 338 6月 19 06:32 HELP.md -rw-r--r-- 1 root root 11 6月 19 14:34 README.md -rw-r--r-- 1 root root 96 6月 19 06:32 settings.gradle drwxr-xr-x 4 root root 30 6月 19 06:32 src # 修改 build.gradle , 主要添加 dependencies 下的几项 buildscript { repositories { mavenCentral() } dependencies { classpath 'org.springframework.boot:spring-boot-gradle-plugin:1.5.21.RELEASE' } } plugins { id 'java' } apply plugin: 'org.springframework.boot' group = 'java' version = '0.0.1-SNAPSHOT' sourceCompatibility = '1.8' repositories { mavenCentral() } dependencies { compile 'org.springframework.boot:spring-boot-starter' compile 'org.springframework.boot:spring-boot-starter-web' compile 'org.springframework.boot:spring-boot-starter-thymeleaf' testCompile 'org.springframework.boot:spring-boot-starter-test' } # 在 src/main/java/java/TestProject 目录下 # 创建一个 HomeController.java 文件 , 内容如下: package java.java.TestProject; import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; @Controller public class HomeController { @RequestMapping(\"/\") public String home(){ return \"index\"; } } # 在 src/main/resources/templates 目录下 # 创建一个 index.html 文件, 内容如下: \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"/\u003e \u003ctitle\u003eTitle\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eTest Java Gradle project\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e # 上传到 gitlab 中 [root@localhost testproject]# git add -A [root@localhost testproject]# git commit -m \"add java project\" [root@localhost testproject]# git push Username for 'http://gitlab.jicki.cn': jicki Password for 'http://jicki@gitlab.jicki.cn': Counting objects: 36, done. Delta compression using up to 4 threads. Compressing objects: 100% (15/15), done. Writing objects: 100% (20/20), 50.06 KiB | 0 bytes/s, done. Total 20 (delta 4), reused 0 (delta 0) To http://gitlab.jicki.cn/java/testproject.git 9a25098..d95ec5f master -\u003e master # Gitlab 中项目 Project 内容如下: GitLab CI 配置 Gitlab CI 的使用 需要使用 Runner 服务, Runner 主要负责CI任务的执行 这里 Runn","date":"2019-06-20","objectID":"/gitlab-ci/:0:0","tags":["kubernetes","docker"],"title":"GitLab CI/CD Docker","uri":"/gitlab-ci/"},{"categories":["docker","kubernetes"],"content":"注册 Runner Runner 注册以后必须要使用命令进行 激活，才能连接到gitlab-ce 服务里面去 # 在注册 Runner 之前，我们需要在 gitlab 里面查看 Runner 的 Token # 一会我们注册的时候需要使用到这个 Token # 进入容器注册 Runner [root@localhost compose]# docker exec -it gitlab-runner gitlab-runner register # 输出如下: Runtime platform arch=amd64 os=linux pid=19 revision=ac2a293c version=11.11.2 Running in system-mode. Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/): http://gitlab.jicki.cn Please enter the gitlab-ci token for this runner: pr5VscoaY2fC_8WiSdc7 Please enter the gitlab-ci description for this runner: [d1f701bae725]: TestProject Please enter the gitlab-ci tags for this runner (comma separated): JavaProject Registering runner... succeeded runner=pr5Vscoa Please enter the executor: parallels, shell, docker+machine, kubernetes, docker, docker-ssh, ssh, virtualbox, docker-ssh+machine: docker Please enter the default Docker image (e.g. ruby:2.1): alpine Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! # Runner registered successfully # 登录 gitlab 后台查看 注册的 Runner 服务 # 配置完成以后~查看 config.toml 生成的配置文件 # 我这里增加了一些配置注释说明 # 配置信息的文档地址 https://docs.gitlab.com/runner/configuration/advanced-configuration.html [root@localhost config]# cat config.toml // 限制同时运行多少个 jobs , 配置为0 为不限制 concurrent = 1 // 配置多就检查一下 jobs,最低为3,设置3以下，也为3. check_interval = 0 [session_server] //jobs 完成以后保持 会话 的时间,默认为 1800 (30分钟). session_timeout = 1800 // 以下为 注册 runner 时填写的信息 [[runners]] name = \"TestProject\" url = \"http://gitlab.jicki.cn\" token = \"V5tiX7JbKmhJx2gSDDcb\" executor = \"docker\" [runners.custom_build_dir] [runners.docker] tls_verify = false image = \"alpine\" privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [\"/cache\"] shm_size = 0 [runners.cache] [runners.cache.s3] [runners.cache.gcs] # 以上 config.toml 为默认生成的配置文件 # 以下为修改以后，的 config.toml # 这里主要修改了一些 runner 中需要使用到的配置 # runners 中添加 # // builds_dir 构建将存储在所选执行程序的上下文中的目录 # builds_dir = \"/gitlab/runner-builds\" # // cache_dir 构建缓存的目录将存储在所选执行程序 # cache_dir = \"/gitlab/runner-cache\" # 由于我们 Runner 使用docker 运行，所以以上目录配置，需要额外增加配置 volumes 这个配置 # volumes = [\"/data/gitlab-runner:/gitlab\",\"/var/run/docker.sock:/var/run/docker.sock\",\"/data/gradle:/data/gradle\",\"/data/sonar_cache:/root/.sonar\"] # extra_hosts 为额外的 dns 配置, 如果有dns 服务器就不需要 # extra_hosts = [\"gitlab.jicki.cn:192.168.168.102\"] concurrent = 1 check_interval = 0 [session_server] session_timeout = 1800 [[runners]] name = \"TestProject\" url = \"http://gitlab.jicki.cn\" token = \"V5tiX7JbKmhJx2gSDDcb\" executor = \"docker\" builds_dir = \"/gitlab/runner-builds\" cache_dir = \"/gitlab/runner-cache\" [runners.custom_build_dir] [runners.docker] tls_verify = false image = \"alpine\" privileged = false disable_entrypoint_overwrite = false oom_kill_disable = false disable_cache = false volumes = [\"/opt/data/gitlab-runner:/gitlab\",\"/var/run/docker.sock:/var/run/docker.sock\",\"/opt/data/gradle:/root/.gradle\",\"/opt/data/sonar_cache:/root/.sonar\"] extra_hosts = [\"gitlab.jicki.cn:192.168.168.102\"] shm_size = 0 [runners.cache] [runners.cache.s3] [runners.cache.gcs] # 修改完配置以后, 删除容器，再重新启动既可. # gitlab-runner 容器为无状态的服务. 创建初始镜像 本项目语言为 java , 使用docker 运行 java 项目的时候，需要一个初始化的环境，这里预先构建一个 alpine 基于 openjdk 的初始镜像, dockerfile 如下: 初始镜像需要预先构建，然后上传到 image 仓库中，方便以后各node pull 下来. FROM alpine ENV JAVA_HOME /usr/lib/jvm/java-1.8-openjdk ENV PATH $PATH:/usr/lib/jvm/java-1.8-openjdk/jre/bin:/usr/lib/jvm/java-1.8-openjdk/bin RUN apk add --update bash curl tar wget ca-certificates unzip \\ openjdk8 font-adobe-100dpi ttf-dejavu fontconfig \\ \u0026\u0026 rm -rf /var/cache/apk/* CMD [\"bash\"] # 这里将镜像构建为 jicki/openjdk:1.8-alpine [root@localhost compose]# docker build -t=\"jicki/openjdk:1.8-alpine\" . [root@localhost compose]# docker run --rm jicki/openjdk:1.8-alpine java -version openjdk version \"1.8.0_212\" OpenJDK Runtime Environment (IcedTea 3.12.0) (Alpine 8.212.04-r0) OpenJDK 64-Bit Server VM (build 25.212-b04, mixed mode) 创建项目镜像 项目镜像按照每个项目运行参数不同会有不同，也可以创建一个基于","date":"2019-06-20","objectID":"/gitlab-ci/:1:0","tags":["kubernetes","docker"],"title":"GitLab CI/CD Docker","uri":"/gitlab-ci/"},{"categories":["docker","kubernetes","devops"],"content":"Drone 1.0 with Gogs CI/CD","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"Drone搭建的私有CI/CD平台 Drone 是基于 Go语言开发的一款用于 CI/CD DevOps自动化平台, 它基于 Docker 配置以及运行. 官方 github : https://github.com/drone/drone 官方文档: https://docs.drone.io/ ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:0:0","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"环境说明 IP 系统 Kernel docker 版本 docker-compose 版本 192.168.168.102 CentOS 7 4.4.181 18.09.6 1.24.0 ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:1:0","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"安装 Drone docker 与 docker-compose 安装就略过了。 Drone 使用 docker-compose 直接安装既可。 Drone 支持三种数据库默认为 sqllite3 , 还支持 Mysql 与 Postgres 这里就使用 mysql 这里关闭 firewalld 与 selinux ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:0","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"mysql 的 compose version: '2' services: mysql: image: mysql:5.7 hostname: mysql container_name: mysql restart: always volumes: - ./data/mysql/data:/var/lib/mysql - ./data/mysql/logs:/opt/local/mysql/logs - ./data/mysql/binlog:/opt/local/mysql/binlog - /etc/localtime:/etc/localtime environment: - MYSQL_ROOT_PASSWORD=123456 - TZ=Asia/Shanghai ports: - \"192.168.168.102:3306:3306\" ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:1","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"创建一个 Gogs gogs: image: gogs/gogs hostname: gogs container_name: gogs restart: always volumes: - /etc/localtime:/etc/localtime - ./data/gogs:/data ports: - \"192.168.168.102:3000:3000\" depends_on: - mysql # 登录 mysql , 创建 gogs 以及 drone 数据库 mysql -uroot -p123456 -h 127.0.0.1 mysql\u003e select version(); +-----------+ | version() | +-----------+ | 5.7.26 | +-----------+ 1 row in set (0.00 sec) mysql\u003e CREATE DATABASE IF NOT EXISTS gogs CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; Query OK, 1 row affected (0.00 sec) mysql\u003e create database drone; Query OK, 1 row affected (0.00 sec) mysql\u003e show databases; +--------------------+ | Database | +--------------------+ | information_schema | | drone | | gogs | | mysql | | performance_schema | | sys | +--------------------+ 6 rows in set (0.00 sec) ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:2","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"配置 gogs # 使用浏览器 访问 http://192.168.168.102:3000 # 这里配置相关数据库信息 并将localhost 修改为 192.168.168.102 # 登录 gogs 以后，创建一个测试的仓库 创建新的仓库: 拥有者: jicki 仓库名称: drone 仓库描述: 测试 drone 仓库 .gitignore: Actionscript 授权许可: Apache License 2.0 自述文档: Default 使用选定的文件和模板初始化仓库 ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:3","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"配置 drone 的compose Drone 支持常见的Git仓库，例如 Github,Gitlab, Bitbucket以及Gogs等。 这里使用 Gogs, 不同的 git 仓库 配置有所不同。 # 创建一个 secret 用于 Agent 与 Server 间的通讯 [root@localhost ~]# openssl rand -hex 16 dfde64837533976ee7dbf4a87e03ad1f services: drone-server: image: drone/drone:latest hostname: drone-server container_name: drone-server restart: always environment: - DRONE_LOGS_DEBUG=true - DRONE_GIT_ALWAYS_AUTH=false - DRONE_AGENTS_ENABLED=true - DRONE_GOGS_SERVER=http://192.168.168.102:3000 - DRONE_RPC_SECRET=dfde64837533976ee7dbf4a87e03ad1f - DRONE_RUNNER_CAPACITY=2 - DRONE_SERVER_HOST=192.168.168.102:8000 - DRONE_SERVER_PROTO=http - DRONE_TLS_AUTOCERT=false volumes: - /etc/localtime:/etc/localtime - /var/run/docker.sock:/var/run/docker.sock - ./data/drone:/var/lib/drone/ ports: - 8000:80 - 6443:443 depends_on: - mysql drone-agent: image: drone/agent:latest hostname: drone-agent container_name: drone-agent restart: always environment: - DRONE_DEBUG=true - DRONE_RPC_SERVER=http://192.168.168.102:8000 - DRONE_RPC_SECRET=dfde64837533976ee7dbf4a87e03ad1f - DRONE_RUNNER_CAPACITY=2 volumes: - /etc/localtime:/etc/localtime - /var/run/docker.sock:/var/run/docker.sock depends_on: - drone-server ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:4","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"验证 Drone 服务 # 使用浏览访问 http://192.168.168.102:8000 # 这里特别注意，因为我们关联的是 gogs 的 git # 所以这里登录 drone 的时候，必须使用 gogs 的账号密码 # 所以这里我们 部署 CI/CD 的时候~可以使用一个 管路员账号 # 或者使用一个有 所有组权限 的账号 ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:5","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"激活仓库 ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:6","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["docker","kubernetes","devops"],"content":"创建一个 .drone.yml 文件 # 创建一个 .drone.yml 并上传到 gogs 仓库中 # 这里是 单 Agent, 的 pipeline # 1.0 版本支持 多 Agent, 多个 node 区分的 pipeline vi .drone.yml kind: pipeline name: default steps: - name: frontend image: alpine commands: - echo \"This My Drone CI Test!\" # 创建一个 后端 golang vi main.go package main import ( \"fmt\" ) func main() { fmt.Printf(\"hello world\"); } func hello() string { return \"hello world\"; } vi main_test.go package main import \"testing\" func TestHello(t *testing.T) { if hello() != \"hello world\" { t.Error(\"Testing error\") } } # 编辑 .drone.yml 文件，增加后端 vi .drone.yml kind: pipeline name: default steps: - name: frontend image: alpine commands: - echo \"This My Drone CI Test!\" - name: backend image: golang commands: - go build - go test ","date":"2019-06-18","objectID":"/drone-1.0-with-gogs/:2:7","tags":["kubernetes","docker","devops"],"title":"Drone 1.0 with Gogs CI/CD","uri":"/drone-1.0-with-gogs/"},{"categories":["kubernetes"],"content":"kubeadm HA to 1.14.1","date":"2019-05-09","objectID":"/kubeadm-1.14.1/","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":" kubeadm HA to 1.14.1 kubernetes 1.14.1 本文基于 kubeadm 方式部署，kubeadm 在1.13 版本以后正式进入 GA. 目前国内各大厂商都有 kubeadm 的镜像源，对于部署 kubernetes 来说是大大的便利. 从官方对 kubeadm 的更新频繁度来看，kubeadm 应该是后面的趋势，毕竟二进制部署确实麻烦了点. 1. 环境说明 系统 IP Docker Kernel 作用 CentOS 7 x64 192.168.168.11 18.09.6 4.4.179 K8s-Master CentOS 7 x64 192.168.168.12 18.09.6 4.4.179 K8s-Master CentOS 7 x64 192.168.168.13 18.09.6 4.4.179 K8s-Master CentOS 7 x64 192.168.168.14 18.09.6 4.4.179 K8s-Node CentOS 7 x64 192.168.168.15 18.09.6 4.4.179 K8s-Node ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:0:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"1.1 初始化环境 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:1:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"1.1.1 配置 hosts hostnamectl --static set-hostname hostname kubernetes-1 192.168.168.11 kubernetes-2 192.168.168.12 kubernetes-3 192.168.168.13 kubernetes-4 192.168.168.14 kubernetes-5 192.168.168.15 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 192.168.168.11 kubernetes-1 192.168.168.12 kubernetes-2 192.168.168.13 kubernetes-3 192.168.168.14 kubernetes-4 192.168.168.15 kubernetes-5 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:1:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"1.1.2 关闭防火墙 sed -ri 's#(SELINUX=).*#\\1disabled#' /etc/selinux/config setenforce 0 systemctl disable firewalld systemctl stop firewalld ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:1:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"1.1.3 关闭虚拟内存 # 临时关闭 swapoff -a # 永久关闭 vi /etc/fstab 注释掉关于 swap 的一段 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:1:3","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"1.1.4 添加内核配置 vi /etc/sysctl.conf net.ipv4.ip_forward=1 net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 vm.swappiness=0 # 生效配置 sysctl -p ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:1:4","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"1.1.5 配置IPVS模块 kube-proxy 使用 ipvs 方式负载 ，所以需要内核加载 ipvs 模块, 否则只会使用 iptables 方式 cat \u003e /etc/sysconfig/modules/ipvs.modules \u003c\u003cEOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF # 授权 chmod 755 /etc/sysconfig/modules/ipvs.modules # 加载模块 bash /etc/sysconfig/modules/ipvs.modules # 查看加载 lsmod | grep -e ip_vs -e nf_conntrack_ipv4 # 输出如下: ----------------------------------------------------------------------- nf_conntrack_ipv4 20480 0 nf_defrag_ipv4 16384 1 nf_conntrack_ipv4 ip_vs_sh 16384 0 ip_vs_wrr 16384 0 ip_vs_rr 16384 0 ip_vs 147456 6 ip_vs_rr,ip_vs_sh,ip_vs_wrr nf_conntrack 110592 2 ip_vs,nf_conntrack_ipv4 libcrc32c 16384 2 xfs,ip_vs ----------------------------------------------------------------------- ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:1:5","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"1.1.6 配置yum源 使用 阿里 的 yum 源 cat \u003c\u003c EOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF # 更新 yum yum makecache 2. 安装 docker The list of validated docker versions has changed. 1.11.1 and 1.12.1 have been removed. The current list is 1.13.1, 17.03, 17.06, 17.09, 18.06, 18.09. # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 官方目前版本为18.09.x 这里直接安装最新版本就行 yum -y install docker-ce # 查看安装 docker version Client: Version: 18.09.6 API version: 1.39 Go version: go1.10.8 Git commit: 481bc77156 Built: Sat May 4 02:34:58 2019 OS/Arch: linux/amd64 Experimental: false ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:1:6","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"2.1 更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min # restart the docker process if it exits prematurely Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target # 修改其他配置 第一种 mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) # docker 版本 17.03.2 之前配置为 --graph=/opt/docker # docker 版本 17.04.x 之后配置为 --data-root=/opt/docker # 10.254.0.0/16 是 kubernetes 预分配的IP，下面会用于 k8s svc 的IP段分配 [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --registry-mirror=https://registry.docker-cn.com \\ --exec-opt native.cgroupdriver=systemd \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local \\ --dns-search svc.cluster.local --dns-search localdomain \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2 \" # 修改配置 第二种 vi /etc/docker/daemon.json { \"insecure-registries\": [\"10.254.0.0/16\"], \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\",\"https://gcr.azk8s.cn\",\"https://quay.azk8s.cn\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"data-root\": \"/opt/docker\", \"log-opts\": { \"max-size\":\"50m\", \"max-file\":\"5\" }, \"dns-search\": [\"default.svc.cluster.local\", \"svc.cluster.local\", \"localdomain\"], \"dns-opts\": [\"ndots:2\", \"timeout:2\", \"attempts:2\"] } # 重新读取配置，启动 docker, 并设置自动启动 systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 3. 部署 kubernetes ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:2:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.1 安装相关软件 所有软件安装都通过 yum 安装 # kubernetes 相关 (Master) yum -y install kubelet-1.14.1 kubeadm-1.14.1 kubectl-1.14.1 # kubernetes 相关 (Node) yum -y install kubelet-1.14.1 kubeadm-1.14.1 # ipvs 相关 yum -y install ipvsadm ipset # 安装的软件列表 socat-1.7.3.2-2.el7.x86_64 libnetfilter_queue-1.0.2-2.el7_2.x86_64 libnetfilter_cttimeout-1.0.0-6.el7.x86_64 kubectl-1.14.1-0.x86_64 libnetfilter_cthelper-1.0.0-9.el7.x86_64 conntrack-tools-1.4.4-4.el7.x86_64 kubernetes-cni-0.7.5-0.x86_64 kubelet-1.14.1-0.x86_64 cri-tools-1.12.0-0.x86_64 kubeadm-1.14.1-0.x86_64 ipvsadm.x86_64 0:1.27-7.el7 ipset-6.38-3.el7_6.x86_64 # 这里由于 kubelet 的驱动默认是 cgroupfs # 但是 docker 在centos 7 下默认驱动是 systemd # 这里必须修改为相同的，否则很多问题 方法一：修改 kubelet vi /var/lib/kubelet/config.yaml cgroupDriver: cgroupfs # 修改为 cgroupDriver: systemd 方法二：修改 docker vi /etc/systemd/system/docker.service.d/docker-options.conf 添加 --exec-opt native.cgroupdriver=cgroupfs \\ # 配置 kubelet 自动启动 (暂时不需要启动) systemctl enable kubelet.service ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:3:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.2 配置 Nginx to API Server 代理 这里可以使用 云上的LB, 物理的 LB 等等, 见仁见智, 我这里是觉得配置虚拟VIP太麻烦。 所以在每个 机器上面都配置一个 Nginx 反向代理 API Server 由于这里API Server 会使用 6443 端口，所以这里 Nginx 代理端口为 8443 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:4:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.2.1 创建 nginx 配置文件 # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 192.168.168.11:6443; server 192.168.168.12:6443; server 192.168.168.13:6443; } server { listen 0.0.0.0:8443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:4:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.2.2 授权配置文件 # 更新权限 chmod +r /etc/nginx/nginx.conf ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:4:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.2.3 创建系统 systemd.service 文件 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:8443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:4:3","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.2.4 启动nginx # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:4:4","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.3 修改 kubeadm 配置信息 # 导出 配置 信息 kubeadm config print init-defaults \u003e kubeadm-init.yaml # 修改相关配置 ，本文配置信息如下 apiVersion: kubeadm.k8s.io/v1beta1 bootstrapTokens: - groups: - system:bootstrappers:kubeadm:default-node-token token: abcdef.0123456789abcdef ttl: 24h0m0s usages: - signing - authentication kind: InitConfiguration localAPIEndpoint: advertiseAddress: 127.0.0.1 bindPort: 6443 nodeRegistration: criSocket: /var/run/dockershim.sock name: kubernetes-1 taints: - effect: NoSchedule key: node-role.kubernetes.io/master --- apiServer: timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta1 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controlPlaneEndpoint: \"127.0.0.1:8443\" controllerManager: {} dns: type: CoreDNS etcd: local: dataDir: /var/lib/etcd imageRepository: registry.cn-hangzhou.aliyuncs.com/google_containers kind: ClusterConfiguration kubernetesVersion: v1.14.1 networking: dnsDomain: cluster.local podSubnet: \"10.254.64.0/18\" serviceSubnet: \"10.254.0.0/18\" scheduler: {} --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration mode: \"ipvs\" ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:5:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.4 初始化集群 kubeadm init --config kubeadm-init.yaml # 输出如下: [bootstrap-token] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root: kubeadm join 127.0.0.1:8443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569 \\ --experimental-control-plane Then you can join any number of worker nodes by running the following on each as root: kubeadm join 127.0.0.1:8443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569 # 拷贝权限文件 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config # 查看集群状态 [root@kubernetes-1 ~]# kubectl get cs NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\"health\":\"true\"} 至此，集群初始化完成，第一个 Master 部署完成。 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:6:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.5 部署其他 Master 这里需要将 第一个 Master 中的证书拷贝到其他 Master 机器中 # 查看 第一个 Master 证书目录 [root@kubernetes-1 ~]# ll /etc/kubernetes/pki/ 总用量 56 -rw-r--r-- 1 root root 1237 5月 9 11:21 apiserver.crt -rw-r--r-- 1 root root 1090 5月 9 11:21 apiserver-etcd-client.crt -rw------- 1 root root 1679 5月 9 11:21 apiserver-etcd-client.key -rw------- 1 root root 1675 5月 9 11:21 apiserver.key -rw-r--r-- 1 root root 1099 5月 9 11:21 apiserver-kubelet-client.crt -rw------- 1 root root 1679 5月 9 11:21 apiserver-kubelet-client.key -rw-r--r-- 1 root root 1025 5月 9 11:21 ca.crt -rw------- 1 root root 1679 5月 9 11:21 ca.key drwxr-xr-x 2 root root 162 5月 9 11:21 etcd -rw-r--r-- 1 root root 1038 5月 9 11:21 front-proxy-ca.crt -rw------- 1 root root 1675 5月 9 11:21 front-proxy-ca.key -rw-r--r-- 1 root root 1058 5月 9 11:21 front-proxy-client.crt -rw------- 1 root root 1675 5月 9 11:21 front-proxy-client.key -rw------- 1 root root 1679 5月 9 11:21 sa.key -rw------- 1 root root 451 5月 9 11:21 sa.pub ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:7:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.5.1 创建证书目录 ssh kubernetes-2 \"mkdir -p /etc/kubernetes/pki/etcd\" ssh kubernetes-3 \"mkdir -p /etc/kubernetes/pki/etcd\" ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:7:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.5.2 拷贝相应证书 scp /etc/kubernetes/pki/ca.* kubernetes-2:/etc/kubernetes/pki/ scp /etc/kubernetes/pki/sa.* kubernetes-2:/etc/kubernetes/pki/ scp /etc/kubernetes/pki/front-proxy-ca.* kubernetes-2:/etc/kubernetes/pki/ scp /etc/kubernetes/pki/etcd/ca.* kubernetes-2:/etc/kubernetes/pki/etcd/ scp /etc/kubernetes/admin.conf kubernetes-2:/etc/kubernetes/ scp /etc/kubernetes/pki/ca.* kubernetes-3:/etc/kubernetes/pki/ scp /etc/kubernetes/pki/sa.* kubernetes-3:/etc/kubernetes/pki/ scp /etc/kubernetes/pki/front-proxy-ca.* kubernetes-3:/etc/kubernetes/pki/ scp /etc/kubernetes/pki/etcd/ca.* kubernetes-3:/etc/kubernetes/pki/etcd/ scp /etc/kubernetes/admin.conf kubernetes-3:/etc/kubernetes/ ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:7:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.5.3 加入 kubernetes 集群 如上有 kubeadm init 后有两条 kubeadm join 命令, –experimental-control-plane 为 加入 Master 另外token 有时效性，如果提示 token 失效，请自行创建一个新的 token. kubeadm token create –print-join-command 创建新的 join token kubeadm join 127.0.0.1:8443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569 \\ --experimental-control-plane # 输出如下 This node has joined the cluster and a new control plane instance was created: * Certificate signing request was sent to apiserver and approval was received. * The Kubelet was informed of the new secure connection details. * Control plane (master) label and taint were applied to the new node. * The Kubernetes control plane instances scaled up. * A new etcd member was added to the local/stacked etcd cluster. To start administering your cluster from this node, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Run 'kubectl get nodes' to see this node join the cluster. # 分别执行 创建配置文件 mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config chown $(id -u):$(id -g) $HOME/.kube/config ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:7:3","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.5.4 验证 Master 节点 这里 STATUS 显示 NotReady 是因为 没有安装网络组件 # 查看 node [root@kubernetes-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-1 NotReady master 11m v1.14.1 kubernetes-2 NotReady master 5m38s v1.14.1 kubernetes-3 NotReady master 2m39s v1.14.1 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:7:4","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.6 部署 Node 节点 kubeadm join 127.0.0.1:8443 --token abcdef.0123456789abcdef \\ --discovery-token-ca-cert-hash sha256:5dff9a228bf55f1e8a6a754f3c087017247734b06c24138c0474d448d8aa5569 # 输出如下: This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the control-plane to see this node join the cluster. ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:8:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.6.1 验证 所有 节点 这里 STATUS 显示 NotReady 是因为 没有安装网络组件 [root@kubernetes-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-1 NotReady master 13m v1.14.1 kubernetes-2 NotReady master 8m20s v1.14.1 kubernetes-3 NotReady master 5m21s v1.14.1 kubernetes-4 NotReady \u003cnone\u003e 45s v1.14.1 kubernetes-5 NotReady \u003cnone\u003e 39s v1.14.1 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:8:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.7 安装网络组件 Calico 网络 官方文档 https://docs.projectcalico.org/v3.7/introduction ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:9:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.7.1 下载 Calico yaml # 下载 yaml 文件 wget https://docs.projectcalico.org/v3.7/manifests/calico.yaml ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:9:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.7.2 修改 Calico 配置 这里只需要修改 分配的 CIDR 就可以 vi calico.yaml # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: \"10.254.64.0/18\" # 导入 yaml 文件 [root@kubernetes-1 calico]# kubectl apply -f calico.yaml configmap/calico-config created customresourcedefinition.apiextensions.k8s.io/felixconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamblocks.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/blockaffinities.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamhandles.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ipamconfigs.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgppeers.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/bgpconfigurations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/ippools.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/hostendpoints.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/clusterinformations.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/globalnetworksets.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networkpolicies.crd.projectcalico.org created customresourcedefinition.apiextensions.k8s.io/networksets.crd.projectcalico.org created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created daemonset.extensions/calico-node created serviceaccount/calico-node created deployment.extensions/calico-kube-controllers created serviceaccount/calico-kube-controllers created # 查看服务 [root@kubernetes-1 calico]# kubectl get pods -n kube-system |grep calico calico-kube-controllers-8646dd497f-4d4hr 1/1 Running 0 28s calico-node-455mf 1/1 Running 0 28s calico-node-b8mrr 1/1 Running 0 28s calico-node-fq9dj 1/1 Running 0 28s calico-node-sk77j 1/1 Running 0 28s calico-node-xxnb8 1/1 Running 0 28s ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:9:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.8 检验整体集群 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:10:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.8.1 查看 状态 所有的 STATUS 都为 Ready ROLES 为 三台 Master, 二台 ，为 Node 节点 [root@kubernetes-1 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-1 Ready master 33m v1.14.1 kubernetes-2 Ready master 28m v1.14.1 kubernetes-3 Ready master 25m v1.14.1 kubernetes-4 Ready \u003cnone\u003e 20m v1.14.1 kubernetes-5 Ready \u003cnone\u003e 20m v1.14.1 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:10:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.8.2 查看 pods 状态 [root@kubernetes-1 ~]# kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-8646dd497f-4d4hr 1/1 Running 0 3m33s kube-system calico-node-455mf 1/1 Running 0 3m33s kube-system calico-node-b8mrr 1/1 Running 0 3m33s kube-system calico-node-fq9dj 1/1 Running 0 3m33s kube-system calico-node-sk77j 1/1 Running 0 3m33s kube-system calico-node-xxnb8 1/1 Running 0 3m33s kube-system coredns-d5947d4b-q4pst 1/1 Running 0 35m kube-system coredns-d5947d4b-sz2bm 1/1 Running 0 35m kube-system etcd-kubernetes-1 1/1 Running 0 34m kube-system etcd-kubernetes-2 1/1 Running 0 30m kube-system etcd-kubernetes-3 1/1 Running 0 27m kube-system kube-apiserver-kubernetes-1 1/1 Running 0 35m kube-system kube-apiserver-kubernetes-2 1/1 Running 0 30m kube-system kube-apiserver-kubernetes-3 1/1 Running 0 26m kube-system kube-controller-manager-kubernetes-1 1/1 Running 1 34m kube-system kube-controller-manager-kubernetes-2 1/1 Running 0 30m kube-system kube-controller-manager-kubernetes-3 1/1 Running 0 26m kube-system kube-proxy-2ht7m 1/1 Running 0 35m kube-system kube-proxy-bkg7z 1/1 Running 0 22m kube-system kube-proxy-sdxnj 1/1 Running 0 23m kube-system kube-proxy-tpc4x 1/1 Running 0 27m kube-system kube-proxy-wvllx 1/1 Running 0 30m kube-system kube-scheduler-kubernetes-1 1/1 Running 1 35m kube-system kube-scheduler-kubernetes-2 1/1 Running 0 30m kube-system kube-scheduler-kubernetes-3 1/1 Running 0 26m ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:10:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.8.3 查看 svc 的状态 [root@kubernetes-1 ~]# kubectl get svc --all-namespaces NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 43m kube-system kube-dns ClusterIP 10.254.0.10 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 43m ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:10:3","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"3.8.3 查看 IPVS 的状态 [root@kubernetes-1 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr -\u003e 192.168.168.11:6443 Masq 1 2 0 -\u003e 192.168.168.12:6443 Masq 1 0 0 -\u003e 192.168.168.13:6443 Masq 1 1 0 TCP 10.254.0.10:53 rr -\u003e 10.254.91.66:53 Masq 1 0 0 -\u003e 10.254.105.65:53 Masq 1 0 0 TCP 10.254.0.10:9153 rr -\u003e 10.254.91.66:9153 Masq 1 0 0 -\u003e 10.254.105.65:9153 Masq 1 0 0 UDP 10.254.0.10:53 rr -\u003e 10.254.91.66:53 Masq 1 0 0 -\u003e 10.254.105.65:53 Masq 1 0 0 4. 测试集群 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:10:4","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"4.1 创建一个 nginx deployment apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-1 nginx]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-dm created service/nginx-svc created [root@kubernetes-1 nginx]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-dm-76cf455886-4drbs 1/1 Running 0 23s 10.254.91.67 kubernetes-4 \u003cnone\u003e \u003cnone\u003e nginx-dm-76cf455886-blldj 1/1 Running 0 23s 10.254.96.66 kubernetes-5 \u003cnone\u003e \u003cnone\u003e nginx-dm-76cf455886-st899 1/1 Running 0 23s 10.254.96.65 kubernetes-5 \u003cnone\u003e \u003cnone\u003e [root@kubernetes-1 nginx]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 56m \u003cnone\u003e nginx-svc ClusterIP 10.254.2.177 \u003cnone\u003e 80/TCP 41s name=nginx # 访问 svc [root@kubernetes-1 nginx]# curl 10.254.2.177 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-1 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.2.177:80 rr -\u003e 10.254.91.67:80 Masq 1 0 0 -\u003e 10.254.96.65:80 Masq 1 0 0 -\u003e 10.254.96.66:80 Masq 1 0 0 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:11:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"4.2 验证 dns 的服务 # 创建一个 pod apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sleep - \"3600\" # 查看 创建的服务 [root@kubernetes-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 3s nginx-dm-76cf455886-4drbs 1/1 Running 0 11m nginx-dm-76cf455886-blldj 1/1 Running 0 11m nginx-dm-76cf455886-st899 1/1 Running 0 11m # 测试 [root@kubernetes-1 ~]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local [root@kubernetes-1 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.2.177 nginx-svc.default.svc.cluster.local 5. 部署 Metrics-Server 官方 https://github.com/kubernetes-incubator/metrics-server ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:12:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"5.1 Metrics-Server 说明 v1.11 以后不再支持通过 heaspter 采集监控数据，支持新的监控数据采集组件metrics-server，比heaspter轻量很多，也不做数据的持久化存储，提供实时的监控数据查询。 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:13:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"5.1.1 创建 Metrics-Server 文件 # vi metrics-server.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:aggregated-metrics-reader labels: rbac.authorization.k8s.io/aggregate-to-view: \"true\" rbac.authorization.k8s.io/aggregate-to-edit: \"true\" rbac.authorization.k8s.io/aggregate-to-admin: \"true\" rules: - apiGroups: [\"metrics.k8s.io\"] resources: [\"pods\", \"nodes\"] verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: metrics-server:system:auth-delegator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:auth-delegator subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: metrics-server-auth-reader namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: extension-apiserver-authentication-reader subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: apiregistration.k8s.io/v1beta1 kind: APIService metadata: name: v1beta1.metrics.k8s.io spec: service: name: metrics-server namespace: kube-system group: metrics.k8s.io version: v1beta1 insecureSkipTLSVerify: true groupPriorityMinimum: 100 versionPriority: 100 --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: system:metrics-server rules: - apiGroups: - \"\" resources: - pods - nodes - nodes/stats - namespaces verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: system:metrics-server roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:metrics-server subjects: - kind: ServiceAccount name: metrics-server namespace: kube-system --- apiVersion: v1 kind: ServiceAccount metadata: name: metrics-server namespace: kube-system --- apiVersion: apps/v1 kind: Deployment metadata: name: metrics-server namespace: kube-system labels: k8s-app: metrics-server spec: selector: matchLabels: k8s-app: metrics-server template: metadata: name: metrics-server labels: k8s-app: metrics-server spec: serviceAccountName: metrics-server volumes: # mount in tmp so we can safely use from-scratch images and/or read-only containers - name: tmp-dir emptyDir: {} containers: - name: metrics-server image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 args: - --cert-dir=/tmp - --secure-port=4443 ports: - name: main-port containerPort: 4443 protocol: TCP securityContext: readOnlyRootFilesystem: true runAsNonRoot: true runAsUser: 1000 imagePullPolicy: Always volumeMounts: - name: tmp-dir mountPath: /tmp command: - /metrics-server - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP nodeSelector: beta.kubernetes.io/os: linux --- apiVersion: v1 kind: Service metadata: name: metrics-server namespace: kube-system labels: kubernetes.io/name: \"Metrics-server\" kubernetes.io/cluster-service: \"true\" spec: selector: k8s-app: metrics-server ports: - port: 443 protocol: TCP targetPort: main-port [root@kubernetes-1 metrics]# kubectl apply -f metrics-server.yaml clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created clusterrole.rbac.authorization.k8s.io/system:metrics-server created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created serviceaccount/metrics-server created deployment.extensions/metrics-server created service/metrics-server created ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:13:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"5.1.2 查看服务 [root@kubernetes-1 metrics]# kubectl get pods -n kube-system |grep metrics metrics-server-5f7bbc8788-6rsj4 1/1 Running 0 54s ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:13:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"5.1.3 测试采集 [root@kubernetes-1 metrics]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% kubernetes-1 159m 3% 1887Mi 11% kubernetes-2 153m 3% 1521Mi 9% kubernetes-3 150m 3% 1419Mi 8% kubernetes-4 113m 2% 1198Mi 7% kubernetes-5 76m 1% 1002Mi 6% 6. Nginx Ingress 官方地址 https://kubernetes.github.io/ingress-nginx/ ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:13:3","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.1 Nginx Ingress 介绍 基于 Nginx 使用 Kubernetes ConfigMap 来存储 Nginx 配置文件 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:14:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.2 部署 Nginx ingress ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:15:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.2.1 下载 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:15:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.2.2 修改 yaml 文件 # 替换 阿里 镜像下载地址 sed -i 's/quay\\.io\\/kubernetes-ingress-controller/registry\\.cn-hangzhou\\.aliyuncs\\.com\\/google_containers/g' mandatory.yaml # 配置 pods 份数 replicas: 3 # 配置 node affinity 与 hostNetwork # 在 如下之间添加 spec: serviceAccountName: nginx-ingress-serviceaccount # 添加完如下: spec: hostNetwork: true affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - kubernetes-1 - kubernetes-2 - kubernetes-3 podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - ingress-nginx topologyKey: \"kubernetes.io/hostname\" tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule serviceAccountName: nginx-ingress-serviceaccount # 如上 affinity 说明 affinity: # 声明 亲和性设置 nodeAffinity: # 声明 为 Node 亲和性设置 requiredDuringSchedulingIgnoredDuringExecution: # 必须满足下面条件 nodeSelectorTerms: # 声明 为 Node 调度选择标签 - matchExpressions: # 设置node拥有的标签 - key: kubernetes.io/hostname # kubernetes内置标签 operator: In # 操作符 values: # 值,既集群 node 名称 - kubernetes-1 - kubernetes-2 - kubernetes-3 podAntiAffinity: # 声明 为 Pod 亲和性设置 requiredDuringSchedulingIgnoredDuringExecution: # 必须满足下面条件 - labelSelector: # 与哪个pod有亲和性，在此设置此pod具有的标签 matchExpressions: # 要匹配如下的pod的,标签定义 - key: app.kubernetes.io/name # 标签定义为 空间名称(namespaces) operator: In values: - ingress-nginx topologyKey: \"kubernetes.io/hostname\" # 节点所属拓朴域 tolerations: # 声明 为 可容忍 的选项 - key: node-role.kubernetes.io/master # 声明 标签为 node-role 选项 effect: NoSchedule # 声明 node-role 为 NoSchedule 也可容忍 serviceAccountName: nginx-ingress-serviceaccount ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:15:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.2.3 apply 导入 文件 [root@kubernetes-1 ingress]# kubectl apply -f mandatory.yaml namespace/ingress-nginx created configmap/nginx-configuration created configmap/tcp-services created configmap/udp-services created serviceaccount/nginx-ingress-serviceaccount created clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created role.rbac.authorization.k8s.io/nginx-ingress-role created rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created deployment.apps/nginx-ingress-controller created ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:15:3","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.2.4 查看服务状态 [root@kubernetes-1 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-ingress-controller-7b5477f4f6-btvqm 1/1 Running 0 118s 192.168.168.13 kubernetes-3 \u003cnone\u003e \u003cnone\u003e nginx-ingress-controller-7b5477f4f6-sgnj8 1/1 Running 0 3m6s 192.168.168.12 kubernetes-2 \u003cnone\u003e \u003cnone\u003e nginx-ingress-controller-7b5477f4f6-xqldf 1/1 Running 0 67s 192.168.168.11 kubernetes-1 \u003cnone\u003e \u003cnone\u003e ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:15:4","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.2.5 测试 ingress # 查看之前创建的 Nginx [root@kubernetes-1 ingress]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 25h nginx-svc ClusterIP 10.254.2.177 \u003cnone\u003e 80/TCP 24h # 创建一个 nginx-svc 的 ingress vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 # 导入 yaml [root@kubernetes-1 yaml]# kubectl apply -f nginx-ingress.yaml ingress.extensions/nginx-ingress created # 查看 ingress [root@kubernetes-1 yaml]# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 3s ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:15:5","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"6.2.6 测试访问 # kubernetes-1 [root@kubernetes-1 ~]# ping nginx.jicki.cn PING nginx.jicki.cn (192.168.168.11) 56(84) bytes of data. 64 bytes from kubernetes-1 (192.168.168.11): icmp_seq=1 ttl=64 time=0.070 ms 64 bytes from kubernetes-1 (192.168.168.11): icmp_seq=2 ttl=64 time=0.031 ms 64 bytes from kubernetes-1 (192.168.168.11): icmp_seq=3 ttl=64 time=0.032 ms 64 bytes from kubernetes-1 (192.168.168.11): icmp_seq=4 ttl=64 time=0.044 ms [root@kubernetes-1 ~]# curl -I nginx.jicki.cn HTTP/1.1 200 OK Server: nginx/1.15.10 Date: Fri, 10 May 2019 09:13:41 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Vary: Accept-Encoding Last-Modified: Tue, 16 Apr 2019 21:22:22 GMT ETag: \"5cb6478e-264\" Accept-Ranges: bytes # kubernetes-2 [root@kubernetes-1 ~]# ping nginx.jicki.cn PING nginx.jicki.cn (192.168.168.12) 56(84) bytes of data. 64 bytes from kubernetes-2 (192.168.168.12): icmp_seq=1 ttl=63 time=0.468 ms 64 bytes from kubernetes-2 (192.168.168.12): icmp_seq=2 ttl=63 time=0.476 ms 64 bytes from kubernetes-2 (192.168.168.12): icmp_seq=3 ttl=63 time=1.15 ms 64 bytes from kubernetes-2 (192.168.168.12): icmp_seq=4 ttl=63 time=0.392 ms [root@kubernetes-1 ~]# curl -I nginx.jicki.cn HTTP/1.1 200 OK Server: nginx/1.15.10 Date: Fri, 10 May 2019 09:21:06 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Vary: Accept-Encoding Last-Modified: Tue, 16 Apr 2019 21:22:22 GMT ETag: \"5cb6478e-264\" Accept-Ranges: bytes # kubernetes-3 [root@kubernetes-1 ~]# ping nginx.jicki.cn PING nginx.jicki.cn (192.168.168.13) 56(84) bytes of data. 64 bytes from kubernetes-3 (192.168.168.13): icmp_seq=1 ttl=64 time=0.273 ms 64 bytes from kubernetes-3 (192.168.168.13): icmp_seq=2 ttl=64 time=0.098 ms 64 bytes from kubernetes-3 (192.168.168.13): icmp_seq=3 ttl=64 time=0.123 ms 64 bytes from kubernetes-3 (192.168.168.13): icmp_seq=4 ttl=64 time=0.092 ms [root@kubernetes-1 ~]# curl -I nginx.jicki.cn HTTP/1.1 200 OK Server: nginx/1.15.10 Date: Fri, 10 May 2019 09:22:31 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Vary: Accept-Encoding Last-Modified: Tue, 16 Apr 2019 21:22:22 GMT ETag: \"5cb6478e-264\" Accept-Ranges: bytes 7. Dashboard 官方 https://github.com/kubernetes/dashboard ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:15:6","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.1 Dashboard 介绍 Dashboard 是 Kubernetes 集群的 通用 WEB UI 它允许用户管理集群中运行的应用程序并对其进行故障排除，以及管理集群本身。 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:16:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2 部署 Dashboard 就目前的版本 v1.10.1 仍然只支持 Heapster 作为监控数据采集显示. 暂时还不支持 Metrics-Server 做为 监控数据采集，这里就不配置 Heapster 了. ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2.1 下载 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.1/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:1","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2.2 修改yaml文件 # 这里主要是将 下载 images 的地址修改为 阿里 的地址 sed -i 's/k8s\\.gcr\\.io/registry\\.cn-hangzhou\\.aliyuncs\\.com\\/google_containers/g' kubernetes-dashboard.yaml # 这里由于我上面 配置的 apiserver 端口为 8443 与这里的端口冲突了 # 这里 关于修改 默认端口的问题, 如下地方: ports: - containerPort: 8443 protocol: TCP ......... livenessProbe: httpGet: scheme: HTTPS path: / port: 8443 .......... ports: - port: 443 targetPort: 8443 # 直接修改 上面端口 是不可以的， 必须在 args 下也更改 # 增加选项, 这里修改为 9443 args: - --port=9443 ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:2","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2.3 apply 导入文件 [root@kubernetes-1 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret/kubernetes-dashboard-certs created serviceaccount/kubernetes-dashboard created role.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal created deployment.apps/kubernetes-dashboard created service/kubernetes-dashboard created ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:3","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2.4 查看服务状态 [root@kubernetes-1 dashboard]# kubectl get pods -n kube-system |grep dashboard kubernetes-dashboard-5d9599dc98-kbhbw 1/1 Running 0 3m6s [root@kubernetes-1 dashboard]# kubectl get svc -n kube-system |grep dashboard kubernetes-dashboard ClusterIP 10.254.28.8 \u003cnone\u003e 443/TCP 3m19s ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:4","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2.5 暴露公网 访问 kubernetes 服务，既暴露 kubernetes 内的端口到 外网，有很多种方案 LoadBlancer ( 支持的公有云服务的负载均衡 ) NodePort (映射所有 node 中的某个端口，暴露到公网中) Ingress ( 支持反向代理软件的对外服务, 如: Nginx , HAproxy 等) # 由于我们已经部署了 Nginx-ingress 所以这里使用 ingress 来暴露出去 # Dashboard 这边 从 svc 上看只 暴露了 443 端口，所以这边需要生成一个证书 # 注: 这里由于测试，所以使用 openssl 生成临时的证书 # 生成证书 # 创建一个 基于 自身域名的 证书 openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout dashboard.jicki.cn-key.key -out dashboard.jicki.cn.pem -subj \"/CN=dashboard.jicki.cn\" # 导入 域名的证书 到 集群 的 secret 中 kubectl create secret tls dashboard-secret --namespace=kube-system --cert dashboard.jicki.cn.pem --key dashboard.jicki.cn-key.key # 查看 secret [root@kubernetes-1 ssl]# kubectl get secret -n kube-system |grep dashboard dashboard-secret kubernetes.io/tls 2 16s [root@kubernetes-1 ssl]# kubectl describe secret/dashboard-secret -n kube-system Name: dashboard-secret Namespace: kube-system Labels: \u003cnone\u003e Annotations: \u003cnone\u003e Type: kubernetes.io/tls Data ==== tls.crt: 1119 bytes tls.key: 1704 bytes # 创建 dashboard ingress # 这里面 annotations 中的 backend 声明,从 v0.21.0 版本开始变更, 一定注意 # nginx-ingress \u003c v0.21.0 使用 nginx.ingress.kubernetes.io/secure-backends: \"true\" # nginx-ingress \u003e v0.21.0 使用 nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard namespace: kube-system annotations: ingress.kubernetes.io/ssl-passthrough: \"true\" nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" spec: tls: - hosts: - dashboard.jicki.cn secretName: dashboard-secret rules: - host: dashboard.jicki.cn http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443 # 导入 yaml [root@kubernetes-1 dashboard]# kubectl apply -f dashboard-ingress.yaml ingress.extensions/kubernetes-dashboard created # 查看 ingress [root@kubernetes-1 dashboard]# kubectl get ingress -n kube-system NAME HOSTS ADDRESS PORTS AGE kubernetes-dashboard dashboard.jicki.cn 80, 443 21s ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:5","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2.6 测试访问 [root@kubernetes-1 dashboard]# ping dashboard.jicki.cn PING dashboard.jicki.cn (192.168.168.11) 56(84) bytes of data. 64 bytes from kubernetes-1 (192.168.168.11): icmp_seq=1 ttl=64 time=0.051 ms 64 bytes from kubernetes-1 (192.168.168.11): icmp_seq=2 ttl=64 time=0.031 ms [root@kubernetes-1 dashboard]# curl -I -k https://dashboard.jicki.cn HTTP/1.1 200 OK Server: nginx/1.15.10 Date: Mon, 13 May 2019 05:57:28 GMT Content-Type: text/html; charset=utf-8 Content-Length: 990 Connection: keep-alive Vary: Accept-Encoding Accept-Ranges: bytes Cache-Control: no-store Last-Modified: Mon, 17 Dec 2018 09:04:43 GMT Strict-Transport-Security: max-age=15724800; includeSubDomains ","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:6","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"7.2.7 令牌 登录认证 # 创建一个 dashboard rbac 超级用户 vi dashboard-admin-rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: kubernetes-dashboard-admin labels: k8s-app: kubernetes-dashboard roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: kubernetes-dashboard-admin namespace: kube-system [root@kubernetes-1 dashboard]# kubectl apply -f dashboard-admin-rbac.yaml serviceaccount/kubernetes-dashboard-admin created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-admin created # 查看 secret [root@kubernetes-1 dashboard]# kubectl -n kube-system get secret | grep kubernetes-dashboard-admin kubernetes-dashboard-admin-token-c4lxz kubernetes.io/service-account-token 3 20s # 查看 token 部分 [root@kubernetes-1 dashboard]# kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-c4lxz error: there is no need to specify a resource type as a separate argument when passing arguments in resource/name form (e.g. 'kubectl get resource/\u003cresource_name\u003e' instead of 'kubectl get resource resource/\u003cresource_name\u003e' [root@kubernetes-1 dashboard]# kubectl describe -n kube-system secret/kubernetes-dashboard-admin-token-c4lxz Name: kubernetes-dashboard-admin-token-c4lxz Namespace: kube-system Labels: \u003cnone\u003e Annotations: kubernetes.io/service-account.name: kubernetes-dashboard-admin kubernetes.io/service-account.uid: 43e785b2-7544-11e9-a1a6-000c29d28269 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 11 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1jNGx4eiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjQzZTc4NWIyLTc1NDQtMTFlOS1hMWE2LTAwMGMyOWQyODI2OSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.TM00Z9oL6YImxPAF1RbrcXjExMB-WqlgcRkpghAyXKTbWOE6IGbIA2WIpZr6VFt-A9hQ719v92C5D4T05MHwH9ZxmNvmR3TnTVwO3TfXkrs6B3wzbLK4mhtzbKOH1H3ZG20iQwukQuaacRhqqttY2b6IvL1fvlVTxYMaBOKpRuTQwt-jPgD--2DJpHOqVezAT-ZgDrpR0Qs0a478pJ283T4dBkVszmpjzsejw64UU8jM2ip1mGtkWp1OMJLufaxNQtyNmOBJGvEjCoxgRe8t91dK0BmC_8ouioTii2ppxeaSJkR7rFctEu-cMbWTIsfzcmQcHJcEDUUMdgt7l9qmYg # 复制 token 如下部分: token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbi10b2tlbi1jNGx4eiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJrdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjQzZTc4NWIyLTc1NDQtMTFlOS1hMWE2LTAwMGMyOWQyODI2OSIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlLXN5c3RlbTprdWJlcm5ldGVzLWRhc2hib2FyZC1hZG1pbiJ9.TM00Z9oL6YImxPAF1RbrcXjExMB-WqlgcRkpghAyXKTbWOE6IGbIA2WIpZr6VFt-A9hQ719v92C5D4T05MHwH9ZxmNvmR3TnTVwO3TfXkrs6B3wzbLK4mhtzbKOH1H3ZG20iQwukQuaacRhqqttY2b6IvL1fvlVTxYMaBOKpRuTQwt-jPgD--2DJpHOqVezAT-ZgDrpR0Qs0a478pJ283T4dBkVszmpjzsejw64UU8jM2ip1mGtkWp1OMJLufaxNQtyNmOBJGvEjCoxgRe8t91dK0BmC_8ouioTii2ppxeaSJkR7rFctEu-cMbWTIsfzcmQcHJcEDUUMdgt7l9qmYg 8. kubeadm upgrade 对于Kubeadm 部署的集群，可以使用命令直接平滑的升级，整个过程非常的简单，不会像二进制部署那样需要重新配置每个点的服务组件。 # 查看可升级的版本，以及升级的步骤，执行以下命令可查看，但是由于国内 k8s.io 被墙，所以国内无法获取可升级的版本。 [root@kubernetes-1 ~]# kubeadm upgrade plan [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrad","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:17:7","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["kubernetes"],"content":"8.1 升级服务组件 # 首先我们需要 升级 kubeadm kubelet kubectl 所有的Node 都需要升级 # 这里要千万注意，跨大版本升级千万要注意，1.14 升级到 1.15 的话，估计会出问题的。 # 查看可升级的版本 [root@kubernetes-1 ~]# yum makecache [root@kubernetes-1 ~]# yum list kubeadm kubelet kubectl --showduplicates |sort -r 已加载插件：fastestmirror 已安装的软件包 可安装的软件包 * updates: centos.ustc.edu.cn Loading mirror speeds from cached hostfile ..... kubelet.x86_64 1.15.0-0 kubernetes kubelet.x86_64 1.14.3-0 kubernetes kubelet.x86_64 1.14.2-0 kubernetes kubelet.x86_64 1.14.1-0 kubernetes .... kubectl.x86_64 1.15.0-0 kubernetes kubectl.x86_64 1.14.3-0 kubernetes kubectl.x86_64 1.14.2-0 kubernetes kubectl.x86_64 1.14.1-0 kubernetes .... kubeadm.x86_64 1.15.0-0 kubernetes kubeadm.x86_64 1.14.3-0 kubernetes kubeadm.x86_64 1.14.2-0 kubernetes kubeadm.x86_64 1.14.1-0 kubernetes # 升级指定的版本, [root@kubernetes-1 ~]# yum -y install kubectl-1.14.3 kubelet-1.14.3 kubeadm-1.14.3 # 如果不小心升级到了大版本，可直接降级 [root@kubernetes-1 ~]# yum downgrade kubectl-1.14.3 kubelet-1.14.3 kubeadm-1.14.3 # 由于我们的 kubelet 是使用 systemctl 托管的服务 所以我们需要在所有Node重启一下 [root@kubernetes-1 ~]# systemctl daemon-reload [root@kubernetes-1 ~]# systemctl restart kubelet # 升级指定集群版本, 这里面由于我们导出了 kubeadm-init.yaml 文件，并做了修改，所以升级的时候也需要指定这个文件 # 当然在 kubeadm-init.yaml 里的(kubernetesVersion: v1.14.3)版本号也需要修改为升级的版本号 [root@kubernetes-1 ~]# kubeadm upgrade apply --config=kubeadm-init.yaml W0625 10:00:45.650868 20357 common.go:150] WARNING: overriding requested API server bind address: requested \"127.0.0.1\", actual \"192.168.168.11\" [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: W0625 10:00:45.670849 20357 common.go:150] WARNING: overriding requested API server bind address: requested \"127.0.0.1\", actual \"192.168.168.11\" [upgrade/version] You have chosen to change the cluster version to \"v1.14.3\" [upgrade/versions] Cluster version: v1.14.1 [upgrade/versions] kubeadm version: v1.14.3 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component etcd. [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [upgrade/prepull] Prepulling image for component kube-scheduler. [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 3 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 3 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 3 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 3 Pods for label selector k8s-app=upgrade-prepull-etcd [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \"v1.14.3\"... Static pod: kube-apiserver-kubernetes-1 hash: 6ad69de30854f1e2a7dded6373d4d803 Static pod: kube-controller-manager-kubernetes-1 hash: 9dd249309899307ea97eae283e64f30b Static pod: kube-scheduler-kubernetes-1 hash: f6038c6cd5a85d4e6bb2b14e4743e83d [upgrade/etcd] Upgrading to TLS for etcd [upgrade/staticpods] Writing new Static Pod manifests to \"/etc/kubernetes/tmp/kubeadm-upgraded-manifests168961842\" [upgrade/staticpods] Moved new manifest to \"/etc/kubernetes/manifests/kube-apiserver.yaml\" and backed up old manifest to \"/etc/kubernetes/tmp/kubeadm-backup-manifests-2019-06-25-10-01-54/kube-apiser","date":"2019-05-09","objectID":"/kubeadm-1.14.1/:18:0","tags":["kubernetes","docker"],"title":"kubeadm HA to 1.14.1","uri":"/kubeadm-1.14.1/"},{"categories":["python"],"content":"Muddery 部署 配置文档","date":"2019-02-27","objectID":"/muddery-0.3.3/","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"Muddery 简介 程序源代码：https://github.com/muddery/muddery Muddery是一个用Python编写的在线文字游戏（如MUD）框架，所有的代码都是开源的，采用BSD许可证发布。它使用Evennia（一个MUD游戏框架）作为其内核。 ","date":"2019-02-27","objectID":"/muddery-0.3.3/:0:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"Muddery 特点 Muddery具有以下特点: 使用Python开发，可以跨平台使用，只需要花几分钟时间就能够安装它。 支持多人在线游戏，游戏内容主要以文字形式展现，但也可以扩展加入多媒体的内容。 内建有基本的任务系统、事件系统、对话系统等，便于游戏的创建。 自带有网页版的游戏编辑器，可以在网页上构建游戏世界。 自带网页客户端，可以轻松地发布游戏。 完全使用点击式的游戏操作模式，便于在智能手机、平板设备上使用。 Muddery 安装部署 ","date":"2019-02-27","objectID":"/muddery-0.3.3/:1:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"安装所需软件 这里使用 CentOS 7.x 64 系统 1. # Python , CentOS 默认都会自带 python 2.7 2. 安装 pip curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py 3. 安装 git yum -y install git 4. 安装 python-devel yum -y install python-devel 5. 安装 virtualenv pip install virtualenv ","date":"2019-02-27","objectID":"/muddery-0.3.3/:2:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"配置 virtualenv # 创建 虚拟环境目录 mkdir -p /opt/muddery # 创建 一个 虚拟环境 virtualenv pyenv # 如果 系统安装了 python 3.x 版本 请使用 使用 2.7 版本来创建 virtualenv 环境 virtualenv -p /usr/bin/python2 pyenv # 创建虚拟环境以后会生成 pyenv 目录 # 激活/进入 虚拟环境 source pyenv/bin/activate # 激活以后~会变成: (pyenv) [root@localhost muddery]# ","date":"2019-02-27","objectID":"/muddery-0.3.3/:3:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"配置 muddery # 下载源码 (pyenv) [root@localhost muddery]# git clone https://github.com/muddery/muddery.git # 进入目录 (pyenv) [root@localhost muddery]# cd /opt/muddery/muddery # 下载 muddery 所需要的依赖 (pyenv) [root@localhost muddery]# pip install -e . # 相关依赖在 requirements.txt 文件中 ","date":"2019-02-27","objectID":"/muddery-0.3.3/:4:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"初始化 模板 # muddery 支持创建 英文 与 中文 版本的模板 # 进入我们的目录 cd /opt/muddery # 创建一个模板 # 英文版 ( mygame 为 模板目录 ) muddery --init mygame # 中文版 ( mygame 为 模板目录 cn 为 中文 ) muddery --init mygame cn # 模板配置文件在 mygame/server/conf/settings.py 中 # 配置文件中可以修改 启动端口, 语言 等参数 # 服务器内部管理端口 admin web-ui WEBSERVER_PORTS = [(8000, 5001)] # websocket 端口, 用于 webclient 连接 WEBSOCKET_CLIENT_PORT = 8001 # AMP 连接端口 AMP_PORT = 5000 # 语言设置, en 与 cn LANGUAGE_CODE = 'zh-cn' # 修改数据库 Muddery 默认使用Sqlite3数据库。 # Sqlite3 数据库只适用于测试， 修改为 mysql 或者其他 # 编辑 mygame/server/conf/settings.py 文件 vi mygame/server/conf/settings.py # 在 最下面 添加 如下 ###################################################################### # Evennia Database config ###################################################################### # Database config syntax: # ENGINE - path to the the database backend. Possible choices are: # 'django.db.backends.sqlite3', (default) # 'django.db.backends.mysql', # 'django.db.backends.postgresql_psycopg2' (see Issue 241), # 'django.db.backends.oracle' (untested). # NAME - database name, or path to the db file for sqlite3 # USER - db admin (unused in sqlite3) # PASSWORD - db admin password (unused in sqlite3) # HOST - empty string is localhost (unused in sqlite3) # PORT - empty string defaults to localhost (unused in sqlite3) DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mygame', 'USER': 'root', 'PASSWORD': '123456789', 'HOST': '127.0.0.1', 'PORT': '3306' }, 'worlddata': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'mygame_worlddata', 'USER': 'root', 'PASSWORD': '123456789', 'HOST': '127.0.0.1', 'PORT': '3306' }} # Database's router DATABASE_ROUTERS = ['muddery.worlddata.db.database_router.DatabaseAppsRouter'] DATABASE_APPS_MAPPING = { 'worlddata': 'worlddata', } # 安装 django 的 mysql 库 # 首先安装 mysql 支持 yum install mysql-devel # 安装 python mysql 库 pip install mysql-python DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7. Collecting mysql-python Using cached https://files.pythonhosted.org/packages/a5/e9/51b544da85a36a68debe7a7091f068d802fc515a3a202652828c73453cad/MySQL-python-1.2.5.zip Building wheels for collected packages: mysql-python Building wheel for mysql-python (setup.py) ... done Stored in directory: /root/.cache/pip/wheels/07/d2/5f/314860e4cb53a44bf0ee0d051d4b34465e4b4fbe9de6d42f42 Successfully built mysql-python Installing collected packages: mysql-python Successfully installed mysql-python-1.2.5 # 登录 mysql 创建 两个数据库 mygame 与 mygame_worlddata mysq -h 127.0.0.1 -uroot -p123456 MySQL [(none)]\u003e create database mygame; Query OK, 1 row affected (0.00 sec) MySQL [(none)]\u003e create database mygame_worlddata; Query OK, 1 row affected (0.00 sec) # 初始化 数据库 的表结构 (pyenv) [root@payment mygame]# cd /opt/mudgame/mygame # 创建 数据库 与 基础数据 (pyenv) [root@payment mygame]# muddery --loaddata Operations to perform: Apply all migrations: accounts, admin, auth, comms, contenttypes, database, flatpages, help, objects, scripts, server, sessions, sites, typeclasses, worlddata .... Import local data success. ","date":"2019-02-27","objectID":"/muddery-0.3.3/:5:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"启动游戏 # 进入刚才的游戏模板目录 cd /opt/muddery/mygame # 执行启动 (初次启动 会让 创建一个 超级管理员 ) (pyenv) [root@payment mygame]# muddery -i start Create a superuser below. The superuser is Account #1, the 'owner' account of the server. Username: Email address: Password: Password (again): Superuser created successfully. Portal starting ... ... Portal started. Server starting ... ... Server started. Evennia running. ----------------------- Evennia --- Muddery Portal 0.8.0 (rev bab4b86) external ports: webserver-proxy: 8000 webclient-websocket: 8001 internal_ports (to Server): webserver: 5001 amp: 5000 Muddery Server 0.8.0 (rev bab4b86) internal ports (to Portal): webserver: 5001 amp : 5000 ----------------------------------- ","date":"2019-02-27","objectID":"/muddery-0.3.3/:6:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"登录 web-ui # 浏览器 访问 http://127.0.0.1:8000/ # 输入 第一次 muddery -i start 时 创建的 管理员账号密码 # 游戏编辑器 http://127.0.0.1:8000/editor/views/main.html ","date":"2019-02-27","objectID":"/muddery-0.3.3/:7:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["python"],"content":"编辑游戏 编辑器中各 选项的属性 请参考 文档 http://www.muddery.org/?cate=docs\u0026content=documentations 点击 web ui 中的 编辑游戏 打开 游戏编辑器 1. 添加 区域 首先我们点击 世界地图 --\u003e 区域 --\u003e Add 添加一个新手村 2. 添加 房间 点击 世界地图 --\u003e 房间 --\u003e Add 添加一个房间 , 该房间创建于上面添加的 新手村区域中 3. 添加 角色模板 点击 角色 --\u003e 角色模板 --\u003e Add 添加 角色模板. 4. 添加 世界NPC 点击 角色 --\u003e 世界NPC --\u003e Add 添加 世界NPC 5. 添加 对话 点击 对话 --\u003e 对话列表 --\u003e Add 添加 对话 # 添加对话内容 必须要保存了 对话才能添加 点击 对话 --\u003e 对话列表 --\u003e 刚才添加的对话 --\u003e Edit 点击 Sentences --\u003e Add 添加完以上东西以后，再回去配置 游戏设置 6. 配置 游戏设置 点击 基础设置 --\u003e 游戏设置 7. 应用到游戏 点击 管理 --\u003e 应用 Changes Applied. Please wait the server to restart. 这里需要手动到 服务器上面 执行重启游戏服务 8. 登录游戏 http://127.0.0.1:8000/webclient/main.html ","date":"2019-02-27","objectID":"/muddery-0.3.3/:8:0","tags":["Muddery","Game","python"],"title":"Muddery 部署 配置文档","uri":"/muddery-0.3.3/"},{"categories":["fabric","kubernetes"],"content":"Hyperledger Fabric to kubernetes","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"Hyperledger Fabric 环境说明 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:0:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"系统说明 IP Hostname 标识 192.168.0.247 kubernetes-1 K8S-Master and Kubectl-Cli 192.168.0.248 kubernetes-2 K8S-Master and Node 192.168.0.249 kubernetes-3 K8S-Node and NFS Server ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:1:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"服务说明 名称 版本 CentOS 7 x64 Hyperledger Fabric 1.4 kubernetes 1.13.2 docker 18.06.1-ce Fabric 环境配置 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:2:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"docker 配置 由于 实例化 chaincode 需要由 docker 创建容器 而 这个容器需要跟 k8s 里的 peer svc 通信, 所以需要额外配置 # 配置 docker 配置 修改 docker 选项 DOCKER_OPTS 在 DOCKER_OPTS 中 增加 --dns 添加 k8s 中 dns 的 ip [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE coredns ClusterIP 10.233.0.3 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 4d23h # 我这个环境中 coredns IP 为 10.233.0.3 # 所以 添加 --dns 10.233.0.3 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:3:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"cryptogen 下载 cryptogen 用于简化生成 fabric 所需要的所有证书 # 官方离线下载地址为 https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/ # 选择相应版本 CentOS 选择 linux-amd64-1.4.0 # Mac 选择 darwin-amd64-1.4.0 # 创建工作目录 ( 后续所有文件都存于此目录中 ) mkdir -p /opt/jicki cd /opt/jicki wget https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/linux-amd64-1.4.0/hyperledger-fabric-linux-amd64-1.4.0.tar.gz # 解压文件 [root@kubernetes-1 /opt/jicki]# tar zxvf hyperledger-fabric-linux-amd64-1.4.0.tar.gz # 删除 config 这个文件夹, 这里用不到 [root@kubernetes-1 /opt/jicki]# rm -rf config # 查看文件 [root@kubernetes-1 /opt/jicki]# tree . └── bin ├── configtxgen ├── configtxlator ├── cryptogen ├── discover ├── get-docker-images.sh ├── idemixgen ├── orderer └── peer 1 directory, 8 files # 为方便使用 我们配置一个 环境变量 vi /etc/profile # fabric env export PATH=$PATH:/opt/jicki/bin # 使文件生效 source /etc/profile ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:4:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"Fabric 源码下载 [root@kubernetes-1 /opt/jicki]# git clone https://github.com/hyperledger/fabric ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:5:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"生成 证书 # 创建 cryptogen.yaml 文件, 1.4版本 crypto-config.yaml 文件更改为 cryptogen.yaml vi cryptogen.yaml OrdererOrgs: - Name: Orderer Domain: orgorderer1 CA: Country: CN Province: GuangDong Locality: ShenZhen Specs: - Hostname: orderer0 PeerOrgs: - Name: Org1 Domain: org1 EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 - Name: Org2 Domain: org2 EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 # 生成证书 [root@kubernetes-1 /opt/jicki]# cryptogen generate --config=./cryptogen.yaml org1 org2 # 查看生成目录结构 [root@kubernetes-1 /opt/jicki]# tree -d crypto-config/ crypto-config/ ├── ordererOrganizations │ └── orgorderer1 │ ├── ca │ ├── msp │ │ ├── admincerts │ │ ├── cacerts │ │ └── tlscacerts │ ├── orderers │ │ └── orderer0.orgorderer1 │ │ ├── msp │ │ │ ├── admincerts │ │ │ ├── cacerts │ │ │ ├── keystore │ │ │ ├── signcerts │ │ │ └── tlscacerts │ │ └── tls │ ├── tlsca │ └── users │ └── Admin@orgorderer1 │ ├── msp │ │ ├── admincerts │ │ ├── cacerts │ │ ├── keystore │ │ ├── signcerts │ │ └── tlscacerts │ └── tls └── peerOrganizations ├── org1 │ ├── ca │ ├── msp │ │ ├── admincerts │ │ ├── cacerts │ │ └── tlscacerts │ ├── peers │ │ ├── peer0.org1 │ │ │ ├── msp │ │ │ │ ├── admincerts │ │ │ │ ├── cacerts │ │ │ │ ├── keystore │ │ │ │ ├── signcerts │ │ │ │ └── tlscacerts │ │ │ └── tls │ │ └── peer1.org1 │ │ ├── msp │ │ │ ├── admincerts │ │ │ ├── cacerts │ │ │ ├── keystore │ │ │ ├── signcerts │ │ │ └── tlscacerts │ │ └── tls │ ├── tlsca │ └── users │ ├── Admin@org1 │ │ ├── msp │ │ │ ├── admincerts │ │ │ ├── cacerts │ │ │ ├── keystore │ │ │ ├── signcerts │ │ │ └── tlscacerts │ │ └── tls │ └── User1@org1 │ ├── msp │ │ ├── admincerts │ │ ├── cacerts │ │ ├── keystore │ │ ├── signcerts │ │ └── tlscacerts │ └── tls └── org2 ├── ca ├── msp │ ├── admincerts │ ├── cacerts │ └── tlscacerts ├── peers │ ├── peer0.org2 │ │ ├── msp │ │ │ ├── admincerts │ │ │ ├── cacerts │ │ │ ├── keystore │ │ │ ├── signcerts │ │ │ └── tlscacerts │ │ └── tls │ └── peer1.org2 │ ├── msp │ │ ├── admincerts │ │ ├── cacerts │ │ ├── keystore │ │ ├── signcerts │ │ └── tlscacerts │ └── tls ├── tlsca └── users ├── Admin@org2 │ ├── msp │ │ ├── admincerts │ │ ├── cacerts │ │ ├── keystore │ │ ├── signcerts │ │ └── tlscacerts │ └── tls └── User1@org2 ├── msp │ ├── admincerts │ ├── cacerts │ ├── keystore │ ├── signcerts │ └── tlscacerts └── tls ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:6:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"生成初始化Fabric文件 # 创建 configtx.yaml 文件 vi configtx.yaml Organizations: - \u0026OrdererOrg Name: Orgorderer1MSP ID: Orgorderer1MSP MSPDir: crypto-config/ordererOrganizations/orgorderer1/msp Policies: Readers: Type: Signature Rule: \"OR('Orgorderer1MSP.member')\" Writers: Type: Signature Rule: \"OR('Orgorderer1MSP.member')\" Admins: Type: Signature Rule: \"OR('Orgorderer1MSP.admin')\" - \u0026Org1 Name: Org1MSP ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1/msp Policies: Readers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.peer', 'Org1MSP.client')\" Writers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.client')\" Admins: Type: Signature Rule: \"OR('Org1MSP.admin')\" AnchorPeers: - Host: peer0.org1 Port: 7051 - \u0026Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2/msp Policies: Readers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.peer', 'Org2MSP.client')\" Writers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.client')\" Admins: Type: Signature Rule: \"OR('Org2MSP.admin')\" AnchorPeers: - Host: peer0.org2 Port: 7051 Capabilities: Channel: \u0026ChannelCapabilities V1_3: true Orderer: \u0026OrdererCapabilities V1_1: true Application: \u0026ApplicationCapabilities V1_3: true V1_2: false V1_1: false Application: \u0026ApplicationDefaults Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ApplicationCapabilities Orderer: \u0026OrdererDefaults OrdererType: kafka Addresses: - orderer0.orgorderer1:7050 BatchTimeout: 2s BatchSize: MaxMessageCount: 10 AbsoluteMaxBytes: 99 MB PreferredMaxBytes: 512 KB Kafka: Brokers: - kafka-0.broker.kafka:9092 - kafka-1.broker.kafka:9092 - kafka-2.broker.kafka:9092 - kafka-3.broker.kafka:9092 Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" BlockValidation: Type: ImplicitMeta Rule: \"ANY Writers\" Channel: \u0026ChannelDefaults Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ChannelCapabilities Profiles: TwoOrgsOrdererGenesis: \u003c\u003c: *ChannelDefaults Orderer: \u003c\u003c: *OrdererDefaults Organizations: - *OrdererOrg Capabilities: \u003c\u003c: *OrdererCapabilities Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: \u003c\u003c: *ApplicationDefaults Organizations: - *Org1 - *Org2 Capabilities: \u003c\u003c: *ApplicationCapabilities SampleDevModeKafka: \u003c\u003c: *ChannelDefaults Capabilities: \u003c\u003c: *ChannelCapabilities Orderer: \u003c\u003c: *OrdererDefaults OrdererType: kafka Kafka: Brokers: - kafka-0.broker.kafka:9092 - kafka-1.broker.kafka:9092 - kafka-2.broker.kafka:9092 - kafka-3.broker.kafka:9092 Organizations: - *OrdererOrg Capabilities: \u003c\u003c: *OrdererCapabilities Application: \u003c\u003c: *ApplicationDefaults Organizations: - \u003c\u003c: *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 # 首先需要创建一个文件夹 mkdir -p /opt/jicki/channel-artifacts # 创建 创世区块 TwoOrgsOrdererGenesis 是 # configtx.yaml 中 Profiles 字段下的名称 [root@kubernetes-1 /opt/jicki]# configtxgen -profile TwoOrgsOrdererGenesis \\ -outputBlock ./channel-artifacts/genesis.block # 下面来生成一个 peer 服务 中使用的 tx 文件 TwoOrgsChannel 名称为 configtx.yaml 中 Profiles 字段下的，这里必须指定上面的 channelID [root@kubernetes-1 /opt/jicki]# configtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel # 定义组织 生成锚节点更新文件 # Org1MSP [root@kubernetes-1 /opt/jicki]# configtxgen -profile TwoOrgsChannel \\ -outputAnchorPeersUpdate ./channel-artifacts/Org1MSPanchors.tx -channelID mychannel -asOrg Org1MSP # Org2MSP [root@kubernetes-1 /opt/jicki]# configtxgen -profile TwoOrgsChannel \\ -outputAnchorPeersUpdate ./channel-artifacts/Org2MSPanchors.tx -channelID mychannel -asOrg Org2MSP # 查看生成文件 [root@kubernetes-1 /opt/jicki]# tree channel-artifacts/ chan","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:7:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"安装NFS # 服务端 安装 NFS 软件 [root@kubernetes-3 ~]# yum -y install nfs-utils rpcbind # k8s 所有节点 安装 NFS 客户端 [root@kubernetes-1 ~]# yum -y install nfs-utils [root@kubernetes-2 ~]# yum -y install nfs-utils ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:8:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"配置 NFS 这里需要创建几个目录 /opt/nfs/data, /opt/nfs/fabric/ /opt/nfs/data 用于存放 zk, kafka 数据 /opt/nfs/fabric 用于存放 fabric 所有的文件 # 创建目录 [root@kubernetes-3 ~]# mkdir -p /opt/nfs/{data,fabric} # 修改配置文件 [root@kubernetes-3 ~]# vi /etc/exports 增加 /opt/nfs/data 192.168.0.0/24(rw,sync,no_root_squash) /opt/nfs/fabric 192.168.0.0/24(rw,sync,no_root_squash) # 启动 NFS 服务 [root@kubernetes-3 ~]# systemctl enable rpcbind.service [root@kubernetes-3 ~]# systemctl enable nfs-server.service [root@kubernetes-3 ~]# systemctl start rpcbind.service [root@kubernetes-3 ~]# systemctl start nfs-server.service [root@kubernetes-3 ~]# systemctl status rpcbind.service [root@kubernetes-3 ~]# systemctl status nfs-server.service # 查看服务 [root@kubernetes-3 ~]# showmount -e 192.168.0.249 Export list for 192.168.0.249: /opt/nfs/fabric 192.168.0.0/24 /opt/nfs/data 192.168.0.0/24 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:9:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"配置 NFS Client Provisioner 官方 说明 https://github.com/kubernetes-incubator/external-storage/tree/master/nfs-client # 下载官方 yaml 文件 wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/rbac.yaml wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/deployment.yaml wget https://raw.githubusercontent.com/kubernetes-incubator/external-storage/master/nfs-client/deploy/class.yaml # 修改 deployment.yaml 文件 vi deployment.yaml apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccountName: nfs-client-provisioner containers: - name: nfs-client-provisioner image: quay.io/external_storage/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 192.168.0.249 - name: NFS_PATH value: /opt/nfs/data volumes: - name: nfs-client-root nfs: server: 192.168.0.249 path: /opt/nfs/data # 修改 class.yaml , 这里修改 name 名称 vi class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: zk-kafka-nfs-storage provisioner: fuseim.pri/ifs parameters: archiveOnDelete: \"false\" # 导入 yaml 文件 [root@kubernetes-1 /opt/yaml/nfs-client]# kubectl apply -f . storageclass.storage.k8s.io/fabric-nfs-storage created serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created deployment.extensions/nfs-client-provisioner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created # 查看服务 [root@kubernetes-1 /opt/yaml/nfs-client]# kubectl get storageclass NAME PROVISIONER AGE zk-kafka-nfs-storage fuseim.pri/ifs 27s [root@kubernetes-1 /opt/yaml/nfs-client]# kubectl get pods NAME READY STATUS RESTARTS AGE nfs-client-provisioner-578785c589-qlgnx 1/1 Running 0 2m24s 部署 fabric ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:10:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"创建 zookeeper [root@kubernetes-1 /opt/jicki/k8s-yaml]# vi kafka-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: kafka # 创建 zk [root@kubernetes-1 /opt/jicki/k8s-yaml]# vi zookeeper.yaml apiVersion: apps/v1 kind: StatefulSet metadata: name: zoo namespace: kafka spec: serviceName: \"zoo\" replicas: 4 template: metadata: labels: app: zookeeper spec: terminationGracePeriodSeconds: 10 containers: - name: zookeeper image: hyperledger/fabric-zookeeper:0.4.14 ports: - containerPort: 2181 name: client - containerPort: 2888 name: peer - containerPort: 3888 name: leader-election volumeMounts: - name: zkdata mountPath: /var/lib/zookeeper volumeClaimTemplates: - metadata: name: zkdata annotations: volume.beta.kubernetes.io/storage-class: \"zk-kafka-nfs-storage\" spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 10Gi --- apiVersion: v1 kind: Service metadata: name: zoo namespace: kafka spec: ports: - port: 2888 name: peer - port: 3888 name: leader-election clusterIP: None selector: app: zookeeper --- apiVersion: v1 kind: Service metadata: name: zookeeper namespace: kafka spec: ports: - port: 2181 name: client selector: app: zookeeper # 导入 yaml 文件 [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f namespace.yaml [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f zookeeper.yaml # 查看服务 [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl get all -n kafka NAME READY STATUS RESTARTS AGE pod/zoo-0 1/1 Running 0 20m pod/zoo-1 1/1 Running 0 9m11s pod/zoo-2 1/1 Running 0 111s pod/zoo-3 1/1 Running 0 108s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/zoo ClusterIP None \u003cnone\u003e 2888/TCP,3888/TCP 21m service/zookeeper ClusterIP 10.233.22.96 \u003cnone\u003e 2181/TCP 21m NAME READY AGE statefulset.apps/zoo 4/4 20m ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:11:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"创建 kafka vi kafka.yaml apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: kafka namespace: kafka spec: serviceName: \"broker\" replicas: 4 template: metadata: labels: app: kafka spec: terminationGracePeriodSeconds: 10 containers: - name: broker image: hyperledger/fabric-kafka:0.4.14 ports: - containerPort: 9092 env: - name: KAFKA_MESSAGE_MAX_BYTES value: \"102760448\" - name: KAFKA_REPLICA_FETCH_MAX_BYTES value: \"102760448\" - name: KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE value: \"false\" - name: KAFKA_ZOOKEEPER_CONNECT value: zoo-0.zoo:2181,zoo-1.zoo:2181,zoo-2.zoo:2181,zoo-3.zoo:2181 - name: KAFKA_PORT value: \"9092\" - name: GODEBUG value: netdns=go - name: KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS value: \"30000\" - name: KAFKA_LOG_DIRS value: /opt/kafka/data - name: KAFKA_LOG_RETENTION_MS value: \"-1\" volumeMounts: - name: kafkadata mountPath: /opt/kafka/data volumeClaimTemplates: - metadata: name: kafkadata annotations: volume.beta.kubernetes.io/storage-class: \"zk-kafka-nfs-storage\" spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 20Gi --- apiVersion: v1 kind: Service metadata: name: kafka namespace: kafka spec: ports: - port: 9092 selector: app: kafka --- apiVersion: v1 kind: Service metadata: name: broker namespace: kafka spec: ports: - port: 9092 clusterIP: None selector: app: kafka # 导入 yaml 文件 [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f kafka.yaml statefulset.apps/kafka created service/kafka created service/broker created # 查看服务 [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl get pods -n kafka NAME READY STATUS RESTARTS AGE kafka-0 1/1 Running 0 2m26s kafka-1 1/1 Running 0 100s kafka-2 1/1 Running 0 53s kafka-3 1/1 Running 0 43s [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl get svc -n kafka NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE broker ClusterIP None \u003cnone\u003e 9092/TCP 2m45s kafka ClusterIP 10.233.48.233 \u003cnone\u003e 9092/TCP 2m45s [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl get statefulset -n kafka NAME READY AGE kafka 4/4 3m13s ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:12:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"创建 fabric 服务 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:13:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"初始化 # 在 操作的机器上 挂载 nfs 目录 /opt/nfs/fabric # 为了预先拷贝一些证书进去 mkdir -p /opt/nfs/fabric mount -t nfs 192.168.0.249:/opt/nfs/fabric /opt/nfs/fabric # 拷贝文件到挂载目录 # 创建两个目录 data 数据目录, 以及 证书，创世区块 等文件目录 mkdir -p /opt/nfs/fabric/{data,fabric} # 拷贝 证书文件 创世区块 文件 mkdir ./channel-artifacts/chaincode cp -r ./fabric/examples/chaincode/go/example* channel-artifacts/chaincode/ cp ./channel-artifacts/genesis.block ./crypto-config/ordererOrganizations/* cp -r ./crypto-config /opt/nfs/fabric/fabric/ cp -r ./channel-artifacts /opt/nfs/fabric/fabric/ # 创建 fabric 运行时生成的共享数据文件夹 mkdir -p /opt/nfs/fabric/data/{orderer,peer} mkdir -p /opt/nfs/fabric/data/orderer/orgorderer1/orderer0 mkdir -p /opt/nfs/fabric/data/peer/org{1,2}/ca mkdir -p /opt/nfs/fabric/data/peer/org{1,2}/peer{0,1}/{couchdb,peerdata} ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:13:1","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"配置 orderer # 创建 orderer1 的 namespaces vi orgorderer1-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: orgorderer1 # 创建 orderer1 的 pv 与 pvc vi orgorderer1-pv-pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: orgorderer1-pv labels: app: orgorderer1-pv spec: capacity: storage: 500Mi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/fabric/crypto-config/ordererOrganizations/orgorderer1 server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: orgorderer1 name: orgorderer1-pv spec: accessModes: - ReadWriteMany resources: requests: storage: 10Mi selector: matchLabels: app: orgorderer1-pv --- apiVersion: v1 kind: PersistentVolume metadata: name: orgorderer1-pvdata labels: app: orgorderer1-pvdata spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/data/orderer/orgorderer1 server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: orgorderer1 name: orgorderer1-pvdata spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi selector: matchLabels: app: orgorderer1-pvdata # 创建 orderer1 Deployment 服务 vi orderer0.orgorderer1.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: orgorderer1 name: orderer0-orgorderer1 spec: replicas: 1 strategy: {} template: metadata: labels: app: hyperledger role: orderer org: orgorderer1 orderer-id: orderer0 spec: containers: - name: orderer0-orgorderer1 image: hyperledger/fabric-orderer:1.4.0 env: - name: ORDERER_GENERAL_LOGLEVEL value: debug - name: ORDERER_GENERAL_LISTENADDRESS value: 0.0.0.0 - name: ORDERER_GENERAL_GENESISMETHOD value: file - name: ORDERER_GENERAL_GENESISFILE value: /var/hyperledger/orderer/orderer.genesis.block - name: ORDERER_GENERAL_LOCALMSPID value: Orgorderer1MSP - name: ORDERER_GENERAL_LOCALMSPDIR value: /var/hyperledger/orderer/msp - name: ORDERER_GENERAL_TLS_ENABLED value: \"false\" - name: ORDERER_GENERAL_TLS_PRIVATEKEY value: /var/hyperledger/orderer/tls/server.key - name: ORDERER_GENERAL_TLS_CERTIFICATE value: /var/hyperledger/orderer/tls/server.crt - name: ORDERER_GENERAL_TLS_ROOTCAS value: '[/var/hyperledger/orderer/tls/ca.crt]' - name: ORDERER_KAFKA_VERSION value: 0.10.0.1 - name: ORDERER_KAFKA_VERBOSE value: \"true\" - name: ORDERER_KAFKA_BROKERS value: \"kafka-0.broker.kafka:9092,kafka-1.broker.kafka:9092,kafka-2.broker.kafka:9092,kafka-3.broker.kafka:9092\" - name: GODEBUG value: netdns=go workingDir: /opt/gopath/src/github.com/hyperledger/fabric ports: - containerPort: 7050 command: [\"orderer\"] volumeMounts: - mountPath: /var/hyperledger/orderer/msp name: certificate subPath: orderers/orderer0.orgorderer1/msp - mountPath: /var/hyperledger/orderer/tls name: certificate subPath: orderers/orderer0.orgorderer1/tls - mountPath: /var/hyperledger/orderer/orderer.genesis.block name: certificate subPath: genesis.block - mountPath: /var/hyperledger/production name: ordererdata subPath: orderer0 volumes: - name: certificate persistentVolumeClaim: claimName: orgorderer1-pv - name: ordererdata persistentVolumeClaim: claimName: orgorderer1-pvdata --- apiVersion: v1 kind: Service metadata: name: orderer0 namespace: orgorderer1 spec: selector: app: hyperledger role: orderer orderer-id: orderer0 org: orgorderer1 type: NodePort ports: - name: listen-endpoint protocol: TCP port: 7050 targetPort: 7050 nodePort: 32000 # 导入 yaml 文件 创建服务 # 创建 namespaces [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f orgorderer1-namespace.yaml namespace/orgorderer1 created # 创建 pv pvc [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f orgorderer1-pv-pvc.yaml persistentvolume/orgorderer1-pv created persistentvolumeclaim/orgorderer1-pv created persistentvolume/orgorderer1-pvdata created persistentvolumeclaim/orgorderer1-pvdata created # 创建 deployment [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f orderer0.orgorderer1.yaml deployment.extensions/orderer0-orgorderer1 created service/orderer0 created # 查看创建的服务 [root@kubernetes-1 /opt","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:13:2","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"配置 org1 服务 每个 org 包含 ca cli peer 三个服务 # 配置 org1 namespaces vi org1-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: org1 # 配置 org1 pv 与 pvc vi org1-pv-pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: org1-pv labels: app: org1-pv spec: capacity: storage: 500Mi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/fabric/crypto-config/peerOrganizations/org1 server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: org1 name: org1-pv spec: accessModes: - ReadWriteMany resources: requests: storage: 10Mi selector: matchLabels: app: org1-pv --- apiVersion: v1 kind: PersistentVolume metadata: name: org1-pvdata labels: app: org1-pvdata spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/data/peer/org1 server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: org1 name: org1-pvdata spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi selector: matchLabels: app: org1-pvdata # 配置 org1 ca # FABRIC_CA_SERVER_TLS_KEYFILE 配置为自己生成的 sk # sk 存在目录 crypto-config/peerOrganizations/org1/ca/ vi org1-ca.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: org1 name: ca spec: replicas: 1 strategy: {} template: metadata: labels: app: hyperledger role: ca org: org1 name: ca spec: containers: - name: ca image: hyperledger/fabric-ca:1.4.0 env: - name: FABRIC_CA_HOME value: /etc/hyperledger/fabric-ca-server - name: FABRIC_CA_SERVER_CA_NAME value: ca - name: FABRIC_CA_SERVER_TLS_ENABLED value: \"false\" - name: FABRIC_CA_SERVER_TLS_CERTFILE value: /etc/hyperledger/fabric-ca-server-config/ca.org1-cert.pem - name: FABRIC_CA_SERVER_TLS_KEYFILE value: /etc/hyperledger/fabric-ca-server-config/904aa978bf690c8198526a6410045cc308468ca7171d02af70ffc7d969eb4c7e_sk - name: GODEBUG value: netdns=go ports: - containerPort: 7054 command: [\"sh\"] args: [\"-c\", \" fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org1-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/904aa978bf690c8198526a6410045cc308468ca7171d02af70ffc7d969eb4c7e_sk -b admin:adminpw -d \"] volumeMounts: - mountPath: /etc/hyperledger/fabric-ca-server-config name: certificate subPath: ca/ - mountPath: /etc/hyperledger/fabric-ca-server name: cadata subPath: ca/ volumes: - name: certificate persistentVolumeClaim: claimName: org1-pv - name: cadata persistentVolumeClaim: claimName: org1-pvdata --- apiVersion: v1 kind: Service metadata: namespace: org1 name: ca spec: selector: app: hyperledger role: ca org: org1 name: ca type: NodePort ports: - name: endpoint protocol: TCP port: 7054 targetPort: 7054 nodePort: 30000 # 配置 org1 peer 服务 # 这里有两个 peer 分别为 peer0, peer1 vi peer0.org1.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: org1 name: peer0-org1 spec: replicas: 1 strategy: {} template: metadata: creationTimestamp: null labels: app: hyperledger role: peer peer-id: peer0 org: org1 spec: containers: - name: couchdb image: hyperledger/fabric-couchdb:0.4.10 env: - name: COUCHDB_USER value: \"\" - name: COUCHDB_PASSWORD value: \"\" ports: - containerPort: 5984 volumeMounts: - mountPath: /opt/couchdb/data name: peerdata subPath: peer0/couchdb - name: peer0-org1 image: hyperledger/fabric-peer:1.4.0 env: - name: CORE_LEDGER_STATE_STATEDATABASE value: \"CouchDB\" - name: CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS value: \"localhost:5984\" - name: CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME value: \"\" - name: CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD value: \"\" - name: CORE_VM_ENDPOINT value: \"unix:///host/var/run/docker.sock\" - name: CORE_LOGGING_LEVEL value: \"DEBUG\" - name: CORE_PEER_TLS_ENABLED value: \"false\" - name: CORE_PEER_GOSSIP_USELEADERELECTION value: \"true\" - name: CORE_PEER_GOSSIP_ORGLEADER value: \"false\" - name: CORE_PEER_PROFILE_ENABLED value: \"true\" - name: CORE_PEER_TLS_CERT_FILE value: \"/etc/hyperledger/fabric/tls/server.crt\" - name: CORE_PEER_TLS_KEY_FILE value: \"/etc/hyperledger/fabric/tls","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:13:3","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"配置 org2 服务 # 配置 org2 namespaces vi org2-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: org2 # 配置 org2 pv pvc vi org2-pv-pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: org2-pv labels: app: org2-pv spec: capacity: storage: 500Mi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/fabric/crypto-config/peerOrganizations/org2 server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: org2 name: org2-pv spec: accessModes: - ReadWriteMany resources: requests: storage: 10Mi selector: matchLabels: app: org2-pv --- apiVersion: v1 kind: PersistentVolume metadata: name: org2-pvdata labels: app: org2-pvdata spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/data/peer/org2 server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: org2 name: org2-pvdata spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi selector: matchLabels: app: org2-pvdata # 配置 org2 ca # FABRIC_CA_SERVER_TLS_KEYFILE 配置为自己生成的 sk # sk 存在目录 crypto-config/peerOrganizations/org2/ca/ vi org2-ca.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: org2 name: ca spec: replicas: 1 strategy: {} template: metadata: labels: app: hyperledger role: ca org: org2 name: ca spec: containers: - name: ca image: hyperledger/fabric-ca:1.4.0 env: - name: FABRIC_CA_HOME value: /etc/hyperledger/fabric-ca-server - name: FABRIC_CA_SERVER_CA_NAME value: ca - name: FABRIC_CA_SERVER_TLS_ENABLED value: \"false\" - name: FABRIC_CA_SERVER_TLS_CERTFILE value: /etc/hyperledger/fabric-ca-server-config/ca.org2-cert.pem - name: FABRIC_CA_SERVER_TLS_KEYFILE value: /etc/hyperledger/fabric-ca-server-config/44299d5dfb204e07b5274a7048269171876157d1efdab11a5ac631bd78d2fe0d_sk - name: GODEBUG value: netdns=go ports: - containerPort: 7054 command: [\"sh\"] args: [\"-c\", \" fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org2-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/44299d5dfb204e07b5274a7048269171876157d1efdab11a5ac631bd78d2fe0d_sk -b admin:adminpw -d \"] volumeMounts: - mountPath: /etc/hyperledger/fabric-ca-server-config name: certificate subPath: ca/ - mountPath: /etc/hyperledger/fabric-ca-server name: cadata subPath: ca/ volumes: - name: certificate persistentVolumeClaim: claimName: org2-pv - name: cadata persistentVolumeClaim: claimName: org2-pvdata --- apiVersion: v1 kind: Service metadata: namespace: org2 name: ca spec: selector: app: hyperledger role: ca org: org2 name: ca type: NodePort ports: - name: endpoint protocol: TCP port: 7054 targetPort: 7054 nodePort: 30100 # 配置 org2 peer 服务 # 这里有两个 peer 分别为 peer0, peer1 vi peer0.org2.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: org2 name: peer0-org2 spec: replicas: 1 strategy: {} template: metadata: creationTimestamp: null labels: app: hyperledger role: peer peer-id: peer0 org: org2 spec: containers: - name: couchdb image: hyperledger/fabric-couchdb:0.4.10 env: - name: COUCHDB_USER value: \"\" - name: COUCHDB_PASSWORD value: \"\" ports: - containerPort: 5984 volumeMounts: - mountPath: /opt/couchdb/data name: peerdata subPath: peer0/couchdb - name: peer0-org2 image: hyperledger/fabric-peer:1.4.0 env: - name: CORE_LEDGER_STATE_STATEDATABASE value: \"CouchDB\" - name: CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS value: \"localhost:5984\" - name: CORE_LEDGER_STATE_COUCHDBCONFIG_USERNAME value: \"\" - name: CORE_LEDGER_STATE_COUCHDBCONFIG_PASSWORD value: \"\" - name: CORE_VM_ENDPOINT value: \"unix:///host/var/run/docker.sock\" - name: CORE_LOGGING_LEVEL value: \"DEBUG\" - name: CORE_PEER_TLS_ENABLED value: \"false\" - name: CORE_PEER_GOSSIP_USELEADERELECTION value: \"true\" - name: CORE_PEER_GOSSIP_ORGLEADER value: \"false\" - name: CORE_PEER_PROFILE_ENABLED value: \"true\" - name: CORE_PEER_TLS_CERT_FILE value: \"/etc/hyperledger/fabric/tls/server.crt\" - name: CORE_PEER_TLS_KEY_FILE value: \"/etc/hyperledger/fabric/tls/server.key\" - name: CORE_PEE","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:13:4","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"配置 cli 服务 cli 服务，用于 命令行操作组织节点的环境。 这里每一个 org 配置一个 cli 服务，方便操作不同的 org 节点 # 创建 org1 cli 的 pv pvc vi org1-cli-pv-pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: org1-cli-pv labels: app: org1-cli-pv spec: capacity: storage: 500Mi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/fabric/channel-artifacts server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: org1 name: org1-cli-pv spec: accessModes: - ReadWriteMany resources: requests: storage: 10Mi selector: matchLabels: app: org1-cli-pv # 创建 org1 cli 服务 vi org1-cli.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: org1 name: cli spec: replicas: 1 strategy: {} template: metadata: labels: app: cli spec: containers: - name: cli image: hyperledger/fabric-tools:1.4.0 env: - name: CORE_PEER_TLS_ENABLED value: \"false\" - name: CORE_VM_ENDPOINT value: unix:///host/var/run/docker.sock - name: GOPATH value: /opt/gopath - name: CORE_LOGGING_LEVEL value: DEBUG - name: CORE_PEER_ID value: cli - name: CORE_PEER_ADDRESS value: peer0.org1:7051 - name: CORE_PEER_LOCALMSPID value: Org1MSP - name: CORE_PEER_MSPCONFIGPATH value: /etc/hyperledger/fabric/msp - name: GODEBUG value: netdns=go workingDir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: [\"/bin/bash\", \"-c\", \"--\"] args: [\"while true; do sleep 30; done;\"] volumeMounts: - mountPath: /host/var/run/ name: run - mountPath: /etc/hyperledger/fabric/msp name: certificate subPath: users/Admin@org1/msp - mountPath: /opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts name: artifacts volumes: - name: certificate persistentVolumeClaim: claimName: org1-pv - name: artifacts persistentVolumeClaim: claimName: org1-cli-pv - name: run hostPath: path: /var/run # 导入 yaml 文件 [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f org1-cli-pv-pvc.yaml persistentvolume/org1-cli-pv created persistentvolumeclaim/org1-cli-pv created [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f org1-cli.yaml deployment.extensions/cli created # 创建 org2 cli 的 pv pvc vi org2-cli-pv-pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: org2-cli-pv labels: app: org2-cli-pv spec: capacity: storage: 500Mi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/fabric/channel-artifacts server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: org2 name: org2-cli-pv spec: accessModes: - ReadWriteMany resources: requests: storage: 10Mi selector: matchLabels: app: org2-cli-pv # 创建 org2 cli 服务 vi org2-cli.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: org2 name: cli spec: replicas: 1 strategy: {} template: metadata: labels: app: cli spec: containers: - name: cli image: hyperledger/fabric-tools:1.4.0 env: - name: CORE_PEER_TLS_ENABLED value: \"false\" - name: CORE_VM_ENDPOINT value: unix:///host/var/run/docker.sock - name: GOPATH value: /opt/gopath - name: CORE_LOGGING_LEVEL value: DEBUG - name: CORE_PEER_ID value: cli - name: CORE_PEER_ADDRESS value: peer0.org2:7051 - name: CORE_PEER_LOCALMSPID value: Org2MSP - name: CORE_PEER_MSPCONFIGPATH value: /etc/hyperledger/fabric/msp - name: GODEBUG value: netdns=go workingDir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: [\"/bin/bash\", \"-c\", \"--\"] args: [\"while true; do sleep 30; done;\"] volumeMounts: - mountPath: /host/var/run/ name: run - mountPath: /etc/hyperledger/fabric/msp name: certificate subPath: users/Admin@org2/msp - mountPath: /opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts name: artifacts volumes: - name: certificate persistentVolumeClaim: claimName: org2-pv - name: artifacts persistentVolumeClaim: claimName: org2-cli-pv - name: run hostPath: path: /var/run # 导入 yaml 文件 [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f org2-cli-pv-pvc.yaml persistentvolume/org2-cli-pv created persistentvolumeclaim/org2-cli-pv created [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl apply -f org2-cli.yaml deployment.extensions/cli c","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:13:5","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"测试 集群 测试集群 初始化 channel 2. 加入 channel 3. 更新为 锚节点 4. 安装 chaincode 5. 实例化 chaincode 6. 操作 chaincode ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:14:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"org1 节点 # 登录 org1 cli [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl exec -it cli-68647b4c-6db4f -n org1 bash 1. 初始化 创建channel peer channel create -o orderer0.orgorderer1:7050 -c mychannel -f ./channel-artifacts/channel.tx 拷贝 mychannel.block 文件到 channel-artifacts 共享目录 # 拷贝 cp mychannel.block channel-artifacts # 加入 channel peer channel join -b channel-artifacts/mychannel.block 2019-01-21 02:40:58.478 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 02:40:58.497 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 02:40:58.500 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2019-01-21 02:40:58.589 UTC [channelCmd] executeJoin -\u003e INFO 004 Successfully submitted proposal to join channel # 更新 锚节点 peer，每个 org 只需执行一次 # 更新对应的 org1 的 peer channel update -o orderer0.orgorderer1:7050 -c mychannel -f ./channel-artifacts/Org1MSPanchors.tx ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:14:1","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"org2 节点 # 登录 org2 节点 cli [root@kubernetes-1 /opt/jicki/k8s-yaml]# kubectl exec -it cli-5bd96dc66d-j2m6d -n org2 bash # 加入 mychannel peer channel join -b channel-artifacts/mychannel.block 2019-01-21 02:47:14.529 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 02:47:14.549 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 02:47:14.551 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2019-01-21 02:47:14.629 UTC [channelCmd] executeJoin -\u003e INFO 004 Successfully submitted proposal to join channel # 更新 锚节点 peer，每个 org 只需执行一次 # 更新对应的 org2 的 peer channel update -o orderer0.orgorderer1:7050 -c mychannel -f ./channel-artifacts/Org2MSPanchors.tx ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:14:2","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"安装 chaincode 安装 chaincode 需要在 org1 org2 的 cli 分别安装 # org1 kubectl exec -it cli-68647b4c-lhdhf -n org1 bash # org2 kubectl exec -it cli-5bd96dc66d-j2m6d -n org2 bash # 分别执行安装 peer chaincode install -n example2 -v 1.0 -p github.com/hyperledger/fabric/peer/channel-artifacts/chaincode/example02 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:14:3","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"实例化 chaincode 实例化 chaincode 只需要在 其中一个 org 执行就可以 peer chaincode instantiate -o orderer0.orgorderer1:7050 \\ -C mychannel -n example2 -v 1.0 \\ -c '{\"Args\":[\"init\",\"A\", \"100\", \"B\",\"200\"]}' \\ -P \"OR ('Org1MSP.peer','Org2MSP.peer')\" # 实例化以后~会在 docker 中生成 一个容器, 脱离于k8s 是由 docker 生成 [root@kubernetes-3 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 26666283f935 dev-peer0.org1-example2-1.0-e8792315af3bc6f98b5be21c4c98ece1ab5c65537e361691af638304c0670cd5 \"chaincode -peer.add…\" 6 minutes ago Up 6 minutes dev-peer0.org1-example2-1.0 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:14:4","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"操作 chaincode 执行 转账，查询 操作 # 查询 A, B 账户的值 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"A\"]}' # 输出如下: 2019-01-21 03:45:18.469 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 03:45:18.485 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 100 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"B\"]}' # 输出如下: 2019-01-21 03:47:21.683 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 03:47:21.701 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 200 # invoke 转账 # 从A账户 转账 50 个币 到 B 账户 peer chaincode invoke -C mychannel -n example2 -c '{\"Args\":[\"invoke\", \"A\", \"B\", \"50\"]}' # 输出如下: 2019-01-21 03:48:43.787 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 03:48:43.806 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 03:48:43.816 UTC [chaincodeCmd] InitCmdFactory -\u003e INFO 003 Retrieved channel (mychannel) orderer endpoint: orderer0.orgorderer1:7050 2019-01-21 03:48:43.830 UTC [chaincodeCmd] chaincodeInvokeOrQuery -\u003e INFO 004 Chaincode invoke successful. result: status:200 # 转账后在查询 A, B 的值 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"A\"]}' # 输出: 2019-01-21 03:49:10.686 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 03:49:10.702 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 50 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"B\"]}' # 输出: 2019-01-21 03:49:41.883 UTC [main] InitCmd -\u003e WARN 001 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 2019-01-21 03:49:41.898 UTC [main] SetOrdererEnv -\u003e WARN 002 CORE_LOGGING_LEVEL is no longer supported, please use the FABRIC_LOGGING_SPEC environment variable 250 blockchain-explorer 部署 blockchain-explorer 是 fabric 的区块浏览器 ","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:14:5","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["fabric","kubernetes"],"content":"配置 postgres # 创建 数据目录 , 创建于 nfs 中 mkdir -p /opt/nfs/fabric/data/postgres # 创建 namesapce vi explorer-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: explorer # 创建 pv pvc vi explorer-pv-pvc.yaml apiVersion: v1 kind: PersistentVolume metadata: name: postgres-pv labels: app: postgres-pv spec: capacity: storage: 10Gi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/data/postgres server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: explorer name: postgres-pv spec: accessModes: - ReadWriteMany resources: requests: storage: 10Gi selector: matchLabels: app: postgres-pv --- apiVersion: v1 kind: PersistentVolume metadata: name: explorer-pv labels: app: explorer-pv spec: capacity: storage: 500Mi accessModes: - ReadWriteMany nfs: path: /opt/nfs/fabric/fabric/crypto-config server: 192.168.0.249 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: namespace: explorer name: explorer-pv spec: accessModes: - ReadWriteMany resources: requests: storage: 100Mi selector: matchLabels: app: explorer-pv # 创建 postgres deployment vi postgres-deployment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: namespace: explorer name: postgres-db spec: replicas: 1 template: metadata: labels: name: explorer-db spec: containers: - name: postgres image: postgres:10-alpine env: - name: TZ value: \"Asia/Shanghai\" - name: DATABASE_DATABASE value: \"fabricexplorer\" - name: DATABASE_USERNAME value: \"jicki\" - name: DATABASE_PASSWORD value: \"password\" volumeMounts: - mountPath: /var/lib/postgresql/data name: postgresdata volumes: - name: postgresdata persistentVolumeClaim: claimName: postgres-pv --- apiVersion: v1 kind: Service metadata: name: postgres-db namespace: explorer labels: run: explorer-db spec: selector: name: explorer-db ports: - protocol: TCP port: 5432 # 查看 服务 [root@kubernetes-1 /opt/jicki/k8s-yaml/explorer]# kubectl get pods -n explorer NAME READY STATUS RESTARTS AGE postgres-db-7884d8c859-s8hqq 1/1 Running 0 32s # 初始化数据库 # 初始化数据库这边只能手动登录 pods 里操作一下, 导入表结构以及初始化数据。 kubectl get pods -n explorer NAME READY STATUS RESTARTS AGE postgres-db-7884d8c859-s8hqq 1/1 Running 0 2m33s kubectl exec -it postgres-db-7884d8c859-s8hqq -n explorer bash cd /tmp/ wget https://raw.githubusercontent.com/hyperledger/blockchain-explorer/master/app/persistence/fabric/postgreSQL/db/createdb.sh wget https://raw.githubusercontent.com/hyperledger/blockchain-explorer/master/app/persistence/fabric/postgreSQL/db/explorerpg.sql wget https://raw.githubusercontent.com/hyperledger/blockchain-explorer/master/app/persistence/fabric/postgreSQL/db/processenv.js wget https://raw.githubusercontent.com/hyperledger/blockchain-explorer/master/app/persistence/fabric/postgreSQL/db/updatepg.sql # 安装依赖软件 apk update apk add jq apk add nodejs apk add sudo rm -rf /var/cache/apk/* # 执行创建脚本 chmod +x ./createdb.sh ./createdb.sh # 看出输出 表示完成 You are now connected to database \"fabricexplorer\" as user \"postgres\". 退出pods # 创建 explorer 配置文件 config.json vi config.json { \"network-configs\": { \"network-1\": { \"version\": \"1.0\", \"clients\": { \"client-1\": { \"tlsEnable\": false, \"organization\": \"Org1MSP\", \"channel\": \"mychannel\", \"credentialStore\": { \"path\": \"./tmp/credentialStore_Org1/credential\", \"cryptoStore\": { \"path\": \"./tmp/credentialStore_Org1/crypto\" } } } }, \"channels\": { \"mychannel\": { \"peers\": { \"peer0.org1\": {}, \"peer1.org1\": {}, \"peer0.org2\": {}, \"peer1.org2\": {} }, \"connection\": { \"timeout\": { \"peer\": { \"endorser\": \"6000\", \"eventHub\": \"6000\", \"eventReg\": \"6000\" } } } } }, \"organizations\": { \"Org1MSP\": { \"mspid\": \"Org1MSP\", \"fullpath\": false, \"adminPrivateKey\": { \"path\": \"/fabric/peerOrganizations/org1/users/Admin@org1/msp/keystore\" }, \"signedCert\": { \"path\": \"/fabric/peerOrganizations/org1/users/Admin@org1/msp/signcerts\" } }, \"Org2MSP\": { \"mspid\": \"Org2MSP\", \"fullpath\": false, \"adminPrivateKey\": { \"path\": \"/fabric/peerOrganizations/org2/users/Admin@org2/msp/keystore\" }, \"signedCert\": { \"path\": \"/fabric/peerOrganizations/org2/users/Admin@org2/ms","date":"2019-01-21","objectID":"/hyperledger-fabric-1.4-to-k8s/:15:0","tags":["kubernetes","fabric"],"title":"Hyperledger Fabric to kubernetes","uri":"/hyperledger-fabric-1.4-to-k8s/"},{"categories":["linux"],"content":"Aliyun SSL 签注 mobileconfig","date":"2019-01-01","objectID":"/aliyunssl-sigend-mobileconfig/","tags":["linux"],"title":"Aliyun SSL 签注 mobileconfig","uri":"/aliyunssl-sigend-mobileconfig/"},{"categories":["linux"],"content":"签注 mobileconfig ","date":"2019-01-01","objectID":"/aliyunssl-sigend-mobileconfig/:0:0","tags":["linux"],"title":"Aliyun SSL 签注 mobileconfig","uri":"/aliyunssl-sigend-mobileconfig/"},{"categories":["linux"],"content":"申请证书 登录阿里云SSL证书申请 https://www.aliyun.com/product/cas?utm_medium=text\u0026utm_source=baidu\u0026utm_campaign=anquan\u0026utm_content=se_1000125802 选择免费版个人DV证书 –\u003e Symantec –\u003e 购买 ","date":"2019-01-01","objectID":"/aliyunssl-sigend-mobileconfig/:1:0","tags":["linux"],"title":"Aliyun SSL 签注 mobileconfig","uri":"/aliyunssl-sigend-mobileconfig/"},{"categories":["linux"],"content":"验证证书 购买成功, 转到控制台 –\u003e 证书申请 –\u003e 填写基本信息 (域名相关) –\u003e 验证服务器(DNS 与 文件) ","date":"2019-01-01","objectID":"/aliyunssl-sigend-mobileconfig/:2:0","tags":["linux"],"title":"Aliyun SSL 签注 mobileconfig","uri":"/aliyunssl-sigend-mobileconfig/"},{"categories":["linux"],"content":"下载证书 下载证书 –\u003e 需要下载 Apache 与 Nginx 这两个证书文件,两个都要下载 需要用到 Apache 证书的 xxx_xxx.com_public.crt 文件 需要用到 Nginx 证书的 xxx_xxx.com.key 与 xxx_xxx.com.pem 文件 ","date":"2019-01-01","objectID":"/aliyunssl-sigend-mobileconfig/:3:0","tags":["linux"],"title":"Aliyun SSL 签注 mobileconfig","uri":"/aliyunssl-sigend-mobileconfig/"},{"categories":["linux"],"content":"修改证书 将 public.crt 重命名为 mbaike.crt (https服务器端使用公钥证书文件） 将 key 重命名为 mbaike.key （https服务器端使用证书对应的key) 将 pem 重命名为 ca-bundle.pem (https服务器端使用证书pem) 需要 unsigned.mobilecofig 文件 （IOS端生成的未签名的配置描述文件） ","date":"2019-01-01","objectID":"/aliyunssl-sigend-mobileconfig/:4:0","tags":["linux"],"title":"Aliyun SSL 签注 mobileconfig","uri":"/aliyunssl-sigend-mobileconfig/"},{"categories":["linux"],"content":"签注 文件 openssl smime -sign -in unsigned.mobileconfig -out signed.mobileconfig -signer mbaike.crt -inkey mbaike.key -certfile ca-bundle.pem -outform der -nodetach 生成 signed.mobileconfig 文件 ","date":"2019-01-01","objectID":"/aliyunssl-sigend-mobileconfig/:5:0","tags":["linux"],"title":"Aliyun SSL 签注 mobileconfig","uri":"/aliyunssl-sigend-mobileconfig/"},{"categories":["kubernetes"],"content":"k8s 1.13.1 to kubespray","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"Kubernetes 1.13.1 kubespray Deploy a Production Ready Kubernetes Cluster kubespray 利用 Ansible 自动部署 更新 kubernetes 集群 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:0:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"环境说明 kubernetes-1: 192.168.0.247 kubernetes-2: 192.168.0.248 kubernetes-3: 192.168.0.249 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:1:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-1: 192.168.0.247 kubernetes-2: 192.168.0.248 kubernetes-3: 192.168.0.249 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:2:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"配置ssh key 认证 # 确保本机也可以 ssh 连接，否则下面部署失败 ssh-keygen -t rsa -N \"\" ssh-copy-id -i /root/.ssh/id_rsa.pub 192.168.0.247 ssh-copy-id -i /root/.ssh/id_rsa.pub 192.168.0.248 ssh-copy-id -i /root/.ssh/id_rsa.pub 192.168.0.249 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:3:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"更新内核 更新内核为 4.4.x , CentOS 默认为3.10.x # 导入 Key rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 安装 Yum 源 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm # 更新 kernel yum --enablerepo=elrepo-kernel install -y kernel-lt kernel-lt-devel # 配置 内核优先 grub2-set-default 0 # 重启系统 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:4:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"增加内核配置 vi /etc/sysctl.conf # docker net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 kubespray ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:5:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"安装依赖 # 安装 centos 额外的yum源 rpm -ivh https://dl.fedoraproject.org/pub/epel/7/x86_64/Packages/e/epel-release-7-11.noarch.rpm # make 缓存 yum clean all \u0026\u0026 yum makecache # 安装 git yum -y install git # ansible 必须 \u003e= 2.7 # 安装 软件 yum install -y python-pip python34 python-netaddr python34-pip ansible ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:6:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"下载源码 # git clone 源码 cd /opt/ git clone https://github.com/kubernetes-sigs/kubespray ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:7:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"安装 kubespray 依赖 cd /opt/kubespray # 安装依赖 pip install -r requirements.txt Successfully installed MarkupSafe-1.1.0 hvac-0.7.1 jinja2-2.10 netaddr-0.7.19 pbr-5.1.1 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:8:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"配置 kubespray ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:9:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"hosts 配置 # 复制一份 自己的配置 cd /opt/kubespray cp -rfp inventory/sample inventory/jicki # 修改配置 hosts.ini cd /opt/kubespray/inventory/jicki rm -rf hosts.ini vi hosts.ini [all] kubernetes-1 ansible_host=kubernetes-1 ansible_port=32 ip=192.168.0.247 etcd_member_name=etcd1 kubernetes-2 ansible_host=kubernetes-2 ansible_port=32 ip=192.168.0.248 etcd_member_name=etcd2 kubernetes-3 ansible_host=kubernetes-3 ansible_port=32 ip=192.168.0.249 etcd_member_name=etcd3 [kube-master] kubernetes-1 kubernetes-2 [etcd] kubernetes-1 kubernetes-2 kubernetes-3 [kube-node] kubernetes-2 kubernetes-3 [k8s-cluster:children] kube-master kube-node [calico-rr] ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:9:1","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"etcd 配置 cd /opt/kubespray/inventory/jicki/group_vars rm -rf etcd.yml vi etcd.yml etcd_compaction_retention: 1 #etcd_metrics: basic # etcd 内存限制 默认 512M etcd_memory_limit: \"2G\" # etcd data 文件大小，默认2G etcd_quota_backend_bytes: \"8G\" # etcd ssl 认证 # etcd_peer_client_auth: true ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:9:2","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"全局配置 all.yaml cd /opt/kubespray/inventory/jicki/group_vars/all vi all.yml # 修改如下配置: loadbalancer_apiserver_localhost: true # 加载内核模块，否则 ceph, gfs 等无法挂载客户端 kubelet_load_modules: true vi docker.yml # 修改如下配置: docker_daemon_graph: \"/opt/docker\" docker_registry_mirrors: - https://registry.docker-cn.com - https://mirror.aliyuncs.com ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:9:3","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"k8s 功能配置 cd /opt/kubespray/inventory/jicki/group_vars/k8s-cluster vi k8s-cluster.yml # 按照自己的需求修改 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:9:4","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"修改镜像 默认镜像 从 gcr.io/google-containers 下载, 修改为国内的地址 # 下载国内镜像 docker pull jicki/kube-proxy:v1.13.1 docker pull jicki/kube-controller-manager:v1.13.1 docker pull jicki/kube-scheduler:v1.13.1 docker pull jicki/kube-apiserver:v1.13.1 docker pull jicki/coredns:1.2.6 docker pull jicki/cluster-proportional-autoscaler-amd64:1.3.0 docker pull jicki/kubernetes-dashboard-amd64:v1.10.0 docker pull jicki/etcd:3.2.24 docker pull jicki/node:v3.1.3 docker pull jicki/ctl:v3.1.3 docker pull jicki/kube-controllers:v3.1.3 docker pull jicki/cni:v3.1.3 docker pull jicki/pause:3.1 docker pull jicki/pause-amd64:3.1 # 修改下载的url inventory/jicki/group_vars/k8s-cluster/k8s-cluster.yml kube_image_repo: \"gcr.io/google-containers\" 修改为 kube_image_repo: \"jicki\" sed -i 's/gcr\\.io\\/google_containers/jicki/g' roles/download/defaults/main.yml sed -i 's/quay\\.io\\/coreos/jicki/g' roles/download/defaults/main.yml sed -i 's/quay\\.io\\/calico/jicki/g' roles/download/defaults/main.yml ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:9:5","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"安装k8s集群 注： 创建集群会需要从 storage.googleapis.com 下载 二进制文件 如 kubeadm, hyperkube 等 cd /opt/kubespray ansible-playbook -i inventory/jicki/hosts.ini --become --become-user=root cluster.yml PLAY RECAP **************************************************************************************************************************************************************************************** kubernetes-1 : ok=355 changed=72 unreachable=0 failed=0 kubernetes-2 : ok=343 changed=70 unreachable=0 failed=0 kubernetes-3 : ok=285 changed=50 unreachable=0 failed=0 localhost : ok=1 changed=0 unreachable=0 failed=0 Friday 21 December 2018 13:48:23 +0800 (0:00:00.059) 0:09:56.520 ******* =============================================================================== kubernetes/preinstall : Update package management cache (YUM) ---------------------------------------------------------------------------------------------------------------------------- 108.02s download : container_download | download images for kubeadm config images ----------------------------------------------------------------------------------------------------------------- 90.71s bootstrap-os : Install pip for bootstrap -------------------------------------------------------------------------------------------------------------------------------------------------- 46.12s gather facts from all instances ----------------------------------------------------------------------------------------------------------------------------------------------------------- 26.64s kubernetes/master : kubeadm | Initialize first master ------------------------------------------------------------------------------------------------------------------------------------- 23.33s kubernetes/master : kubeadm | Init other uninitialized masters ---------------------------------------------------------------------------------------------------------------------------- 21.14s etcd : reload etcd ------------------------------------------------------------------------------------------------------------------------------------------------------------------------ 10.83s kubernetes/kubeadm : Restart all kube-proxy pods to ensure that they load the new configmap ------------------------------------------------------------------------------------------------ 9.07s etcd : wait for etcd up -------------------------------------------------------------------------------------------------------------------------------------------------------------------- 7.29s etcd : Gen_certs | Write etcd master certs ------------------------------------------------------------------------------------------------------------------------------------------------- 6.76s kubernetes-apps/ansible : Kubernetes Apps | Start Resources -------------------------------------------------------------------------------------------------------------------------------- 5.10s etcd : Refresh Time Fact ------------------------------------------------------------------------------------------------------------------------------------------------------------------- 4.52s container-engine/docker : Ensure old versions of Docker are not installed. | RedHat -------------------------------------------------------------------------------------------------------- 3.91s kubernetes/master : kubeadm | write out kubeadm certs -------------------------------------------------------------------------------------------------------------------------------------- 3.89s kubernetes/preinstall : Install packages requirements -------------------------------------------------------------------------------------------------------------------------------------- 3.74s kubernetes-apps/ansible : Kubernetes Apps | Lay Down CoreDNS Template ---------------------------------------------------------------------------------------------------------------------- 3.50s network_plugin/calico : Calico | Copy cni plugins ------------------------------------------------------------------------------------","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:10:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"验证k8s集群 # 查看集群状态 [root@kubernetes-1 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME kubernetes-1 Ready master 20m v1.13.1 192.168.0.247 \u003cnone\u003e CentOS Linux 7 (Core) 4.4.168-1.el7.elrepo.x86_64 docker://18.6.1 kubernetes-2 Ready master,node 19m v1.13.1 192.168.0.248 \u003cnone\u003e CentOS Linux 7 (Core) 4.4.168-1.el7.elrepo.x86_64 docker://18.6.1 kubernetes-3 Ready node 19m v1.13.1 192.168.0.249 \u003cnone\u003e CentOS Linux 7 (Core) 4.4.168-1.el7.elrepo.x86_64 docker://18.6.1 # 查看集群组件 [root@kubernetes-1 ~]# kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/calico-kube-controllers-7c499f67b4-tldlj 1/1 Running 0 18m kube-system pod/calico-node-6psl6 1/1 Running 0 18m kube-system pod/calico-node-dh5rv 1/1 Running 0 18m kube-system pod/calico-node-mhb2m 1/1 Running 0 18m kube-system pod/coredns-6fd7dbf94c-vvf5h 1/1 Running 0 18m kube-system pod/coredns-6fd7dbf94c-zns9w 1/1 Running 0 17m kube-system pod/dns-autoscaler-5b547856bc-t6brh 1/1 Running 0 17m kube-system pod/kube-apiserver-kubernetes-1 1/1 Running 0 19m kube-system pod/kube-apiserver-kubernetes-2 1/1 Running 0 19m kube-system pod/kube-controller-manager-kubernetes-1 1/1 Running 0 19m kube-system pod/kube-controller-manager-kubernetes-2 1/1 Running 0 19m kube-system pod/kube-proxy-dgt8r 1/1 Running 0 17m kube-system pod/kube-proxy-skmf9 1/1 Running 0 18m kube-system pod/kube-proxy-wj584 1/1 Running 0 17m kube-system pod/kube-scheduler-kubernetes-1 1/1 Running 0 19m kube-system pod/kube-scheduler-kubernetes-2 1/1 Running 0 19m kube-system pod/kubernetes-dashboard-d7978b5cc-2s8bw 1/1 Running 0 17m kube-system pod/nginx-proxy-kubernetes-3 1/1 Running 0 19m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.233.0.1 \u003cnone\u003e 443/TCP 20m kube-system service/coredns ClusterIP 10.233.0.3 \u003cnone\u003e 53/UDP,53/TCP,9153/TCP 18m kube-system service/kubernetes-dashboard ClusterIP 10.233.47.27 \u003cnone\u003e 443/TCP 17m NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/calico-node 3 3 3 3 3 \u003cnone\u003e 18m kube-system daemonset.apps/kube-proxy 3 3 3 3 3 beta.kubernetes.io/os=linux 20m NAMESPACE NAME READY UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/calico-kube-controllers 1/1 1 1 18m kube-system deployment.apps/coredns 2/2 2 2 18m kube-system deployment.apps/dns-autoscaler 1/1 1 1 17m kube-system deployment.apps/kubernetes-dashboard 1/1 1 1 17m NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/calico-kube-controllers-7c499f67b4 1 1 1 18m kube-system replicaset.apps/coredns-6fd7dbf94c 2 2 2 18m kube-system replicaset.apps/dns-autoscaler-5b547856bc 1 1 1 17m kube-system replicaset.apps/kubernetes-dashboard-d7978b5cc 1 1 1 17m # 查看 ipvs [root@kubernetes-1 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.233.0.1:443 rr -\u003e 192.168.0.247:6443 Masq 1 0 0 -\u003e 192.168.0.248:6443 Masq 1 0 0 TCP 10.233.0.3:53 rr -\u003e 10.233.90.131:53 Masq 1 0 0 -\u003e 10.233.101.2:53 Masq 1 0 0 TCP 10.233.0.3:9153 rr -\u003e 10.233.90.131:9153 Masq 1 0 0 -\u003e 10.233.101.2:9153 Masq 1 0 0 TCP 10.233.47.27:443 rr -\u003e 10.233.101.1:8443 Masq 1 0 0 UDP 10.233.0.3:53 rr -\u003e 10.233.90.131:53 Masq 1 0 0 -\u003e 10.233.101.2:53 Masq 1 0 0 # 创建一个 deplpyment vi nginx-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-1 ~]# kubectl apply -f nginx-deployment.yaml deployment.apps/nginx-dm created service/nginx-svc created [root@kubernetes","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:11:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"update 1.13.2 优雅更新 版本 git fetch origin git checkout origin/master ansible-playbook -i inventory/jicki/hosts.ini --become --become-user=root upgrade-cluster.yml -e kube_version=v1.13.2 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:12:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"FAQ 问题 # FAQ 1: TASK [download : file_download | Download item] *************************************************************************************************************************************************** Friday 21 December 2018 10:37:05 +0800 (0:00:00.250) 0:03:05.805 ******* FAILED - RETRYING: file_download | Download item (4 retries left). # 多试几次~ 国内~是真的没办法, 国内网络一时可以一时不行，取决于它解析的IP # 或者去其他地方下载了放过去 分别是: https://storage.googleapis.com/kubernetes-release/release/v1.13.1/bin/linux/amd64/hyperkube https://storage.googleapis.com/kubernetes-release/release/v1.13.1/bin/linux/amd64/kubeadm https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz # 下载到 所有服务器的 /tmp/releases 目录下 # 并且授权 执行权限 chmod +x hyperkube kubeadm cni-plugins-amd64-v0.6.0.tgz # 懒人~做法~人就是要懒，不想写脚本 echo \"pull images\" docker pull gcr.io/google-containers/kube-proxy:v1.13.2 docker pull gcr.io/google-containers/kube-controller-manager:v1.13.2 docker pull gcr.io/google-containers/kube-scheduler:v1.13.2 docker pull gcr.io/google-containers/kube-apiserver:v1.13.2 docker pull gcr.io/google-containers/kubernetes-dashboard-amd64:v1.10.1 echo \"tag images\" docker tag gcr.io/google-containers/kube-proxy:v1.13.2 jicki/kube-proxy:v1.13.2 docker tag gcr.io/google-containers/kube-controller-manager:v1.13.2 jicki/kube-controller-manager:v1.13.2 docker tag gcr.io/google-containers/kube-scheduler:v1.13.2 jicki/kube-scheduler:v1.13.2 docker tag gcr.io/google-containers/kube-apiserver:v1.13.2 jicki/kube-apiserver:v1.13.2 docker tag gcr.io/google-containers/kubernetes-dashboard-amd64:v1.10.1 jicki/kubernetes-dashboard-amd64:v1.10.1 echo \"push images\" docker push jicki/kube-proxy:v1.13.2 docker push jicki/kube-controller-manager:v1.13.2 docker push jicki/kube-scheduler:v1.13.2 docker push jicki/kube-apiserver:v1.13.2 docker push jicki/kubernetes-dashboard-amd64:v1.10.1 echo \"rmi images\" docker rmi jicki/kube-proxy:v1.13.2 docker rmi jicki/kube-controller-manager:v1.13.2 docker rmi jicki/kube-scheduler:v1.13.2 docker rmi jicki/kube-apiserver:v1.13.2 docker rmi jicki/kubernetes-dashboard-amd64:v1.10.1 docker rmi gcr.io/google-containers/kube-proxy:v1.13.2 docker rmi gcr.io/google-containers/kube-controller-manager:v1.13.2 docker rmi gcr.io/google-containers/kube-scheduler:v1.13.2 docker rmi gcr.io/google-containers/kube-apiserver:v1.13.2 docker rmi gcr.io/google-containers/kubernetes-dashboard-amd64:v1.10.1 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:13:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"update 1.13.5 优雅更新 版本 git fetch origin git checkout origin/master ansible-playbook -i inventory/jicki/hosts.ini --become --become-user=root upgrade-cluster.yml -e kube_version=v1.13.5 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:14:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"FAQ 问题 # FAQ 1: TASK [download : file_download | Download item] *************************************************************************************************************************************************** Friday 21 December 2018 10:37:05 +0800 (0:00:00.250) 0:03:05.805 ******* FAILED - RETRYING: file_download | Download item (4 retries left). # 多试几次~ 国内~是真的没办法, 国内网络一时可以一时不行，取决于它解析的IP # 或者去其他地方下载了放过去 分别是: https://storage.googleapis.com/kubernetes-release/release/v1.13.5/bin/linux/amd64/hyperkube https://storage.googleapis.com/kubernetes-release/release/v1.13.5/bin/linux/amd64/kubeadm https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz # 下载到 所有服务器的 /tmp/releases 目录下 # 并且授权 执行权限 chmod +x hyperkube kubeadm cni-plugins-amd64-v0.6.0.tgz # 懒人~做法~人就是要懒，不想写脚本 echo \"pull images\" docker pull gcr.io/google-containers/kube-proxy:v1.13.5 docker pull gcr.io/google-containers/kube-controller-manager:v1.13.5 docker pull gcr.io/google-containers/kube-scheduler:v1.13.5 docker pull gcr.io/google-containers/kube-apiserver:v1.13.5 docker pull quay.io/calico/kube-controllers:v3.4.0 docker pull quay.io/calico/node:v3.4.0 docker pull quay.io/calico/cni:v3.4.0 docker pull quay.io/calico/ctl:v3.4.0 docker pull quay.io/calico/routereflector:v0.6.1 echo \"tag images\" docker tag gcr.io/google-containers/kube-proxy:v1.13.5 jicki/kube-proxy:v1.13.5 docker tag gcr.io/google-containers/kube-controller-manager:v1.13.5 jicki/kube-controller-manager:v1.13.5 docker tag gcr.io/google-containers/kube-scheduler:v1.13.5 jicki/kube-scheduler:v1.13.5 docker tag gcr.io/google-containers/kube-apiserver:v1.13.5 jicki/kube-apiserver:v1.13.5 docker tag quay.io/calico/kube-controllers:v3.4.0 jicki/kube-controllers:v3.4.0-amd64 docker tag quay.io/calico/node:v3.4.0 jicki/node:v3.4.0-amd64 docker tag quay.io/calico/cni:v3.4.0 jicki/cni:v3.4.0-amd64 docker tag quay.io/calico/ctl:v3.4.0 jicki/ctl:v3.4.0-amd64 docker tag quay.io/calico/routereflector:v0.6.1 jicki/routereflector:v0.6.1-amd64 echo \"push images\" docker push jicki/kube-proxy:v1.13.5 docker push jicki/kube-controller-manager:v1.13.5 docker push jicki/kube-scheduler:v1.13.5 docker push jicki/kube-apiserver:v1.13.5 docker push jicki/node:v3.4.0-amd64 docker push jicki/cni:v3.4.0-amd64 docker push jicki/ctl:v3.4.0-amd64 docker push jicki/routereflector:v0.6.1-amd64 docker push jicki/kube-controllers:v3.4.0-amd64 echo \"rmi images\" docker rmi jicki/kube-proxy:v1.13.5 docker rmi jicki/kube-controller-manager:v1.13.5 docker rmi jicki/kube-scheduler:v1.13.5 docker rmi jicki/kube-apiserver:v1.13.5 docker rmi gcr.io/google-containers/kube-proxy:v1.13.5 docker rmi gcr.io/google-containers/kube-controller-manager:v1.13.5 docker rmi gcr.io/google-containers/kube-scheduler:v1.13.5 docker rmi gcr.io/google-containers/kube-apiserver:v1.13.5 docker rmi quay.io/calico/kube-controllers:v3.4.0 docker rmi quay.io/calico/node:v3.4.0 docker rmi quay.io/calico/cni:v3.4.0 docker rmi quay.io/calico/ctl:v3.4.0 docker rmi quay.io/calico/routereflector:v0.6.1 docker rmi jicki/kube-controllers:v3.4.0-amd64 docker rmi jicki/node:v3.4.0-amd64 docker rmi jicki/cni:v3.4.0-amd64 docker rmi jicki/ctl:v3.4.0-amd64 docker rmi jicki/routereflector:v0.6.1-amd64 ","date":"2018-12-21","objectID":"/k8s-1.13.1-kubespray/:15:0","tags":["kubernetes","docker"],"title":"k8s 1.13.1 to kubespray","uri":"/k8s-1.13.1-kubespray/"},{"categories":["kubernetes"],"content":"kubernetes 1.13.1","date":"2018-12-19","objectID":"/kubernetes-1.13.1/","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":" Update kubernetes 1.13.1 CHANGELOG 0. 创建 deploymoent DaemonSet 的apiVersion: extensions/v1beta1 更新为 apiVersion: apps/v1, 否则日志抛出错误: watch of *v1beta1.Event ended with: The resourceVersion for the provided watch is too old. 1. 更新一个 CVE-2018-1002105 安全问题，必须更新(v1.10.11，v1.11.5和v1.12.3)子版本更新 2. etcd 版本 更新为 v3.2.24 , 删除 etcd v2 支持, kube-apiserver 配置 --storage-backend=etcd3 3. kube-apiserver 中 标签: --service-account-api-audiences 更改为 --api-audiences --experimental-encryption-provider-config 更改为 --encryption-provider-config --encryption-provider-config 标签引用的配置文件中: kind: EncryptionConfig 更改为 kind: EncryptionConfiguration apiVersion: v1 更改为 apiVersion: apiserver.config.k8s.io/v1 kubernetes 1.13.1 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:0:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"环境说明 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node, node-66 只做单纯 Node kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:1:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.64 kubernetes-64 172.16.1.65 kubernetes-65 172.16.1.66 kubernetes-66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:2:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:3:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:4:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | \\ /opt/local/cfssl/cfssljson -bare ca [root@kubernetes-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:5:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 官方最新版本 docker 为 18.06.1 , 官方验证最高版本支持到 18.06.0 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 18.06 被 docker-ce-selinux 依赖 # 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/\\ Packages/docker-ce-18.06.0.ce-3.el7.x86_64.rpm rpm -ivh docker-ce-18.06.0.ce-3.el7.x86_64.rpm yum -y install docker-ce-18.06.0.ce # 查看安装 docker version Client: Version: 18.06.0-ce API version: 1.38 Go version: go1.10.3 Git commit: 0ffa825 Built: Wed Jul 18 19:08:18 2018 OS/Arch: linux/amd64 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:6:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) # docker 版本 17.03.2 之前配置为 --graph=/opt/docker # docker 版本 17.04.x 之后配置为 --data-root=/opt/docker [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --registry-mirror=http://b438f72b.m.daocloud.io \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local \\ --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.13.1 etcd 支持最新版本为 v3.2.24 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:7:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.2.24/\\ etcd-v3.2.24-linux-amd64.tar.gz tar zxvf etcd-v3.2.24-linux-amd64.tar.gz cd etcd-v3.2.24-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:8:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | \\ /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@kubernetes-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 检查证书 [root@kubernetes-64 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:9:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:10:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:11:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:12:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.13.1/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:1","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@kubernetes-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:2","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"生成 kubernetes 配置文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:3","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:4","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@kubernetes-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:5","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 22a762c6fd1e636c3b1c7248980e4b93 # 创建 encryption-config.yaml 配置 cat \u003e encryption-config.yaml \u003c\u003cEOF kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 40179b02a8f6da07d90392ae966f7749 - identity: {} EOF # 拷贝 cp encryption-config.yaml /etc/kubernetes/ scp encryption-config.yaml 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 \u003e 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ \u003e \u003e 如下为最低限度的日志审核 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:6","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --anonymous-auth=false \\ --encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # --encryption-provider-config ，替代之前 token.csv 文件 # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:7","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver # 如果报错 请使用 journalctl -f -t kube-apiserver 和 journalctl -u kube-apiserver 来定位问题 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:8","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --feature-gates=RotateKubeletServerCertificate=true \\ --controllers=*,tokencleaner,bootstrapsigner \\ --experimental-cluster-signing-duration=86700h0m0s \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=5m0s \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:9","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager # 如果报错 请使用 journalctl -f -t kube-controller-manager 和 journalctl -u kube-controller-manager 来定位问题 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:10","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-scheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:11","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:12","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"验证 kube-scheduler 的 ha kube-scheduler 通过配置 leader-elect=true 自动选择 leader # 使用如下命令 可以查看 holderIdentity 字段中的前缀 来判断 leader kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml apiVersion: v1 kind: Endpoints metadata: annotations: control-plane.alpha.kubernetes.io/leader: '{\"holderIdentity\":\"kubernetes-2_84a37531-f846-11e8-a04c-000c29e78616\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2018-12-05T04:33:02Z\",\"renewTime\":\"2018-12-05T04:33:10Z\",\"leaderTransitions\":4}' creationTimestamp: \"2018-10-09T02:38:12Z\" name: kube-scheduler namespace: kube-system resourceVersion: \"7718283\" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler uid: 5dbb5a57-cb6c-11e8-8f42-000c2947006b ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:13","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@kubernetes-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@kubernetes-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:14","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 kubelet 认证 kubelet 授权 kube-apiserver 的一些操作 exec run logs 等 # RBAC 只需创建一次就可以 kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:15","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 bootstrap kubeconfig 文件 注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token # 创建 集群所有 kubelet 的 token [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-64 --kubeconfig ~/.kube/config I1009 10:39:16.623409 3117 version.go:89] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: dial tcp 172.217.25.16:443: connect: connection timed out I1009 10:39:16.623486 3117 version.go:94] falling back to the local client version: v1.13.1 ado3mb.00vde0vkgvfbpz30 [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-65 --kubeconfig ~/.kube/config I1009 10:40:14.199418 3126 version.go:89] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: dial tcp 172.217.25.16:443: connect: connection timed out I1009 10:40:14.199487 3126 version.go:94] falling back to the local client version: v1.13.1 6xkesn.bmym9293ty2r1umr [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-66 --kubeconfig ~/.kube/config I1009 10:40:42.919424 3136 version.go:89] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: dial tcp 172.217.25.16:443: connect: connection timed out I1009 10:40:42.919501 3136 version.go:94] falling back to the local client version: v1.13.1 6jj682.j7wgboa50f6agith # 查看生成的 token [root@kubernetes-64 kubernetes]# kubeadm token list --kubeconfig ~/.kube/config TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 6jj682.j7wgboa50f6agith 23h 2018-10-10T10:40:42+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-64 6xkesn.bmym9293ty2r1umr 23h 2018-10-10T10:40:14+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-65 ado3mb.00vde0vkgvfbpz30 23h 2018-10-10T10:39:16+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-66 以下为了区分 会先生成 node 名称加 bootstrap.kubeconfig 生成 kubernetes-64 # 生成 64 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=6jj682.j7wgboa50f6agith \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 拷贝生成的 kubernetes-64-bootstrap.kubeconfig 文件 mv kubernetes-64-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-65 # 生成 65 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=1ua4d4.9bluufy3esw4lch6 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 拷贝生成的 kubernetes-65-bootstrap.kubeconfig 文件 scp kubernetes-65-bootstrap.kubeconfig 172.16.1.65:/etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-66 # 生成 66 的 bootstrap.kubeconfig # 配置集群参数 kubectl config se","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:16","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建自动批准相关 CSR 请求的 ClusterRole vi /etc/kubernetes/tls-instructs-csr.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"] # 导入 yaml 文件 [root@kubernetes-64 opt]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml clusterrole.rbac.authorization.k8s.io \"system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\" created # 查看 [root@kubernetes-64 opt]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"name\":\"system:certificates.k8s.io:certificatesigningreq... PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] # 将 ClusterRole 绑定到适当的用户组 # 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers # 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes # 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求 kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:17","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 关于 kubectl get node 中的 ROLES 的标签 单 Master 打标签 kubectl label node kubernetes-64 node-role.kubernetes.io/master=”” 这里需要将 单Master 更改为 NoSchedule 更新标签命令为 kubectl taint nodes kubernetes-64 node-role.kubernetes.io/master=:NoSchedule 既 Master 又是 node 打标签 kubectl label node kubernetes-65 node-role.kubernetes.io/master=”” 单 Node 打标签 kubectl label node kubernetes-66 node-role.kubernetes.io/node=”” 关于删除 label 可使用 - 号相连 如: kubectl label nodes kubernetes-65 node-role.kubernetes.io/node- 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ https://github.com/kubernetes/kubernetes/blob/release-1.12/pkg/kubelet/apis/config/types.go # 创建 kubelet 目录 mkdir -p /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.64\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"RotateCertificates\": true, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } # 如上配置: kubernetes-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。 \"clusterDNS\": [\"10.254.0.2\"] 可配置多个 dns地址，逗号可开, 可配置宿主机dns. ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:18","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:19","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"验证 nodes [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 17h v1.13.1 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:20","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"查看 kubelet 生成文件 [root@kubernetes-64 ~]# ls -lt /etc/kubernetes/ssl/kubelet-* -rw------- 1 root root 1374 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem lrwxrwxrwx 1 root root 58 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-current.pem -\u003e /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem -rw-r--r-- 1 root root 1050 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.crt -rw------- 1 root root 227 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.key ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:13:21","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:14:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@kubernetes-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:14:1","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy* /etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:15:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.65:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:15:1","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 node 中安装 yum install ipset ipvsadm conntrack-tools.x86_64 -y yaml 配置文件中的 参数如下: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/config/types.go cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:15:2","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 # 检查 ipvs [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:15:3","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理； 在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server; node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口; 当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA; ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:16:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:16:1","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:16:2","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 systemd kubelet 配置 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ https://github.com/kubernetes/kubernetes/blob/release-1.12/pkg/kubelet/apis/config/types.go # 创建 kubelet 目录 vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.66\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } # 启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:16:3","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy 配置 Flannel 网络 公有云如 阿里云 华为云 可能无法使用 flannel 的 host-gw 模式，请使用 vxlan 或 calico 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1_A3zzurG5vV40-FnyA8uWg rpm -ivh flannel-0.10.0-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=em1\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl enable docker systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 配置 Calico 网络 官方文档 https://docs.projectcalico.org/v3.4/introduction ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:16:4","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"下载 Calico yaml # 下载 yaml 文件 wget https://docs.projectcalico.org/v3.4/getting-started/kubernetes/installation/hosted/calico.yaml ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:17:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"下载镜像 # 下载 镜像 # 国外镜像 有墙 quay.io/calico/node:v3.3.1 quay.io/calico/cni:v3.3.1 quay.io/calico/kube-controllers:v3.3.1 # 国内镜像 jicki/node:v3.3.1 jicki/cni:v3.3.1 jicki/kube-controllers:v3.3.1 # 替换镜像 sed -i 's/quay\\.io\\/calico/jicki/g' calico.yaml ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:18:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"修改配置 vi calico.yaml # 注意修改如下选项: # etcd 地址 etcd_endpoints: \"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # etcd 证书路径 # If you're using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: \"/calico-secrets/etcd-ca\" etcd_cert: \"/calico-secrets/etcd-cert\" etcd_key: \"/calico-secrets/etcd-key\" # etcd 证书 base64 地址 (执行里面的命令生成的证书 base64 码，填入里面) data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d '\\n') etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d '\\n') etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d '\\n') # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: \"10.254.64.0/18\" # 导入 yaml 文件 [root@kubernetes-64 ~]# kubectl apply -f calico.yaml configmap/calico-config created secret/calico-etcd-secrets created daemonset.extensions/calico-node created serviceaccount/calico-node created deployment.extensions/calico-kube-controllers created serviceaccount/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created # 查看服务 [root@kubernetes-64 ~]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-5d94b577bb-rzl7d 1/1 Running 0 52s calico-node-4cbrl 1/1 Running 0 52s calico-node-9t7kj 1/1 Running 0 52s calico-node-hc4x6 1/1 Running 0 52s ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:19:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"修改 kubelet 配置 # kubelet 需要增加 cni 插件 --network-plugin=cni vi /etc/systemd/system/kubelet.service --network-plugin=cni \\ # 重新加载配置 systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:20:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"检查网络 # 查看 node 中网络状况 [root@kubernetes-64 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.95.64 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 2 bytes 168 (168.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 168 (168.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@kubernetes-65 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.116.128 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@kubernetes-66 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.70.64 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:21:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"安装 calicoctl calicoctl 是 calico 网络的管理客户端, 只需要在一台 node 里配置既可。 # 下载 二进制文件 curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.2.3/calicoctl mv calicoctl /usr/local/bin/ chmod +x /usr/local/bin/calicoctl # 创建 calicoctl.cfg 配置文件 mkdir /etc/calico vi /etc/calico/calicoctl.cfg apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"kubernetes\" kubeconfig: \"/root/.kube/config\" # 查看 calico 状态 [root@kubernetes-64 ~]# calicoctl node status Calico process is running. IPv4 BGP status +----------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +----------------+-------------------+-------+----------+-------------+ | 172.16.1.65 | node-to-node mesh | up | 01:10:35 | Established | | 172.16.1.66 | node-to-node mesh | up | 01:10:35 | Established | +----------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. [root@kubernetes-64 ~]# calicoctl get node NAME kubernetes-64 kubernetes-65 kubernetes-66 测试集群 # 创建一个 nginx deplyment apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 name: http targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 kubernetes-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@kubernetes-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 网络的节点 里 curl [root@kubernetes-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:22:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml 1.2.x 版本中 Corefile 部分更新了点东西，使用如下替换整个 Corefile 部分 # vi coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/18 { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 loop reload loadbalance } ... clusterIP: 10.254.0.2 # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:23:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@kubernetes-64 coredns]# kubectl apply -f coredns.yaml serviceaccount/coredns created clusterrole.rbac.authorization.k8s.io/system:coredns created clusterrolebinding.rbac.authorization.k8s.io/system:coredns created configmap/coredns created deployment.extensions/coredns created service/kube-dns created ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:24:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"查看 coredns 服务 [root@kubernetes-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE pod/coredns-6975654877-nzhgr 1/1 Running 0 23s pod/coredns-6975654877-qn4bp 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 23s ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:25:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"检查日志 [root@kubernetes-64 coredns]# kubectl logs -n kube-system pod/coredns-6975654877-nzhgr .:53 2018/08/09 02:11:11 [INFO] CoreDNS-1.2.0 2018/08/09 02:11:11 [INFO] linux/amd64, go1.10.3, 2e322f6 CoreDNS-1.2.0 linux/amd64, go1.10.3, 2e322f6 2018/08/09 02:11:11 [INFO] plugin/reload: Running configuration MD5 = 271feea1e1cf54e66a65c7ffcf2b89ad ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:26:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sleep - \"3600\" # 查看 创建的服务 [root@kubernetes-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@kubernetes-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@kubernetes-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:27:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"部署 DNS 自动伸缩 按照 node 数量 自动伸缩 dns 数量 vi dns-auto-scaling.yaml kind: ServiceAccount apiVersion: v1 metadata: name: kube-dns-autoscaler namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\"] - apiGroups: [\"\"] resources: [\"replicationcontrollers/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"extensions\"] resources: [\"deployments/scale\", \"replicasets/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"create\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-cluster-critical containers: - name: autoscaler image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 resources: requests: cpu: \"20m\" memory: \"10Mi\" command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kube-dns-autoscaler - --target=Deployment/coredns - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}} - --logtostderr=true - --v=2 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" serviceAccountName: kube-dns-autoscaler # 导入文件 [root@kubernetes-64 coredns]# kubectl apply -f dns-auto-scaling.yaml serviceaccount/kube-dns-autoscaler created clusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler created deployment.apps/kube-dns-autoscaler created 部署 Dashboard 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:28:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0 # 个人的镜像 jicki/kubernetes-dashboard-amd64:v1.10.0 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:28:1","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:28:2","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/k8s\\.gcr\\.io/jicki/g' * # 导入文件 [root@kubernetes-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@kubernetes-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-5984fb8cbb-77dl4 1/1 Running 0 3h po/coredns-5984fb8cbb-9hdwt 1/1 Running 0 3h po/kubernetes-dashboard-78bcdc4d64-x6fhq 1/1 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 3h svc/kubernetes-dashboard ClusterIP 10.254.18.143 \u003cnone\u003e 443/TCP 14s 部署 Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方地址 https://kubernetes.github.io/ingress-nginx/ ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:28:3","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@kubernetes-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 1d v1.13.1 kubernetes-65 Ready master 1d v1.13.1 kubernetes-66 Ready node 1d v1.13.1 # 对 65 与 66 打上 label [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-65 ingress=proxy node \"kubernetes-65\" labeled [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-66 ingress=proxy node \"kubernetes-66\" labeled # 打完标签以后 [root@kubernetes-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 32m v1.13.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubernetes-64 kubernetes-65 Ready \u003cnone\u003e 17m v1.13.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-65 kubernetes-66 Ready \u003cnone\u003e 4m v1.13.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-66 # 下载镜像 # 官方镜像 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0 # 国内镜像 jicki/nginx-ingress-controller:0.21.0 # 下载 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/mandatory.yaml # 替换所有的 images sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi mandatory.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 导入 yaml 文件 [root@kubernetes-64 nginx-ingress]# kubectl apply -f mandatory.yaml namespace/ingress-nginx created configmap/nginx-configuration created configmap/tcp-services created configmap/udp-services created serviceaccount/nginx-ingress-serviceaccount created clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created role.rbac.authorization.k8s.io/nginx-ingress-role created rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created deployment.extensions/nginx-ingress-controller created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 66 中 [root@kubernetes-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-ingress-controller-8476958f94-8fh5h 1/1 Running 0 5m 172.16.1.66 kubernetes-66 nginx-ingress-controller-8476958f94-qfhhp 1/1 Running 0 5m 172.16.1.65 kubernetes-65 # 查看我们原有的 svc [root@kubernetes-64 ingress]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 24m nginx-dm-84f8f49555-tmqzm 1/1 Running 0 24m nginx-dm-84f8f49555-wdk67 1/1 Running 0 24m # 创建一个 基于 nginx-dm 的 ingress vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 # 查看服务 [root@kubernetes-64 ingress]# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 6s # 测试访问 [root@kubernetes-64 ingress]# curl nginx.jicki.cn \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 配置一个基于域名的 https , ingress # 创建一个 基于 自身域名的 证书 openssl req -x509 -nodes -days 3650 -newkey rsa:2048 -keyout dashboard.jicki.cn-key.key -out dashboard.jick","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:29:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"部署 monitoring k8s 运维相关 ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:30:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:31:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2018-12-19","objectID":"/kubernetes-1.13.1/:32:0","tags":["kubernetes","docker"],"title":"kubernetes 1.13.1","uri":"/kubernetes-1.13.1/"},{"categories":["kubernetes"],"content":"helm 包管理","date":"2018-12-07","objectID":"/helm/","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":" Helm 是 Kubernetes 的包管理器，可以帮我们简化 kubernetes 的操作，一键部署应用。 helm ","date":"2018-12-07","objectID":"/helm/:0:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":"安装依赖 # 首先要在所有 node 里安装 socat 软件 yum -y install socat ","date":"2018-12-07","objectID":"/helm/:1:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":"下载文件 # 下载 安装包 (googleapis.com 有可能访问不到，多试几次，或者自己翻墙下载) wget https://storage.googleapis.com/kubernetes-helm/helm-v2.11.0-linux-amd64.tar.gz tar zxvf helm-v2.11.0-linux-amd64.tar.gz cd linux-amd64/ mv helm /usr/local/bin/ # 验证服务 [root@kubernetes-1 ~]# helm version Client: \u0026version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} ","date":"2018-12-07","objectID":"/helm/:2:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":"初始化 helm # 初始化 helm [root@kubernetes-1 ~]# helm init --skip-refresh Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. To prevent this, run `helm init` with the --tiller-tls-verify flag. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! ","date":"2018-12-07","objectID":"/helm/:3:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":"替换 repo # helm 的 repo 默认是使用 谷歌的 repo ，国内访问不到，修改为 阿里云 的 repo [root@kubernetes-1 ~]# helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts [root@kubernetes-1 ~]# helm repo remove stable \"stable\" has been removed from your repositories [root@kubernetes-1 ~]# helm repo add stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts \"stable\" has been added to your repositories [root@kubernetes-1 ~]# helm repo update Hang tight while we grab the latest from your chart repositories... ...Skip local chart repository ...Successfully got an update from the \"stable\" chart repository Update Complete. ⎈ Happy Helming!⎈ [root@kubernetes-1 ~]# helm repo list NAME URL local http://127.0.0.1:8879/charts stable https://kubernetes.oss-cn-hangzhou.aliyuncs.com/charts ","date":"2018-12-07","objectID":"/helm/:4:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":"替换 tiller 镜像 # 默认 tiller 会使用 gcr.io 中的镜像，修改为自己的否则下载不到 helm init --upgrade --tiller-image=jicki/tiller:v2.11.0 ","date":"2018-12-07","objectID":"/helm/:5:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":"修改 tiller rbac 为admin # 创建 rbac 并修改为 admin 权限 kubectl create serviceaccount --namespace=kube-system tiller kubectl create clusterrolebinding tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:tiller kubectl patch deploy --namespace=kube-system tiller-deploy -p '{\"spec\":{\"template\":{\"spec\":{\"serviceAccount\":\"tiller\"}}}}' # 查看 kubernetes pods [root@kubernetes-1 ~]# kubectl get pods -n kube-system | grep tiller tiller-deploy-7fc579878c-67vf8 1/1 Running 0 4m44s # 验证tiller [root@kubernetes-1 ~]# helm version Client: \u0026version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} Server: \u0026version.Version{SemVer:\"v2.11.0\", GitCommit:\"2e55dbe1fdb5fdb96b75ff144a339489417b146b\", GitTreeState:\"clean\"} ","date":"2018-12-07","objectID":"/helm/:6:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["kubernetes"],"content":"helm 命令 Helm 一些常用命令 # 查询 charts 包 [root@kubernetes-1 ~]# helm search mysql NAME CHART VERSION APP VERSION DESCRIPTION stable/mysql 0.3.5 Fast, reliable, scalable, and easy to use open-source rel... stable/percona 0.3.0 free, fully compatible, enhanced, open source drop-in rep... stable/percona-xtradb-cluster 0.0.2 5.7.19 free, fully compatible, enhanced, open source drop-in rep... stable/gcloud-sqlproxy 0.2.3 Google Cloud SQL Proxy stable/mariadb 2.1.6 10.1.31 Fast, reliable, scalable, and easy to use open-source rel... # 查询 包 的具体信息 [root@kubernetes-1 ~]# helm inspect stable/mysql description: Fast, reliable, scalable, and easy to use open-source relational database system. engine: gotpl home: https://www.mysql.com/ icon: https://www.mysql.com/common/logos/logo-mysql-170x115.png keywords: ...... # 部署 程序 helm install stable/mysql # 查询支持的选项,用于set 参数 [root@kubernetes-1 ~]# helm inspect values stable/mysql ## mysql image version ## ref: https://hub.docker.com/r/library/mysql/tags/ ## image: \"mysql\" imageTag: \"5.7.14\" ## Specify password for root user ## ## Default: random 10 character string # mysqlRootPassword: testing ....... # 根据上面 查询 values 的mysqlRootPassword ，修改为 jicki helm install --name mysql --set mysqlRootPassword=jicki stable/mysql # 升级 设置，或者版本 helm upgrade db-mysql --set mysqlRootPassword=passwd --set image=\"jicki/mysql\" --set imageTag=\"5.7.20\" stable/mysql # 回滚 到upgrade 之前的版本 helm rollback db-mysql 1 # 添加 额外 repo helm repo add my-repo https://kubernetes-charts.jicki.cn/ # 查询 repo 列表 helm repo list ","date":"2018-12-07","objectID":"/helm/:7:0","tags":["kubernetes","docker"],"title":"helm 包管理","uri":"/helm/"},{"categories":["linux"],"content":"Let's Encrypt 证书 申请","date":"2018-12-04","objectID":"/certbot/","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":" Let’s Encrypt 是一个免费、开放，自动化的证书颁发机构，由 ISRG（Internet Security Research Group）运作。 ISRG 是一个关注网络安全的公益组织，其赞助商从非商业组织到财富100强公司都有，包括 Mozilla、Akamai、Cisco、Facebook，密歇根大学等等。 ISRG 以消除资金，技术领域的障碍，全面推进加密连接成为互联网标配为自己的使命。 Let’s Encrypt 项目于2012年由 Mozilla 的两个员工发起，2014年11年对外宣布公开，2015年12月3日开启公测。 Certbot Certboot 是官方提供的一个 申请 Let’s Encrypt 的工具。 官方文档 https://certbot.eff.org/docs/ ","date":"2018-12-04","objectID":"/certbot/:0:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"安装 Certbot # 下载 二进制文件 wget https://dl.eff.org/certbot-auto chmod a+x ./certbot-auto ./certbot-auto --help # 初始化环境 ./certbot-auto -n 会初始化生成环境，会创建 virtualenv env # 注: 如果系统存在两个版本 virtualenv 会出现问题 # cerbot 会使用 yum 与 pip 下载 virtualenv # 请使用 pip install virtualenv ","date":"2018-12-04","objectID":"/certbot/:1:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"生成证书 生成证书的方式有多种，webroot, nginx, apache, standalone, DNS 的方式 ","date":"2018-12-04","objectID":"/certbot/:2:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"standalone 模式 # 独立模式 --standalone ./certbot-auto certonly --standalone --email jicki@qq.com --agree-tos -d jicki.cn -d www.jicki.cn # 独立模式需要 占用本机的 80 以及 443 端口 用来认证 证书， # 所以需要先关闭 本机 服务 ","date":"2018-12-04","objectID":"/certbot/:2:1","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"webroot 模式 # 网站目录 --webroot 模式 # 不同域名需要配置再不同的 --webroot 目录下 ./certbot-auto certonly --agree-tos --email jicki@qq.com --webroot -w /var/www/html/ -d jicki.cn -d www.jicki.cn -w /var/www/wiki -d wiki.jicki.cn # --webroot 模式 不需要关闭正在运行的服务, 但是会在 网站文件目录下 创建一个 .well-known 目录 对于这个目录需要配置外部禁止访问。 # 这里面注意，配置反向代理的https这模式不适用。 # nginx 配置 在相关域名下配置 location ~ \\.well-known{ allow all; } location ^~ /.well-known/acme-challenge/ { alias /var/www/html/; try_files $uri =404; } ","date":"2018-12-04","objectID":"/certbot/:2:2","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"nginx apache 模式 ./certbot-auto --nginx ./certbot-auto --apache # nginx 与 apache 两种模式 会自动修改 nginx 与 apache 配置文件， # 所以对 nginx 与 apache 的安装有要求，配置文件必须在固定位置。 ","date":"2018-12-04","objectID":"/certbot/:2:3","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"Dns 模式 dns 模式支持 范域名 的证书 ./certbot-auto --server https://acme-v02.api.letsencrypt.org/directory -d \"*.jicki.cn\" --manual --preferred-challenges dns-01 certonly # 这里执行命令后~需要 交互输入 一些 配置 如下: Saving debug log to /var/log/letsencrypt/letsencrypt.log Plugins selected: Authenticator manual, Installer None Obtaining a new certificate Performing the following challenges: dns-01 challenge for jicki.cn - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - NOTE: The IP of this machine will be publicly logged as having requested this certificate. If you're running certbot in manual mode on a machine that is not your server, please ensure you're okay with that. Are you OK with your IP being logged? - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - (Y)es/(N)o: y - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Please deploy a DNS TXT record under the name _acme-challenge.jicki.cn with the following value: tdoCC636Cel1wQPY-LB-FURPvNSloFhBdWyEoqkQZNU Before continuing, verify the record is deployed. - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Press Enter to Continue Waiting for verification... ## 这里面提示在 dns 里配置一下 认证 Please deploy a DNS TXT record under the name _acme-challenge.jicki.cn with the following value: tdoCC636Cel1wQPY-LB-FURPvNSloFhBdWyEoqkQZNU Before continuing, verify the record is deployed. 主机记录: _acme-challenge.jicki.cn 记录类型: TXT 记录值: tdoCC636Cel1wQPY-LB-FURPvNSloFhBdWyEoqkQZNU # 配置完以后~~等待认证 ","date":"2018-12-04","objectID":"/certbot/:2:4","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"配置 https # 上面生成好证书以后会将证书生成在 /etc/letsencrypt 目录下 # Nginx 配置 https { listen 80; listen 443 ssl; listen [::]:443 ssl ipv6only=on; server_name jicki.cn www.jicki.cn; root /var/www/html; index index.html index.htm index.php; access_log /var/logs/nginx/jicki.log main; # ssl setting ssl_protocols TLSv1.2 TLSv1.1 TLSv1; ssl_certificate /etc/letsencrypt/live/www.jicki.cn/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/www.jicki.cn/privkey.pem; ssl_trusted_certificate /etc/letsencrypt/live/www.jicki.cn/chain.pem; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; server_tokens off; ssl_prefer_server_ciphers on; fastcgi_param HTTPS on; fastcgi_param HTTP_SCHEME https; # 强制跳转到 https if ($scheme = http) { return 301 https://$server_name$request_uri; } # 禁止 webroot 模式目录 location ~ \\.well-known{ allow all; } # 禁止 webroot 模式目录 location ^~ /.well-known/acme-challenge/ { alias /var/www/html/; try_files $uri =404; } ","date":"2018-12-04","objectID":"/certbot/:3:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"配置续签证书 crontab -e 添加如下: 每周1检测一次 30 2 * * 1 /opt/certbot/certbot-auto renew \u003e\u003e /var/log/certbot-renew.log docker 方式 docker 方式只做简单的介绍，需要懂docker的人使用，不懂docker 建议使用上面方式 # 这里放一个 脚本 #!/bin/bash case $1 in \"make\") docker stop nginx docker run --rm -p 80:80 -p 443:443 \\ -v /opt/data/nginx/ssl/:/etc/letsencrypt \\ certbot/certbot certonly \\ --standalone -m jicki@qq.com --agree-tos \\ -d www.jicki.cn -d jicki.cn docker start nginx ;; \"renew\") docker stop nginx docker run --rm -p 80:80 -p 443:443 \\ -v /opt/nginx/ssl/:/etc/letsencrypt \\ -v /var/log/letsencrypt:/var/log/letsencrypt \\ certbot/certbot renew \\ --standalone docker start nginx ;; *) echo \"Please choose make/renew\" ;; esac acem.sh 也是一个签发工具，这个对于 泛域名配置 会比较简单，可以自动添加到dns记录里 第一次成功之后，acme.sh会记录下App_Key跟App_Secret，并且生成一个定时任务，每天凌晨0：00自动检测过期域名并且自动续期。 # 泛域名 最好使用 acem.sh 这个容器来配置 # 这里面 App_Key 与 App_Secret 是dns商里面的一个 api # acme.sh 支持很多个 dns 商 # 如下是 aliyun 的配置 docker run --rm -it \\ -v /opt/nginx/ssl:/acme.sh \\ -e Ali_Key=\"xxxxxx\" \\ -e Ali_Secret=\"xxxx\" \\ neilpang/acme.sh --issue --dns dns_ali -d jicki.cn -d *.jicki.cn # DNSPod 配置如下: docker run --rm -it \\ -v /opt/nginx/ssl:/acme.sh \\ -e DP_Id=\"xxxxxx\" \\ -e DP_Key=\"xxxx\" \\ neilpang/acme.sh --issue --dns dns_dp -d jicki.cn -d *.jicki.cn # GoDaddy 配置如下: docker run --rm -it \\ -v /opt/nginx/ssl:/acme.sh \\ -e GD_Key=\"xxxxxx\" \\ -e GD_Secret=\"xxxx\" \\ neilpang/acme.sh --issue --dns dns_gd -d jicki.cn -d *.jicki.cn Kubernetes 方式 Kubernetes 通过 Cert manager 进行自动申请 Let’s Encrypt 。 github 地址 https://github.com/jetstack/cert-manager ","date":"2018-12-04","objectID":"/certbot/:4:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"部署 cert-manager 这里官方使用 helm 来直接部署 ","date":"2018-12-04","objectID":"/certbot/:5:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"安装 helm Helm 是 Kubernetes 的包管理器，可以帮我们简化 kubernetes 的操作，一键部署应用。 helm 部署 请参考 https://jicki.cn/kubernetes/docker/2018/12/07/helm/ ","date":"2018-12-04","objectID":"/certbot/:5:1","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"安装 cert-manager # 执行 helm install helm install \\ --name cert-manager \\ --namespace kube-system \\ --set ingressShim.defaultIssuerName=letsencrypt-prod \\ --set ingressShim.defaultIssuerKind=ClusterIssuer \\ --set image.repository=jicki/cert-manager-controller \\ --set ingressShim.image.repository=jicki/cert-manager-ingress-shim \\ stable/cert-manager # 输出如下信息: NAME: cert-manager LAST DEPLOYED: Fri Dec 7 14:46:20 2018 NAMESPACE: kube-system STATUS: DEPLOYED RESOURCES: ==\u003e v1beta1/Deployment NAME AGE cert-manager-cert-manager 0s ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE cert-manager-cert-manager-6b58f97c65-dl2j9 0/2 ContainerCreating 0 0s ==\u003e v1/ServiceAccount NAME AGE cert-manager-cert-manager 0s ==\u003e v1beta1/CustomResourceDefinition certificates.certmanager.k8s.io 0s clusterissuers.certmanager.k8s.io 0s issuers.certmanager.k8s.io 0s ==\u003e v1beta1/ClusterRole cert-manager-cert-manager 0s ==\u003e v1beta1/ClusterRoleBinding cert-manager-cert-manager 0s NOTES: cert-manager has been deployed successfully! In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer). More information on the different types of issuers and how to configure them can be found in our documentation: https://github.com/jetstack/cert-manager/tree/v0.2.3/docs/api-types/issuer For information on how to configure cert-manager to automatically provision Certificates for Ingress resources, take a look at the `ingress-shim` documentation: https://github.com/jetstack/cert-manager/blob/v0.2.3/docs/user-guides/ingress-shim.md # 这里 cert-manager 的 image 是国外地址 quay.io/jetstack/cert-manager-controller:v0.2.3 quay.io/jetstack/cert-manager-ingress-shim:v0.2.3 # 替换为官方最新版本，否则有可能报协议不匹配 jicki/cert-manager-controller:canary jicki/cert-manager-ingress-shim:canary # 查询服务可用的 values 修改 image 需要用到 helm inspect values stable/cert-manager # 修改 image [root@kubernetes-1 ~]# helm upgrade cert-manager --set image.repository=jicki/cert-manager-controller --set image.tag=canary --set ingressShim.image.repository=jicki/cert-manager-ingress-shim --set ingressShim.image.tag=canary stable/cert-manager helm upgrade cert-manager --set image.repository=jicki/cert-manager-controller --set image.tag=canary --set ingressShim.image.repository=jicki/cert-manager-ingress-shim --set ingressShim.image.tag=canary stable/cert-manager Release \"cert-manager\" has been upgraded. Happy Helming! LAST DEPLOYED: Fri Dec 7 12:14:05 2018 NAMESPACE: kube-system STATUS: DEPLOYED RESOURCES: ==\u003e v1/Pod(related) NAME READY STATUS RESTARTS AGE cert-manager-cert-manager-6b58f97c65-rk68q 2/2 Running 0 50m cert-manager-cert-manager-766fb987fc-l5b7f 0/2 ContainerCreating 0 0s ==\u003e v1/ServiceAccount NAME AGE cert-manager-cert-manager 59m ==\u003e v1beta1/CustomResourceDefinition certificates.certmanager.k8s.io 59m clusterissuers.certmanager.k8s.io 59m issuers.certmanager.k8s.io 59m ==\u003e v1beta1/ClusterRole cert-manager-cert-manager 59m ==\u003e v1beta1/ClusterRoleBinding cert-manager-cert-manager 59m ==\u003e v1beta1/Deployment cert-manager-cert-manager 59m NOTES: cert-manager has been deployed successfully! In order to begin issuing certificates, you will need to set up a ClusterIssuer or Issuer resource (for example, by creating a 'letsencrypt-staging' issuer). More information on the different types of issuers and how to configure them can be found in our documentation: https://github.com/jetstack/cert-manager/tree/v0.2.3/docs/api-types/issuer For information on how to configure cert-manager to automatically provision Certificates for Ingress resources, take a look at the `ingress-shim` documentation: https://github.com/jetstack/cert-manager/blob/v0.2.3/docs/user-guides/ingress-shim.md ","date":"2018-12-04","objectID":"/certbot/:5:2","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"验证服务 [root@kubernetes-1 ~]# kubectl get pods -n kube-system --selector=app=cert-manager NAME READY STATUS RESTARTS AGE cert-manager-cert-manager-766fb987fc-l5b7f 2/2 Running 0 45s # 查看具体信息 [root@kubernetes-1 ~]# kubectl describe pods/cert-manager-cert-manager-766fb987fc-l5b7f -n kube-system Name: cert-manager-cert-manager-766fb987fc-l5b7f Namespace: kube-system Node: kubernetes-2/192.168.0.248 Start Time: Fri, 07 Dec 2018 12:14:06 +0800 Labels: app=cert-manager pod-template-hash=766fb987fc release=cert-manager Annotations: \u003cnone\u003e Status: Running IP: 10.254.90.167 Controlled By: ReplicaSet/cert-manager-cert-manager-766fb987fc Containers: cert-manager: Container ID: docker://7f6a6ed2257567c1a92dfe2ef583ddf275cc51bd8e5454ca694079f551aa6b17 Image: jicki/cert-manager-controller:canary Image ID: docker-pullable://jicki/cert-manager-controller@sha256:e894e0965c974e526c489fc69e8536d55893610085c46f9ff59f6c75480f521d Port: \u003cnone\u003e Host Port: \u003cnone\u003e State: Running Started: Fri, 07 Dec 2018 12:14:18 +0800 Ready: True Restart Count: 0 Environment: \u003cnone\u003e Mounts: /var/run/secrets/kubernetes.io/serviceaccount from cert-manager-cert-manager-token-cpwvg (ro) ingress-shim: Container ID: docker://21e37d4317c9083624b8fbe078433d53135d9f0715769110c362aeef69b2f9ed Image: jicki/cert-manager-ingress-shim:canary Image ID: docker-pullable://jicki/cert-manager-ingress-shim@sha256:d798681aae440fadde653559605f9d2d1c006da83caf6e86aced79faf3de2aa7 Port: \u003cnone\u003e Host Port: \u003cnone\u003e State: Running Started: Fri, 07 Dec 2018 12:14:32 +0800 Ready: True Restart Count: 0 Environment: \u003cnone\u003e Mounts: /var/run/secrets/kubernetes.io/serviceaccount from cert-manager-cert-manager-token-cpwvg (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: cert-manager-cert-manager-token-cpwvg: Type: Secret (a volume populated by a Secret) SecretName: cert-manager-cert-manager-token-cpwvg Optional: false QoS Class: BestEffort Node-Selectors: \u003cnone\u003e Tolerations: \u003cnone\u003e Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 57s default-scheduler Successfully assigned kube-system/cert-manager-cert-manager-766fb987fc-l5b7f to kubernetes-2 Normal Pulling 57s kubelet, kubernetes-2 pulling image \"jicki/cert-manager-controller:canary\" Normal Pulled 45s kubelet, kubernetes-2 Successfully pulled image \"jicki/cert-manager-controller:canary\" Normal Created 45s kubelet, kubernetes-2 Created container Normal Started 45s kubelet, kubernetes-2 Started container Normal Pulling 45s kubelet, kubernetes-2 pulling image \"jicki/cert-manager-ingress-shim:canary\" Normal Pulled 31s kubelet, kubernetes-2 Successfully pulled image \"jicki/cert-manager-ingress-shim:canary\" Normal Created 31s kubelet, kubernetes-2 Created container Normal Started 31s kubelet, kubernetes-2 Started container # 查看生成的 crd [root@kubernetes-1 ~]# kubectl get crd NAME CREATED AT certificates.certmanager.k8s.io 2018-12-07T03:14:36Z clusterissuers.certmanager.k8s.io 2018-12-07T03:14:36Z issuers.certmanager.k8s.io 2018-12-07T03:14:36Z ","date":"2018-12-04","objectID":"/certbot/:6:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"创建签发证书服务 创建一个基于上面 crd 中 certificates.certmanager.k8s.io 的 api 的服务 # 这里请特别注意 server: 这个地址, 官方版本是 0.2.3 使用v01的api 不要用 v02的~否则报错 vi letsencrypt-clusterissuer.yaml apiVersion: certmanager.k8s.io/v1alpha1 kind: ClusterIssuer metadata: name: letsencrypt-prod namespace: kube-system spec: acme: srver: https://acme-v01.api.letsencrypt.org/directory email: jicki@qq.com privateKeySecretRef: name: letsencrypt-prod http01: {} # 创建服务 [root@kubernetes-1 ~]# kubectl apply -f letsencrypt-clusterissuer.yaml clusterissuer.certmanager.k8s.io/letsencrypt-prod created clusterissuer.certmanager.k8s.io/letsencrypt-staging created # 查看 [root@kubernetes-1 ~]# kubectl get clusterissuer NAME AGE letsencrypt-prod 10s letsencrypt-staging 10s ","date":"2018-12-04","objectID":"/certbot/:7:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["linux"],"content":"创建基于 ingress 的 https # 查看 svc [root@kubernetes-1 ~]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard ClusterIP 10.254.53.66 \u003cnone\u003e 443/TCP 56d # 编辑一个 ingress [root@kubernetes-1 ~]# vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kubernetes-dashboard namespace: kube-system annotations: kubernetes.io/ingress.class: \"nginx\" certmanager.k8s.io/cluster-issuer: \"letsencrypt-prod\" spec: tls: - hosts: - dashboard.jicki.cn secretName: dashboard-tls rules: - host: dashboard.jicki.cn http: paths: - path: / backend: serviceName: kubernetes-dashboard servicePort: 443 # 查看 ingress [root@kubernetes-1 ~]# kubectl get ingress -n kube-system NAME HOSTS ADDRESS PORTS AGE kubernetes-dashboard dashboard.jicki.cn 80, 443 11s # 查看 pods [root@kubernetes-1 ~]# kubectl get pods -n kube-system NAME HOSTS ADDRESS PORTS AGE cm-acme-http-solver-prcdn dashboard.jicki.cn 80 8s # 这个 cm-acme 是用来创建认证 证书的, 认证通过以后~会自动删除 # 查看具体信息 [root@kubernetes-1 ~]# kubectl describe ingress/kubernetes-dashboard -n kube-system Name: kubernetes-dashboard Namespace: kube-system Address: Default backend: default-http-backend:80 (\u003cnone\u003e) TLS: dashboard-tls terminates dashboard.jicki.cn Rules: Host Path Backends ---- ---- -------- dashboard.jicki.cn / kubernetes-dashboard:443 (10.254.101.26:8443) Annotations: certmanager.k8s.io/cluster-issuer: letsencrypt-prod ingress.kubernetes.io/ssl-passthrough: true kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"extensions/v1beta1\",\"kind\":\"Ingress\",\"metadata\":{\"annotations\":{\"certmanager.k8s.io/cluster-issuer\":\"letsencrypt-prod\",\"ingress.kubernetes.io/ssl-passthrough\":\"true\",\"kubernetes.io/ingress.class\":\"nginx\",\"nginx.ingress.kubernetes.io/secure-backends\":\"true\"},\"name\":\"kubernetes-dashboard\",\"namespace\":\"kube-system\"},\"spec\":{\"rules\":[{\"host\":\"dashboard.jicki.cn\",\"http\":{\"paths\":[{\"backend\":{\"serviceName\":\"kubernetes-dashboard\",\"servicePort\":443},\"path\":\"/\"}]}}],\"tls\":[{\"hosts\":[\"dashboard.jicki.cn\"],\"secretName\":\"dashboard-tls\"}]}} kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/secure-backends: true Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal CREATE 76s nginx-ingress-controller Ingress kube-system/kubernetes-dashboard Normal CreateCertificate 76s cert-manager Successfully created Certificate \"dashboard-tls\" Normal CREATE 62s nginx-ingress-controller Ingress kube-system/kubernetes-dashboard # 查看生成证书 [root@kubernetes-1 ~]# kubectl get certificate -n kube-system NAME AGE dashboard-tls 3m [root@kubernetes-1 ~]# kubectl get secret -n kube-system NAME TYPE DATA AGE dashboard-tls kubernetes.io/tls 3 4m38s # 查看具体日志 （所以需要域名可以正常使用） [root@kubernetes-1 ~]# kubectl logs pods/cert-manager-7859bc8fd7-lhhgb -n cert-manager I1206 07:05:06.579320 1 controller.go:176] ingress-shim controller: Finished processing work item \"kube-system/kubernetes-dashboard\" I1206 07:05:06.579763 1 controller.go:148] certificates controller: Finished processing work item \"kube-system/dashboard-tls\" I1206 07:05:06.579794 1 controller.go:142] certificates controller: syncing item 'kube-system/dashboard-tls' I1206 07:05:06.585618 1 controller.go:181] orders controller: syncing item 'kube-system/dashboard-tls-3718435272' I1206 07:05:06.585697 1 controller.go:148] certificates controller: Finished processing work item \"kube-system/dashboard-tls\" I1206 07:05:06.585718 1 controller.go:142] certificates controller: syncing item 'kube-system/dashboard-tls' # 如果域名未配置，会报错 (因为申请证书需要认证 域名下的 .well-known/acme-challenge 目录) I1206 07:58:57.570703 1 http.go:110] could not reach 'http://dashboard.jicki.cn/.well-known/acme-challenge/0beMNTSzGirQygofZ2kyiexLjqPDSV3-XGUGpokFSNM': failed to GET 'http://dashboard.jicki.cn/.well-known/acme-challenge/0beMNTSzGirQygofZ2kyiexLjqPDSV3-XGUGpokFSNM': Get http://dashboard.jicki.cn/.well-known/acme-challenge/0beMNTSzGirQygofZ2kyiexLjqPDSV3-XGUGpokFSNM: dial tcp: lookup dashboard.jicki.cn on 10.254.0.2:53: no such","date":"2018-12-04","objectID":"/certbot/:8:0","tags":["linux","docker"],"title":"Let's Encrypt 证书 申请","uri":"/certbot/"},{"categories":["fabric"],"content":"hyperledger-fabric v 1.3","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":" fabric v1.3 , 单机 多节点 kafka 手动部署, 所有服务均 开启 SSL 认证。 部署 hyperledger-fabric v1.3 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:0:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"环境规划 相关hostname 必须配置 dns 关于 orderer 集群 当orderer 向peer节点提交Transaction的时候，peer节点会得到或返回一个读写集结果，该结果会发送给orderer节点进行共识和排序，此时如果orderer节点突然down掉，就会使请求服务失效而引发的数据丢失等问题，且目前的sdk对orderer发送的Transaction的回调会占用极长的时间，当大批量数据导入的时候该回调可认为不可用。 节点标识 hostname IP 开放端口 系统 orderer0节点 orderer0.jicki.cn 192.168.100.100 7050 CentOS 7 x64 orderer1节点 orderer1.jicki.cn 192.168.100.100 7050 CentOS 7 x64 orderer2节点 orderer2.jicki.cn 192.168.100.100 7050 CentOS 7 x64 peer0节点 peer0.org1.jicki.cn 192.168.100.100 7051, 7052, 7053 CentOS 7 x64 peer0节点 peer0.org2.jicki.cn 192.168.100.100 7051, 7052, 7053 CentOS 7 x64 zk0节点 zookeeper0 192.168.100.100 2181 CentOS 7 x64 zk1节点 zookeeper1 192.168.100.100 2181 CentOS 7 x64 zk2节点 zookeeper2 192.168.100.100 2181 CentOS 7 x64 kafka0节点 kafka0 192.168.100.100 9092 CentOS 7 x64 kafka1节点 kafka1 192.168.100.100 9092 CentOS 7 x64 kafka2节点 kafka2 192.168.100.100 9092 CentOS 7 x64 # 配置一下 hosts 后面需要调用 # fabric 192.168.168.100 orderer0.jicki.cn 192.168.168.100 orderer1.jicki.cn 192.168.168.100 orderer2.jicki.cn 192.168.168.100 peer0.org1.jicki.cn 192.168.168.100 peer0.org2.jicki.cn # fabric ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:1:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"官方地址 文档以官方文档为主 http://hyperledger-fabric.readthedocs.io/en/release-1.3/prereqs.html # 官网 github https://github.com/hyperledger/fabric ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:2:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"环境准备 所有机器 安装 Docker (用于 fabric 服务启动) # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装 docker yum -y install docker-ce # 启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 查看 docker 版本 docker version Client: Version: 18.06.1-ce API version: 1.38 Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:23:03 2018 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 18.06.1-ce API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: e68fc7a Built: Tue Aug 21 17:25:29 2018 OS/Arch: linux/amd64 Experimental: false 安装 Docker-compose (用于 docker 容器服务统一管理 编排) # 安装 pip curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py # 安装 docker-compose pip install docker-compose --ignore-installed requests docker-compose version docker-compose version 1.32.0, build f46880f docker-py version: 3.4.1 CPython version: 2.7.5 OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013 Golang (用于 fabric cli 服务的调用， ca 服务证书生成 ) mkdir -p /opt/golang mkdir -p /opt/gopath # 国外地址 wget https://storage.googleapis.com/golang/go1.10.linux-amd64.tar.gz # 国内地址 wget https://studygolang.com/dl/golang/go1.10.linux-amd64.tar.gz # 解压 tar zxvf go1.10.linux-amd64.tar.gz # 配置环境变量 vi /etc/profile 添加如下 # golang env export PATH=$PATH:/opt/golang/go/bin export GOPATH=/opt/gopath # 生效配置 source /etc/profile # 查看配置 go version go version go1.10 linux/amd64 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:3:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"Hyperledger Fabric 源码 fabric 源码用于 cli 智能合约安装时的依赖, 这里只用于第一个节点 # 下载 Fabric 源码, 源码中 import 的路径为github.com/hyperledger/fabric ,所以我们要按照这个路径 mkdir -p /opt/gopath/src/github.com/hyperledger cd /opt/gopath/src/github.com/hyperledger git clone https://github.com/hyperledger/fabric # 查看分支 git branch -a # 查看本地分支 git branch # 切换分支 git checkout -b release-1.3 remotes/origin/release-1.3 # 文件如下: [root@localhost fabric]# ls -lt 总用量 1240 drwxr-xr-x 8 root root 153 11月 5 11:41 vendor drwxr-xr-x 4 root root 96 11月 5 11:41 token -rw-r--r-- 1 root root 301 11月 5 11:41 tox.ini drwxr-xr-x 2 root root 56 11月 5 11:41 unit-test -rw-r--r-- 1 root root 3816 11月 5 11:41 testingInfo.rst -rw-r--r-- 1 root root 438053 11月 5 11:41 test-pyramid.png drwxr-xr-x 2 root root 4096 11月 5 11:41 scripts -rw-r--r-- 1 root root 316 11月 5 11:41 settings.gradle drwxr-xr-x 4 root root 110 11月 5 11:41 sampleconfig drwxr-xr-x 3 root root 22 11月 5 11:41 release drwxr-xr-x 2 root root 4096 11月 5 11:41 release_notes drwxr-xr-x 14 root root 4096 11月 5 11:41 protos drwxr-xr-x 11 root root 4096 11月 5 11:41 peer drwxr-xr-x 6 root root 126 11月 5 11:41 orderer drwxr-xr-x 6 root root 4096 11月 5 11:41 msp drwxr-xr-x 12 root root 4096 11月 5 11:41 integration drwxr-xr-x 8 root root 112 11月 5 11:41 images drwxr-xr-x 2 root root 4096 11月 5 11:41 idemix drwxr-xr-x 16 root root 4096 11月 5 11:41 gossip -rw-r--r-- 1 root root 2944 11月 5 11:41 gotools.mk drwxr-xr-x 8 root root 126 11月 5 11:41 examples drwxr-xr-x 5 root root 156 11月 5 11:41 docs drwxr-xr-x 7 root root 4096 11月 5 11:41 discovery -rw-r--r-- 1 root root 3356 11月 5 11:41 docker-env.mk drwxr-xr-x 4 root root 4096 11月 5 11:41 devenv drwxr-xr-x 22 root root 4096 11月 5 11:41 core drwxr-xr-x 24 root root 4096 11月 5 11:41 common drwxr-xr-x 4 root root 46 11月 5 11:41 cmd -rw-r--r-- 1 root root 14 11月 5 11:41 ci.properties drwxr-xr-x 8 root root 4096 11月 5 11:41 bccsp -rw-r--r-- 1 root root 3205 11月 5 11:41 Gopkg.toml -rw-r--r-- 1 root root 11358 11月 5 11:41 LICENSE -rwxr-xr-x 1 root root 18176 11月 5 11:41 Makefile -rw-r--r-- 1 root root 6482 11月 5 11:41 README.md -rw-r--r-- 1 root root 670001 11月 5 11:41 CHANGELOG.md -rw-r--r-- 1 root root 597 11月 5 11:41 CODE_OF_CONDUCT.md -rw-r--r-- 1 root root 664 11月 5 11:41 CONTRIBUTING.md -rw-r--r-- 1 root root 26423 11月 5 11:41 Gopkg.lock ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:4:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"生成 Hyperledger Fabric 证书 证书生成只需要生成一次，这里只在第一个节点配置 # 下载官方证书生成软件(均为二进制文件) # 官方离线下载地址为 https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/ # 选择相应版本 CentOS 选择 linux-amd64-1.3.0 Mac 选择 darwin-amd64-1.3.0 # 下载地址为: https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/linux-amd64-1.3.0/hyperledger-fabric-linux-amd64-1.3.0.tar.gz mkdir /opt/jicki/ cd /opt/jicki wget https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/linux-amd64-1.3.0/hyperledger-fabric-linux-amd64-1.3.0.tar.gz tar zxvf hyperledger-fabric-linux-amd64-1.3.0.tar.gz # 解压后是 一个 bin 与 一个 config 目录 [root@localhost jicki]# tree bin/ bin/ ├── configtxgen ├── configtxlator ├── cryptogen ├── discover ├── get-docker-images.sh ├── idemixgen ├── orderer └── peer 0 directories, 8 files # 为方便使用 我们配置一个 环境变量 vi /etc/profile # fabric env export PATH=$PATH:/opt/jicki/bin # 使文件生效 source /etc/profile # 拷贝 configtx.yaml 与 crypto-config.yaml 。 cd /opt/jicki/ cp /opt/gopath/src/github.com/hyperledger/fabric/examples/e2e_cli/crypto-config.yaml . cp /opt/gopath/src/github.com/hyperledger/fabric/examples/e2e_cli/configtx.yaml . # 这里修改相应 jicki.cn 为 jicki.cn sed -i 's/example\\.com/jicki\\.me/g' *.yaml # 编辑 crypto-config.yaml 增加 orderer 为三个如下: OrdererOrgs: - Name: Orderer Domain: jicki.cn CA: Country: CN Province: GuangDong Locality: ShenZhen Specs: - Hostname: orderer0 - Hostname: orderer1 - Hostname: orderer2 PeerOrgs: - Name: Org1 Domain: org1.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 - Name: Org2 Domain: org2.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 # 然后这里使用 cryptogen 软件来生成相应的证书了 [root@localhost jicki]# cryptogen generate --config=./crypto-config.yaml org1.jicki.cn org2.jicki.cn # 生成一个 crypto-config 证书目录 [root@payment jicki]# tree crypto-config crypto-config ├── ordererOrganizations │ └── jicki.cn │ ├── ca │ │ ├── 1b50e9236c72ccbdf474ecd39ceb300f90b983b8044981895ee8ba4fda466c38_sk │ │ └── ca.jicki.cn-cert.pem │ ├── msp │ │ ├── admincerts │ │ │ └── Admin@jicki.cn-cert.pem │ │ ├── cacerts │ │ │ └── ca.jicki.cn-cert.pem │ │ └── tlscacerts │ │ └── tlsca.jicki.cn-cert.pem │ ├── orderers │ │ ├── orderer0.jicki.cn │ │ │ ├── msp │ │ │ │ ├── admincerts │ │ │ │ │ └── Admin@jicki.cn-cert.pem │ │ │ │ ├── cacerts │ │ │ │ │ └── ca.jicki.cn-cert.pem │ │ │ │ ├── keystore │ │ │ │ │ └── 4dd2ea13c77739776e2e70a82516fce9836c25c92ab6d762333b468b8824fdfd_sk │ │ │ │ ├── signcerts │ │ │ │ │ └── orderer0.jicki.cn-cert.pem │ │ │ │ └── tlscacerts │ │ │ │ └── tlsca.jicki.cn-cert.pem │ │ │ └── tls │ │ │ ├── ca.crt │ │ │ ├── server.crt │ │ │ └── server.key │ │ ├── orderer1.jicki.cn │ │ │ ├── msp │ │ │ │ ├── admincerts │ │ │ │ │ └── Admin@jicki.cn-cert.pem │ │ │ │ ├── cacerts │ │ │ │ │ └── ca.jicki.cn-cert.pem │ │ │ │ ├── keystore │ │ │ │ │ └── 2a55c127a61858ddf36cdffe66782f6a3976955ed6ccafa7dd602b9f393cee77_sk │ │ │ │ ├── signcerts │ │ │ │ │ └── orderer1.jicki.cn-cert.pem │ │ │ │ └── tlscacerts │ │ │ │ └── tlsca.jicki.cn-cert.pem │ │ │ └── tls │ │ │ ├── ca.crt │ │ │ ├── server.crt │ │ │ └── server.key │ │ └── orderer2.jicki.cn │ │ ├── msp │ │ │ ├── admincerts │ │ │ │ └── Admin@jicki.cn-cert.pem │ │ │ ├── cacerts │ │ │ │ └── ca.jicki.cn-cert.pem │ │ │ ├── keystore │ │ │ │ └── d85da317a39fc3c95c5ddce183bf492e1310decee79dd215ef4cfd4f7130d0c4_sk │ │ │ ├── signcerts │ │ │ │ └── orderer2.jicki.cn-cert.pem │ │ │ └── tlscacerts │ │ │ └── tlsca.jicki.cn-cert.pem │ │ └── tls │ │ ├── ca.crt │ │ ├── server.crt │ │ └── server.key │ ├── tlsca │ │ ├── e03e3c85138178da8a03c3cc00aecb0627aa299bdd0b92c4bc82591cd47a2672_sk │ │ └── tlsca.jicki.cn-cert.pem │ └── users │ └── Admin@jicki.cn │ ├── msp │ │ ├── admincerts │ │ │ └── Admin@jicki.cn-cert.pem │ │ ├── cacerts │ │ │ └── ca.jicki.cn-cert.pem │ │ ├── keystore │ │ │ └── fac87a53e4255e8007bf947bda","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:5:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"生成 Hyperledger Fabric 创世区块 # 这里使用 configtxgen 来创建 创世区块 # 首先需要创建一个文件夹 mkdir -p /opt/jicki/channel-artifacts # 修改 configtx.yaml 文件增加 orderer 多节点 以及 peer 多节点, 删除了 \u0026Org3 节点 # 完整 configtx.yaml 如下: # configtx.yaml 文件格式 请千万注意 空格 与 tab 键 里的缩进，否则会报错。 Organizations: - \u0026OrdererOrg Name: OrdererOrg ID: OrdererMSP MSPDir: crypto-config/ordererOrganizations/jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Writers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Admins: Type: Signature Rule: \"OR('OrdererMSP.admin')\" - \u0026Org1 Name: Org1MSP ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.peer', 'Org1MSP.client')\" Writers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.client')\" Admins: Type: Signature Rule: \"OR('Org1MSP.admin')\" AnchorPeers: - Host: peer0.org1.jicki.cn Port: 7051 - \u0026Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.peer', 'Org2MSP.client')\" Writers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.client')\" Admins: Type: Signature Rule: \"OR('Org2MSP.admin')\" AnchorPeers: - Host: peer0.org2.jicki.cn Port: 7051 Capabilities: Channel: \u0026ChannelCapabilities V1_3: true Orderer: \u0026OrdererCapabilities V1_1: true Application: \u0026ApplicationCapabilities V1_3: true V1_2: false V1_1: false Application: \u0026ApplicationDefaults Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ApplicationCapabilities Orderer: \u0026OrdererDefaults OrdererType: kafka Addresses: - orderer0.jicki.cn:7050 - orderer1.jicki.cn:7050 - orderer2.jicki.cn:7050 BatchTimeout: 2s BatchSize: MaxMessageCount: 10 AbsoluteMaxBytes: 99 MB PreferredMaxBytes: 512 KB Kafka: Brokers: - kafka0:9092 - kafka1:9092 - kafka2:9092 Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" BlockValidation: Type: ImplicitMeta Rule: \"ANY Writers\" Capabilities: \u003c\u003c: *OrdererCapabilities Channel: \u0026ChannelDefaults Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ChannelCapabilities Profiles: TwoOrgsOrdererGenesis: \u003c\u003c: *ChannelDefaults Orderer: \u003c\u003c: *OrdererDefaults Organizations: - *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: \u003c\u003c: *ApplicationDefaults Organizations: - *Org1 - *Org2 # 创建 创世区块 TwoOrgsOrdererGenesis 名称为 configtx.yaml 中 Profiles 字段下的 [root@localhost jicki]# configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./channel-artifacts/genesis.block 2018-11-05 12:06:21.780 CST [common/tools/configtxgen] main -\u003e WARN 001 Omitting the channel ID for configtxgen for output operations is deprecated. Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'. 2018-11-05 12:06:21.780 CST [common/tools/configtxgen] main -\u003e INFO 002 Loading configuration 2018-11-05 12:06:21.829 CST [common/tools/configtxgen] doOutputBlock -\u003e INFO 003 Generating genesis block 2018-11-05 12:06:21.829 CST [common/tools/configtxgen] doOutputBlock -\u003e INFO 004 Writing genesis block # 创世区块 是在 orderer 服务中使用 [root@localhost jicki]# ls -lt channel-artifacts/ 总用量 16 -rw-r--r-- 1 root root 12479 11月 5 12:06 genesis.block # 下面来生成一个 peer 服务 中使用的 tx 文件 TwoOrgsChannel 名称为 configtx.yaml 中 Profiles 字段下的，这里必须指定上面的 channelID [root@localhost jicki]# configtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/channel.tx -channelID mychannel 2018-11-05 12:08:11.241 CST [common/tools/configtxgen] main -\u003e INFO 001 Loading configuration 2018-11-05 12:08:11.286 CST [common/tools/configtxgen] doOutp","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:6:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"配置 Zookeeper Kafka 集群 启动 zookeeper 与 kafka 集群 vi docker-compose-zk-kafka.yaml version: '2' services: zookeeper1: container_name: zookeeper1 hostname: zookeeper1 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=1 - ZOO_SERVERS=server.1=zookeeper1:2888:3888 server.2=zookeeper2:2888:3888 server.3=zookeeper3:2888:3888 #volumes: # 存储数据与日志 #- ./data/zookeeper1/data:/data #- ./data/zookeeper1/datalog:/datalog networks: default: aliases: - jicki zookeeper2: container_name: zookeeper2 hostname: zookeeper2 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=2 - ZOO_SERVERS=server.1=zookeeper1:2888:3888 server.2=zookeeper2:2888:3888 server.3=zookeeper3:2888:3888 #volumes: # 存储数据与日志 #- ./data/zookeeper2/data:/data #- ./data/zookeeper2/datalog:/datalog networks: default: aliases: - jicki zookeeper3: container_name: zookeeper3 hostname: zookeeper3 image: hyperledger/fabric-zookeeper restart: always environment: - ZOO_MY_ID=3 - ZOO_SERVERS=server.1=zookeeper1:2888:3888 server.2=zookeeper2:2888:3888 server.3=zookeeper3:2888:3888 #volumes: # 存储数据与日志 #- ./data/zookeeper3/data:/data #- ./data/zookeeper3/datalog:/datalog networks: default: aliases: - jicki kafka0: container_name: kafka0 hostname: kafka0 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=1 # 设置一个M值,数据提交时会写入至少M个副本(这里M=2)（这些数据会被同步并且归属到in-sync 副本集合或ISR）M 必须小于 如下 N 值,并且大于1,既最小为2。 - KAFKA_MIN_INSYNC_REPLICAS=2 # 设置一个N值, N代表着每个channel都保存N个副本的数据到Kafka的代理上。N 必须大于如上 M 值, 既 N 值最小值为 3。 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181 # 如下99为configtx.yaml中会设置最大的区块大小(参考configtx.yaml中AbsoluteMaxBytes参数) # 每个区块最大有Orderer.AbsoluteMaxBytes个字节 # 99 * 1024 * 1024 B - KAFKA_MESSAGE_MAX_BYTES=103809024 # 每个通道获取的消息的字节数 如上一样 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 # 数据一致性在区块链环境中是至关重要的, 我们不能从in-sync 副本（ISR）集合之外选取channel leader , 否则我们将会面临对于之前的leader产生的offsets覆盖的风险 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false # 关闭基于时间的日志保留方式并且避免分段到期。 - KAFKA_LOG_RETENTION_MS=-1 #volumes: # 存储数据与日志. #- ./data/kafka1/data:/data #- ./data/kafka1/data:/logs networks: default: aliases: - jicki depends_on: - zookeeper1 - zookeeper2 - zookeeper3 kafka1: container_name: kafka1 hostname: kafka1 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=2 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181 - KAFKA_MESSAGE_MAX_BYTES=103809024 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false - KAFKA_LOG_RETENTION_MS=-1 #volumes: # 存储数据与日志. #- ./data/kafka1/data:/data #- ./data/kafka1/data:/logs networks: default: aliases: - jicki depends_on: - zookeeper1 - zookeeper2 - zookeeper3 kafka2: container_name: kafka2 hostname: kafka2 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=3 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper1:2181,zookeeper2:2181,zookeeper3:2181 - KAFKA_MESSAGE_MAX_BYTES=103809024 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false # 关闭基于时间的日志保留方式并且避免分段到期。 - KAFKA_LOG_RETENTION_MS=-1 #volumes: # 存储数据与日志. #- ./data/kafka2/data:/data #- ./data/kafka2/data:/logs networks: default: aliases: - jicki depends_on: - zookeeper1 - zookeeper2 - zookeeper3 # 分别启动服务 docker-compose -f docker-compose-zk-kafka.yaml up -d ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:7:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric Orderer # 创建文件 docker-compose-orderer.yaml # 创建于 /opt/jicki 目录下 vi docker-compose-orderer.yaml version: '2' services: orderer0.jicki.cn: container_name: orderer0.jicki.cn image: hyperledger/fabric-orderer environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default # - ORDERER_GENERAL_LOGLEVEL=error - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 #- ORDERER_GENERAL_GENESISPROFILE=AntiMothOrdererGenesis - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #- ORDERER_GENERAL_LEDGERTYPE=ram #- ORDERER_GENERAL_LEDGERTYPE=file # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt, /etc/hyperledger/crypto/peerOrg1/tls/ca.crt, /etc/hyperledger/crypto/peerOrg2/tls/ca.crt] # KAFKA - ORDERER_KAFKA_RETRY_LONGINTERVAL=10s - ORDERER_KAFKA_RETRY_LONGTOTAL=100s - ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s - ORDERER_KAFKA_RETRY_SHORTTOTAL=30s - ORDERER_KAFKA_VERBOSE=true - ORDERER_KAFKA_BROKERS=[kafka0:9092,kafka1:9092,kafka2:9092] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/:/var/hyperledger/orderer/tls - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/:/etc/hyperledger/crypto/peerOrg1 - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/:/etc/hyperledger/crypto/peerOrg2 networks: default: aliases: - jicki ports: - 7050:7050 depends_on: - kafka0 - kafka1 - kafka2 orderer1.jicki.cn: container_name: orderer1.jicki.cn image: hyperledger/fabric-orderer environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default # - ORDERER_GENERAL_LOGLEVEL=error - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 #- ORDERER_GENERAL_GENESISPROFILE=AntiMothOrdererGenesis - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #- ORDERER_GENERAL_LEDGERTYPE=ram #- ORDERER_GENERAL_LEDGERTYPE=file # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=true - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt, /etc/hyperledger/crypto/peerOrg1/tls/ca.crt, /etc/hyperledger/crypto/peerOrg2/tls/ca.crt] # KAFKA - ORDERER_KAFKA_RETRY_LONGINTERVAL=10s - ORDERER_KAFKA_RETRY_LONGTOTAL=100s - ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s - ORDERER_KAFKA_RETRY_SHORTTOTAL=30s - ORDERER_KAFKA_VERBOSE=true - ORDERER_KAFKA_BROKERS=[kafka0:9092,kafka1:9092,kafka2:9092] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/:/var/hyperledger/orderer/tls - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/:/etc/hyperledger/crypto/peerOrg1 - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/:/etc/hyperledger/crypto/peerOrg2 networks: default: aliases: - jicki ports: - 8050:7050 depends_on: - kafka0 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:8:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"启动服务 所有节点启动服务 docker-compose -f docker-compose-orderer.yaml up -d # 启动完毕，查看docker logs 如下表示成功 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processMessagesToBlocks -\u003e DEBU 2fd [channel: testchainid] Successfully unmarshalled consumed message, offset is 0. Inspecting type... 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processConnect -\u003e DEBU 2fe [channel: testchainid] It's a connect message - ignoring 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processMessagesToBlocks -\u003e DEBU 2ff [channel: testchainid] Successfully unmarshalled consumed message, offset is 1. Inspecting type... 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processConnect -\u003e DEBU 300 [channel: testchainid] It's a connect message - ignoring 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processMessagesToBlocks -\u003e DEBU 301 [channel: testchainid] Successfully unmarshalled consumed message, offset is 2. Inspecting type... 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processConnect -\u003e DEBU 302 [channel: testchainid] It's a connect message - ignoring ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:8:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric ca 节点 配置两个 ca 节点 vi docker-compose-ca.yaml version: '2' services: ca.org1.jicki.cn: container_name: ca.org1.jicki.cn image: hyperledger/fabric-ca environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca-org1 - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.jicki.cn-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/aa167e5dcb62e9e1068487484c08f0bc0a13e13cebf53a0fa2f64038a1a76662_sk - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.jicki.cn-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/aa167e5dcb62e9e1068487484c08f0bc0a13e13cebf53a0fa2f64038a1a76662_sk ports: - \"7054:7054\" command: sh -c 'fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org1.jicki.cn-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/aa167e5dcb62e9e1068487484c08f0bc0a13e13cebf53a0fa2f64038a1a76662_sk -b admin:adminpw -d' volumes: - ./crypto-config/peerOrganizations/org1.jicki.cn/ca/:/etc/hyperledger/fabric-ca-server-config depends_on: - orderer0.jicki.cn - orderer1.jicki.cn - orderer2.jicki.cn ca.org2.jicki.cn: container_name: ca.org2.jicki.cn image: hyperledger/fabric-ca environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca-org2 - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_CA_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.jicki.cn-cert.pem - FABRIC_CA_SERVER_CA_KEYFILE=/etc/hyperledger/fabric-ca-server-config/bcbc85a3992715502c7e7e7d21b792678a9d03668a52bb18a0c10862d45222c3_sk - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org2.jicki.cn-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/bcbc85a3992715502c7e7e7d21b792678a9d03668a52bb18a0c10862d45222c3_sk ports: - \"8054:7054\" command: sh -c 'fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org2.jicki.cn-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/bcbc85a3992715502c7e7e7d21b792678a9d03668a52bb18a0c10862d45222c3_sk -b admin:adminpw -d' volumes: - ./crypto-config/peerOrganizations/org2.jicki.cn/ca/:/etc/hyperledger/fabric-ca-server-config depends_on: - orderer0.jicki.cn - orderer1.jicki.cn - orderer2.jicki.cn ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:9:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric peer peer 节点下都必须启动一个数据存储，如 file 或者 couchdb 等 vi docker-compose-peer.yaml version: '2' services: couchdb0: container_name: couchdb0 image: hyperledger/fabric-couchdb environment: - COUCHDB_USER= - COUCHDB_PASSWORD= ports: - \"5984:5984\" #volumes: # 数据持久化，用于存储链码值 #- ./data/couchdb0/data:/opt/couchdb/data networks: default: aliases: - jicki couchdb1: container_name: couchdb1 image: hyperledger/fabric-couchdb environment: - COUCHDB_USER= - COUCHDB_PASSWORD= ports: - \"6984:5984\" #volumes: # 数据持久化，用于存储链码值 #- ./data/couchdb1/data:/opt/couchdb/data networks: default: aliases: - jicki peer0.org1.jicki.cn: container_name: peer0.org1.jicki.cn image: hyperledger/fabric-peer environment: - CORE_LEDGER_STATE_STATEDATABASE=CouchDB - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984 - CORE_PEER_ID=peer0.org1.jicki.cn - CORE_PEER_NETWORKID=jicki - CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 - CORE_PEER_CHAINCODELISTENADDRESS=peer0.org1.jicki.cn:7052 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.jicki.cn:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_PROFILE_ENABLED=false - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt volumes: - /var/run/:/host/var/run/ - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/msp:/etc/hyperledger/fabric/msp - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls:/etc/hyperledger/fabric/tls # 数据持久化, 存储安装，以及实例化智能合约的数据 #- ./data/peer0org1:/var/hyperledger/production working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start ports: - 7051:7051 - 7052:7052 - 7053:7053 networks: default: aliases: - jicki depends_on: - couchdb0 - ca.org1.jicki.cn - ca.org2.jicki.cn - orderer0.jicki.cn - orderer1.jicki.cn - orderer2.jicki.cn peer0.org2.jicki.cn: container_name: peer0.org2.jicki.cn image: hyperledger/fabric-peer environment: - CORE_LEDGER_STATE_STATEDATABASE=CouchDB - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb1:5984 - CORE_PEER_ID=peer0.org2.jicki.cn - CORE_PEER_NETWORKID=jicki - CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 - CORE_PEER_CHAINCODELISTENADDRESS=peer0.org2.jicki.cn:7052 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org2.jicki.cn:7051 - CORE_PEER_LOCALMSPID=Org2MSP - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_PROFILE_ENABLED=false - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt volumes: - /var/run/:/host/var/run/ - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/msp:/etc/hyperledger/fabric/msp - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls:/etc/hyperledger/fabric/tls # 数据持久化, 存储安装，以及实例化智能合约的数据 #- ./data/peer0org2:/var/hyperledger/production working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start ports: - 8051:7051 - 8052:7052 - 8053:7053 networks: default: aliases: - jicki depends_on: - couchdb1 - ca.org1.jicki.cn - ca.org2.jicki.cn - orderer0.jicki.cn - orderer1.jicki.cn - orderer2.jicki.cn ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:10:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"启动服务 docker-compose -f docker-compose-peer.yaml up -d ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:10:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric cli 客户端 客户端服务，只需要配置一个既可，用于调用创建 channel 与 智能合约 vi docker-compose-cli.yaml version: '2' services: cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=true - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer volumes: - /var/run/:/host/var/run/ - /opt/golang/go:/opt/go - /opt/gopath:/opt/gopath - ./peer:/opt/gopath/src/github.com/hyperledger/fabric/peer - ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/jicki/chaincode/go - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts networks: default: aliases: - jicki ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:11:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"启动服务 docker-compose -f docker-compose-cli.yaml up -d ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:11:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"Hyperledger Fabric 创建 Channel # 上面我们创建了 cli 容器，我们可以直接进入 容器里操作 [root@localhost jicki]# docker exec -it cli bash root@0b55c64a9853:/opt/gopath/src/github.com/hyperledger/fabric/peer# # 执行 创建命令 (未启动 认证) peer channel create -c mychannel -f ./channel-artifacts/channel.tx --orderer orderer0.jicki.cn:7050 # 提示如下表示认证不通过 Error: failed to create deliver client: rpc error: code = Unavailable desc = all SubConns are in TransientFailure, latest connection error: \u003cnil\u003e # 以下为启用认证 peer channel create -o orderer0.jicki.cn:7050 -c mychannel -f ./channel-artifacts/channel.tx --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输出如下: 2018-11-05 04:29:11.416 UTC [grpc] DialContext -\u003e DEBU 060 parsed scheme: \"\" 2018-11-05 04:29:11.416 UTC [grpc] DialContext -\u003e DEBU 061 scheme \"\" not registered, fallback to default scheme 2018-11-05 04:29:11.416 UTC [grpc] watcher -\u003e DEBU 062 ccResolverWrapper: sending new addresses to cc: [{orderer0.jicki.cn:7050 0 \u003cnil\u003e}] 2018-11-05 04:29:11.416 UTC [grpc] switchBalancer -\u003e DEBU 063 ClientConn switching balancer to \"pick_first\" 2018-11-05 04:29:11.417 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 064 pickfirstBalancer: HandleSubConnStateChange: 0xc420389240, CONNECTING 2018-11-05 04:29:11.417 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 065 pickfirstBalancer: HandleSubConnStateChange: 0xc420389240, READY 2018-11-05 04:29:11.417 UTC [channelCmd] InitCmdFactory -\u003e INFO 066 Endorser and orderer connections initialized 2018-11-05 04:29:11.618 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 067 Obtaining default signing identity 2018-11-05 04:29:11.618 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 068 Obtaining default signing identity 2018-11-05 04:29:11.618 UTC [msp/identity] Sign -\u003e DEBU 069 Sign: plaintext: 0AD1060A1508051A0608978EFFDE0522...3561A3D8F91812080A021A0012021A00 2018-11-05 04:29:11.618 UTC [msp/identity] Sign -\u003e DEBU 06a Sign: digest: 8E158641AEF7FC331BDF972FB71525AA5B119D2ABD26A6B4932A6BE6E0179D4F 2018-11-05 04:29:11.622 UTC [cli/common] readBlock -\u003e INFO 06b Received block: 0 # 创建以后生成文件 mychannel.block total 16 -rw-r--r-- 1 root root 15407 Nov 5 04:29 mychannel.block drwxr-xr-x 2 root root 111 Nov 5 04:09 channel-artifacts drwxr-xr-x 4 root root 69 Nov 5 03:53 crypto ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:12:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"Hyperledger Fabric 加入 Channel 我们这边有2个 peer 所以需要分别加入, 后续有多少个 peer 都需要加入到 Channel 中 # peer0.org1.jicki.cn 加入 此 channel 中，首先需要查看如下 环境变量 echo $CORE_PEER_LOCALMSPID echo $CORE_PEER_ADDRESS echo $CORE_PEER_MSPCONFIGPATH echo $CORE_PEER_TLS_ROOTCERT_FILE # 加入 channel (未开启认证) peer channel join -b mychannel.block # 加入 channel (开启认证) peer channel join -b mychannel.block -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输出如下: 2018-11-07 03:12:21.050 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-11-02 03:49:00 +0000 UTC 2018-11-07 03:12:21.050 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-11-07 03:12:21.050 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 037 Obtaining default signing identity 2018-11-07 03:12:21.051 UTC [grpc] DialContext -\u003e DEBU 038 parsed scheme: \"\" 2018-11-07 03:12:21.051 UTC [grpc] DialContext -\u003e DEBU 039 scheme \"\" not registered, fallback to default scheme 2018-11-07 03:12:21.051 UTC [grpc] watcher -\u003e DEBU 03a ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-07 03:12:21.051 UTC [grpc] switchBalancer -\u003e DEBU 03b ClientConn switching balancer to \"pick_first\" 2018-11-07 03:12:21.051 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc42053dd30, CONNECTING 2018-11-07 03:12:21.056 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03d pickfirstBalancer: HandleSubConnStateChange: 0xc42053dd30, READY 2018-11-07 03:12:21.056 UTC [channelCmd] InitCmdFactory -\u003e INFO 03e Endorser and orderer connections initialized 2018-11-07 03:12:21.057 UTC [msp/identity] Sign -\u003e DEBU 03f Sign: plaintext: 0A97070A5B08011A0B0895B089DF0510...FD5719255DE61A080A000A000A000A00 2018-11-07 03:12:21.057 UTC [msp/identity] Sign -\u003e DEBU 040 Sign: digest: 4F2C696D018EC053894B7911B363A9D58394B6DF795CBDBEFDF52A1775AF87F3 2018-11-07 03:12:21.163 UTC [channelCmd] executeJoin -\u003e INFO 041 Successfully submitted proposal to join channel # peer1.org2.jicki.cn 加入 此 channel 中，这里配置一下环境变量 export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp # 加入 channel (未开启认证) peer channel join -b mychannel.block # 加入 channel (开启认证) peer channel join -b mychannel.block -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输入如下: 2018-11-07 03:13:15.350 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-11-02 03:49:00 +0000 UTC 2018-11-07 03:13:15.350 UTC [msp] Validate -\u003e DEBU 036 MSP Org2MSP validating identity 2018-11-07 03:13:15.351 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 037 Obtaining default signing identity 2018-11-07 03:13:15.351 UTC [grpc] DialContext -\u003e DEBU 038 parsed scheme: \"\" 2018-11-07 03:13:15.351 UTC [grpc] DialContext -\u003e DEBU 039 scheme \"\" not registered, fallback to default scheme 2018-11-07 03:13:15.352 UTC [grpc] watcher -\u003e DEBU 03a ccResolverWrapper: sending new addresses to cc: [{peer0.org2.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-07 03:13:15.352 UTC [grpc] switchBalancer -\u003e DEBU 03b ClientConn switching balancer to \"pick_first\" 2018-11-07 03:13:15.352 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc420311d00, CONNECTING 2018-11-07 03:13:15.356 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03d pickfirstBalancer: HandleSubConnStateChange: 0xc420311d00, READY 2018-11-07 03:13:15.356 UTC [channelCmd] ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:13:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"Hyperledger Fabric 实例化测试 在上面我们已经拷贝了官方的例子，在 chaincode 下, 下面我们来测试一下 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:14:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"安装智能合约 # cli 部分 ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/jicki/chaincode/go # 为 智能合约的目录 我们约定为这个目录 需要预先创建 mkdir -p /opt/jicki/chaincode/go cd /opt/jicki/chaincode/go # 创建以后~我们拷贝官方的 例子进来，方便后面进行合约测试 cp -r /opt/gopath/src/github.com/hyperledger/fabric/examples/chaincode/go/example0* /opt/jicki/chaincode/go/ # 官方这里有5个例子 [root@localhost jicki]# ls -lt chaincode/go/ 总用量 0 drwxr-xr-x 3 root root 75 11月 5 12:32 example05 drwxr-xr-x 3 root root 75 11月 5 12:32 example03 drwxr-xr-x 3 root root 75 11月 5 12:32 example04 drwxr-xr-x 3 root root 47 11月 5 12:32 example01 drwxr-xr-x 3 root root 75 11月 5 12:32 example02 # 如上我们挂载的地址为 github.com/hyperledger/fabric/jicki/chaincode/go # 注: 这里面的 example02 的 package 为 example02 会报错 Error: could not assemble transaction, err Proposal response was not successful, error code 500, msg failed to execute transaction 819b581ce88604e9b6651764324876f2ca7a47d7aeb7ee307f273af867a4a134: error starting container: error starting container: API error (404): oci runtime error: container_linux.go:247: starting container process caused \"exec: \\\"chaincode\\\": executable file not found in $PATH\" # 将 chaincode.go chaincode_test.go 中 package 修改成 main 然后在最下面增加 main()函数 package example02 修改为 package main # 在最后添加如下: func main() { err := shim.Start(new(SimpleChaincode)) if err != nil { fmt.Printf(\"Error starting Simple chaincode: %s\", err) } } # 安装指定合约到 所有的 peer 节点中，每个节点都必须安装一次 # 同样需要先配置变量 export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp # 安装 合约 peer chaincode install -n example2 -p github.com/hyperledger/fabric/jicki/chaincode/go/example02 -v 1.0 # 输出如下: 2018-11-05 04:34:32.461 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-11-02 03:49:00 +0000 UTC 2018-11-05 04:34:32.461 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-11-05 04:34:32.462 UTC [grpc] DialContext -\u003e DEBU 037 parsed scheme: \"\" 2018-11-05 04:34:32.462 UTC [grpc] DialContext -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-11-05 04:34:32.463 UTC [grpc] watcher -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-05 04:34:32.463 UTC [grpc] switchBalancer -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-11-05 04:34:32.463 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc42054f730, CONNECTING 2018-11-05 04:34:32.464 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc42054f730, READY 2018-11-05 04:34:32.466 UTC [grpc] DialContext -\u003e DEBU 03d parsed scheme: \"\" 2018-11-05 04:34:32.466 UTC [grpc] DialContext -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-11-05 04:34:32.466 UTC [grpc] watcher -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-05 04:34:32.466 UTC [grpc] switchBalancer -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-11-05 04:34:32.466 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc4205ad630, CONNECTING 2018-11-05 04:34:32.467 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc4205ad630, READY 2018-11-05 04:34:32.468 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-11-05 04:34:32.468 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 044 Using default escc 2018-11-05 04:34:32.468 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 045 Using default vscc 2018-11-05 04:34:32.545 UTC [golang-platform] getCodeFromFS -\u003e DEBU 046 getCodeFromFS github.com/hyp","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:14:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"实例化 Chaincode 这里无论多少个 peer 节点, 实例化只需要实例化一次，就可以。 # 实例化合约 (未认证) peer chaincode instantiate -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"200\",\"B\",\"500\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.0 # 实例化合约 (已认证) peer chaincode instantiate -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"200\",\"B\",\"500\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.0 # 输出如下: 2018-11-07 03:17:30.242 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-11-02 03:49:00 +0000 UTC 2018-11-07 03:17:30.242 UTC [msp] Validate -\u003e DEBU 036 MSP Org2MSP validating identity 2018-11-07 03:17:30.243 UTC [grpc] DialContext -\u003e DEBU 037 parsed scheme: \"\" 2018-11-07 03:17:30.243 UTC [grpc] DialContext -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-11-07 03:17:30.244 UTC [grpc] watcher -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org2.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-07 03:17:30.244 UTC [grpc] switchBalancer -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-11-07 03:17:30.244 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc4205e5be0, CONNECTING 2018-11-07 03:17:30.248 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc4205e5be0, READY 2018-11-07 03:17:30.249 UTC [grpc] DialContext -\u003e DEBU 03d parsed scheme: \"\" 2018-11-07 03:17:30.249 UTC [grpc] DialContext -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-11-07 03:17:30.249 UTC [grpc] watcher -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org2.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-07 03:17:30.249 UTC [grpc] switchBalancer -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-11-07 03:17:30.249 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc4205b0780, CONNECTING 2018-11-07 03:17:30.252 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc4205b0780, READY 2018-11-07 03:17:30.253 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-11-07 03:17:30.254 UTC [grpc] DialContext -\u003e DEBU 044 parsed scheme: \"\" 2018-11-07 03:17:30.254 UTC [grpc] DialContext -\u003e DEBU 045 scheme \"\" not registered, fallback to default scheme 2018-11-07 03:17:30.254 UTC [grpc] watcher -\u003e DEBU 046 ccResolverWrapper: sending new addresses to cc: [{orderer0.jicki.cn:7050 0 \u003cnil\u003e}] 2018-11-07 03:17:30.254 UTC [grpc] switchBalancer -\u003e DEBU 047 ClientConn switching balancer to \"pick_first\" 2018-11-07 03:17:30.254 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 048 pickfirstBalancer: HandleSubConnStateChange: 0xc42029ee50, CONNECTING 2018-11-07 03:17:30.257 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 049 pickfirstBalancer: HandleSubConnStateChange: 0xc42029ee50, READY 2018-11-07 03:17:30.257 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 04a Using default escc 2018-11-07 03:17:30.257 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 04b Using default vscc 2018-11-07 03:17:30.258 UTC [msp/identity] Sign -\u003e DEBU 04c Sign: plaintext: 0AA6070A6608031A0B08CAB289DF0510...324D53500A04657363630A0476736363 2018-11-07 03:17:30.258 UTC [msp/identity] Sign -\u003e DEBU 04d Sign: digest: 4CADBFD99999E7E35A2C6C221EE15A5D6D1531AEC62B35B70F47B3C638049B67 2018-11-07 03:17:30.267 UTC [msp/identity] Sign -\u003e DEBU 04e Sign: plaintext: 0AA6070A6608031A0B08CAB289DF0510...A39B1E366186F53CCB32CCEC0B05E851 2018-11-07 03:17:30.267 UTC [msp/identity] Sign -\u003e DEBU 04f Sign: digest: 382322A56C5F45E0E6432CBFB026334DDA4106A3DFEABE7A14802614F04D5914 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:14:2","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"操作智能合约 # query 查询方法 # 查询 A 账户里的余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"A\"]}' # 输出如下: 2018-11-05 04:35:47.211 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-11-02 03:49:00 +0000 UTC 2018-11-05 04:35:47.212 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-11-05 04:35:47.212 UTC [grpc] DialContext -\u003e DEBU 037 parsed scheme: \"\" 2018-11-05 04:35:47.212 UTC [grpc] DialContext -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-11-05 04:35:47.213 UTC [grpc] watcher -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-05 04:35:47.213 UTC [grpc] switchBalancer -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-11-05 04:35:47.213 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc4205355e0, CONNECTING 2018-11-05 04:35:47.214 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc4205355e0, READY 2018-11-05 04:35:47.216 UTC [grpc] DialContext -\u003e DEBU 03d parsed scheme: \"\" 2018-11-05 04:35:47.216 UTC [grpc] DialContext -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-11-05 04:35:47.216 UTC [grpc] watcher -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-05 04:35:47.216 UTC [grpc] switchBalancer -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-11-05 04:35:47.216 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc4203d2790, CONNECTING 2018-11-05 04:35:47.217 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc4203d2790, READY 2018-11-05 04:35:47.218 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-11-05 04:35:47.218 UTC [msp/identity] Sign -\u003e DEBU 044 Sign: plaintext: 0AA6070A6A08031A0B08A391FFDE0510...706C65321A0A0A0571756572790A0141 2018-11-05 04:35:47.219 UTC [msp/identity] Sign -\u003e DEBU 045 Sign: digest: F4FF4D4EE24CDFE6D2EF369DEF6AA927717916A8076FAD3A32396C2EE39D70B6 200 # 可以看到 返回 200 # 查询 B 账户里的余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"B\"]}' # 输出如下: 2018-11-05 04:36:00.734 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-11-02 03:49:00 +0000 UTC 2018-11-05 04:36:00.734 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-11-05 04:36:00.735 UTC [grpc] DialContext -\u003e DEBU 037 parsed scheme: \"\" 2018-11-05 04:36:00.735 UTC [grpc] DialContext -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-11-05 04:36:00.735 UTC [grpc] watcher -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-05 04:36:00.735 UTC [grpc] switchBalancer -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-11-05 04:36:00.735 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc4203e5470, CONNECTING 2018-11-05 04:36:00.737 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc4203e5470, READY 2018-11-05 04:36:00.738 UTC [grpc] DialContext -\u003e DEBU 03d parsed scheme: \"\" 2018-11-05 04:36:00.739 UTC [grpc] DialContext -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-11-05 04:36:00.739 UTC [grpc] watcher -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-11-05 04:36:00.739 UTC [grpc] switchBalancer -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-11-05 04:36:00.739 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc42039b0c0, CONNECTING 2018-11-05 04:36:00.740 UTC [grpc] HandleSubConnStateChange -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc42039b0c0, READY 2018-11-05 04:36:00.740 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default s","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:14:3","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"Hyperledger Fabric 操作命令 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:15:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"peer 命令 peer chaincode # 对链进行操作 peer channel # channel相关操作 peer logging # 设置日志级别 peer node # 启动、管理节点 peer version # 查看版本信息 upgrade 更新合约 更新合约相当于将合约重新实例化，并带有一个新的版本号。 更新合约之前，需要在所有的 peer节点 上安装(install)最新的合约，并使用新的版本号。 # 更新合约 # 首先安装(install)新的合约, 以本文为例, chaincode_example02, 初次安装版本号为 1.0 peer chaincode install -n example2 -p github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02 -v 1.1 # 更新版本为 1.1 的合约 (未开启认证) peer chaincode upgrade -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"100\",\"B\",\"50\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.1 # 更新版本为 1.1 的合约 (开启认证) peer chaincode upgrade -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"100\",\"B\",\"50\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.1 -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 旧版本的合约, 目前，fabric不支持合约的启动与暂停。要暂停或删除合约，只能到peer上手动删除容器。 # 查看 已经创建的 通道 (channel) peer channel list # 查看通道(channel) 的状态 -c(小写) 加 通道名称 peer channel getinfo -c mychannel # 查看已经 安装的 智能合约(chincode) peer chaincode list --installed # 查看已经 实例化的 智能合约(chincode) 需要使用 -C(大写) 加通道名称 peer chaincode -C mychannel list --instantiated 配置 Hyperledger Fabric balance-transfer ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:15:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"安装 node.js # 安装NodeJS curl --silent --location https://rpm.nodesource.com/setup_8.x | sudo bash - yum install -y nodejs # 验证 node -v v8.11.4 npm -v 5.6.0 # 更改 npm 源为 taobao 源 npm install node-gyp --registry=https://registry.npm.taobao.org npm install node-pre-gyp --registry=https://registry.npm.taobao.org npm install grpc --registry=https://registry.npm.taobao.org npm install --registry=https://registry.npm.taobao.org npm rebuild # 安装一下 jq yum install jq -y ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:16:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"下载源码 cd /opt/jicki git clone https://github.com/hyperledger/fabric-samples.git mv fabric-samples/balance-transfer . ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:17:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"修改配置文件 mv network-config.yaml network-config.yaml-bak # 增加 network-config 文件 vi network-config.yaml --- name: \"balance-transfer\" x-type: \"hlfv1\" description: \"Balance Transfer Network\" version: \"1.0\" channels: mychannel: orderers: - orderer0.jicki.cn peers: peer0.org1.jicki.cn: endorsingPeer: true chaincodeQuery: true ledgerQuery: true eventSource: true #peer1.org1.jicki.cn: # endorsingPeer: false # chaincodeQuery: true # ledgerQuery: true # eventSource: false peer0.org2.jicki.cn: endorsingPeer: true chaincodeQuery: true ledgerQuery: true eventSource: true #peer1.org2.jicki.cn: # endorsingPeer: false # chaincodeQuery: true # ledgerQuery: true # eventSource: false chaincodes: - mycc:v0 organizations: Org1: mspid: Org1MSP peers: - peer0.org1.jicki.cn #- peer1.org1.jicki.cn certificateAuthorities: - ca-org1 adminPrivateKey: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp/keystore/a9666f561d211e7b7cc170bfe854721431a1038b7914a67555d82dcf6b9eaaf8_sk signedCert: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp/signcerts/Admin@org1.jicki.cn-cert.pem Org2: mspid: Org2MSP peers: - peer0.org2.jicki.cn #- peer1.org2.jicki.cn certificateAuthorities: - ca-org2 adminPrivateKey: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp/keystore/d402b684b807080653511978a51fcd2326668156cd8a10fb612b80bc49c9b354_sk signedCert: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp/signcerts/Admin@org2.jicki.cn-cert.pem orderers: orderer0.jicki.cn: url: grpcs://localhost:7050 grpcOptions: ssl-target-name-override: orderer0.jicki.cn tlsCACerts: path: artifacts/channel/crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/ca.crt peers: peer0.org1.jicki.cn: # this URL is used to send endorsement and query requests url: grpcs://localhost:7051 grpcOptions: ssl-target-name-override: peer0.org1.jicki.cn tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt #peer1.org1.jicki.cn: # url: grpcs://localhost:7056 # grpcOptions: # ssl-target-name-override: peer1.org1.jicki.cn # tlsCACerts: # path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/peers/peer1.org1.jicki.cn/tls/ca.crt peer0.org2.jicki.cn: url: grpcs://localhost:8051 grpcOptions: ssl-target-name-override: peer0.org2.jicki.cn tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt #peer1.org2.jicki.cn: # url: grpcs://localhost:8056 # eventUrl: grpcs://localhost:8058 # grpcOptions: # ssl-target-name-override: peer1.org2.jicki.cn # tlsCACerts: # path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/peers/peer1.org2.jicki.cn/tls/ca.crt certificateAuthorities: ca-org1: url: https://localhost:7054 httpOptions: verify: false tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org1.jicki.cn/ca/ca.org1.jicki.cn-cert.pem registrar: - enrollId: admin enrollSecret: adminpw caName: ca-org1 ca-org2: url: https://localhost:8054 httpOptions: verify: false tlsCACerts: path: artifacts/channel/crypto-config/peerOrganizations/org2.jicki.cn/ca/ca.org2.jicki.cn-cert.pem registrar: - enrollId: admin enrollSecret: adminpw caName: ca-org2 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:18:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"拷贝证书文件 cd /opt/jicki/balance-transfer/artifacts/ mv channel channel-bak mkdir channel # 拷贝自行生成的证书文件以及tx，等文件 [root@localhost channel]# ls -lt 总用量 56 -rw-r--r-- 1 root root 281 11月 5 16:40 Org1MSPanchors.tx -rw-r--r-- 1 root root 281 11月 5 16:40 Org2MSPanchors.tx -rw-r--r-- 1 root root 346 11月 5 16:40 channel.tx -rw-r--r-- 1 root root 12479 11月 5 16:40 genesis.block -rw-r--r-- 1 root root 15407 11月 5 16:39 mychannel.block -rw-r--r-- 1 root root 645 11月 5 16:39 crypto-config.yaml -rw-r--r-- 1 root root 3859 11月 5 16:39 configtx.yaml drwxr-xr-x 4 root root 69 11月 5 16:38 crypto-config ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:19:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"启动 balance-transfer runApp.sh 脚本里包含了 启动 ca 节点 以及 peer 节点的docker-compose 如上我们已经启动过了，只需要直接运行 node app 既可，所以不需要用到此脚本 # 安装依赖 npm install # 导入环境变量 PORT=4000 HOST=192.168.168.100 # 启动 node app ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:20:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"测试 调用 API # 运行 testAPIs.sh 可全部API跑一次 # 本文中只运行了 2个 peer # 所以需要 编辑 testAPIs.sh 修改文件中 删除 peer1.org1.jicki.cn 以及 peer1.org2.jicki.cn # 修改 \"channelConfigPath\":\"../artifacts/channel/mychannel.tx\" 中 mychannel.tx 为 channel.tx ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:0","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"创建用户 # 创建用户 curl -s -X POST http://localhost:4000/users -H \"content-type: application/x-www-form-urlencoded\" -d 'username=jicki\u0026orgName=Org1' {\"success\":true,\"secret\":\"\",\"message\":\"jicki enrolled Successfully\",\"token\":\"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDE2MTIwNzAsInVzZXJuYW1lIjoiSmltIiwib3JnTmFtZSI6Ik9yZzEiLCJpYXQiOjE1NDE1NzYwNzB9.pQZAqkeRW7vtwvNSrSTy4SKsVt5yFGafzlWTQjjONWE\"} # 注册成功以后~~取 token 部分,配置成环境变量, 如下操作需用使用此 token ORG1_TOKEN=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE1NDE2MTIwNzAsInVzZXJuYW1lIjoiSmltIiwib3JnTmFtZSI6Ik9yZzEiLCJpYXQiOjE1NDE1NzYwNzB9.pQZAqkeRW7vtwvNSrSTy4SKsVt5yFGafzlWTQjjONWE ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:1","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"创建 channel # 创建 channel # 此处 channelName 如果存在会失败~ 报如下错误 response ::{\"status\":\"BAD_REQUEST\",\"info\":\"error authorizing update: error validating ReadSet: readset expected key [Group] /Channel/Application at version 0, but got version 1\"} curl -s -X POST \\ http://localhost:4000/channels \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"channelName\":\"mychannel\", \"channelConfigPath\":\"../artifacts/channel/channel.tx\" }' # 关于 channelName 需要跟 创建 channel.tx 时的 channelID 对应，否则报如下错误: [2018-11-09 12:03:28.276] [DEBUG] Create-Channel - response ::{\"status\":\"BAD_REQUEST\",\"info\":\"Failing initial channel config creation: mismatched channel IDs: 'mychannel' != 'youchannel'\"} [2018-11-09 12:03:28.277] [ERROR] Create-Channel - !!!!!!!!! Failed to create the channel 'youchannel' !!!!!!!!! [2018-11-09 12:03:28.277] [ERROR] Create-Channel - Error: Failed to create the channel 'youchannel' ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:2","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"加入 channel # 加入 ORG1 加入 channel # 如果已加入 channel 报如下错误: [DEBUG] Join-Channel - Join Channel R E S P O N S E : [null,[{\"status\":500,\"payload\":{\"type\":\"Buffer\",\"data\":[]},\"isProposalResponse\":true}]] [ERROR] Join-Channel - Failed to join peer to the channel mychannel [ERROR] Join-Channel - Failed to join all peers to channel. cause:Failed to join peer to the channel mychannel curl -s -X POST \\ http://localhost:4000/channels/mychannel/peers \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\"] }' ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:3","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"安装 chaincode # 为 ORG1 安装 chaincode # 已存在报如下错误: [2018-11-07 16:10:44.011] [ERROR] install-chaincode - TypeError: proposalResponses.toJSON is not a function at Object.installChaincode (/opt/jicki/balance-transfer/app/install-chaincode.js:58:66) at \u003canonymous\u003e [2018-11-07 16:10:44.011] [ERROR] install-chaincode - Failed to install due to:TypeError: proposalResponses.toJSON is not a function curl -s -X POST \\ http://localhost:4000/chaincodes \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\"], \"chaincodeName\":\"mycc\", \"chaincodePath\":\"github.com/example_cc/go\", \"chaincodeType\": \"golang\", \"chaincodeVersion\":\"v0\" }' ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:4","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"实例化chaincode # 如果已经实例化 报如下错误: [2018-11-07 16:12:10.040] [ERROR] instantiate-chaincode - instantiate proposal was bad [2018-11-07 16:12:10.040] [DEBUG] instantiate-chaincode - Failed to send Proposal and receive all good ProposalResponse [2018-11-07 16:12:10.040] [ERROR] instantiate-chaincode - Failed to instantiate. cause:Failed to send Proposal and receive all good ProposalResponse curl -s -X POST \\ http://localhost:4000/channels/mychannel/chaincodes \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\"], \"chaincodeName\":\"mycc\", \"chaincodeVersion\":\"v0\", \"chaincodeType\": \"golang\", \"args\":[\"a\",\"100\",\"b\",\"200\"] }' ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:5","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"查询已创建的 channel curl -s -X GET \\ \"http://localhost:4000/channels?peer=peer0.org1.jicki.cn\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:19:29.592] [DEBUG] Query - \u003c\u003c\u003c channels \u003e\u003e\u003e [2018-11-07 16:19:29.592] [DEBUG] Query - [ 'channel id: mychannel' ] ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:6","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"查询 chaincode # %5B%22a%22%5D Escape 加密以后 等于 [\"a\"] # %5B%22b%22%5D Escape 加密以后 等于 [\"b\"] # 查询 a 的值 curl -s -X GET \\ \"http://localhost:4000/channels/mychannel/chaincodes/mycc?peer=peer0.org1.jicki.cn\u0026fcn=query\u0026args=%5B%22a%22%5D\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 输出如下: 2018-11-07 16:33:29.266] [INFO] Query - a now has 100 after the move # 查询 b 的值 curl -s -X GET \\ \"http://localhost:4000/channels/mychannel/chaincodes/mycc?peer=peer0.org1.jicki.cn\u0026fcn=query\u0026args=%5B%22b%22%5D\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 输出如下: [2018-11-07 16:34:12.378] [INFO] Query - b now has 200 after the move ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:7","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"操作交易 # 从 A账号 转账 10 到 B账号中 # 必须配置 org1 与 org2 节点的 peers 否则报错: [2018-11-07 17:17:53.267] [ERROR] invoke-chaincode - The invoke chaincode transaction was invalid, code:ENDORSEMENT_POLICY_FAILURE [2018-11-07 17:17:53.268] [ERROR] invoke-chaincode - Error: The invoke chaincode transaction was invalid, code:ENDORSEMENT_POLICY_FAILURE [2018-11-07 17:17:53.268] [ERROR] invoke-chaincode - Failed to invoke chaincode. cause:Error: The invoke chaincode transaction was invalid, code:ENDORSEMENT_POLICY_FAILURE curl -s -X POST \\ http://localhost:4000/channels/mychannel/chaincodes/mycc \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" \\ -d '{ \"peers\": [\"peer0.org1.jicki.cn\", \"peer0.org2.jicki.cn\"], \"fcn\":\"move\", \"args\":[\"a\",\"b\",\"10\"] }' # 输出如下: POST invoke chaincode on peers of Org1 and Org2 Transaction ID is 70a5157704b950cca09a6a46f5be7fca61355b43ed83f3d9a5b633f3e38b3619 ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:8","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"查询 chainInfo curl -s -X GET \\ \"http://localhost:4000/channels/mychannel?peer=peer0.org1.jicki.cn\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:14:55.648] [DEBUG] Query - { height: Long { low: 5, high: 0, unsigned: true }, currentBlockHash: ByteBuffer { buffer: \u003cBuffer 08 05 12 20 c0 c4 f3 65 a3 8a 66 d4 1e ff a4 45 3e 1c e6 2b 90 d3 38 4f 18 3f d5 b3 38 76 0e 26 30 32 e0 f9 1a 20 da 35 eb d4 1b 11 b2 7f 1a 07 c5 30 ... \u003e, offset: 4, markedOffset: -1, limit: 36, littleEndian: true, noAssert: false }, previousBlockHash: ByteBuffer { buffer: \u003cBuffer 08 05 12 20 c0 c4 f3 65 a3 8a 66 d4 1e ff a4 45 3e 1c e6 2b 90 d3 38 4f 18 3f d5 b3 38 76 0e 26 30 32 e0 f9 1a 20 da 35 eb d4 1b 11 b2 7f 1a 07 c5 30 ... \u003e, offset: 38, markedOffset: -1, limit: 70, littleEndian: true, noAssert: false } } ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:9","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"查询已安装 chaincode curl -s -X GET \\ \"http://localhost:4000/chaincodes?peer=peer0.org1.jicki.cn\u0026type=installed\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:17:01.758] [DEBUG] Query - \u003c\u003c\u003c Installed Chaincodes \u003e\u003e\u003e [2018-11-07 16:17:01.758] [DEBUG] Query - name: mycc, version: v0, path: github.com/example_cc/go ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:10","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["fabric"],"content":"查询实例化 chaincode curl -s -X GET \\ \"http://localhost:4000/chaincodes?peer=peer0.org1.jicki.cn\u0026type=instantiated\" \\ -H \"authorization: Bearer $ORG1_TOKEN\" \\ -H \"content-type: application/json\" # 返回如下信息: [2018-11-07 16:18:06.494] [DEBUG] Query - \u003c\u003c\u003c Installed Chaincodes \u003e\u003e\u003e [2018-11-07 16:18:06.494] [DEBUG] Query - name: mycc, version: v0, path: github.com/example_cc/go ","date":"2018-11-06","objectID":"/hyperledger-fabric-1.3/:21:11","tags":["fabric","docker"],"title":"hyperledger-fabric v 1.3","uri":"/hyperledger-fabric-1.3/"},{"categories":["kubernetes"],"content":"kubernetes 1.12.1","date":"2018-10-09","objectID":"/kubernetes-1.12.1/","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":" Update kubernetes 1.12.1 CHANGELOG 1. docker 版本 更新为 1.11.1,1.12.1,1.13.1,17.03,17.06,17.09,18.06 2. etcd 版本 更新为 v3.2.24 , 并且在1.13.x 版本中删除 etcd v2 支持 3. [ POD 自动伸缩 ] kubectl autoscale deployment nginx --min=2 --max=10 kubectl get horizontalpodautoscalers 查看伸缩的服务 获取 伸缩的指标 需要部署 metrics-server 到集群中 https://github.com/kubernetes-incubator/metrics-server 4. 已知问题BUG: kubelet: E1009 17:24:03.820462 10457 azure_dd.go:147] failed to get azure cloud in GetVolumeLimits kubernetes 1.12.1 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:0:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"环境说明 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node, node-66 只做单纯 Node kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:1:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.64 kubernetes-64 172.16.1.65 kubernetes-65 172.16.1.66 kubernetes-66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:2:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:3:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:4:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@kubernetes-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:5:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 官方最新版本 docker 为 18.06.1 , 官方验证最高版本支持到 18.06.0 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 18.06 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-18.06.0.ce-3.el7.x86_64.rpm rpm -ivh docker-ce-18.06.0.ce-3.el7.x86_64.rpm yum -y install docker-ce-18.06.0.ce # 查看安装 docker version Client: Version: 18.06.0-ce API version: 1.38 Go version: go1.10.3 Git commit: 0ffa825 Built: Wed Jul 18 19:08:18 2018 OS/Arch: linux/amd64 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:6:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) # docker 版本 17.03.2 之前配置为 --graph=/opt/docker # docker 版本 17.04.x 之后配置为 --data-root=/opt/docker [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --registry-mirror=http://b438f72b.m.daocloud.io \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.12.1 etcd 支持最新版本为 v3.2.24 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:7:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.2.24/etcd-v3.2.24-linux-amd64.tar.gz tar zxvf etcd-v3.2.24-linux-amd64.tar.gz cd etcd-v3.2.24-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:8:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@kubernetes-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 检查证书 [root@kubernetes-64 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:9:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:10:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:11:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:12:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.12.1/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:1","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@kubernetes-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:2","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"生成 kubernetes 配置文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:3","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:4","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@kubernetes-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:5","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 22a762c6fd1e636c3b1c7248980e4b93 # 创建 encryption-config.yaml 配置 cat \u003e encryption-config.yaml \u003c\u003cEOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 40179b02a8f6da07d90392ae966f7749 - identity: {} EOF # 拷贝 cp encryption-config.yaml /etc/kubernetes/ scp encryption-config.yaml 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 \u003e 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ \u003e \u003e 如下为最低限度的日志审核 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:6","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --anonymous-auth=false \\ --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # --experimental-encryption-provider-config ，替代之前 token.csv 文件 # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:7","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver # 如果报错 请使用 journalctl -f -t kube-apiserver 和 journalctl -u kube-apiserver 来定位问题 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:8","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager 新增几个配置，用于自动 续期证书 –feature-gates=RotateKubeletServerCertificate=true –experimental-cluster-signing-duration=86700h0m0s # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --feature-gates=RotateKubeletServerCertificate=true \\ --controllers=*,tokencleaner,bootstrapsigner \\ --experimental-cluster-signing-duration=86700h0m0s \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=5m0s \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:9","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager # 如果报错 请使用 journalctl -f -t kube-controller-manager 和 journalctl -u kube-controller-manager 来定位问题 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:10","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:11","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:12","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@kubernetes-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@kubernetes-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:13","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 kubelet 认证 kubelet 授权 kube-apiserver 的一些操作 exec run logs 等 # RBAC 只需创建一次就可以 kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:14","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 bootstrap kubeconfig 文件 注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token # 创建 集群所有 kubelet 的 token [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-64 --kubeconfig ~/.kube/config I1009 10:39:16.623409 3117 version.go:89] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: dial tcp 172.217.25.16:443: connect: connection timed out I1009 10:39:16.623486 3117 version.go:94] falling back to the local client version: v1.12.1 ado3mb.00vde0vkgvfbpz30 [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-65 --kubeconfig ~/.kube/config I1009 10:40:14.199418 3126 version.go:89] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: dial tcp 172.217.25.16:443: connect: connection timed out I1009 10:40:14.199487 3126 version.go:94] falling back to the local client version: v1.12.1 6xkesn.bmym9293ty2r1umr [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-66 --kubeconfig ~/.kube/config I1009 10:40:42.919424 3136 version.go:89] could not fetch a Kubernetes version from the internet: unable to get URL \"https://dl.k8s.io/release/stable-1.txt\": Get https://storage.googleapis.com/kubernetes-release/release/stable-1.txt: dial tcp 172.217.25.16:443: connect: connection timed out I1009 10:40:42.919501 3136 version.go:94] falling back to the local client version: v1.12.1 6jj682.j7wgboa50f6agith # 查看生成的 token [root@kubernetes-64 kubernetes]# kubeadm token list --kubeconfig ~/.kube/config TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 6jj682.j7wgboa50f6agith 23h 2018-10-10T10:40:42+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-64 6xkesn.bmym9293ty2r1umr 23h 2018-10-10T10:40:14+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-65 ado3mb.00vde0vkgvfbpz30 23h 2018-10-10T10:39:16+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-66 以下为了区分 会先生成 node 名称加 bootstrap.kubeconfig 生成 kubernetes-64 # 生成 64 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=6jj682.j7wgboa50f6agith \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 拷贝生成的 kubernetes-64-bootstrap.kubeconfig 文件 mv kubernetes-64-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-65 # 生成 65 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=1ua4d4.9bluufy3esw4lch6 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 拷贝生成的 kubernetes-65-bootstrap.kubeconfig 文件 scp kubernetes-65-bootstrap.kubeconfig 172.16.1.65:/etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-66 # 生成 66 的 bootstrap.kubeconfig # 配置集群参数 kubectl config se","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:15","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建自动批准相关 CSR 请求的 ClusterRole vi /etc/kubernetes/tls-instructs-csr.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"] # 导入 yaml 文件 [root@kubernetes-64 opt]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml clusterrole.rbac.authorization.k8s.io \"system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\" created # 查看 [root@kubernetes-64 opt]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"name\":\"system:certificates.k8s.io:certificatesigningreq... PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] # 将 ClusterRole 绑定到适当的用户组 # 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers # 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes # 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求 kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:16","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 关于 kubectl get node 中的 ROLES 的标签 单 Master 打标签 kubectl label node kubernetes-64 node-role.kubernetes.io/master=”” 这里需要将 单Master 更改为 NoSchedule 更新标签命令为 kubectl taint nodes kubernetes-64 node-role.kubernetes.io/master=:NoSchedule 既 Master 又是 node 打标签 kubectl label node kubernetes-65 node-role.kubernetes.io/master=”” 单 Node 打标签 kubectl label node kubernetes-66 node-role.kubernetes.io/node=”” 关于删除 label 可使用 - 号相连 如: kubectl label nodes kubernetes-65 node-role.kubernetes.io/node- 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ https://github.com/kubernetes/kubernetes/blob/release-1.12/pkg/kubelet/apis/config/types.go # 创建 kubelet 目录 mkdir -p /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.64\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"RotateCertificates\": true, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } # 如上配置: kubernetes-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。 \"clusterDNS\": [\"10.254.0.2\"] 可配置多个 dns地址，逗号可开, 可配置宿主机dns. ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:17","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:18","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"验证 nodes [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 17h v1.12.1 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:19","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"查看 kubelet 生成文件 [root@kubernetes-64 ~]# ls -lt /etc/kubernetes/ssl/kubelet-* -rw------- 1 root root 1374 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem lrwxrwxrwx 1 root root 58 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-current.pem -\u003e /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem -rw-r--r-- 1 root root 1050 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.crt -rw------- 1 root root 227 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.key ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:13:20","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:14:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@kubernetes-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:14:1","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy* /etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:15:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.65:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:15:1","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 node 中安装 yum install ipset ipvsadm conntrack-tools.x86_64 -y yaml 配置文件中的 参数如下: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/config/types.go cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:15:2","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 # 检查 ipvs [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:15:3","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理； 在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server; node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口; 当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA; ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:16:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:16:1","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:16:2","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 systemd kubelet 配置 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ https://github.com/kubernetes/kubernetes/blob/release-1.12/pkg/kubelet/apis/config/types.go # 创建 kubelet 目录 vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.66\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } # 启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:16:3","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy 配置 Flannel 网络 公有云如 阿里云 华为云 可能无法使用 flannel 的 host-gw 模式，请使用 vxlan 或 calico 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1_A3zzurG5vV40-FnyA8uWg rpm -ivh flannel-0.10.0-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=em1\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl enable docker systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 配置 Calico 网络 官方文档 https://docs.projectcalico.org/v3.2/introduction ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:16:4","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"下载 Calico yaml # 下载 yaml 文件 wget http://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/hosted/calico.yaml wget http://docs.projectcalico.org/v3.2/getting-started/kubernetes/installation/rbac.yaml ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:17:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"下载镜像 # 下载 镜像 # 国外镜像 有墙 quay.io/calico/node:v3.2.3 quay.io/calico/cni:v3.2.3 quay.io/calico/kube-controllers:v3.2.3 # 国内镜像 jicki/node:v3.1.3 jicki/cni:v3.1.3 jicki/kube-controllers:v3.1.3 # 替换镜像 sed -i 's/quay\\.io\\/calico/jicki/g' calico.yaml ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:18:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"修改配置 vi calico.yaml # 注意修改如下选项: # etcd 地址 etcd_endpoints: \"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # etcd 证书路径 # If you're using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: \"/calico-secrets/etcd-ca\" etcd_cert: \"/calico-secrets/etcd-cert\" etcd_key: \"/calico-secrets/etcd-key\" # etcd 证书 base64 地址 (执行里面的命令生成的证书 base64 码，填入里面) data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d '\\n') etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d '\\n') etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d '\\n') # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: \"10.254.64.0/18\" # 导入 yaml 文件 [root@kubernetes-64 ~]# kubectl apply -f . configmap/calico-config created secret/calico-etcd-secrets created daemonset.extensions/calico-node created deployment.extensions/calico-kube-controllers created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created # 查看服务 [root@kubernetes-64 ~]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-79cfd7887-nbgdv 1/1 Running 0 1m calico-node-9mkrt 2/2 Running 0 1m calico-node-dzf4c 2/2 Running 0 1m calico-node-gdxnn 2/2 Running 0 1m ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:19:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"修改 kubelet 配置 # kubelet 需要增加 cni 插件 --network-plugin=cni vi /etc/systemd/system/kubelet.service --network-plugin=cni \\ # 重新加载配置 systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:20:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"检查网络 # 查看 node 中网络状况 [root@kubernetes-64 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.95.64 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 2 bytes 168 (168.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 168 (168.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@kubernetes-65 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.116.128 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@kubernetes-66 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.70.64 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:21:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"安装 calicoctl calicoctl 是 calico 网络的管理客户端, 只需要在一台 node 里配置既可。 # 下载 二进制文件 curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.2.3/calicoctl mv calicoctl /usr/local/bin/ chmod +x /usr/local/bin/calicoctl # 创建 calicoctl.cfg 配置文件 mkdir /etc/calico vi /etc/calico/calicoctl.cfg apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"kubernetes\" kubeconfig: \"/root/.kube/config\" # 查看 calico 状态 [root@kubernetes-64 ~]# calicoctl node status Calico process is running. IPv4 BGP status +----------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +----------------+-------------------+-------+----------+-------------+ | 172.16.1.65 | node-to-node mesh | up | 01:10:35 | Established | | 172.16.1.66 | node-to-node mesh | up | 01:10:35 | Established | +----------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. [root@kubernetes-64 ~]# calicoctl get node NAME kubernetes-64 kubernetes-65 kubernetes-66 测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 kubernetes-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@kubernetes-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 网络的节点 里 curl [root@kubernetes-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:22:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml 1.2.x 版本中 Corefile 部分更新了点东西，使用如下替换整个 Corefile 部分 # vi coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/18 { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 } ... clusterIP: 10.254.0.2 # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:23:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@kubernetes-64 coredns]# kubectl apply -f coredns.yaml serviceaccount/coredns created clusterrole.rbac.authorization.k8s.io/system:coredns created clusterrolebinding.rbac.authorization.k8s.io/system:coredns created configmap/coredns created deployment.extensions/coredns created service/kube-dns created ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:24:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"查看 coredns 服务 [root@kubernetes-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE pod/coredns-6975654877-nzhgr 1/1 Running 0 23s pod/coredns-6975654877-qn4bp 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 23s ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:25:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"检查日志 [root@kubernetes-64 coredns]# kubectl logs -n kube-system pod/coredns-6975654877-nzhgr .:53 2018/08/09 02:11:11 [INFO] CoreDNS-1.2.0 2018/08/09 02:11:11 [INFO] linux/amd64, go1.10.3, 2e322f6 CoreDNS-1.2.0 linux/amd64, go1.10.3, 2e322f6 2018/08/09 02:11:11 [INFO] plugin/reload: Running configuration MD5 = 271feea1e1cf54e66a65c7ffcf2b89ad ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:26:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sleep - \"3600\" # 查看 创建的服务 [root@kubernetes-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@kubernetes-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@kubernetes-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:27:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"部署 DNS 自动伸缩 按照 node 数量 自动伸缩 dns 数量 vi dns-auto-scaling.yaml kind: ServiceAccount apiVersion: v1 metadata: name: kube-dns-autoscaler namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\"] - apiGroups: [\"\"] resources: [\"replicationcontrollers/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"extensions\"] resources: [\"deployments/scale\", \"replicasets/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"create\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-cluster-critical containers: - name: autoscaler image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 resources: requests: cpu: \"20m\" memory: \"10Mi\" command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kube-dns-autoscaler - --target=Deployment/coredns - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}} - --logtostderr=true - --v=2 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" serviceAccountName: kube-dns-autoscaler # 导入文件 [root@kubernetes-64 coredns]# kubectl apply -f dns-auto-scaling.yaml serviceaccount/kube-dns-autoscaler created clusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler created deployment.apps/kube-dns-autoscaler created 部署 Ingress 与 Dashboard ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:28:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"部署 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard 官方 heapster 的github https://github.com/kubernetes/heapster ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:29:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"下载 heapster 相关 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:29:1","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"下载 heapster 镜像下载 # 官方镜像 k8s.gcr.io/heapster-grafana-amd64:v4.4.3 k8s.gcr.io/heapster-amd64:v1.5.3 k8s.gcr.io/heapster-influxdb-amd64:v1.3.3 # 个人的镜像 jicki/heapster-grafana-amd64:v4.4.3 jicki/heapster-amd64:v1.5.3 jicki/heapster-influxdb-amd64:v1.3.3 # 替换所有yaml 镜像地址 sed -i 's/k8s\\.gcr\\.io/jicki/g' *.yaml ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:29:2","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"修改 yaml 文件 # heapster.yaml 文件 #### 修改如下部分 ##### 因为 kubelet 启用了 https 所以如下配置需要增加 https 端口 - --source=kubernetes:https://kubernetes.default 修改为 - --source=kubernetes:https://kubernetes.default?kubeletHttps=true\u0026kubeletPort=10250\u0026insecure=true # heapster-rbac.yaml 文件 #### 修改为部分 ##### 将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限； kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: heapster roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:heapster subjects: - kind: ServiceAccount name: heapster namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: heapster-kubelet-api roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-admin subjects: - kind: ServiceAccount name: heapster namespace: kube-system # 导入所有的文件 [root@kubernetes-64 heapster]# kubectl apply -f . deployment.extensions/monitoring-grafana created service/monitoring-grafana created clusterrolebinding.rbac.authorization.k8s.io/heapster created serviceaccount/heapster created deployment.extensions/heapster created service/heapster created deployment.extensions/monitoring-influxdb created service/monitoring-influxdb created # 查看运行 [root@kubernetes-64 heapster]# kubectl get pods -n kube-system | grep -E 'heapster|monitoring' heapster-545d9555d4-lm5fs 1/1 Running 0 1m monitoring-grafana-59b4f6d8b7-ft2gv 1/1 Running 0 1m monitoring-influxdb-f6bcc9795-9zjnl 1/1 Running 0 1m ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:29:3","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"部署 dashboard ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:30:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0 # 个人的镜像 jicki/kubernetes-dashboard-amd64:v1.10.0 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:30:1","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:30:2","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/k8s\\.gcr\\.io/jicki/g' * # 导入文件 [root@kubernetes-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@kubernetes-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-5984fb8cbb-77dl4 1/1 Running 0 3h po/coredns-5984fb8cbb-9hdwt 1/1 Running 0 3h po/kubernetes-dashboard-78bcdc4d64-x6fhq 1/1 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 3h svc/kubernetes-dashboard ClusterIP 10.254.18.143 \u003cnone\u003e 443/TCP 14s ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:30:3","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:31:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@kubernetes-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 1d v1.12.1 kubernetes-65 Ready master 1d v1.12.1 kubernetes-66 Ready node 1d v1.12.1 # 对 65 与 66 打上 label [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-65 ingress=proxy node \"kubernetes-65\" labeled [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-66 ingress=proxy node \"kubernetes-66\" labeled # 打完标签以后 [root@kubernetes-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 32m v1.12.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubernetes-64 kubernetes-65 Ready \u003cnone\u003e 17m v1.12.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-65 kubernetes-66 Ready \u003cnone\u003e 4m v1.12.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-66 # 下载镜像 # 官方镜像 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.20.0 # 国内镜像 jicki/nginx-ingress-controller:0.20.0 # 下载 yaml 文件 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务 # tcp 例子 apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 9000: \"default/tomcat:8080\" # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中 # udp 例子 apiVersion: v1 kind: ConfigMap metadata: name: udp-services namespace: ingress-nginx data: 53: \"kube-system/kube-dns:53\" # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi with-rbac.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 # 导入 yaml 文件 [root@kubernetes-64 ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@kubernetes-64 nginx-ingress]# kubectl apply -f . configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created namespace \"ingress-nginx\" configured serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 66 中 [root@kubernetes-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-ingress-controller-8476958f94-8fh5h 1/1 Running 0 5m 172.16.1.66 kubernetes-66 nginx-ingress-controller-8476958f94-qfhhp 1/1 Running 0 5m 172.16.1.65 kubernetes-65 # 查看我们原有的 svc [root@kubernetes-64 ingress]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 24m nginx-dm-84f8f49555-tmqzm 1/1 Running 0 24m nginx-dm-84f8f49555-wdk67 1/1 Running 0 24m # 创建一个 基于 nginx-dm 的 ingress vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:32:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"部署 monitoring k8s 运维相关 ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:33:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:34:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2018-10-09","objectID":"/kubernetes-1.12.1/:35:0","tags":["kubernetes","docker"],"title":"kubernetes 1.12.1","uri":"/kubernetes-1.12.1/"},{"categories":["kubernetes"],"content":"kubernetes 1.11.2","date":"2018-08-10","objectID":"/kubernetes-1.11.2/","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":" 1.11.2 更新元数据的一些bug 以及 优化日志, 增加 calico 网络配置, 1.11.1 修复了一些BUG 其中包括 pod 抢占 导致循环错误， kubectl delete 等待依赖删除时间等待长，改为后台。 kubernetes 1.11.2 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:0:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"Urgent Upgrade Notes ACTION REQUIRED: Removes defaulting of CSI file system type to ext4. All the production drivers listed under https://kubernetes-csi.github.io/docs/Drivers.html were inspected and should not be impacted after this change. If you are using a driver not in that list, please test the drivers on an updated test cluster first. kube-apiserver: the Priority admission plugin is now enabled by default when using –enable-admission-plugins. If using –admission-control to fully specify the set of admission plugins, the Priority admission plugin should be added if using the PodPriority feature, which is enabled by default in 1.11. The system-node-critical and system-cluster-critical priority classes are now limited to the kube-system namespace by the PodPriority admission plugin. ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:1:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"环境说明 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node, node-66 只做单纯 Node kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:2:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.64 kubernetes-64 172.16.1.65 kubernetes-65 172.16.1.66 kubernetes-66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:3:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:4:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:5:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@kubernetes-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:6:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm rpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm yum -y install docker-ce-17.03.2.ce # 查看安装 docker version Client: Version: 17.03.2-ce API version: 1.27 Go version: go1.7.5 Git commit: f5ec1e2 Built: Tue Jun 27 02:21:36 2017 OS/Arch: linux/amd64 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:7:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) # docker 版本 17.03.2 之前配置为 --graph=/opt/docker # docker 版本 17.04.x 之后配置为 --data-root=/opt/docker [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.11.2 etcd 支持最新版本为 v3.2.18 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:8:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz tar zxvf etcd-v3.2.18-linux-amd64.tar.gz cd etcd-v3.2.18-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:9:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@kubernetes-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 检查证书 [root@kubernetes-64 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:10:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:11:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:12:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:13:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.11.2/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:1","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@kubernetes-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:2","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"生成 kubernetes 配置文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:3","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:4","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@kubernetes-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:5","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 40179b02a8f6da07d90392ae966f7749 # 创建 encryption-config.yaml 配置 cat \u003e encryption-config.yaml \u003c\u003cEOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 40179b02a8f6da07d90392ae966f7749 - identity: {} EOF # 拷贝 cp encryption-config.yaml /etc/kubernetes/ scp encryption-config.yaml 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 \u003e 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ \u003e \u003e 如下为最低限度的日志审核 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:6","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --anonymous-auth=false \\ --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # --experimental-encryption-provider-config ，替代之前 token.csv 文件 # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:7","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver # 如果报错 请使用 journalctl -f -t kube-apiserver 和 journalctl -u kube-apiserver 来定位问题 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:8","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager 新增几个配置，用于自动 续期证书 –feature-gates=RotateKubeletServerCertificate=true –experimental-cluster-signing-duration=86700h0m0s # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --feature-gates=RotateKubeletServerCertificate=true \\ --controllers=*,tokencleaner,bootstrapsigner \\ --experimental-cluster-signing-duration=86700h0m0s \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=5m0s \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:9","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager # 如果报错 请使用 journalctl -f -t kube-controller-manager 和 journalctl -u kube-controller-manager 来定位问题 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:10","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:11","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:12","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@kubernetes-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@kubernetes-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:13","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 kubelet 认证 kubelet 授权 kube-apiserver 的一些操作 exec run logs 等 # RBAC 只需创建一次就可以 kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:14","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 bootstrap kubeconfig 文件 注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token # 创建 集群所有 kubelet 的 token [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-64 --kubeconfig ~/.kube/config I0705 14:42:22.587674 90997 feature_gate.go:230] feature gates: \u0026{map[]} 1jezb7.izm7refwnj3umncy [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-65 --kubeconfig ~/.kube/config I0705 14:42:30.553287 91021 feature_gate.go:230] feature gates: \u0026{map[]} 1ua4d4.9bluufy3esw4lch6 [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-66 --kubeconfig ~/.kube/config I0705 14:42:35.681003 91047 feature_gate.go:230] feature gates: \u0026{map[]} r8llj2.itme3y54ok531ops # 查看生成的 token [root@kubernetes-64 kubernetes]# kubeadm token list --kubeconfig ~/.kube/config TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 1jezb7.izm7refwnj3umncy 23h 2018-07-06T14:42:22+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-64 1ua4d4.9bluufy3esw4lch6 23h 2018-07-06T14:42:30+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-65 r8llj2.itme3y54ok531ops 23h 2018-07-06T14:42:35+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-66 以下为了区分 会先生成 node 名称加 bootstrap.kubeconfig 生成 kubernetes-64 # 生成 64 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=1jezb7.izm7refwnj3umncy \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 拷贝生成的 kubernetes-64-bootstrap.kubeconfig 文件 mv kubernetes-64-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-65 # 生成 65 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=1ua4d4.9bluufy3esw4lch6 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 拷贝生成的 kubernetes-65-bootstrap.kubeconfig 文件 scp kubernetes-65-bootstrap.kubeconfig 172.16.1.65:/etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-66 # 生成 66 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=r8llj2.itme3y54ok531ops \\ --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 拷贝生成的 kubernetes-66-bootstrap.kubeconfig 文件 scp kubernetes-66-bootstrap.kubeconfig 172.16.1.66:/etc/kubernetes/bootstrap.kubeconfig # 配置 bootstrap RBAC 权限 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers # 否则报如下错误 failed to run Kubelet: cannot create certificate signing request: certificatesigningreques","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:15","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建自动批准相关 CSR 请求的 ClusterRole vi /etc/kubernetes/tls-instructs-csr.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"] # 导入 yaml 文件 [root@kubernetes-64 opt]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml clusterrole.rbac.authorization.k8s.io \"system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\" created # 查看 [root@kubernetes-64 opt]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"name\":\"system:certificates.k8s.io:certificatesigningreq... PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] # 将 ClusterRole 绑定到适当的用户组 # 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers # 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes # 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求 kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:16","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 关于 kubectl get node 中的 ROLES 的标签 单 Master 打标签 kubectl label node kubernetes-64 \\ node-role.kubernetes.io/master=”” 这里需要将 单Master 更改为 NoSchedule 更新标签命令为 kubectl taint nodes kubernetes-64 node-role.kubernetes.io/master=:NoSchedule 既 Master 又是 node 打标签 kubectl label node kubernetes-65 node-role.kubernetes.io/master=”” 单 Node 打标签 kubectl label node kubernetes-66 node-role.kubernetes.io/node=”” 关于删除 label 可使用 - 号相连 如: kubectl label nodes kubernetes-65 node-role.kubernetes.io/node- 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 目前官方还只是 beta 阶段, 动态配置 json 的具体参数可以参考 https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go # 创建 kubelet 目录 mkdir -p /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.64\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"RotateCertificates\": true, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } # 如上配置: kubernetes-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。 \"clusterDNS\": [\"10.254.0.2\"] 可配置多个 dns地址，逗号可开, 可配置宿主机dns. ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:17","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:18","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"验证 nodes [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 17h v1.11.2 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:19","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"查看 kubelet 生成文件 [root@kubernetes-64 ~]# ls -lt /etc/kubernetes/ssl/kubelet-* -rw------- 1 root root 1374 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem lrwxrwxrwx 1 root root 58 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-current.pem -\u003e /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem -rw-r--r-- 1 root root 1050 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.crt -rw------- 1 root root 227 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.key ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:14:20","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:15:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@kubernetes-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:15:1","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 scp kube-proxy* 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:16:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.65:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:16:1","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 node 中安装 yum install ipset ipvsadm conntrack-tools.x86_64 -y yaml 配置文件中的 参数如下: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:16:2","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 # 检查 ipvs [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:16:3","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理； 在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server; node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口; 当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA; ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:17:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:17:1","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:17:2","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 systemd kubelet 配置 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 目前官方还只是 beta 阶段, 动态配置 json 的具体参数可以参考 https://github.com/kubernetes/kubernetes/blob/release-1.11/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go # 创建 kubelet 目录 vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.66\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } # 启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:17:3","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl start kube-proxy systemctl status kube-proxy 配置 Flannel 网络 公有云如 阿里云 华为云 可能无法使用 flannel 的 host-gw 模式，请使用 vxlan 或 calico 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1_A3zzurG5vV40-FnyA8uWg rpm -ivh flannel-0.10.0-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=em1\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl enable docker systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 配置 Calico 网络 官方文档 https://docs.projectcalico.org/v3.1/introduction ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:17:4","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"下载 Calico yaml # 下载 yaml 文件 wget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/calico.yaml wget http://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/rbac.yaml ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:18:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"下载镜像 # 下载 镜像 # 国外镜像 有墙 quay.io/calico/node:v3.1.3 quay.io/calico/cni:v3.1.3 quay.io/calico/kube-controllers:v3.1.3 # 国内镜像 jicki/node:v3.1.3 jicki/cni:v3.1.3 jicki/kube-controllers:v3.1.3 # 替换镜像 sed -i 's/quay\\.io\\/calico/jicki/g' calico.yaml ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:19:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"修改配置 vi calico.yaml # 注意修改如下选项: # etcd 地址 etcd_endpoints: \"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # etcd 证书路径 # If you're using TLS enabled etcd uncomment the following. # You must also populate the Secret below with these files. etcd_ca: \"/calico-secrets/etcd-ca\" etcd_cert: \"/calico-secrets/etcd-cert\" etcd_key: \"/calico-secrets/etcd-key\" # etcd 证书 base64 地址 (执行里面的命令生成的证书 base64 码，填入里面) data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d '\\n') etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d '\\n') etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d '\\n') # 修改 pods 分配的 IP 段 - name: CALICO_IPV4POOL_CIDR value: \"10.254.64.0/18\" # 导入 yaml 文件 [root@kubernetes-64 ~]# kubectl apply -f . configmap/calico-config created secret/calico-etcd-secrets created daemonset.extensions/calico-node created deployment.extensions/calico-kube-controllers created serviceaccount/calico-kube-controllers created serviceaccount/calico-node created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-node created clusterrolebinding.rbac.authorization.k8s.io/calico-node created # 查看服务 [root@kubernetes-64 ~]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-79cfd7887-nbgdv 1/1 Running 0 1m calico-node-9mkrt 2/2 Running 0 1m calico-node-dzf4c 2/2 Running 0 1m calico-node-gdxnn 2/2 Running 0 1m ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:20:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"修改 kubelet 配置 # kubelet 需要增加 cni 插件 --network-plugin=cni vi /etc/systemd/system/kubelet.service --network-plugin=cni \\ # 重新加载配置 systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:21:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"检查网络 # 查看 node 中网络状况 [root@kubernetes-64 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.95.64 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 2 bytes 168 (168.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 2 bytes 168 (168.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@kubernetes-65 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.116.128 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 [root@kubernetes-66 ~]# ifconfig tunl0: flags=193\u003cUP,RUNNING,NOARP\u003e mtu 1440 inet 10.254.70.64 netmask 255.255.255.255 tunnel txqueuelen 1 (IPIP Tunnel) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:22:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"安装 calicoctl calicoctl 是 calico 网络的管理客户端, 只需要在一台 node 里配置既可。 # 下载 二进制文件 curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.1.3/calicoctl mv calicoctl /usr/local/bin/ chmod +x /usr/local/bin/calicoctl # 创建 calicoctl.cfg 配置文件 mkdir /etc/calico vi /etc/calico/calicoctl.cfg apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"kubernetes\" kubeconfig: \"/root/.kube/config\" # 查看 calico 状态 [root@kubernetes-64 ~]# calicoctl node status Calico process is running. IPv4 BGP status +----------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +----------------+-------------------+-------+----------+-------------+ | 172.16.1.65 | node-to-node mesh | up | 01:10:35 | Established | | 172.16.1.66 | node-to-node mesh | up | 01:10:35 | Established | +----------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. [root@kubernetes-64 ~]# calicoctl get node NAME kubernetes-64 kubernetes-65 kubernetes-66 测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 kubernetes-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@kubernetes-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 网络的节点 里 curl [root@kubernetes-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:23:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml # vi coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/18 { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 } ... clusterIP: 10.254.0.2 # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:24:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@kubernetes-64 coredns]# kubectl apply -f coredns.yaml serviceaccount/coredns created clusterrole.rbac.authorization.k8s.io/system:coredns created clusterrolebinding.rbac.authorization.k8s.io/system:coredns created configmap/coredns created deployment.extensions/coredns created service/kube-dns created ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:25:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"查看 coredns 服务 [root@kubernetes-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE pod/coredns-6975654877-nzhgr 1/1 Running 0 23s pod/coredns-6975654877-qn4bp 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 23s ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:26:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"检查日志 [root@kubernetes-64 coredns]# kubectl logs -n kube-system pod/coredns-6975654877-nzhgr .:53 2018/08/09 02:11:11 [INFO] CoreDNS-1.2.0 2018/08/09 02:11:11 [INFO] linux/amd64, go1.10.3, 2e322f6 CoreDNS-1.2.0 linux/amd64, go1.10.3, 2e322f6 2018/08/09 02:11:11 [INFO] plugin/reload: Running configuration MD5 = 271feea1e1cf54e66a65c7ffcf2b89ad ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:27:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sleep - \"3600\" # 查看 创建的服务 [root@kubernetes-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@kubernetes-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@kubernetes-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:28:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"部署 DNS 自动伸缩 按照 node 数量 自动伸缩 dns 数量 vi dns-auto-scaling.yaml kind: ServiceAccount apiVersion: v1 metadata: name: kube-dns-autoscaler namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\"] - apiGroups: [\"\"] resources: [\"replicationcontrollers/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"extensions\"] resources: [\"deployments/scale\", \"replicasets/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"create\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-cluster-critical containers: - name: autoscaler image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 resources: requests: cpu: \"20m\" memory: \"10Mi\" command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kube-dns-autoscaler - --target=Deployment/coredns - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}} - --logtostderr=true - --v=2 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" serviceAccountName: kube-dns-autoscaler # 导入文件 [root@kubernetes-64 coredns]# kubectl apply -f dns-auto-scaling.yaml serviceaccount/kube-dns-autoscaler created clusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler created deployment.apps/kube-dns-autoscaler created 部署 Ingress 与 Dashboard ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:29:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"部署 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard 官方 heapster 的github https://github.com/kubernetes/heapster ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:30:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"下载 heapster 相关 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:30:1","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"下载 heapster 镜像下载 # 官方镜像 k8s.gcr.io/heapster-grafana-amd64:v4.4.3 k8s.gcr.io/heapster-amd64:v1.5.3 k8s.gcr.io/heapster-influxdb-amd64:v1.3.3 # 个人的镜像 jicki/heapster-grafana-amd64:v4.4.3 jicki/heapster-amd64:v1.5.3 jicki/heapster-influxdb-amd64:v1.3.3 # 替换所有yaml 镜像地址 sed -i 's/k8s\\.gcr\\.io/jicki/g' *.yaml ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:30:2","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"修改 yaml 文件 # heapster.yaml 文件 #### 修改如下部分 ##### 因为 kubelet 启用了 https 所以如下配置需要增加 https 端口 - --source=kubernetes:https://kubernetes.default 修改为 - --source=kubernetes:https://kubernetes.default?kubeletHttps=true\u0026kubeletPort=10250\u0026insecure=true # heapster-rbac.yaml 文件 #### 修改为部分 ##### 将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限； kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: heapster roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:heapster subjects: - kind: ServiceAccount name: heapster namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: heapster-kubelet-api roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-admin subjects: - kind: ServiceAccount name: heapster namespace: kube-system # 导入所有的文件 [root@kubernetes-64 heapster]# kubectl apply -f . deployment.extensions/monitoring-grafana created service/monitoring-grafana created clusterrolebinding.rbac.authorization.k8s.io/heapster created serviceaccount/heapster created deployment.extensions/heapster created service/heapster created deployment.extensions/monitoring-influxdb created service/monitoring-influxdb created # 查看运行 [root@kubernetes-64 heapster]# kubectl get pods -n kube-system | grep -E 'heapster|monitoring' heapster-545d9555d4-lm5fs 1/1 Running 0 1m monitoring-grafana-59b4f6d8b7-ft2gv 1/1 Running 0 1m monitoring-influxdb-f6bcc9795-9zjnl 1/1 Running 0 1m ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:30:3","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"部署 dashboard ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:31:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3 # 个人的镜像 jicki/kubernetes-dashboard-amd64:v1.8.3 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:31:1","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:31:2","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/k8s\\.gcr\\.io/jicki/g' * # 导入文件 [root@kubernetes-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@kubernetes-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-5984fb8cbb-77dl4 1/1 Running 0 3h po/coredns-5984fb8cbb-9hdwt 1/1 Running 0 3h po/kubernetes-dashboard-78bcdc4d64-x6fhq 1/1 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 3h svc/kubernetes-dashboard ClusterIP 10.254.18.143 \u003cnone\u003e 443/TCP 14s ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:31:3","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:32:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@kubernetes-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 1d v1.11.2 kubernetes-65 Ready master 1d v1.11.2 kubernetes-66 Ready node 1d v1.11.2 # 对 65 与 66 打上 label [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-65 ingress=proxy node \"kubernetes-65\" labeled [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-66 ingress=proxy node \"kubernetes-66\" labeled # 打完标签以后 [root@kubernetes-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 32m v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubernetes-64 kubernetes-65 Ready \u003cnone\u003e 17m v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-65 kubernetes-66 Ready \u003cnone\u003e 4m v1.11.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-66 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.4 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.16.2 # 国内镜像 jicki/defaultbackend:1.4 jicki/nginx-ingress-controller:0.16.2 # 下载 yaml 文件 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务 # tcp 例子 apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 9000: \"default/tomcat:8080\" # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中 # udp 例子 apiVersion: v1 kind: ConfigMap metadata: name: udp-services namespace: ingress-nginx data: 53: \"kube-system/kube-dns:53\" # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi with-rbac.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 # 导入 yaml 文件 [root@kubernetes-64 ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@kubernetes-64 nginx-ingress]# kubectl apply -f . configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created namespace \"ingress-nginx\" configured serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 66 中 [root@kubernetes-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE default","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:33:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"部署 monitoring k8s 运维相关 ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:34:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:35:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2018-08-10","objectID":"/kubernetes-1.11.2/:36:0","tags":null,"title":"kubernetes 1.11.2","uri":"/kubernetes-1.11.2/"},{"categories":["fabric"],"content":"hyperledger-fabric v 1.2 多机多节点","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":" fabric v1.2 , 多机，多节点，多 Orderer ，生产环境配置。 部署 hyperledger-fabric v1.2 ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:0:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"环境规划 相关hostname 必须配置 dns 关于 orderer 集群 当orderer 向peer节点提交Transaction的时候，peer节点会得到或返回一个读写集结果，该结果会发送给orderer节点进行共识和排序，此时如果orderer节点突然down掉，就会使请求服务失效而引发的数据丢失等问题，且目前的sdk对orderer发送的Transaction的回调会占用极长的时间，当大批量数据导入的时候该回调可认为不可用。 节点标识 hostname IP 开放端口 系统 orderer0节点 orderer0.jicki.cn 192.168.100.67 7050 CentOS 7 x64 orderer1节点 orderer1.jicki.cn 192.168.100.15 7050 CentOS 7 x64 orderer2节点 orderer2.jicki.cn 192.168.100.91 7050 CentOS 7 x64 peer0节点 peer0.org1.jicki.cn 192.168.100.15 7051, 7052, 7053 CentOS 7 x64 peer0节点 peer0.org2.jicki.cn 192.168.100.91 7051, 7052, 7053 CentOS 7 x64 zk0节点 zookeeper0 192.168.100.67 2181 CentOS 7 x64 zk1节点 zookeeper1 192.168.100.15 2181 CentOS 7 x64 zk2节点 zookeeper2 192.168.100.91 2181 CentOS 7 x64 kafka0节点 kafka0 192.168.100.67 9092 CentOS 7 x64 kafka1节点 kafka1 192.168.100.15 9092 CentOS 7 x64 kafka2节点 kafka2 192.168.100.91 9092 CentOS 7 x64 所有节点 docker 互为单机，不做网络集群，连接使用 hostname or IP 所有需要配置 hosts 域名 # 所有节点均需要配置 vi /etc/hosts # fabric hostname 192.168.100.67 orderer0.jicki.cn 192.168.100.15 orderer1.jicki.cn 192.168.100.91 orderer2.jicki.cn 192.168.100.15 peer0.org1.jicki.cn 192.168.100.91 peer0.org2.jicki.cn 192.168.100.67 zookeeper0 192.168.100.15 zookeeper1 192.168.100.91 zookeeper2 192.168.100.67 kafka0 192.168.100.15 kafka1 192.168.100.91 kafka2 # fabric hostname ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:1:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"官方地址 文档以官方文档为主 http://hyperledger-fabric.readthedocs.io/en/release-1.2/prereqs.html # 官网 github https://github.com/hyperledger/fabric ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:2:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"环境准备 所有机器 安装 Docker (用于 fabric 服务启动) # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装 docker yum -y install docker-ce # 启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 查看 docker 版本 docker version Client: Version: 18.06.0-ce API version: 1.38 Go version: go1.10.3 Git commit: 0ffa825 Built: Wed Jul 18 19:08:18 2018 OS/Arch: linux/amd64 Experimental: false Server: Engine: Version: 18.06.0-ce API version: 1.38 (minimum version 1.12) Go version: go1.10.3 Git commit: 0ffa825 Built: Wed Jul 18 19:10:42 2018 OS/Arch: linux/amd64 Experimental: false 安装 Docker-compose (用于 docker 容器服务统一管理 编排) # 安装 pip curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py # 安装 docker-compose pip install docker-compose --ignore-installed requests docker-compose version docker-compose version 1.22.0, build f46880f docker-py version: 3.4.1 CPython version: 2.7.5 OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013 Golang (用于 fabric cli 服务的调用， ca 服务证书生成 ) mkdir -p /opt/golang mkdir -p /opt/gopath # 国外地址 wget https://storage.googleapis.com/golang/go1.10.linux-amd64.tar.gz # 国内地址 wget https://studygolang.com/dl/golang/go1.10.linux-amd64.tar.gz # 解压 tar zxvf go1.10.linux-amd64.tar.gz # 配置环境变量 vi /etc/profile 添加如下 # golang env export PATH=$PATH:/opt/golang/go/bin export GOPATH=/opt/gopath # 生效配置 source /etc/profile # 查看配置 go version go version go1.10 linux/amd64 ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:3:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"Hyperledger Fabric 源码 fabric 源码用于 cli 智能合约安装时的依赖, 这里只用于第一个节点 # 下载 Fabric 源码, 源码中 import 的路径为github.com/hyperledger/fabric ,所以我们要按照这个路径 mkdir -p /opt/gopath/src/github.com/hyperledger cd /opt/gopath/src/github.com/hyperledger git clone https://github.com/hyperledger/fabric mkdir -p /opt/gopath/src cp -r github.com /opt/gopath/src # 文件如下: [root@localhost fabric]# ls -lt 总用量 580 drwxr-xr-x 7 root root 138 6月 5 09:29 vendor -rw-r--r-- 1 root root 301 6月 5 09:29 tox.ini drwxr-xr-x 2 root root 56 6月 5 09:29 unit-test drwxr-xr-x 7 root root 134 6月 5 09:29 test drwxr-xr-x 2 root root 4096 6月 5 09:29 scripts -rw-r--r-- 1 root root 316 6月 5 09:29 settings.gradle drwxr-xr-x 3 root root 91 6月 5 09:29 sampleconfig drwxr-xr-x 2 root root 4096 6月 5 09:29 release_notes drwxr-xr-x 11 root root 4096 6月 5 09:29 protos drwxr-xr-x 3 root root 30 6月 5 09:29 release drwxr-xr-x 9 root root 154 6月 5 09:29 peer drwxr-xr-x 3 root root 23 6月 5 09:29 proposals drwxr-xr-x 6 root root 126 6月 5 09:29 orderer drwxr-xr-x 6 root root 4096 6月 5 09:29 msp drwxr-xr-x 9 root root 130 6月 5 09:29 images drwxr-xr-x 2 root root 29 6月 5 09:29 gotools drwxr-xr-x 2 root root 4096 6月 5 09:29 idemix drwxr-xr-x 15 root root 4096 6月 5 09:29 gossip drwxr-xr-x 10 root root 4096 6月 5 09:29 examples drwxr-xr-x 4 root root 48 6月 5 09:29 events drwxr-xr-x 4 root root 137 6月 5 09:29 docs drwxr-xr-x 4 root root 4096 6月 5 09:29 devenv -rw-r--r-- 1 root root 3356 6月 5 09:29 docker-env.mk drwxr-xr-x 20 root root 4096 6月 5 09:29 core drwxr-xr-x 23 root root 4096 6月 5 09:29 common drwxr-xr-x 10 root root 4096 6月 5 09:29 bddtests -rw-r--r-- 1 root root 13 6月 5 09:29 ci.properties drwxr-xr-x 8 root root 4096 6月 5 09:29 bccsp -rw-r--r-- 1 root root 11358 6月 5 09:29 LICENSE -rwxr-xr-x 1 root root 17068 6月 5 09:29 Makefile -rw-r--r-- 1 root root 5678 6月 5 09:29 README.md -rw-r--r-- 1 root root 475471 6月 5 09:29 CHANGELOG.md -rw-r--r-- 1 root root 597 6月 5 09:29 CODE_OF_CONDUCT.md -rw-r--r-- 1 root root 664 6月 5 09:29 CONTRIBUTING.md ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:4:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"生成 Hyperledger Fabric 证书 证书生成只需要生成一次，这里只在第一个节点配置 # 下载官方证书生成软件(均为二进制文件) # 官方离线下载地址为 https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/ # 选择相应版本 CentOS 选择 linux-amd64-1.2.0 Mac 选择 darwin-amd64-1.2.0 # 下载地址为: https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/linux-amd64-1.2.0/hyperledger-fabric-linux-amd64-1.2.0.tar.gz mkdir /opt/jicki/ cd /opt/jicki wget https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/linux-amd64-1.2.0/hyperledger-fabric-linux-amd64-1.2.0.tar.gz tar zxvf hyperledger-fabric-linux-amd64-1.2.0.tar.gz # 解压后是 一个 bin 与 一个 config 目录 [root@localhost jicki]# tree bin/ bin/ ├── configtxgen ├── configtxlator ├── cryptogen ├── get-docker-images.sh ├── orderer └── peer 0 directories, 6 files # 为方便使用 我们配置一个 环境变量 vi /etc/profile # fabric env export PATH=$PATH:/opt/jicki/bin # 使文件生效 source /etc/profile # 拷贝 configtx.yaml 与 crypto-config.yaml 。 cd /opt/jicki/ cp /opt/gopath/src/github.com/hyperledger/fabric/examples/e2e_cli/crypto-config.yaml . cp /opt/gopath/src/github.com/hyperledger/fabric/examples/e2e_cli/configtx.yaml . # 这里修改相应 jicki.cn 为 jicki.cn sed -i 's/example\\.com/jicki\\.me/g' *.yaml # 编辑 crypto-config.yaml 增加 orderer 为三个如下: OrdererOrgs: - Name: Orderer Domain: jicki.cn CA: Country: CN Province: GuangDong Locality: ShenZhen Specs: - Hostname: orderer0 - Hostname: orderer1 - Hostname: orderer2 PeerOrgs: - Name: Org1 Domain: org1.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 - Name: Org2 Domain: org2.jicki.cn EnableNodeOUs: true CA: Country: CN Province: GuangDong Locality: ShenZhen Template: Count: 2 Users: Count: 1 # 然后这里使用 cryptogen 软件来生成相应的证书了 [root@localhost jicki]# cryptogen generate --config=./crypto-config.yaml org1.jicki.cn org2.jicki.cn # 生成一个 crypto-config 证书目录 [root@payment jicki]# tree crypto-config crypto-config ├── ordererOrganizations │ └── jicki.cn │ ├── ca │ │ ├── ca.jicki.cn-cert.pem │ │ └── ea7afe99d56e6c2e839113909651f8b3531a6447e4615eed208c7aa86b211ad0_sk │ ├── msp │ │ ├── admincerts │ │ │ └── Admin@jicki.cn-cert.pem │ │ ├── cacerts │ │ │ └── ca.jicki.cn-cert.pem │ │ └── tlscacerts │ │ └── tlsca.jicki.cn-cert.pem │ ├── orderers │ │ ├── orderer0.jicki.cn │ │ │ ├── msp │ │ │ │ ├── admincerts │ │ │ │ │ └── Admin@jicki.cn-cert.pem │ │ │ │ ├── cacerts │ │ │ │ │ └── ca.jicki.cn-cert.pem │ │ │ │ ├── keystore │ │ │ │ │ └── c4ff1a8f55af9e4108596a26471a2a748eeb370b00300abddac692d5c5f72f57_sk │ │ │ │ ├── signcerts │ │ │ │ │ └── orderer0.jicki.cn-cert.pem │ │ │ │ └── tlscacerts │ │ │ │ └── tlsca.jicki.cn-cert.pem │ │ │ └── tls │ │ │ ├── ca.crt │ │ │ ├── server.crt │ │ │ └── server.key │ │ ├── orderer1.jicki.cn │ │ │ ├── msp │ │ │ │ ├── admincerts │ │ │ │ │ └── Admin@jicki.cn-cert.pem │ │ │ │ ├── cacerts │ │ │ │ │ └── ca.jicki.cn-cert.pem │ │ │ │ ├── keystore │ │ │ │ │ └── f057fd116621213b9d06cf05d1b04b3b53057b4df4338ca5bb3b59e36acf074a_sk │ │ │ │ ├── signcerts │ │ │ │ │ └── orderer1.jicki.cn-cert.pem │ │ │ │ └── tlscacerts │ │ │ │ └── tlsca.jicki.cn-cert.pem │ │ │ └── tls │ │ │ ├── ca.crt │ │ │ ├── server.crt │ │ │ └── server.key │ │ └── orderer2.jicki.cn │ │ ├── msp │ │ │ ├── admincerts │ │ │ │ └── Admin@jicki.cn-cert.pem │ │ │ ├── cacerts │ │ │ │ └── ca.jicki.cn-cert.pem │ │ │ ├── keystore │ │ │ │ └── a0395ee7823af3ab3ae56fa4569051a5525324f506cd38b0d0dcaa3e484ae084_sk │ │ │ ├── signcerts │ │ │ │ └── orderer2.jicki.cn-cert.pem │ │ │ └── tlscacerts │ │ │ └── tlsca.jicki.cn-cert.pem │ │ └── tls │ │ ├── ca.crt │ │ ├── server.crt │ │ └── server.key │ ├── tlsca │ │ ├── 4164a39aaee758c85e9d4458bae4d51f0d01e58116003cd5782dbbaff68b1545_sk │ │ └── tlsca.jicki.cn-cert.pem │ └── users │ └── Admin@jicki.cn │ ├── msp │ │ ├── admincerts │ │ │ └── Admin@jicki.cn-cert.pem │ │ ├── cacerts │ │ │ └── ca.jicki.cn-cert.pem │ │ ├── keystore │ │ │ └── 30d0b23ab85ede3d4424a336d839067bf7cad70f538fcf87d2a9c","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:5:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"生成 Hyperledger Fabric 创世区块 只在第一个节点配置 # 这里使用 configtxgen 来创建 创世区块 # 首先需要创建一个文件夹 mkdir -p /opt/jicki/channel-artifacts # 修改 configtx.yaml 文件增加 orderer 多节点 以及 peer 多节点, 删除了 \u0026Org3 节点 # 完整 configtx.yaml 如下: # configtx.yaml 文件格式 请千万注意 空格 与 tab 键 里的缩进，否则会报错。 Organizations: - \u0026OrdererOrg Name: OrdererOrg ID: OrdererMSP MSPDir: crypto-config/ordererOrganizations/jicki.cn/msp Policies: Readers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Writers: Type: Signature Rule: \"OR('OrdererMSP.member')\" Admins: Type: Signature Rule: \"OR('OrdererMSP.admin')\" - \u0026Org1 Name: Org1MSP ID: Org1MSP MSPDir: crypto-config/peerOrganizations/org1.jicki.cn/msp AnchorPeers: - Host: peer0.org1.jicki.cn Port: 7051 Policies: Readers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.peer', 'Org1MSP.client')\" Writers: Type: Signature Rule: \"OR('Org1MSP.admin', 'Org1MSP.client')\" Admins: Type: Signature Rule: \"OR('Org1MSP.admin')\" - \u0026Org2 Name: Org2MSP ID: Org2MSP MSPDir: crypto-config/peerOrganizations/org2.jicki.cn/msp AnchorPeers: - Host: peer0.org2.jicki.cn Port: 7051 Policies: Readers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.peer', 'Org2MSP.client')\" Writers: Type: Signature Rule: \"OR('Org2MSP.admin', 'Org2MSP.client')\" Admins: Type: Signature Rule: \"OR('Org2MSP.admin')\" Capabilities: Global: \u0026ChannelCapabilities V1_1: true Orderer: \u0026OrdererCapabilities V1_1: true Application: \u0026ApplicationCapabilities V1_2: true Application: \u0026ApplicationDefaults Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ApplicationCapabilities Orderer: \u0026OrdererDefaults OrdererType: kafka Addresses: - orderer0.jicki.cn:7050 - orderer1.jicki.cn:7050 - orderer2.jicki.cn:7050 BatchTimeout: 2s BatchSize: MaxMessageCount: 10 # 设置最大的区块大小。每个区块最大有Orderer.AbsoluteMaxBytes个字节 # 假定这里设置的值为 99MB，记住这个值，这会影响怎样配置Kafka代理。 AbsoluteMaxBytes: 99 MB # 设置每个区块建议的大小。Kafka对于相对小的消息提供更高的吞吐量。 # 区块大小最好不要超过1MB。 PreferredMaxBytes: 512 KB Kafka: Brokers: # 这里如果非容器内建议使用 [ip:port]配置kafka 集群。 # 如下为 单机版本集群内的配置。 # kafka 集群最少为3节点, 建议配置4个以上。 - kafka0:9092 - kafka1:9092 - kafka2:9092 Organizations: Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" BlockValidation: Type: ImplicitMeta Rule: \"ANY Writers\" Capabilities: \u003c\u003c: *OrdererCapabilities Channel: \u0026ChannelDefaults Policies: Readers: Type: ImplicitMeta Rule: \"ANY Readers\" Writers: Type: ImplicitMeta Rule: \"ANY Writers\" Admins: Type: ImplicitMeta Rule: \"MAJORITY Admins\" Capabilities: \u003c\u003c: *ChannelCapabilities Profiles: TwoOrgsOrdererGenesis: \u003c\u003c: *ChannelDefaults Orderer: \u003c\u003c: *OrdererDefaults Organizations: - *OrdererOrg Consortiums: SampleConsortium: Organizations: - *Org1 - *Org2 TwoOrgsChannel: Consortium: SampleConsortium Application: \u003c\u003c: *ApplicationDefaults Organizations: - *Org1 - *Org2 # 创建 创世区块 TwoOrgsOrdererGenesis 名称为 configtx.yaml 中 Profiles 字段下的 [root@localhost jicki]# configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./channel-artifacts/genesis.block 2018-07-24 14:17:28.681 CST [common/tools/configtxgen] main -\u003e WARN 001 Omitting the channel ID for configtxgen is deprecated. Explicitly passing the channel ID will be required in the future, defaulting to 'testchainid'. 2018-07-24 14:17:28.681 CST [common/tools/configtxgen] main -\u003e INFO 002 Loading configuration 2018-07-24 14:17:28.726 CST [msp] getMspConfig -\u003e INFO 003 Loading NodeOUs 2018-07-24 14:17:28.727 CST [msp] getMspConfig -\u003e INFO 004 Loading NodeOUs 2018-07-24 14:17:28.727 CST [common/tools/configtxgen] doOutputBlock -\u003e INFO 005 Generating genesis block 2018-07-24 14:17:28.728 CST [common/tools/configtxgen] doOutputBlock -\u003e INFO 006 Writing genesis block # 创世区块 是在 orderer 服务中使用 [root@localhost jicki]# ls -lt channel-artifacts/ 总用量 16 -rw-r--r-- 1 root root 12484 7月 24 14:17 genesis.block # 下面来生成一个 peer 服务 中使用的 tx 文件 TwoOrgsChannel 名称为 conf","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:6:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"配置 Zookeeper Kafka 集群 zookeeper 与 kafka 集群 必须在 所有服务器之间启动。 ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:7:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第一个节点 (192.168.100.67) ZOO_SERVERS 本机因为在 docker 里，需要配置为 0.0.0.0:2888:3888 vi docker-compose-zk-kafka.yaml version: '2' services: zookeeper0: container_name: zookeeper0 hostname: zookeeper0 image: zookeeper restart: always environment: - ZOO_MY_ID=1 - ZOO_SERVERS=server.1=0.0.0.0:2888:3888 server.2=zookeeper1:2888:3888 server.3=zookeeper2:2888:3888 #volumes: # 存储数据与日志 #- ./data/zookeeper0/data:/data #- ./data/zookeeper0/datalog:/datalog ports: - 192.168.100.67:2181:2181 - 192.168.100.67:2888:2888 - 192.168.100.67:3888:3888 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" kafka0: container_name: kafka0 hostname: kafka0 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=1 # 设置一个M值,数据提交时会写入至少M个副本(这里M=2)（这些数据会被同步并且归属到in-sync 副本集合或ISR）M 必须小于 如下 N 值,并且大于1,既最小为2。 - KAFKA_MIN_INSYNC_REPLICAS=2 # 设置一个N值, N代表着每个channel都保存N个副本的数据到Kafka的代理上。N 必须大于如上 M 值, 既 N 值最小值为 3。 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181 # 如下99为configtx.yaml中会设置最大的区块大小(参考configtx.yaml中AbsoluteMaxBytes参数) # 每个区块最大有Orderer.AbsoluteMaxBytes个字节 # 99 * 1024 * 1024 B - KAFKA_MESSAGE_MAX_BYTES=103809024 # 每个通道获取的消息的字节数 如上一样 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 # 数据一致性在区块链环境中是至关重要的, 我们不能从in-sync 副本（ISR）集合之外选取channel leader , 否则我们将会面临对于之前的leader产生的offsets覆盖的风险 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false # 关闭基于时间的日志保留方式并且避免分段到期。 - KAFKA_LOG_RETENTION_MS=-1 #volumes: # 存储数据与日志. #- ./data/kafka0/data:/data #- ./data/kafka0/data:/logs ports: - 192.168.100.67:9092:9092 depends_on: - zookeeper0 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" - \"kafka0:192.168.100.67\" - \"kafka1:192.168.100.15\" - \"kafka2:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:7:1","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第二个节点 (192.168.100.15) vi docker-compose-zk-kafka.yaml version: '2' services: zookeeper1: container_name: zookeeper1 hostname: zookeeper1 image: zookeeper restart: always environment: - ZOO_MY_ID=2 - ZOO_SERVERS=server.1=zookeeper0:2888:3888 server.2=0.0.0.0:2888:3888 server.3=zookeeper2:2888:3888 #volumes: # 存储数据与日志 #- ./data/zookeeper1/data:/data #- ./data/zookeeper1/datalog:/datalog ports: - 192.168.100.15:2181:2181 - 192.168.100.15:2888:2888 - 192.168.100.15:3888:3888 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" kafka1: container_name: kafka1 hostname: kafka1 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=2 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181 - KAFKA_MESSAGE_MAX_BYTES=103809024 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false - KAFKA_LOG_RETENTION_MS=-1 #volumes: # 存储数据与日志. #- ./data/kafka1/data:/data #- ./data/kafka1/data:/logs ports: - 192.168.100.15:9092:9092 depends_on: - zookeeper1 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" - \"kafka0:192.168.100.67\" - \"kafka1:192.168.100.15\" - \"kafka2:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:7:2","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第三个节点 (192.168.100.91) vi docker-compose-zk-kafka.yaml version: '2' services: zookeeper2: container_name: zookeeper2 hostname: zookeeper2 image: zookeeper restart: always environment: - ZOO_MY_ID=3 - ZOO_SERVERS=server.1=zookeeper0:2888:3888 server.2=zookeeper1:2888:3888 server.3=0.0.0.0:2888:3888 #volumes: # 存储数据与日志 #- ./data/zookeeper2/data:/data #- ./data/zookeeper2/datalog:/datalog ports: - 192.168.100.91:2181:2181 - 192.168.100.91:2888:2888 - 192.168.100.91:3888:3888 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" kafka2: container_name: kafka2 hostname: kafka2 image: hyperledger/fabric-kafka restart: always environment: - KAFKA_BROKER_ID=3 - KAFKA_MIN_INSYNC_REPLICAS=2 - KAFKA_DEFAULT_REPLICATION_FACTOR=3 - KAFKA_ZOOKEEPER_CONNECT=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181 - KAFKA_MESSAGE_MAX_BYTES=103809024 - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024 - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false - KAFKA_LOG_RETENTION_MS=-1 #volumes: # 存储数据与日志. #- ./data/kafka2/data:/data #- ./data/kafka2/data:/logs ports: - 192.168.100.91:9092:9092 depends_on: - zookeeper2 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" - \"kafka0:192.168.100.67\" - \"kafka1:192.168.100.15\" - \"kafka2:192.168.100.91\" # 分别启动服务 docker-compose -f docker-compose-zk-kafka.yaml up -d ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:7:3","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric Orderer ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:8:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第一个节点 (192.168.100.67) # 创建文件 docker-compose-orderer.yaml # 创建于 /opt/jicki 目录下 vi docker-compose-orderer.yaml version: '2' services: orderer0.jicki.cn: container_name: orderer0.jicki.cn image: hyperledger/fabric-orderer environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default # - ORDERER_GENERAL_LOGLEVEL=error - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 #- ORDERER_GENERAL_GENESISPROFILE=AntiMothOrdererGenesis - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #- ORDERER_GENERAL_LEDGERTYPE=ram #- ORDERER_GENERAL_LEDGERTYPE=file # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=false - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] # KAFKA - ORDERER_KAFKA_RETRY_LONGINTERVAL=10s - ORDERER_KAFKA_RETRY_LONGTOTAL=100s - ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s - ORDERER_KAFKA_RETRY_SHORTTOTAL=30s - ORDERER_KAFKA_VERBOSE=true - ORDERER_KAFKA_BROKERS=[kafka0:9092,kafka1:9092,kafka2:9092] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/tls/:/var/hyperledger/orderer/tls ports: - 7050:7050 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" - \"kafka0:192.168.100.67\" - \"kafka1:192.168.100.15\" - \"kafka2:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:8:1","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第二个节点 (192.168.100.15) # 创建文件 docker-compose-orderer.yaml # 创建于 /opt/jicki 目录下 vi docker-compose-orderer.yaml version: '2' services: orderer1.jicki.cn: container_name: orderer1.jicki.cn image: hyperledger/fabric-orderer environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default # - ORDERER_GENERAL_LOGLEVEL=error - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 #- ORDERER_GENERAL_GENESISPROFILE=AntiMothOrdererGenesis - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #- ORDERER_GENERAL_LEDGERTYPE=ram #- ORDERER_GENERAL_LEDGERTYPE=file # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=false - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] # KAFKA - ORDERER_KAFKA_RETRY_LONGINTERVAL=10s - ORDERER_KAFKA_RETRY_LONGTOTAL=100s - ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s - ORDERER_KAFKA_RETRY_SHORTTOTAL=30s - ORDERER_KAFKA_VERBOSE=true - ORDERER_KAFKA_BROKERS=[kafka0:9092,kafka1:9092,kafka2:9092] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer1.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer1.jicki.cn/tls/:/var/hyperledger/orderer/tls ports: - 7050:7050 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" - \"kafka0:192.168.100.67\" - \"kafka1:192.168.100.15\" - \"kafka2:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:8:2","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第三个节点 (192.168.100.91) # 创建文件 docker-compose-orderer.yaml # 创建于 /opt/jicki 目录下 vi docker-compose-orderer.yaml version: '2' services: orderer2.jicki.cn: container_name: orderer2.jicki.cn image: hyperledger/fabric-orderer environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default # - ORDERER_GENERAL_LOGLEVEL=error - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 #- ORDERER_GENERAL_GENESISPROFILE=AntiMothOrdererGenesis - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #- ORDERER_GENERAL_LEDGERTYPE=ram #- ORDERER_GENERAL_LEDGERTYPE=file # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=false - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] # KAFKA - ORDERER_KAFKA_RETRY_LONGINTERVAL=10s - ORDERER_KAFKA_RETRY_LONGTOTAL=100s - ORDERER_KAFKA_RETRY_SHORTINTERVAL=1s - ORDERER_KAFKA_RETRY_SHORTTOTAL=30s - ORDERER_KAFKA_VERBOSE=true - ORDERER_KAFKA_BROKERS=[kafka0:9092,kafka1:9092,kafka2:9092] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer2.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer2.jicki.cn/tls/:/var/hyperledger/orderer/tls ports: - 7050:7050 extra_hosts: - \"zookeeper0:192.168.100.67\" - \"zookeeper1:192.168.100.15\" - \"zookeeper2:192.168.100.91\" - \"kafka0:192.168.100.67\" - \"kafka1:192.168.100.15\" - \"kafka2:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:8:3","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"启动服务 所有节点启动服务 docker-compose -f docker-compose-orderer.yaml up -d # 启动完毕，查看docker logs 如下表示成功 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processMessagesToBlocks -\u003e DEBU 2fd [channel: testchainid] Successfully unmarshalled consumed message, offset is 0. Inspecting type... 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processConnect -\u003e DEBU 2fe [channel: testchainid] It's a connect message - ignoring 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processMessagesToBlocks -\u003e DEBU 2ff [channel: testchainid] Successfully unmarshalled consumed message, offset is 1. Inspecting type... 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processConnect -\u003e DEBU 300 [channel: testchainid] It's a connect message - ignoring 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processMessagesToBlocks -\u003e DEBU 301 [channel: testchainid] Successfully unmarshalled consumed message, offset is 2. Inspecting type... 2018-08-07 01:42:14.431 UTC [orderer/consensus/kafka] processConnect -\u003e DEBU 302 [channel: testchainid] It's a connect message - ignoring ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:8:4","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric peer 节点下都必须启动一个数据存储，如 file 或者 couchdb 等 ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:9:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第二个节点 (192.168.100.15) vi docker-compose-peer.yaml version: '2' services: couchdb0: container_name: couchdb0 image: hyperledger/fabric-couchdb environment: - COUCHDB_USER= - COUCHDB_PASSWORD= ports: - \"5984:5984\" #volumes: # 数据持久化，用于存储链码值 #- ./data/couchdb0/data:/opt/couchdb/data peer0.org1.jicki.cn: container_name: peer0.org1.jicki.cn image: hyperledger/fabric-peer environment: - CORE_LEDGER_STATE_STATEDATABASE=CouchDB - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb0:5984 - CORE_PEER_ID=peer0.org1.jicki.cn - CORE_PEER_NETWORKID=jicki - CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 - CORE_PEER_CHAINCODELISTENADDRESS=peer0.org1.jicki.cn:7052 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.jicki.cn:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_PROFILE_ENABLED=false - CORE_PEER_TLS_ENABLED=false - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt volumes: - /var/run/:/host/var/run/ - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/msp:/etc/hyperledger/fabric/msp - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls:/etc/hyperledger/fabric/tls # 数据持久化, 存储安装，以及实例化智能合约的数据 - ./data/peer0org1:/var/hyperledger/production working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start ports: - 7051:7051 - 7052:7052 - 7053:7053 depends_on: - couchdb0 extra_hosts: - \"couchdb0:192.168.100.15\" - \"orderer0.jicki.cn:192.168.100.67\" - \"orderer1.jicki.cn:192.168.100.15\" - \"orderer2.jicki.cn:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:9:1","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"第三个节点 (192.168.100.91) vi docker-compose-peer.yaml version: '2' services: couchdb1: container_name: couchdb1 image: hyperledger/fabric-couchdb environment: - COUCHDB_USER= - COUCHDB_PASSWORD= ports: - \"5984:5984\" #volumes: # 数据持久化，用于存储链码值 #- ./data/couchdb1/data:/opt/couchdb/data peer0.org2.jicki.cn: container_name: peer0.org2.jicki.cn image: hyperledger/fabric-peer environment: - CORE_LEDGER_STATE_STATEDATABASE=CouchDB - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb1:5984 - CORE_PEER_ID=peer0.org2.jicki.cn - CORE_PEER_NETWORKID=jicki - CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 - CORE_PEER_CHAINCODELISTENADDRESS=peer0.org2.jicki.cn:7052 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org2.jicki.cn:7051 - CORE_PEER_LOCALMSPID=Org2MSP - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_PROFILE_ENABLED=false - CORE_PEER_TLS_ENABLED=false - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt volumes: - /var/run/:/host/var/run/ - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/msp:/etc/hyperledger/fabric/msp - ./crypto-config/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls:/etc/hyperledger/fabric/tls # 数据持久化, 存储安装，以及实例化智能合约的数据 - ./data/peer0org2:/var/hyperledger/production working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start ports: - 7051:7051 - 7052:7052 - 7053:7053 depends_on: - couchdb1 extra_hosts: - \"couchdb1:192.168.100.91\" - \"orderer0.jicki.cn:192.168.100.67\" - \"orderer1.jicki.cn:192.168.100.15\" - \"orderer2.jicki.cn:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:9:2","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"启动服务 docker-compose -f docker-compose-peer.yaml up -d # 查看 docker logs 输出如下, 既为成功 # 第二个节点 2018-08-07 02:05:20.638 UTC [discovery] NewService -\u003e INFO 122 Created with config TLS: false, authCacheMaxSize: 1000, authCachePurgeRatio: 0.750000 2018-08-07 02:05:20.638 UTC [nodeCmd] registerDiscoveryService -\u003e INFO 123 Discovery service activated 2018-08-07 02:05:20.638 UTC [nodeCmd] serve -\u003e INFO 124 Starting peer with ID=[name:\"peer0.org1.jicki.cn\" ], network ID=[jicki], address=[peer0.org1.jicki.cn:7051] 2018-08-07 02:05:20.638 UTC [nodeCmd] serve -\u003e INFO 125 Started peer with ID=[name:\"peer0.org1.jicki.cn\" ], network ID=[jicki], address=[peer0.org1.jicki.cn:7051] # 第三个节点 2018-08-07 02:07:23.628 UTC [discovery] NewService -\u003e INFO 133 Created with config TLS: false, authCacheMaxSize: 1000, authCachePurgeRatio: 0.750000 2018-08-07 02:07:23.628 UTC [nodeCmd] registerDiscoveryService -\u003e INFO 134 Discovery service activated 2018-08-07 02:07:23.628 UTC [nodeCmd] serve -\u003e INFO 135 Starting peer with ID=[name:\"peer0.org2.jicki.cn\" ], network ID=[jicki], address=[peer0.org2.jicki.cn:7051] 2018-08-07 02:07:23.628 UTC [nodeCmd] serve -\u003e INFO 136 Started peer with ID=[name:\"peer0.org2.jicki.cn\" ], network ID=[jicki], address=[peer0.org2.jicki.cn:7051] ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:9:3","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric cli 客户端 客户端服务，只需要配置一个既可，这里配置再 第一个节点中，用于调用创建 channel 与 智能合约 vi docker-compose-cli.yaml version: '2' services: cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_PEER_TLS_ENABLED=false - CORE_PEER_TLS_CERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt - CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer volumes: - /var/run/:/host/var/run/ - /opt/golang/go:/opt/go - /opt/gopath:/opt/gopath - ./peer:/opt/gopath/src/github.com/hyperledger/fabric/peer - ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/jicki/chaincode/go - ./crypto-config:/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ - ./channel-artifacts:/opt/gopath/src/github.com/hyperledger/fabric/peer/channel-artifacts extra_hosts: - \"orderer0.jicki.cn:192.168.100.67\" - \"orderer1.jicki.cn:192.168.100.15\" - \"orderer2.jicki.cn:192.168.100.91\" - \"peer0.org1.jicki.cn:192.168.100.15\" - \"peer0.org2.jicki.cn:192.168.100.91\" ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:10:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"启动服务 docker-compose -f docker-compose-cli.yaml up -d ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:10:1","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"Hyperledger Fabric 创建 Channel # 上面我们创建了 cli 容器，我们可以直接进入 容器里操作 [root@localhost jicki]# docker exec -it cli bash root@77962643125a:/opt/gopath/src/github.com/hyperledger/fabric/peer# # 执行 创建命令 (未启动 认证) peer channel create -c mychannel -f ./channel-artifacts/channel.tx --orderer orderer0.jicki.cn:7050 # 以下为启用认证 peer channel create -o orderer0.jicki.cn:7050 -c mychannel -f ./channel-artifacts/channel.tx --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输出如下: 2018-08-07 02:20:49.574 UTC [grpc] Printf -\u003e DEBU 094 parsed scheme: \"\" 2018-08-07 02:20:49.574 UTC [grpc] Printf -\u003e DEBU 095 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:20:49.574 UTC [grpc] Printf -\u003e DEBU 096 ccResolverWrapper: sending new addresses to cc: [{orderer0.jicki.cn:7050 0 \u003cnil\u003e}] 2018-08-07 02:20:49.574 UTC [grpc] Printf -\u003e DEBU 097 ClientConn switching balancer to \"pick_first\" 2018-08-07 02:20:49.574 UTC [grpc] Printf -\u003e DEBU 098 pickfirstBalancer: HandleSubConnStateChange: 0xc420281c80, CONNECTING 2018-08-07 02:20:49.575 UTC [grpc] Printf -\u003e DEBU 099 pickfirstBalancer: HandleSubConnStateChange: 0xc420281c80, READY 2018-08-07 02:20:49.575 UTC [channelCmd] InitCmdFactory -\u003e INFO 09a Endorser and orderer connections initialized 2018-08-07 02:20:49.775 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 09b Obtaining default signing identity 2018-08-07 02:20:49.775 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 09c Obtaining default signing identity 2018-08-07 02:20:49.775 UTC [msp/identity] Sign -\u003e DEBU 09d Sign: plaintext: 0AD1060A1508051A06088184A4DB0522...2FB3601B298B12080A021A0012021A00 2018-08-07 02:20:49.775 UTC [msp/identity] Sign -\u003e DEBU 09e Sign: digest: DE4E562C31BC2BA09C70978648AAE7EC6F81BC49BF5D6AE8760FD64B84F3541C 2018-08-07 02:20:49.777 UTC [cli/common] readBlock -\u003e INFO 09f Received block: 0 # 创建以后生成文件 mychannel.block total 24 -rw-r--r-- 1 root root 15382 Aug 7 02:20 mychannel.block drwxr-xr-x 2 root root 4096 Aug 6 09:16 channel-artifacts drwxr-xr-x 4 root root 4096 Aug 6 06:45 crypto ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:11:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"Hyperledger Fabric 加入 Channel 我们这边有2个 peer 所以需要分别加入, 后续有多少个 peer 都需要加入到 Channel 中 # peer0.org1.jicki.cn 加入 此 channel 中，首先需要查看如下 环境变量 echo $CORE_PEER_LOCALMSPID echo $CORE_PEER_ADDRESS echo $CORE_PEER_MSPCONFIGPATH echo $CORE_PEER_TLS_ROOTCERT_FILE # 加入 channel peer channel join -b mychannel.block # 输出如下: 2018-08-07 02:31:52.810 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-08-03 06:40:54 +0000 UTC 2018-08-07 02:31:52.811 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-08-07 02:31:52.811 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 037 Obtaining default signing identity 2018-08-07 02:31:52.811 UTC [grpc] Printf -\u003e DEBU 038 parsed scheme: \"\" 2018-08-07 02:31:52.811 UTC [grpc] Printf -\u003e DEBU 039 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:31:52.811 UTC [grpc] Printf -\u003e DEBU 03a ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:31:52.811 UTC [grpc] Printf -\u003e DEBU 03b ClientConn switching balancer to \"pick_first\" 2018-08-07 02:31:52.811 UTC [grpc] Printf -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc4203cb530, CONNECTING 2018-08-07 02:31:52.812 UTC [grpc] Printf -\u003e DEBU 03d pickfirstBalancer: HandleSubConnStateChange: 0xc4203cb530, READY 2018-08-07 02:31:52.812 UTC [channelCmd] InitCmdFactory -\u003e INFO 03e Endorser and orderer connections initialized 2018-08-07 02:31:52.813 UTC [msp/identity] Sign -\u003e DEBU 03f Sign: plaintext: 0A98070A5C08011A0C089889A4DB0510...131231D39FB81A080A000A000A000A00 2018-08-07 02:31:52.813 UTC [msp/identity] Sign -\u003e DEBU 040 Sign: digest: E9D23EC60B2EDFA54AD140AB09299DCF3FA11A657D85743E58A15BC02FE38743 2018-08-07 02:31:52.902 UTC [channelCmd] executeJoin -\u003e INFO 041 Successfully submitted proposal to join channel # peer1.org2.jicki.cn 加入 此 channel 中，这里配置一下环境变量 export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0.org2.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/users/Admin@org2.jicki.cn/msp # 加入 channel peer channel join -b mychannel.block # 输入如下: 2018-08-07 02:32:23.864 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-08-03 06:40:54 +0000 UTC 2018-08-07 02:32:23.864 UTC [msp] Validate -\u003e DEBU 036 MSP Org2MSP validating identity 2018-08-07 02:32:23.865 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 037 Obtaining default signing identity 2018-08-07 02:32:23.865 UTC [grpc] Printf -\u003e DEBU 038 parsed scheme: \"\" 2018-08-07 02:32:23.865 UTC [grpc] Printf -\u003e DEBU 039 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:32:23.865 UTC [grpc] Printf -\u003e DEBU 03a ccResolverWrapper: sending new addresses to cc: [{peer0.org2.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:32:23.865 UTC [grpc] Printf -\u003e DEBU 03b ClientConn switching balancer to \"pick_first\" 2018-08-07 02:32:23.865 UTC [grpc] Printf -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc420ca8bf0, CONNECTING 2018-08-07 02:32:23.866 UTC [grpc] Printf -\u003e DEBU 03d pickfirstBalancer: HandleSubConnStateChange: 0xc420ca8bf0, READY 2018-08-07 02:32:23.866 UTC [channelCmd] InitCmdFactory -\u003e INFO 03e Endorser and orderer connections initialized 2018-08-07 02:32:23.867 UTC [msp/identity] Sign -\u003e DEBU 03f Sign: plaintext: 0A98070A5C08011A0C08B789A4DB0510...131231D39FB81A080A000A000A000A00 2018-08-07 02:32:23.867 UTC [msp/identity] Sign -\u003e DEBU 040 Sign: digest: 620FDAF5EA0CAC6FB6FEC4B16910082B6E471EA03688AB0095A71DC58F280E0B 2018-08-07 02:32:23.955 UTC [channelCmd] executeJoin -\u003e INFO 041 Successfully submitted proposal to join channel ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:12:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"Hyperledger Fabric 锚节点 锚节点通过广播的方式通知有新节点加入 # 使用Org1的管理员身份更新锚节点配置 # 同样需要先配置变量 export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp # 未开启认证的方式 peer channel update -o orderer0.jicki.cn:7050 -c mychannel -f ./channel-artifacts/Org1MSPanchors.tx # 开启认证的方式 peer channel update -o orderer0.jicki.cn:7050 -c mychannel -f ./channel-artifacts/Org1MSPanchors.tx --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem # 输出如下: 2018-08-07 02:45:16.093 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-08-03 06:40:54 +0000 UTC 2018-08-07 02:45:16.093 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-08-07 02:45:16.093 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 037 Obtaining default signing identity 2018-08-07 02:45:16.093 UTC [grpc] Printf -\u003e DEBU 038 parsed scheme: \"\" 2018-08-07 02:45:16.093 UTC [grpc] Printf -\u003e DEBU 039 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:45:16.093 UTC [grpc] Printf -\u003e DEBU 03a ccResolverWrapper: sending new addresses to cc: [{orderer0.jicki.cn:7050 0 \u003cnil\u003e}] 2018-08-07 02:45:16.093 UTC [grpc] Printf -\u003e DEBU 03b ClientConn switching balancer to \"pick_first\" 2018-08-07 02:45:16.093 UTC [grpc] Printf -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc4201fe5e0, CONNECTING 2018-08-07 02:45:16.094 UTC [grpc] Printf -\u003e DEBU 03d pickfirstBalancer: HandleSubConnStateChange: 0xc4201fe5e0, READY 2018-08-07 02:45:16.094 UTC [channelCmd] InitCmdFactory -\u003e INFO 03e Endorser and orderer connections initialized 2018-08-07 02:45:16.094 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 03f Obtaining default signing identity 2018-08-07 02:45:16.094 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 040 Obtaining default signing identity 2018-08-07 02:45:16.094 UTC [msp/identity] Sign -\u003e DEBU 041 Sign: plaintext: 0A9A060A074F7267314D5350128E062D...2A0641646D696E732A0641646D696E73 2018-08-07 02:45:16.094 UTC [msp/identity] Sign -\u003e DEBU 042 Sign: digest: B9EBDD95115ECC8722C7E5D917CE7AB428AF8F7921A55D01D988488B24EBCA64 2018-08-07 02:45:16.094 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-08-07 02:45:16.094 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 044 Obtaining default signing identity 2018-08-07 02:45:16.094 UTC [msp/identity] Sign -\u003e DEBU 045 Sign: plaintext: 0AD1060A1508021A0608BC8FA4DB0522...223072D3C517446F35F9431FCE53B9AD 2018-08-07 02:45:16.094 UTC [msp/identity] Sign -\u003e DEBU 046 Sign: digest: 6B483C98171C37B2FBD318E27AC73DD68277C3719CAA04719225672B028ABABD 2018-08-07 02:45:16.094 UTC [grpc] Printf -\u003e DEBU 047 parsed scheme: \"\" 2018-08-07 02:45:16.094 UTC [grpc] Printf -\u003e DEBU 048 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:45:16.094 UTC [grpc] Printf -\u003e DEBU 049 ccResolverWrapper: sending new addresses to cc: [{orderer0.jicki.cn:7050 0 \u003cnil\u003e}] 2018-08-07 02:45:16.094 UTC [grpc] Printf -\u003e DEBU 04a ClientConn switching balancer to \"pick_first\" 2018-08-07 02:45:16.095 UTC [grpc] Printf -\u003e DEBU 04b pickfirstBalancer: HandleSubConnStateChange: 0xc4201fef50, CONNECTING 2018-08-07 02:45:16.095 UTC [grpc] Printf -\u003e DEBU 04c pickfirstBalancer: HandleSubConnStateChange: 0xc4201fef50, READY 2018-08-07 02:45:16.214 UTC [channelCmd] update -\u003e INFO 04d Successfully submitted channel update # 使用Org2的管理员身份更新锚节点配置 # 同样需要先配置变量 export CORE_PEER_LOCALMSPID=\"Org2MSP\" export CORE_PEER_ADDRESS=peer0.org2.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org2.jicki.cn/peers/peer0","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:13:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"Hyperledger Fabric 实例化测试 在上面我们已经拷贝了官方的例子，在 chaincode 下, 下面我们来测试一下 ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:14:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"安装智能合约 # cli 部分 ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/jicki/chaincode/go # 为 智能合约的目录 我们约定为这个目录 需要预先创建 mkdir -p /opt/jicki/chaincode/go cd /opt/jicki/chaincode/go # 创建以后~我们拷贝官方的 例子进来，方便后面进行合约测试 cp -r /opt/gopath/src/github.com/hyperledger/fabric/examples/chaincode/go/example0* /opt/jicki/chaincode/go/ # 官方这里有5个例子 [root@localhost jicki]# ls -lt chaincode/go/ total 20 drwxr-xr-x 3 root root 4096 Aug 7 10:46 example01 drwxr-xr-x 3 root root 4096 Aug 7 10:46 example02 drwxr-xr-x 3 root root 4096 Aug 7 10:46 example03 drwxr-xr-x 3 root root 4096 Aug 7 10:46 example04 drwxr-xr-x 3 root root 4096 Aug 7 10:46 example05 # 如上我们挂载的地址为 github.com/hyperledger/fabric/jicki/chaincode/go # 注: 这里面的 example02 的 package 为 example02 会报错 Error: could not assemble transaction, err Proposal response was not successful, error code 500, msg failed to execute transaction 819b581ce88604e9b6651764324876f2ca7a47d7aeb7ee307f273af867a4a134: error starting container: error starting container: API error (404): oci runtime error: container_linux.go:247: starting container process caused \"exec: \\\"chaincode\\\": executable file not found in $PATH\" # 将 chaincode.go chaincode_test.go 中 package 修改成 main 然后在最下面增加 main()函数 func main() { err := shim.Start(new(SimpleChaincode)) if err != nil { fmt.Printf(\"Error starting Simple chaincode: %s\", err) } } # 安装指定合约到 所有的 peer 节点中，每个节点都必须安装一次 # 同样需要先配置变量 export CORE_PEER_LOCALMSPID=\"Org1MSP\" export CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 export CORE_PEER_TLS_ROOTCERT_FILE=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls/ca.crt export CORE_PEER_MSPCONFIGPATH=/opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/peerOrganizations/org1.jicki.cn/users/Admin@org1.jicki.cn/msp # 安装 合约 peer chaincode install -n example2 -p github.com/hyperledger/fabric/jicki/chaincode/go/example02 -v 1.0 # 输出如下: 2018-08-07 02:48:05.296 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-08-03 06:40:54 +0000 UTC 2018-08-07 02:48:05.296 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-08-07 02:48:05.297 UTC [grpc] Printf -\u003e DEBU 037 parsed scheme: \"\" 2018-08-07 02:48:05.297 UTC [grpc] Printf -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:48:05.297 UTC [grpc] Printf -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:48:05.297 UTC [grpc] Printf -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-08-07 02:48:05.297 UTC [grpc] Printf -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc42049c2f0, CONNECTING 2018-08-07 02:48:05.298 UTC [grpc] Printf -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc42049c2f0, READY 2018-08-07 02:48:05.298 UTC [grpc] Printf -\u003e DEBU 03d parsed scheme: \"\" 2018-08-07 02:48:05.298 UTC [grpc] Printf -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-08-07 02:48:05.298 UTC [grpc] Printf -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:48:05.298 UTC [grpc] Printf -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-08-07 02:48:05.299 UTC [grpc] Printf -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc4202ce020, CONNECTING 2018-08-07 02:48:05.299 UTC [grpc] Printf -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc4202ce020, READY 2018-08-07 02:48:05.300 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-08-07 02:48:05.300 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 044 Using default escc 2018-08-07 02:48:05.300 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 045 Using default vscc 2018-08-07 02:48:05.300 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 046 java chaincode disabled 2018-08-07 02:48:05.338 UTC [golang-platform] getCodeFromFS -\u003e DEBU 047 getCodeFromFS github.com/hyperledger/fabric/jicki/chaincode/go/example02 20","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:14:1","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"实例化 Chaincode 这里无论多少个 peer 节点, 实例化只需要实例化一次，就可以。 # 实例化合约 (未认证) peer chaincode instantiate -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"200\",\"B\",\"500\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.0 # 实例化合约 (已认证) peer chaincode instantiate -o orderer0.jicki.cn:7050 --tls $CORE_PEER_TLS_ENABLED --cafile /opt/gopath/src/github.com/hyperledger/fabric/peer/crypto/ordererOrganizations/jicki.cn/orderers/orderer0.jicki.cn/msp/tlscacerts/tlsca.jicki.cn-cert.pem -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"100\",\"B\",\"50\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.0 # 输出如下: 2018-08-07 02:49:00.544 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-08-03 06:40:54 +0000 UTC 2018-08-07 02:49:00.544 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-08-07 02:49:00.545 UTC [grpc] Printf -\u003e DEBU 037 parsed scheme: \"\" 2018-08-07 02:49:00.545 UTC [grpc] Printf -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:49:00.545 UTC [grpc] Printf -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:49:00.545 UTC [grpc] Printf -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-08-07 02:49:00.545 UTC [grpc] Printf -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc4204ca910, CONNECTING 2018-08-07 02:49:00.546 UTC [grpc] Printf -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc4204ca910, READY 2018-08-07 02:49:00.546 UTC [grpc] Printf -\u003e DEBU 03d parsed scheme: \"\" 2018-08-07 02:49:00.546 UTC [grpc] Printf -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-08-07 02:49:00.546 UTC [grpc] Printf -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:49:00.546 UTC [grpc] Printf -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-08-07 02:49:00.547 UTC [grpc] Printf -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc420549310, CONNECTING 2018-08-07 02:49:00.548 UTC [grpc] Printf -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc420549310, READY 2018-08-07 02:49:00.548 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-08-07 02:49:00.548 UTC [grpc] Printf -\u003e DEBU 044 parsed scheme: \"\" 2018-08-07 02:49:00.548 UTC [grpc] Printf -\u003e DEBU 045 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:49:00.548 UTC [grpc] Printf -\u003e DEBU 046 ccResolverWrapper: sending new addresses to cc: [{orderer0.jicki.cn:7050 0 \u003cnil\u003e}] 2018-08-07 02:49:00.548 UTC [grpc] Printf -\u003e DEBU 047 ClientConn switching balancer to \"pick_first\" 2018-08-07 02:49:00.548 UTC [grpc] Printf -\u003e DEBU 048 pickfirstBalancer: HandleSubConnStateChange: 0xc42006b380, CONNECTING 2018-08-07 02:49:00.548 UTC [grpc] Printf -\u003e DEBU 049 pickfirstBalancer: HandleSubConnStateChange: 0xc42006b380, READY 2018-08-07 02:49:00.548 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 04a Using default escc 2018-08-07 02:49:00.548 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 04b Using default vscc 2018-08-07 02:49:00.549 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 04c java chaincode disabled 2018-08-07 02:49:00.549 UTC [msp/identity] Sign -\u003e DEBU 04d Sign: plaintext: 0AA3070A6708031A0C089C91A4DB0510...324D53500A04657363630A0476736363 2018-08-07 02:49:00.549 UTC [msp/identity] Sign -\u003e DEBU 04e Sign: digest: 1852003550AFAEB4A9DD2F01A3F4DA89346E64FCC1B6056D45CCC297A9763A1F 2018-08-07 02:49:13.285 UTC [msp/identity] Sign -\u003e DEBU 04f Sign: plaintext: 0AA3070A6708031A0C089C91A4DB0510...46C5367A5E41CCC7C4704B72B4D5F2D4 2018-08-07 02:49:13.285 UTC [msp/identity] Sign -\u003e DEBU 050 Sign: digest: 8D57F12A907CA041F15725217B3EC56FC52C2E4A5A9FF8578FD90C18AC65F817 ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:14:2","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"操作智能合约 # query 查询方法 # 查询 A 账户里的余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"A\"]}' # 输出如下: 2018-08-07 02:49:40.807 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-08-03 06:40:54 +0000 UTC 2018-08-07 02:49:40.807 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-08-07 02:49:40.807 UTC [grpc] Printf -\u003e DEBU 037 parsed scheme: \"\" 2018-08-07 02:49:40.807 UTC [grpc] Printf -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:49:40.807 UTC [grpc] Printf -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:49:40.807 UTC [grpc] Printf -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-08-07 02:49:40.808 UTC [grpc] Printf -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc4201b5510, CONNECTING 2018-08-07 02:49:40.809 UTC [grpc] Printf -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc4201b5510, READY 2018-08-07 02:49:40.809 UTC [grpc] Printf -\u003e DEBU 03d parsed scheme: \"\" 2018-08-07 02:49:40.809 UTC [grpc] Printf -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-08-07 02:49:40.809 UTC [grpc] Printf -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:49:40.809 UTC [grpc] Printf -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-08-07 02:49:40.809 UTC [grpc] Printf -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc420232a20, CONNECTING 2018-08-07 02:49:40.809 UTC [grpc] Printf -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc420232a20, READY 2018-08-07 02:49:40.811 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-08-07 02:49:40.811 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 044 java chaincode disabled 2018-08-07 02:49:40.811 UTC [msp/identity] Sign -\u003e DEBU 045 Sign: plaintext: 0AA7070A6B08031A0C08C491A4DB0510...706C65321A0A0A0571756572790A0141 2018-08-07 02:49:40.811 UTC [msp/identity] Sign -\u003e DEBU 046 Sign: digest: CD3360D1248F37AC518F8B9D55B46BA976526A6D115AE68A2EC3A93423C22D7C 200 # 可以看到 返回 200 # 查询 B 账户里的余额 peer chaincode query -C mychannel -n example2 -c '{\"Args\":[\"query\",\"B\"]}' # 输出如下: 2018-08-07 02:50:09.596 UTC [msp] setupSigningIdentity -\u003e DEBU 035 Signing identity expires at 2028-08-03 06:40:54 +0000 UTC 2018-08-07 02:50:09.596 UTC [msp] Validate -\u003e DEBU 036 MSP Org1MSP validating identity 2018-08-07 02:50:09.597 UTC [grpc] Printf -\u003e DEBU 037 parsed scheme: \"\" 2018-08-07 02:50:09.597 UTC [grpc] Printf -\u003e DEBU 038 scheme \"\" not registered, fallback to default scheme 2018-08-07 02:50:09.597 UTC [grpc] Printf -\u003e DEBU 039 ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:50:09.597 UTC [grpc] Printf -\u003e DEBU 03a ClientConn switching balancer to \"pick_first\" 2018-08-07 02:50:09.597 UTC [grpc] Printf -\u003e DEBU 03b pickfirstBalancer: HandleSubConnStateChange: 0xc42053fc70, CONNECTING 2018-08-07 02:50:09.597 UTC [grpc] Printf -\u003e DEBU 03c pickfirstBalancer: HandleSubConnStateChange: 0xc42053fc70, READY 2018-08-07 02:50:09.598 UTC [grpc] Printf -\u003e DEBU 03d parsed scheme: \"\" 2018-08-07 02:50:09.598 UTC [grpc] Printf -\u003e DEBU 03e scheme \"\" not registered, fallback to default scheme 2018-08-07 02:50:09.598 UTC [grpc] Printf -\u003e DEBU 03f ccResolverWrapper: sending new addresses to cc: [{peer0.org1.jicki.cn:7051 0 \u003cnil\u003e}] 2018-08-07 02:50:09.598 UTC [grpc] Printf -\u003e DEBU 040 ClientConn switching balancer to \"pick_first\" 2018-08-07 02:50:09.598 UTC [grpc] Printf -\u003e DEBU 041 pickfirstBalancer: HandleSubConnStateChange: 0xc420247180, CONNECTING 2018-08-07 02:50:09.599 UTC [grpc] Printf -\u003e DEBU 042 pickfirstBalancer: HandleSubConnStateChange: 0xc420247180, READY 2018-08-07 02:50:09.599 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 043 Obtaining default signing identity 2018-08-07 02:50:09.599 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 044 java chaincode disabled 2018-08-07 0","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:14:3","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"Hyperledger Fabric 操作命令 ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:15:0","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["fabric"],"content":"peer 命令 peer chaincode # 对链进行操作 peer channel # channel相关操作 peer logging # 设置日志级别 peer node # 启动、管理节点 peer version # 查看版本信息 upgrade 更新合约 更新合约相当于将合约重新实例化，并带有一个新的版本号。 更新合约之前，需要在所有的 peer节点 上安装(install)最新的合约，并使用新的版本号。 # 更新合约 # 首先安装(install)新的合约, 以本文为例, chaincode_example02, 初次安装版本号为 1.0 peer chaincode install -n example2 -p github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02 -v 1.1 # 更新版本为 1.1 的合约 peer chaincode upgrade -o orderer0.jicki.cn:7050 -C mychannel -n example2 -c '{\"Args\":[\"init\",\"A\",\"100\",\"B\",\"50\"]}' -P \"OR ('Org1MSP.member','Org2MSP.member')\" -v 1.1 # 旧版本的合约, 目前，fabric不支持合约的启动与暂停。要暂停或删除合约，只能到peer上手动删除容器。 # 查看 已经创建的 通道 (channel) peer channel list # 查看通道(channel) 的状态 -c(小写) 加 通道名称 peer channel getinfo -c mychannel # 查看已经 安装的 智能合约(chincode) peer chaincode list --installed # 查看已经 实例化的 智能合约(chincode) 需要使用 -C(大写) 加通道名称 peer chaincode -C mychannel list --instantiated ","date":"2018-08-07","objectID":"/hyperledger-fabric-1.2/:15:1","tags":null,"title":"hyperledger-fabric v 1.2 多机 多节点","uri":"/hyperledger-fabric-1.2/"},{"categories":["kubernetes"],"content":"kubernetes 1.11.0","date":"2018-06-29","objectID":"/kubernetes-1.11.0/","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":" Update kubernetes 1.11.0 , Update kubelet , kube-proxy 动态配置文件，减少 systemd 配置, 更新 kubelet https 连接，关闭 http. kubernetes 1.11.0 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:0:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"Urgent Upgrade Notes JSON configuration files that contain fields with incorrect case will no longer be valid. You must correct these files before upgrading. Pod priority and preemption is now enabled by default. pod 优先级 官方说明 https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:1:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"环境说明 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node, node-66 只做单纯 Node kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:2:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.64 kubernetes-64 172.16.1.65 kubernetes-65 172.16.1.66 kubernetes-66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:3:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:4:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:5:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@kubernetes-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:6:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm rpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm yum -y install docker-ce-17.03.2.ce # 查看安装 docker version Client: Version: 17.03.2-ce API version: 1.27 Go version: go1.7.5 Git commit: f5ec1e2 Built: Tue Jun 27 02:21:36 2017 OS/Arch: linux/amd64 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:7:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) # docker 版本 17.03.2 之前配置为 --graph=/opt/docker # docker 版本 17.04.x 之后配置为 --data-root=/opt/docker [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 1.11.0 etcd 支持最新版本为 v3.2.18 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:8:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz tar zxvf etcd-v3.2.18-linux-amd64.tar.gz cd etcd-v3.2.18-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:9:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@kubernetes-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 检查证书 [root@kubernetes-64 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:10:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:11:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:12:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:13:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.11.0/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet,kubeadm} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet,kubeadm} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:1","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@kubernetes-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:2","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"生成 kubernetes 配置文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:3","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:4","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@kubernetes-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:5","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 40179b02a8f6da07d90392ae966f7749 # 创建 encryption-config.yaml 配置 cat \u003e encryption-config.yaml \u003c\u003cEOF kind: EncryptionConfig apiVersion: v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: 40179b02a8f6da07d90392ae966f7749 - identity: {} EOF # 拷贝 cp encryption-config.yaml /etc/kubernetes/ scp encryption-config.yaml 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 \u003e 官方说明 https://kubernetes.io/docs/tasks/debug-application-cluster/audit/ \u003e \u003e 如下为最低限度的日志审核 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:6","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --anonymous-auth=false \\ --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # --experimental-encryption-provider-config ，替代之前 token.csv 文件 # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:7","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver # 如果报错 请使用 journalctl -f -t kube-apiserver 和 journalctl -u kube-apiserver 来定位问题 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:8","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager 新增几个配置，用于自动 续期证书 –feature-gates=RotateKubeletServerCertificate=true –experimental-cluster-signing-duration=86700h0m0s # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --feature-gates=RotateKubeletServerCertificate=true \\ --controllers=*,tokencleaner,bootstrapsigner \\ --experimental-cluster-signing-duration=86700h0m0s \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=5m0s \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:9","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager # 如果报错 请使用 journalctl -f -t kube-controller-manager 和 journalctl -u kube-controller-manager 来定位问题 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:10","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:11","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:12","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@kubernetes-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@kubernetes-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:13","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 kubelet 认证 kubelet 授权 kube-apiserver 的一些操作 exec run logs 等 # RBAC 只需创建一次就可以 kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:14","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 bootstrap kubeconfig 文件 注意: token 生效时间为 1day , 超过时间未创建自动失效，需要重新创建 token # 创建 集群所有 kubelet 的 token [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-64 --kubeconfig ~/.kube/config I0705 14:42:22.587674 90997 feature_gate.go:230] feature gates: \u0026{map[]} 1jezb7.izm7refwnj3umncy [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-65 --kubeconfig ~/.kube/config I0705 14:42:30.553287 91021 feature_gate.go:230] feature gates: \u0026{map[]} 1ua4d4.9bluufy3esw4lch6 [root@kubernetes-64 kubernetes]# kubeadm token create --description kubelet-bootstrap-token --groups system:bootstrappers:kubernetes-66 --kubeconfig ~/.kube/config I0705 14:42:35.681003 91047 feature_gate.go:230] feature gates: \u0026{map[]} r8llj2.itme3y54ok531ops # 查看生成的 token [root@kubernetes-64 kubernetes]# kubeadm token list --kubeconfig ~/.kube/config TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS 1jezb7.izm7refwnj3umncy 23h 2018-07-06T14:42:22+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-64 1ua4d4.9bluufy3esw4lch6 23h 2018-07-06T14:42:30+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-65 r8llj2.itme3y54ok531ops 23h 2018-07-06T14:42:35+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:kubernetes-66 以下为了区分 会先生成 node 名称加 bootstrap.kubeconfig 生成 kubernetes-64 # 生成 64 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=1jezb7.izm7refwnj3umncy \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-64-bootstrap.kubeconfig # 拷贝生成的 kubernetes-64-bootstrap.kubeconfig 文件 mv kubernetes-64-bootstrap.kubeconfig /etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-65 # 生成 65 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=1ua4d4.9bluufy3esw4lch6 \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-65-bootstrap.kubeconfig # 拷贝生成的 kubernetes-65-bootstrap.kubeconfig 文件 scp kubernetes-65-bootstrap.kubeconfig 172.16.1.65:/etc/kubernetes/bootstrap.kubeconfig 生成 kubernetes-66 # 生成 66 的 bootstrap.kubeconfig # 配置集群参数 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=r8llj2.itme3y54ok531ops \\ --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kubernetes-66-bootstrap.kubeconfig # 拷贝生成的 kubernetes-66-bootstrap.kubeconfig 文件 scp kubernetes-66-bootstrap.kubeconfig 172.16.1.66:/etc/kubernetes/bootstrap.kubeconfig # 配置 bootstrap RBAC 权限 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers # 否则报如下错误 failed to run Kubelet: cannot create certificate signing request: certificatesigningreques","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:15","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建自动批准相关 CSR 请求的 ClusterRole vi /etc/kubernetes/tls-instructs-csr.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"] # 导入 yaml 文件 [root@kubernetes-64 opt]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml clusterrole.rbac.authorization.k8s.io \"system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\" created # 查看 [root@kubernetes-64 opt]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"name\":\"system:certificates.k8s.io:certificatesigningreq... PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] # 将 ClusterRole 绑定到适当的用户组 # 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers # 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes # 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求 kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:16","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 关于 kubectl get node 中的 ROLES 的标签 单 Master 打标签 kubectl label node kubernetes-64 \\ node-role.kubernetes.io/master=”” 这里需要将 单Master 更改为 NoSchedule 更新标签命令为 kubectl taint nodes kubernetes-64 node-role.kubernetes.io/master=:NoSchedule 既 Master 又是 node 打标签 kubectl label node kubernetes-65 node-role.kubernetes.io/master=”” 单 Node 打标签 kubectl label node kubernetes-66 node-role.kubernetes.io/node=”” 关于删除 label 可使用 - 号相连 如: kubectl label nodes kubernetes-65 node-role.kubernetes.io/node- 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 目前官方还只是 beta 阶段, 动态配置 json 的具体参数可以参考 https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go # 创建 kubelet 目录 vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.64\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"RotateCertificates\": true, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } # 如上配置: kubernetes-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:17","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:18","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"验证 nodes [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 17h v1.11.0 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:19","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"查看 kubelet 生成文件 [root@kubernetes-64 ~]# ls -lt /etc/kubernetes/ssl/kubelet-* -rw------- 1 root root 1374 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem lrwxrwxrwx 1 root root 58 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-current.pem -\u003e /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem -rw-r--r-- 1 root root 1050 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.crt -rw------- 1 root root 227 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.key ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:14:20","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:15:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@kubernetes-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:15:1","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 scp kube-proxy* 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:16:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.65:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:16:1","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm ipset conntrack 软件， 在 node 中安装 yum install ipset ipvsadm conntrack-tools.x86_64 -y yaml 配置文件中的 参数如下: https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/apis/kubeproxyconfig/types.go cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:16:2","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 # 检查 ipvs [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:16:3","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理； 在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server; node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口; 当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA; ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:17:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:17:1","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:17:2","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 systemd kubelet 配置 动态 kubelet 配置 官方说明 https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/ 目前官方还只是 beta 阶段, 动态配置 json 的具体参数可以参考 https://github.com/kubernetes/kubernetes/blob/release-1.10/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go # 创建 kubelet 目录 vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --config=/etc/kubernetes/kubelet.config.json \\ --cert-dir=/etc/kubernetes/ssl \\ --logtostderr=true \\ --v=2 [Install] WantedBy=multi-user.target # 创建 kubelet config 配置文件 vi /etc/kubernetes/kubelet.config.json { \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": { \"x509\": { \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" }, \"webhook\": { \"enabled\": true, \"cacheTTL\": \"2m0s\" }, \"anonymous\": { \"enabled\": false } }, \"authorization\": { \"mode\": \"Webhook\", \"webhook\": { \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" } }, \"address\": \"172.16.1.66\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"featureGates\": { \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true }, \"MaxPods\": \"512\", \"failSwapOn\": false, \"containerLogMaxSize\": \"10Mi\", \"containerLogMaxFiles\": 5, \"clusterDomain\": \"cluster.local.\", \"clusterDNS\": [\"10.254.0.2\"] } ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:17:3","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service cd /etc/kubernetes/ vi kube-proxy.config.yaml apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 172.16.1.66 clientConnection: kubeconfig: /etc/kubernetes/kube-proxy.kubeconfig clusterCIDR: 10.254.64.0/18 healthzBindAddress: 172.16.1.66:10256 hostnameOverride: kubernetes-66 kind: KubeProxyConfiguration metricsBindAddress: 172.16.1.66:10249 mode: \"ipvs\" # 创建 kube-proxy 目录 vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/kube-proxy.config.yaml \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl start kube-proxy systemctl status kube-proxy 配置 Flannel 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1_A3zzurG5vV40-FnyA8uWg rpm -ivh flannel-0.10.0-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=em1\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl enable docker systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:17:4","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 kubernetes-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@kubernetes-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 Flannel 网络的节点 里 curl [root@kubernetes-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:18:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml # vi coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/18 { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 } ... clusterIP: 10.254.0.2 # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:19:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@kubernetes-64 coredns]# kubectl apply -f coredns.yaml serviceaccount \"coredns\" created clusterrole \"system:coredns\" created clusterrolebinding \"system:coredns\" created configmap \"coredns\" created deployment \"coredns\" created service \"coredns\" created ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:20:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"查看 coredns 服务 [root@kubernetes-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-6bd7d5dbb5-jh4fj 1/1 Running 0 19s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/coredns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 19s ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:21:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"检查日志 [root@kubernetes-64 coredns]# kubectl logs -n kube-system coredns-6bd7d5dbb5-jh4fj .:53 CoreDNS-1.1.3 linux/amd64, go1.10, 231c2c0e 2018/04/23 04:26:47 [INFO] CoreDNS-1.1.3 2018/04/23 04:26:47 [INFO] linux/amd64, go1.10, 231c2c0e ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:22:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 创建的服务 [root@kubernetes-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@kubernetes-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@kubernetes-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:23:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"部署 DNS 自动伸缩 按照 node 数量 自动伸缩 dns 数量 vi dns-auto-scaling.yaml kind: ServiceAccount apiVersion: v1 metadata: name: kube-dns-autoscaler namespace: kube-system labels: addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups: [\"\"] resources: [\"nodes\"] verbs: [\"list\"] - apiGroups: [\"\"] resources: [\"replicationcontrollers/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"extensions\"] resources: [\"deployments/scale\", \"replicasets/scale\"] verbs: [\"get\", \"update\"] - apiGroups: [\"\"] resources: [\"configmaps\"] verbs: [\"get\", \"create\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:kube-dns-autoscaler labels: addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount name: kube-dns-autoscaler namespace: kube-system roleRef: kind: ClusterRole name: system:kube-dns-autoscaler apiGroup: rbac.authorization.k8s.io --- apiVersion: apps/v1 kind: Deployment metadata: name: kube-dns-autoscaler namespace: kube-system labels: k8s-app: kube-dns-autoscaler kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile spec: selector: matchLabels: k8s-app: kube-dns-autoscaler template: metadata: labels: k8s-app: kube-dns-autoscaler annotations: scheduler.alpha.kubernetes.io/critical-pod: '' spec: priorityClassName: system-cluster-critical containers: - name: autoscaler image: jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 resources: requests: cpu: \"20m\" memory: \"10Mi\" command: - /cluster-proportional-autoscaler - --namespace=kube-system - --configmap=kube-dns-autoscaler - --target=Deployment/coredns - --default-params={\"linear\":{\"coresPerReplica\":256,\"nodesPerReplica\":16,\"preventSinglePointFailure\":true}} - --logtostderr=true - --v=2 tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" serviceAccountName: kube-dns-autoscaler # 导入文件 [root@kubernetes-64 coredns]# kubectl apply -f dns-auto-scaling.yaml serviceaccount/kube-dns-autoscaler created clusterrole.rbac.authorization.k8s.io/system:kube-dns-autoscaler created clusterrolebinding.rbac.authorization.k8s.io/system:kube-dns-autoscaler created deployment.apps/kube-dns-autoscaler created 部署 Ingress 与 Dashboard ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:24:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"部署 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard 官方 heapster 的github https://github.com/kubernetes/heapster ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:25:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"下载 heapster 相关 yaml 文件 wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml wget https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:25:1","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"下载 heapster 镜像下载 # 官方镜像 k8s.gcr.io/heapster-grafana-amd64:v4.4.3 k8s.gcr.io/heapster-amd64:v1.5.3 k8s.gcr.io/heapster-influxdb-amd64:v1.3.3 # 个人的镜像 jicki/heapster-grafana-amd64:v4.4.3 jicki/heapster-amd64:v1.5.3 jicki/heapster-influxdb-amd64:v1.3.3 # 替换所有yaml 镜像地址 sed -i 's/k8s\\.gcr\\.io/jicki/g' *.yaml ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:25:2","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"修改 yaml 文件 # heapster.yaml 文件 #### 修改如下部分 ##### 因为 kubelet 启用了 https 所以如下配置需要增加 https 端口 - --source=kubernetes:https://kubernetes.default 修改为 - --source=kubernetes:https://kubernetes.default?kubeletHttps=true\u0026kubeletPort=10250\u0026insecure=true # heapster-rbac.yaml 文件 #### 修改为部分 ##### 将 serviceAccount kube-system:heapster 与 ClusterRole system:kubelet-api-admin 绑定，授予它调用 kubelet API 的权限； kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: heapster roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:heapster subjects: - kind: ServiceAccount name: heapster namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: heapster-kubelet-api roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kubelet-api-admin subjects: - kind: ServiceAccount name: heapster namespace: kube-system # 导入所有的文件 [root@kubernetes-64 heapster]# kubectl apply -f . deployment.extensions/monitoring-grafana created service/monitoring-grafana created clusterrolebinding.rbac.authorization.k8s.io/heapster created serviceaccount/heapster created deployment.extensions/heapster created service/heapster created deployment.extensions/monitoring-influxdb created service/monitoring-influxdb created # 查看运行 [root@kubernetes-64 heapster]# kubectl get pods -n kube-system | grep -E 'heapster|monitoring' heapster-545d9555d4-lm5fs 1/1 Running 0 1m monitoring-grafana-59b4f6d8b7-ft2gv 1/1 Running 0 1m monitoring-influxdb-f6bcc9795-9zjnl 1/1 Running 0 1m ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:25:3","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"部署 dashboard ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:26:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3 # 个人的镜像 jicki/kubernetes-dashboard-amd64:v1.8.3 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:26:1","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:26:2","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/k8s\\.gcr\\.io/jicki/g' * # 导入文件 [root@kubernetes-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@kubernetes-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-5984fb8cbb-77dl4 1/1 Running 0 3h po/coredns-5984fb8cbb-9hdwt 1/1 Running 0 3h po/kubernetes-dashboard-78bcdc4d64-x6fhq 1/1 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 3h svc/kubernetes-dashboard ClusterIP 10.254.18.143 \u003cnone\u003e 443/TCP 14s ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:26:3","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:27:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@kubernetes-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready master 1d v1.11.0 kubernetes-65 Ready master 1d v1.11.0 kubernetes-66 Ready node 1d v1.11.0 # 对 65 与 66 打上 label [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-65 ingress=proxy node \"kubernetes-65\" labeled [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-66 ingress=proxy node \"kubernetes-66\" labeled # 打完标签以后 [root@kubernetes-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 32m v1.11.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubernetes-64 kubernetes-65 Ready \u003cnone\u003e 17m v1.11.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-65 kubernetes-66 Ready \u003cnone\u003e 4m v1.11.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-66 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.4 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.16.2 # 国内镜像 jicki/defaultbackend:1.4 jicki/nginx-ingress-controller:0.16.2 # 下载 yaml 文件 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务 # tcp 例子 apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 9000: \"default/tomcat:8080\" # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中 # udp 例子 apiVersion: v1 kind: ConfigMap metadata: name: udp-services namespace: ingress-nginx data: 53: \"kube-system/kube-dns:53\" # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi with-rbac.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 # 导入 yaml 文件 [root@kubernetes-64 ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@kubernetes-64 nginx-ingress]# kubectl apply -f . configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created namespace \"ingress-nginx\" configured serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 66 中 [root@kubernetes-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE default","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:28:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"部署 monitoring k8s 运维相关 ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:29:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:30:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2018-06-29","objectID":"/kubernetes-1.11.0/:31:0","tags":null,"title":"kubernetes 1.11.0","uri":"/kubernetes-1.11.0/"},{"categories":["fabric"],"content":"hyperledger-fabric v1.1.0","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":" 单机版 手动部署 fabric v1.1.0 实例为1.0 版本的, 使用以及说明。 部署 hyperledger-fabric v1.1.0 ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:0:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"官方地址 文档以官方文档为主 http://hyperledger-fabric.readthedocs.io/en/release-1.1/prereqs.html # 官网 github https://github.com/hyperledger/fabric ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:1:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"环境准备 安装 Docker (用于 fabric 服务启动) # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 安装 docker yum -y install docker-ce # 启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 查看 docker 版本 docker version Client: Version: 17.05.0-ce API version: 1.29 Go version: go1.7.5 Git commit: 89658be Built: Thu May 4 22:06:25 2017 OS/Arch: linux/amd64 Server: Version: 17.05.0-ce API version: 1.29 (minimum version 1.12) Go version: go1.7.5 Git commit: 89658be Built: Thu May 4 22:06:25 2017 OS/Arch: linux/amd64 Experimental: false 安装 Docker-compose (用于 docker 容器服务统一管理 编排) # 安装 pip curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py python get-pip.py # 安装 docker-compose pip install docker-compose docker-compose version docker-compose version 1.16.1, build 6d1ac219 docker-py version: 2.5.1 CPython version: 2.7.5 OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013 Golang (用于 fabric cli 服务的调用， ca 服务证书生成 ) mkdir -p /opt/golang mkdir -p /opt/gopath # 国外地址 curl -O https://storage.googleapis.com/golang/go1.10.linux-amd64.tar.gz # 国内地址 curl -O https://studygolang.com/dl/golang/go1.10.linux-amd64.tar.gz # 解压 tar zxvf go1.10.linux-amd64.tar.gz # 配置环境变量 vi /etc/profile 添加如下 # golang env export PATH=$PATH:/opt/golang/go/bin export GOPATH=/opt/gopath # 生效配置 source /etc/profile # 查看配置 go version go version go1.10 linux/amd64 ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:2:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"环境规划 相关hostname 必须配置 dns 节点标识 hostname IP 开放端口 系统 orderer节点 orderer.jicki.cn 192.168.168.100 7050 CentOS 7 x64 ca节点 ca0.org1.jicki.cn 192.168.168.100 7054 CentOS 7 x64 peer节点 peer0.org1.jicki.cn 192.168.168.100 7051, 7052, 7053 CentOS 7 x64 ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:3:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"Hyperledger Fabric 源码 # 下载 Fabric 源码, 源码中 import 的路径为github.com/hyperledger/fabric ,所以我们要按照这个路径 mkdir -p /opt/jicki/github.com/hyperledger cd /opt/jicki/github.com/hyperledger git clone https://github.com/hyperledger/fabric # 文件如下: [root@localhost fabric]# ls -lt 总用量 580 drwxr-xr-x 7 root root 138 6月 5 09:29 vendor -rw-r--r-- 1 root root 301 6月 5 09:29 tox.ini drwxr-xr-x 2 root root 56 6月 5 09:29 unit-test drwxr-xr-x 7 root root 134 6月 5 09:29 test drwxr-xr-x 2 root root 4096 6月 5 09:29 scripts -rw-r--r-- 1 root root 316 6月 5 09:29 settings.gradle drwxr-xr-x 3 root root 91 6月 5 09:29 sampleconfig drwxr-xr-x 2 root root 4096 6月 5 09:29 release_notes drwxr-xr-x 11 root root 4096 6月 5 09:29 protos drwxr-xr-x 3 root root 30 6月 5 09:29 release drwxr-xr-x 9 root root 154 6月 5 09:29 peer drwxr-xr-x 3 root root 23 6月 5 09:29 proposals drwxr-xr-x 6 root root 126 6月 5 09:29 orderer drwxr-xr-x 6 root root 4096 6月 5 09:29 msp drwxr-xr-x 9 root root 130 6月 5 09:29 images drwxr-xr-x 2 root root 29 6月 5 09:29 gotools drwxr-xr-x 2 root root 4096 6月 5 09:29 idemix drwxr-xr-x 15 root root 4096 6月 5 09:29 gossip drwxr-xr-x 10 root root 4096 6月 5 09:29 examples drwxr-xr-x 4 root root 48 6月 5 09:29 events drwxr-xr-x 4 root root 137 6月 5 09:29 docs drwxr-xr-x 4 root root 4096 6月 5 09:29 devenv -rw-r--r-- 1 root root 3356 6月 5 09:29 docker-env.mk drwxr-xr-x 20 root root 4096 6月 5 09:29 core drwxr-xr-x 23 root root 4096 6月 5 09:29 common drwxr-xr-x 10 root root 4096 6月 5 09:29 bddtests -rw-r--r-- 1 root root 13 6月 5 09:29 ci.properties drwxr-xr-x 8 root root 4096 6月 5 09:29 bccsp -rw-r--r-- 1 root root 11358 6月 5 09:29 LICENSE -rwxr-xr-x 1 root root 17068 6月 5 09:29 Makefile -rw-r--r-- 1 root root 5678 6月 5 09:29 README.md -rw-r--r-- 1 root root 475471 6月 5 09:29 CHANGELOG.md -rw-r--r-- 1 root root 597 6月 5 09:29 CODE_OF_CONDUCT.md -rw-r--r-- 1 root root 664 6月 5 09:29 CONTRIBUTING.md ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:4:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"生成 Hyperledger Fabric 证书 # 下载官方证书生成软件(均为二进制文件) # 官方离线下载地址为 https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/ # 选择相应版本 CentOS 选择 linux-amd64-1.1.0 Mac 选择 darwin-amd64-1.1.0 # 下载地址为: https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/linux-amd64-1.1.0/hyperledger-fabric-linux-amd64-1.1.0.tar.gz cd /opt/jicki wget https://nexus.hyperledger.org/content/repositories/releases/org/hyperledger/fabric/hyperledger-fabric/linux-amd64-1.1.0/hyperledger-fabric-linux-amd64-1.1.0.tar.gz tar zxvf hyperledger-fabric-linux-amd64-1.1.0.tar.gz # 解压后是 一个 bin 与 一个 config 目录 [root@localhost jicki]# tree bin/ bin/ ├── configtxgen ├── configtxlator ├── cryptogen ├── get-docker-images.sh ├── orderer └── peer 0 directories, 6 files # 为方便使用 我们配置一个 环境变量 vi /etc/profile # fabric env export PATH=$PATH:/opt/jicki/bin # 使文件生效 source /etc/profile # 下载1.0 版本的 configtx.yaml 与 crypto-config.yaml 。 cd /opt/jicki/ wget https://raw.githubusercontent.com/hyperledger/fabric/v1.0.0/examples/e2e_cli/crypto-config.yaml wget https://raw.githubusercontent.com/hyperledger/fabric/v1.0.0/examples/e2e_cli/configtx.yaml # 这里修改相应 jicki.cn 为 jicki.cn sed -i 's/example\\.com/jicki\\.me/g' *.yaml # 然后这里使用 cryptogen 软件来生成相应的证书了 [root@localhost jicki]# cryptogen generate --config=./crypto-config.yaml org1.jicki.cn org2.jicki.cn # 生成一个 crypto-config 证书目录 # 这里使用 configtxgen 来创建 创世区块 # 首先需要创建一个文件夹 mkdir -p /opt/jicki/channel-artifacts # 创建 创世区块 TwoOrgsOrdererGenesis 名称为 configtx.yaml 中 Profiles 字段下的 [root@localhost jicki]# configtxgen -profile TwoOrgsOrdererGenesis -outputBlock ./channel-artifacts/genesis.block 2018-06-05 10:26:12.714 CST [common/tools/configtxgen] main -\u003e INFO 001 Loading configuration 2018-06-05 10:26:12.726 CST [common/tools/configtxgen] doOutputBlock -\u003e INFO 002 Generating genesis block 2018-06-05 10:26:12.726 CST [common/tools/configtxgen] doOutputBlock -\u003e INFO 003 Writing genesis block # 创世区块 是在 orderer 服务中使用 [root@localhost jicki]# ls -lt channel-artifacts/ 总用量 12 -rw-r--r-- 1 root root 8951 6月 5 10:26 genesis.block # 下面来生成一个 peer 服务 中使用的 tx 文件 TwoOrgsChannel 名称为 configtx.yaml 中 Profiles 字段下的 # 这里配置的 channelID 为 mychannel , 后续使用到 [root@localhost jicki]# configtxgen -profile TwoOrgsChannel -outputCreateChannelTx ./channel-artifacts/mychannel.tx -channelID mychannel 2018-06-05 10:30:25.279 CST [common/tools/configtxgen] main -\u003e INFO 001 Loading configuration 2018-06-05 10:30:25.291 CST [common/tools/configtxgen] doOutputChannelCreateTx -\u003e INFO 002 Generating new channel configtx 2018-06-05 10:30:25.323 CST [common/tools/configtxgen] doOutputChannelCreateTx -\u003e INFO 003 Writing new channel tx [root@localhost jicki]# ls -lt channel-artifacts/ 总用量 16 -rw-r--r-- 1 root root 308 6月 5 10:30 mychannel.tx -rw-r--r-- 1 root root 8951 6月 5 10:26 genesis.block ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:5:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric Orderer # 创建文件 docker-compose-orderer.yaml # 创建于 /opt/jicki 目录下 vi docker-compose-orderer.yaml version: '2' services: orderer.jicki.cn: container_name: orderer.jicki.cn image: hyperledger/fabric-orderer environment: - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default # - ORDERER_GENERAL_LOGLEVEL=error - ORDERER_GENERAL_LOGLEVEL=debug - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_GENERAL_LISTENPORT=7050 #- ORDERER_GENERAL_GENESISPROFILE=AntiMothOrdererGenesis - ORDERER_GENERAL_GENESISMETHOD=file - ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block - ORDERER_GENERAL_LOCALMSPID=OrdererMSP - ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #- ORDERER_GENERAL_LEDGERTYPE=ram #- ORDERER_GENERAL_LEDGERTYPE=file # enabled TLS - ORDERER_GENERAL_TLS_ENABLED=false - ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.key - ORDERER_GENERAL_TLS_CERTIFICATE=/var/hyperledger/orderer/tls/server.crt - ORDERER_GENERAL_TLS_ROOTCAS=[/var/hyperledger/orderer/tls/ca.crt] working_dir: /opt/gopath/src/github.com/hyperledger/fabric command: orderer volumes: - ./channel-artifacts/genesis.block:/var/hyperledger/orderer/orderer.genesis.block - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer.jicki.cn/msp:/var/hyperledger/orderer/msp - ./crypto-config/ordererOrganizations/jicki.cn/orderers/orderer.jicki.cn/tls/:/var/hyperledger/orderer/tls networks: default: aliases: - jicki ports: - 7050:7050 ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:6:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"配置 Hyperledger Fabric Peer # 创建 peer yaml 文件 # 创建于 /opt/jicki 目录下 # 注: 如下目录中相关的 ca 证书请替换为 各自生成的, 本文目录为 /opt/jicki/crypto-config/peerOrganizations/org1.jicki.cn/ca # cli 部分 ./chaincode/go/:/opt/gopath/src/github.com/hyperledger/fabric/jicki/chaincode/go # 为 智能合约的目录 我们约定为这个目录 需要预先创建 mkdir -p /opt/jicki/chaincode/go cd /opt/jicki/chaincode/go # 创建以后~我们拷贝官方的 例子进来，方便后面进行合约测试 cp -r /opt/jicki/github.com/hyperledger/fabric/examples/chaincode/go/chaincode_example0* /opt/jicki/chaincode/go/ # 官方这里有5个例子 [root@localhost jicki]# ls -lt chaincode/go/ 总用量 0 drwxr-xr-x 2 root root 81 6月 5 11:06 chaincode_example02 drwxr-xr-x 2 root root 69 6月 5 11:06 chaincode_example03 drwxr-xr-x 2 root root 69 6月 5 11:06 chaincode_example04 drwxr-xr-x 2 root root 81 6月 5 11:06 chaincode_example05 drwxr-xr-x 2 root root 43 6月 5 11:06 chaincode_example01 vi docker-compose-peer.yaml version: '2' services: couchdb: container_name: couchdb image: hyperledger/fabric-couchdb # Comment/Uncomment the port mapping if you want to hide/expose the CouchDB service, # for example map it to utilize Fauxton User Interface in dev environments. ports: - \"5984:5984\" volumes: # 数据持久化，用于存储链码值 - /opt/jicki/couchdb/data:/opt/couchdb/data ca0.org1.jicki.cn: container_name: ca0.org1.jicki.cn image: hyperledger/fabric-ca environment: - FABRIC_CA_HOME=/etc/hyperledger/fabric-ca-server - FABRIC_CA_SERVER_CA_NAME=ca - FABRIC_CA_SERVER_TLS_ENABLED=false - FABRIC_CA_SERVER_TLS_CERTFILE=/etc/hyperledger/fabric-ca-server-config/ca.org1.jicki.cn-cert.pem - FABRIC_CA_SERVER_TLS_KEYFILE=/etc/hyperledger/fabric-ca-server-config/ebed980e19743358897c6c7e9069dbc789a872097af8d142bdc69f0ac60c89d5_sk ports: - \"7054:7054\" command: sh -c 'fabric-ca-server start --ca.certfile /etc/hyperledger/fabric-ca-server-config/ca.org1.jicki.cn-cert.pem --ca.keyfile /etc/hyperledger/fabric-ca-server-config/ebed980e19743358897c6c7e9069dbc789a872097af8d142bdc69f0ac60c89d5_sk -b admin:adminpw -d' volumes: - ./crypto-config/peerOrganizations/org1.jicki.cn/ca/:/etc/hyperledger/fabric-ca-server-config peer0.org1.jicki.cn: container_name: peer0.org1.jicki.cn image: hyperledger/fabric-peer environment: - CORE_LEDGER_STATE_STATEDATABASE=CouchDB - CORE_LEDGER_STATE_COUCHDBCONFIG_COUCHDBADDRESS=couchdb:5984 - CORE_PEER_ID=peer0.org1.jicki.cn - CORE_PEER_NETWORKID=jicki - CORE_PEER_ADDRESS=peer0.org1.jicki.cn:7051 - CORE_PEER_CHAINCODELISTENADDRESS=peer0.org1.jicki.cn:7052 - CORE_PEER_GOSSIP_EXTERNALENDPOINT=peer0.org1.jicki.cn:7051 - CORE_PEER_LOCALMSPID=Org1MSP - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # the following setting starts chaincode containers on the same # bridge network as the peers # https://docs.docker.com/compose/networking/ - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_VM_DOCKER_HOSTCONFIG_NETWORKMODE=jicki_default - CORE_PEER_GOSSIP_SKIPHANDSHAKE=true - CORE_PEER_GOSSIP_USELEADERELECTION=true - CORE_PEER_GOSSIP_ORGLEADER=false - CORE_PEER_PROFILE_ENABLED=false - CORE_PEER_TLS_ENABLED=false - CORE_PEER_TLS_CERT_FILE=/etc/hyperledger/fabric/tls/server.crt - CORE_PEER_TLS_KEY_FILE=/etc/hyperledger/fabric/tls/server.key - CORE_PEER_TLS_ROOTCERT_FILE=/etc/hyperledger/fabric/tls/ca.crt volumes: - /var/run/:/host/var/run/ - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/msp:/etc/hyperledger/fabric/msp - ./crypto-config/peerOrganizations/org1.jicki.cn/peers/peer0.org1.jicki.cn/tls:/etc/hyperledger/fabric/tls # 数据持久化, 存储安装，以及实例化智能合约的数据 - /opt/jicki/peer0:/var/hyperledger/production working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start ports: - 7051:7051 - 7052:7052 - 7053:7053 depends_on: - couchdb networks: default: aliases: - jicki cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # - CORE_LOGGING_LEVEL=ERROR - CORE_LOGGING_LEVEL=DEBUG - CORE_PEER_ID=cli - CORE_PEER_ADDRESS=peer0.","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:7:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"启动 Hyperledger Fabric 服务 # 启动 orderer 服务 [root@localhost jicki]# docker-compose -f docker-compose-orderer.yaml up -d Creating network \"jicki_default\" with the default driver Pulling orderer.jicki.cn (hyperledger/fabric-orderer:latest)... latest: Pulling from hyperledger/fabric-orderer 1be7f2b886e8: Pull complete 6fbc4a21b806: Pull complete c71a6f8e1378: Pull complete 4be3072e5a37: Pull complete 06c6d2f59700: Pull complete 4d536120d8a5: Pull complete 0baaf9ec263e: Pull complete 770563795186: Pull complete 61d33418a569: Pull complete b1b98004e7c6: Pull complete Digest: sha256:7a0a6ca2bbddff69ddf63615cdddfe46bf5f2fe7c55530a092d597d99bd2a4bb Status: Downloaded newer image for hyperledger/fabric-orderer:latest Creating orderer.jicki.cn ... Creating orderer.jicki.cn ... done # 启动 peer 服务 [root@localhost jicki]# docker-compose -f docker-compose-peer.yaml up -d Creating ca0.org1.jicki.cn ... Creating couchdb ... Creating couchdb ... donee ... done Creating ca0.org1.jicki.cn Creating peer0.org1.jicki.cn ... Creating peer0.org1.jicki.cn ... done Creating cli ... Creating cli ... done # 下载的镜像 [root@payment jicki]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE hyperledger/fabric-couchdb latest 35228d48a25a 2 months ago 1.56GB hyperledger/fabric-ca latest 72617b4fa9b4 2 months ago 299MB hyperledger/fabric-tools latest b7bfddf508bc 2 months ago 1.46GB hyperledger/fabric-orderer latest ce0c810df36a 2 months ago 180MB hyperledger/fabric-peer latest b023f9be0771 2 months ago 187MB # 查看启动的服务 [root@localhost jicki]# docker ps -a 77962643125a hyperledger/fabric-tools \"/bin/bash\" About a minute ago Up About a minute cli 8334cd884f99 hyperledger/fabric-peer \"peer node start\" About a minute ago Up About a minute 0.0.0.0:7051-7053-\u003e7051-7053/tcp peer0.org1.jicki.cn 7faf1692b1c8 hyperledger/fabric-ca \"sh -c 'fabric-ca-...\" About a minute ago Up About a minute 0.0.0.0:7054-\u003e7054/tcp ca0.org1.jicki.cn c3d581ca1957 hyperledger/fabric-couchdb \"tini -- /docker-e...\" About a minute ago Up About a minute 4369/tcp, 9100/tcp, 0.0.0.0:5984-\u003e5984/tcp couchdb 435f6268ed57 hyperledger/fabric-orderer \"orderer\" 39 minutes ago Up 39 minutes 0.0.0.0:7050-\u003e7050/tcp ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:8:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"Hyperledger Fabric 创建Channel 加盟 # 上面我们创建了 cli 容器，我们可以直接进入 容器里操作 [root@localhost jicki]# docker exec -it cli bash root@77962643125a:/opt/gopath/src/github.com/hyperledger/fabric/peer# # 执行 创建命令 peer channel create -o orderer.jicki.cn:7050 -c mychannel -t 50 -f ./channel-artifacts/mychannel.tx # 输出如下: 2018-06-05 03:58:27.812 UTC [msp] GetLocalMSP -\u003e DEBU 001 Returning existing local MSP 2018-06-05 03:58:27.812 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 002 Obtaining default signing identity 2018-06-05 03:58:27.813 UTC [channelCmd] InitCmdFactory -\u003e INFO 003 Endorser and orderer connections initialized 2018-06-05 03:58:27.813 UTC [msp] GetLocalMSP -\u003e DEBU 004 Returning existing local MSP 2018-06-05 03:58:27.813 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 005 Obtaining default signing identity 2018-06-05 03:58:27.814 UTC [msp] GetLocalMSP -\u003e DEBU 006 Returning existing local MSP 2018-06-05 03:58:27.814 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 007 Obtaining default signing identity 2018-06-05 03:58:27.814 UTC [msp/identity] Sign -\u003e DEBU 008 Sign: plaintext: 0A96060A074F7267314D5350128A062D...53616D706C65436F6E736F727469756D 2018-06-05 03:58:27.816 UTC [msp/identity] Sign -\u003e DEBU 009 Sign: digest: B98D97859C943C258DC17F79C1AFDB97259206D8459A523B0D2063DD6A6EDE78 2018-06-05 03:58:27.817 UTC [msp] GetLocalMSP -\u003e DEBU 00a Returning existing local MSP 2018-06-05 03:58:27.817 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 00b Obtaining default signing identity 2018-06-05 03:58:27.817 UTC [msp] GetLocalMSP -\u003e DEBU 00c Returning existing local MSP 2018-06-05 03:58:27.817 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 00d Obtaining default signing identity 2018-06-05 03:58:27.817 UTC [msp/identity] Sign -\u003e DEBU 00e Sign: plaintext: 0ACD060A1508021A0608E394D8D80522...38F301FCA102E04B2C4451A710338CE6 2018-06-05 03:58:27.817 UTC [msp/identity] Sign -\u003e DEBU 00f Sign: digest: 5DF2DEECA8C542FB555728C1AF37A7135D2E73EC5A70C01DA110FA5FB7A4350B 2018-06-05 03:58:27.855 UTC [msp] GetLocalMSP -\u003e DEBU 010 Returning existing local MSP 2018-06-05 03:58:27.855 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 011 Obtaining default signing identity 2018-06-05 03:58:27.855 UTC [msp] GetLocalMSP -\u003e DEBU 012 Returning existing local MSP 2018-06-05 03:58:27.855 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 013 Obtaining default signing identity 2018-06-05 03:58:27.855 UTC [msp/identity] Sign -\u003e DEBU 014 Sign: plaintext: 0ACD060A1508021A0608E394D8D80522...7AC84BD8C96D12080A021A0012021A00 2018-06-05 03:58:27.855 UTC [msp/identity] Sign -\u003e DEBU 015 Sign: digest: C895D56A7881A4C5DB5BB30224F3F481B5F164E328F1220F63453B280888D492 2018-06-05 03:58:27.856 UTC [channelCmd] readBlock -\u003e DEBU 016 Got status: \u0026{NOT_FOUND} 2018-06-05 03:58:27.856 UTC [msp] GetLocalMSP -\u003e DEBU 017 Returning existing local MSP 2018-06-05 03:58:27.856 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 018 Obtaining default signing identity 2018-06-05 03:58:27.857 UTC [channelCmd] InitCmdFactory -\u003e INFO 019 Endorser and orderer connections initialized 2018-06-05 03:58:28.057 UTC [msp] GetLocalMSP -\u003e DEBU 01a Returning existing local MSP 2018-06-05 03:58:28.057 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 01b Obtaining default signing identity 2018-06-05 03:58:28.058 UTC [msp] GetLocalMSP -\u003e DEBU 01c Returning existing local MSP 2018-06-05 03:58:28.058 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 01d Obtaining default signing identity 2018-06-05 03:58:28.058 UTC [msp/identity] Sign -\u003e DEBU 01e Sign: plaintext: 0ACD060A1508021A0608E494D8D80522...0EE8DDD220F812080A021A0012021A00 2018-06-05 03:58:28.058 UTC [msp/identity] Sign -\u003e DEBU 01f Sign: digest: 3025E2B47E9058D643182FD23B581EA8504099AD34B3CDF7B672D62FC8CEDC0B 2018-06-05 03:58:28.062 UTC [channelCmd] readBlock -\u003e DEBU 020 Received block: 0 2018-06-05 03:58:28.062 UTC [main] main -\u003e INFO 021 Exiting..... # 创建以后生成文件 mychannel.block # ls -lt total 12 -rw-r--r-- 1 root root 11804 Jun 5 03:58 mychannel.block drwxr-xr-x 2 root root 57 Jun 5 02:30 channel-artifacts drwxr","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:9:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"Hyperledger Fabric 实例化测试 在上面我们已经拷贝了官方的例子，在 chaincode 下, 下面我们来测试一下 ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:10:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"安装智能合约 # 如上我们挂载的地址为 github.com/hyperledger/fabric/jicki/chaincode/go # 指定安装路径 安装 peer chaincode install -n mychannel -p github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02 -v 1.0 # 输出如下: 2018-06-05 04:03:55.916 UTC [msp] GetLocalMSP -\u003e DEBU 001 Returning existing local MSP 2018-06-05 04:03:55.916 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 002 Obtaining default signing identity 2018-06-05 04:03:55.917 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 003 Using default escc 2018-06-05 04:03:55.917 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 004 Using default vscc 2018-06-05 04:03:55.917 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 005 java chaincode disabled 2018-06-05 04:03:55.963 UTC [golang-platform] getCodeFromFS -\u003e DEBU 006 getCodeFromFS github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02 2018-06-05 04:03:56.105 UTC [golang-platform] func1 -\u003e DEBU 007 Discarding GOROOT package fmt 2018-06-05 04:03:56.105 UTC [golang-platform] func1 -\u003e DEBU 008 Discarding provided package github.com/hyperledger/fabric/core/chaincode/shim 2018-06-05 04:03:56.105 UTC [golang-platform] func1 -\u003e DEBU 009 Discarding provided package github.com/hyperledger/fabric/protos/peer 2018-06-05 04:03:56.105 UTC [golang-platform] func1 -\u003e DEBU 00a Discarding GOROOT package strconv 2018-06-05 04:03:56.106 UTC [golang-platform] GetDeploymentPayload -\u003e DEBU 00b done 2018-06-05 04:03:56.106 UTC [container] WriteFileToPackage -\u003e DEBU 00c Writing file to tarball: src/github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02/chaincode_example02.go 2018-06-05 04:03:56.107 UTC [container] WriteFileToPackage -\u003e DEBU 00d Writing file to tarball: src/github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02/chaincode_example02_test.go 2018-06-05 04:03:56.108 UTC [msp/identity] Sign -\u003e DEBU 00e Sign: plaintext: 0A93070A5B08031A0B08AC97D8D80510...3F79FC370000FFFF2584A7C4002A0000 2018-06-05 04:03:56.108 UTC [msp/identity] Sign -\u003e DEBU 00f Sign: digest: 00863649A2C1F4DCF7913323182FF13ADEA767FC30C25D83179D1ECFBEF06180 2018-06-05 04:03:56.124 UTC [chaincodeCmd] install -\u003e DEBU 010 Installed remotely response:\u003cstatus:200 payload:\"OK\" \u003e ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:10:1","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"实例化 Chaincode peer chaincode instantiate -o orderer.jicki.cn:7050 -C mychannel -n mychannel -c '{\"Args\":[\"init\",\"A\",\"10\",\"B\",\"10\"]}' -P \"OR ('Org1MSP.member')\" -v 1.0 # 输出如下: 2018-06-05 04:05:46.217 UTC [msp] GetLocalMSP -\u003e DEBU 001 Returning existing local MSP 2018-06-05 04:05:46.217 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 002 Obtaining default signing identity 2018-06-05 04:05:46.220 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 003 Using default escc 2018-06-05 04:05:46.221 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 004 Using default vscc 2018-06-05 04:05:46.221 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 005 java chaincode disabled 2018-06-05 04:05:46.221 UTC [msp/identity] Sign -\u003e DEBU 006 Sign: plaintext: 0A9E070A6608031A0B089A98D8D80510...314D53500A04657363630A0476736363 2018-06-05 04:05:46.221 UTC [msp/identity] Sign -\u003e DEBU 007 Sign: digest: 461D88DFC289669A30F4B32BCCAE9AC22EF7F473A991796D71C515FE3C9E710E 2018-06-05 04:06:07.391 UTC [msp/identity] Sign -\u003e DEBU 008 Sign: plaintext: 0A9E070A6608031A0B089A98D8D80510...743A646439C8D2AC5A7A105153A92C88 2018-06-05 04:06:07.391 UTC [msp/identity] Sign -\u003e DEBU 009 Sign: digest: 16B5C05D014187EE64710445C16DFD912C9BFFA637AB098A392D3FD8DA23D293 2018-06-05 04:06:07.393 UTC [main] main -\u003e INFO 00a Exiting..... ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:10:2","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"操作智能合约 # query 查询方法 # 查询 A 账户里的余额 peer chaincode query -C mychannel -n mychannel -c '{\"Args\":[\"query\",\"A\"]}' # 输出如下: 2018-06-05 04:07:20.190 UTC [msp] GetLocalMSP -\u003e DEBU 001 Returning existing local MSP 2018-06-05 04:07:20.190 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 002 Obtaining default signing identity 2018-06-05 04:07:20.190 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 003 Using default escc 2018-06-05 04:07:20.190 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 004 Using default vscc 2018-06-05 04:07:20.191 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 005 java chaincode disabled 2018-06-05 04:07:20.191 UTC [msp/identity] Sign -\u003e DEBU 006 Sign: plaintext: 0AA3070A6B08031A0B08F898D8D80510...6E6E656C1A0A0A0571756572790A0141 2018-06-05 04:07:20.191 UTC [msp/identity] Sign -\u003e DEBU 007 Sign: digest: 26BD0EFDB8015795543FDFBEF605A71870580A6DCE9B4E1285B009642116594F Query Result: 10 2018-06-05 04:07:20.206 UTC [main] main -\u003e INFO 008 Exiting..... # 可以看到 返回 Query Result: 10 账户上有10个币 # 查询 B 账户里的余额 peer chaincode query -C mychannel -n mychannel -c '{\"Args\":[\"query\",\"B\"]}' # 输出如下: 2018-06-05 04:09:08.238 UTC [msp] GetLocalMSP -\u003e DEBU 001 Returning existing local MSP 2018-06-05 04:09:08.238 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 002 Obtaining default signing identity 2018-06-05 04:09:08.238 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 003 Using default escc 2018-06-05 04:09:08.238 UTC [chaincodeCmd] checkChaincodeCmdParams -\u003e INFO 004 Using default vscc 2018-06-05 04:09:08.239 UTC [chaincodeCmd] getChaincodeSpec -\u003e DEBU 005 java chaincode disabled 2018-06-05 04:09:08.239 UTC [msp/identity] Sign -\u003e DEBU 006 Sign: plaintext: 0AA3070A6B08031A0B08E499D8D80510...6E6E656C1A0A0A0571756572790A0142 2018-06-05 04:09:08.239 UTC [msp/identity] Sign -\u003e DEBU 007 Sign: digest: 8DEB3D74945B3C274B215FB16B0B5EA468EB0B2BB8121949796EE0A7DD31EAC7 Query Result: 10 2018-06-05 04:09:08.256 UTC [main] main -\u003e INFO 008 Exiting..... # 可以看到 返回 Query Result: 10 账户上有10个币 # invoke 转账方法 # 从A账户 转账 5个币 到 B 账户 peer chaincode invoke -C mychannel -n mychannel -c '{\"Args\":[\"invoke\", \"A\", \"B\", \"5\"]}' # 输出如下: 2018-06-05 04:10:23.536 UTC [msp] GetLocalMSP -\u003e DEBU 001 Returning existing local MSP 2018-06-05 04:10:23.536 UTC [msp] GetDefaultSigningIdentity -\u003e DEBU 002 Obtaining default signing identity 2018-06-05 04:10:23.536 UTC [msp/identity] Sign -\u003e DEBU 003 Sign: plaintext: 0A94070A5C08011A0C08AF9AD8D80510...426C6F636B0A096D796368616E6E656C 2018-06-05 04:10:23.536 UTC [msp/identity] Sign -\u003e DEBU 004 Sign: digest: 39756FDF0F4C2116D88E1E929192F7F73A5693EA3AA7CB6D6088E8330F408847 2018-06-05 04:10:23.543 UTC [common/channelconfig] NewStandardValues -\u003e DEBU 005 Initializing protos for *channelconfig.ChannelProtos 2018-06-05 04:10:23.543 UTC [common/channelconfig] initializeProtosStruct -\u003e DEBU 006 Processing field: HashingAlgorithm 2018-06-05 04:10:23.543 UTC [common/channelconfig] initializeProtosStruct -\u003e DEBU 007 Processing field: BlockDataHashingStructure 2018-06-05 04:10:23.543 UTC [common/channelconfig] initializeProtosStruct -\u003e DEBU 008 Processing field: OrdererAddresses 2018-06-05 04:10:23.543 UTC [common/channelconfig] initializeProtosStruct -\u003e DEBU 009 Processing field: Consortium 2018-06-05 04:10:23.543 UTC [common/channelconfig] initializeProtosStruct -\u003e DEBU 00a Processing field: Capabilities 2018-06-05 04:10:23.543 UTC [common/channelconfig] NewStandardValues -\u003e DEBU 00b Initializing protos for *channelconfig.ApplicationProtos 2018-06-05 04:10:23.543 UTC [common/channelconfig] initializeProtosStruct -\u003e DEBU 00c Processing field: Capabilities 2018-06-05 04:10:23.543 UTC [common/channelconfig] NewStandardValues -\u003e DEBU 00d Initializing protos for *channelconfig.ApplicationOrgProtos 2018-06-05 04:10:23.543 UTC [common/channelconfig] initializeProtosStruct -\u003e DEBU 00e Processing field: AnchorPeers 2018-06-05 04:10:23.543 UTC [common/channelconfig] NewStandardValues -\u003e DEBU 00f Initializing protos for *channelconfig.OrganizationProtos 2018-06-05 0","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:10:3","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"Hyperledger Fabric 操作命令 ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:11:0","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["fabric"],"content":"peer 命令 peer chaincode # 对链进行操作 peer channel # channel相关操作 peer logging # 设置日志级别 peer node # 启动、管理节点 peer version # 查看版本信息 upgrade 更新合约 更新合约相当于将合约重新实例化，并带有一个新的版本号。 更新合约之前，需要在所有的 peer节点 上安装(install)最新的合约，并使用新的版本号。 # 更新合约 # 首先安装(install)新的合约, 以本文为例, chaincode_example02, 初次安装版本号为 1.0 peer chaincode install -n mychannel -p github.com/hyperledger/fabric/jicki/chaincode/go/chaincode_example02 -v 1.1 # 更新版本为 1.1 的合约 peer chaincode upgrade -o orderer.jicki.cn:7050 -C mychannel -n mychannel -c '{\"Args\":[\"init\",\"A\",\"10\",\"B\",\"10\"]}' -P \"OR ('Org1MSP.member')\" -v 1.1 # 旧版本的合约, 目前，fabric不支持合约的启动与暂停。要暂停或删除合约，只能到peer上手动删除容器。 # 查看 已经创建的 通道 (channel) peer channel list # 查看通道(channel) 的状态 -c(小写) 加 通道名称 peer channel getinfo -c mychannel # 查看已经 安装的 智能合约(chincode) peer chaincode list --installed # 查看已经 实例化的 智能合约(chincode) 需要使用 -C(大写) 加通道名称 peer chaincode -C mychannel list --instantiated ","date":"2018-06-05","objectID":"/hyperledger-fabric-1.1.0/:11:1","tags":null,"title":"hyperledger-fabric v1.1.0","uri":"/hyperledger-fabric-1.1.0/"},{"categories":["kubernetes"],"content":"kubernetes 1.10.1","date":"2018-04-23","objectID":"/kubernetes-1.10.1/","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":" 更新版本到 1.10.1 ，修复了之前 beta.4 文章中 kubelet.kubeconfig 无法生成的问题。 更新了kubelet 自动续签证书，以及 自动注册 csr kubernetes 1.10.1 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:0:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"环境说明 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node, node-66 只做单纯 Node kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:1:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.64 kubernetes-64 172.16.1.65 kubernetes-65 172.16.1.66 kubernetes-66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:2:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:3:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:4:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@kubernetes-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:5:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm rpm -ivh docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm yum -y install docker-ce-17.03.1.ce # 查看安装 docker version Client: Version: 17.03.1-ce API version: 1.27 Go version: go1.7.5 Git commit: f5ec1e2 Built: Tue Jun 27 02:21:36 2017 OS/Arch: linux/amd64 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:6:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 官方 log 提示 etcd 版本为 3.1.12 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:7:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz tar zxvf etcd-v3.1.11-linux-amd64.tar.gz cd etcd-v3.1.11-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:8:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@kubernetes-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 检查证书 [root@kubernetes-64 ssl]# /opt/local/cfssl/cfssl-certinfo -cert etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:9:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:10:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:11:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:12:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.10.1/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kubelet} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:1","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@kubernetes-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:2","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"生成 kubernetes 配置文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:3","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:4","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@kubernetes-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:5","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' a2ae287d4417361144a67ac192e75016 # 创建 token.csv 文件 cd /opt/ssl vi token.csv a2ae287d4417361144a67ac192e75016,kubelet-bootstrap,10001,\"system:bootstrappers\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:6","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # k8s 1.8 开始需要 添加 --authorization-mode=Node # k8s 1.8 开始需要 添加 --admission-control=NodeRestriction # k8s 1.8 开始需要 添加 --audit-policy-file=/etc/kubernetes/audit-policy.yaml # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:7","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:8","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager 新增几个配置，用于自动 续期证书 –feature-gates=RotateKubeletServerCertificate=true –experimental-cluster-signing-duration=86700h0m0s # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --feature-gates=RotateKubeletServerCertificate=true \\ --experimental-cluster-signing-duration=86700h0m0s \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --node-monitor-grace-period=40s \\ --node-monitor-period=5s \\ --pod-eviction-timeout=5m0s \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:9","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:10","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:11","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:12","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@kubernetes-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@kubernetes-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:13","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:14","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 bootstrap kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=a2ae287d4417361144a67ac192e75016 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ # 拷贝到其他 scp bootstrap.kubeconfig 172.16.1.65:/etc/kubernetes/ scp bootstrap.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:15","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建自动批准相关 CSR 请求的 ClusterRole vi /etc/kubernetes/tls-instructs-csr.yaml kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver rules: - apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"] # 导入 yaml 文件 [root@kubernetes-64 opt]# kubectl apply -f /etc/kubernetes/tls-instructs-csr.yaml clusterrole.rbac.authorization.k8s.io \"system:certificates.k8s.io:certificatesigningrequests:selfnodeserver\" created # 查看 [root@kubernetes-64 opt]# kubectl describe ClusterRole/system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeserver Labels: \u003cnone\u003e Annotations: kubectl.kubernetes.io/last-applied-configuration={\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRole\",\"metadata\":{\"annotations\":{},\"name\":\"system:certificates.k8s.io:certificatesigningreq... PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- certificatesigningrequests.certificates.k8s.io/selfnodeserver [] [] [create] # 将 ClusterRole 绑定到适当的用户组 # 自动批准 system:bootstrappers 组用户 TLS bootstrapping 首次申请证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --group=system:bootstrappers # 自动批准 system:nodes 组用户更新 kubelet 自身与 apiserver 通讯证书的 CSR 请求 kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes # 自动批准 system:nodes 组用户更新 kubelet 10250 api 端口证书的 CSR 请求 kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:16","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 \u003e 配置为 node 本机 IP vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --feature-gates=RotateKubeletClientCertificate=true,RotateKubeletServerCertificate=true \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=2 [Install] WantedBy=multi-user.target # 如上配置: kubernetes-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.1 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.1 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:17","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:18","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"查看 TLS 认证 # 查看 csr 因为上面配置了 自签, 所以不需要手动 approved [root@kubernetes-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION csr-mz2hc 1m system:node:kubernetes-64 Approved,Issued node-csr-rhFVOW9po4i1VXjveDp4zzwPrkKeThdvW0JIVa7T0dk 1m kubelet-bootstrap Approved,Issued # 手动 增加 认证 # kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:19","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"验证 nodes [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready \u003cnone\u003e 1m v1.10.1 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:20","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"查看 kubelet 生成文件 [root@kubernetes-64 ~]# ls -lt /etc/kubernetes/ssl/kubelet-* -rw------- 1 root root 1374 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem lrwxrwxrwx 1 root root 58 4月 23 11:55 /etc/kubernetes/ssl/kubelet-server-current.pem -\u003e /etc/kubernetes/ssl/kubelet-server-2018-04-23-11-55-38.pem -rw-r--r-- 1 root root 1050 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.crt -rw------- 1 root root 227 4月 23 11:55 /etc/kubernetes/ssl/kubelet-client.key ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:13:21","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:14:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@kubernetes-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:14:1","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 scp kube-proxy* 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:15:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.65:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:15:1","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm 软件， 在 node 中安装 yum install ipvsadm -y # 创建 kube-proxy 目录 vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.65 \\ --hostname-override=kubernetes-65 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:15:2","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 检查 ipvs [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:15:3","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:16:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:16:1","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:16:2","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=kubernetes-66 \\ --pod-infra-container-image=jicki/pause-amd64:3.1 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=1 [Install] WantedBy=multi-user.target # 启动 kubelet 与 kube-proxy systemctl daemon-reload systemctl start kubelet systemctl status kubelet ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:16:3","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.66 \\ --hostname-override=kubernetes-66 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl start kube-proxy systemctl status kube-proxy ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:16:4","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"限制 POD 的调度 由于 master-64 只做 master 不做 pod 调度，所以禁止调度到 master-64中, Pod 的调度是通过 kubelet 服务来启动的，但是不启动 kubelet 的话，节点在 node 里是不可见的。 [root@kubernetes-64 ~]# kubectl cordon kubernetes-64 node \"kubernetes-64\" cordoned [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 29m v1.10.1 kubernetes-65 Ready \u003cnone\u003e 14m v1.10.1 kubernetes-66 Ready \u003cnone\u003e 34s v1.10.1 配置 Flannel 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1eStojia rpm -ivh flannel-0.9.1-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=em1\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:16:5","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 kubernetes-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@kubernetes-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 Flannel 网络的节点 里 curl [root@kubernetes-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:17:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml # vi coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/18 { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 } ... clusterIP: 10.254.0.2 # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:18:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@kubernetes-64 coredns]# kubectl apply -f coredns.yaml serviceaccount \"coredns\" created clusterrole \"system:coredns\" created clusterrolebinding \"system:coredns\" created configmap \"coredns\" created deployment \"coredns\" created service \"coredns\" created ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:19:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@kubernetes-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-6bd7d5dbb5-jh4fj 1/1 Running 0 19s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/coredns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 19s ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:20:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"检查日志 [root@kubernetes-64 coredns]# kubectl logs -n kube-system coredns-6bd7d5dbb5-jh4fj .:53 CoreDNS-1.1.1 linux/amd64, go1.10, 231c2c0e 2018/04/23 04:26:47 [INFO] CoreDNS-1.1.1 2018/04/23 04:26:47 [INFO] linux/amd64, go1.10, 231c2c0e ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:21:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 创建的服务 [root@kubernetes-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@kubernetes-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@kubernetes-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:22:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"部署 dashboard \u0026\u0026 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:23:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.8.3 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:24:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:25:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/k8s\\.gcr\\.io/jicki/g' * # 导入文件 [root@kubernetes-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@kubernetes-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-5984fb8cbb-77dl4 1/1 Running 0 3h po/coredns-5984fb8cbb-9hdwt 1/1 Running 0 3h po/kubernetes-dashboard-78bcdc4d64-x6fhq 1/1 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 3h svc/kubernetes-dashboard ClusterIP 10.254.18.143 \u003cnone\u003e 443/TCP 14s ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:26:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:27:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@kubernetes-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 32m v1.10.1 kubernetes-65 Ready \u003cnone\u003e 17m v1.10.1 kubernetes-66 Ready \u003cnone\u003e 3m v1.10.1 # 对 65 与 66 打上 label [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-65 ingress=proxy node \"kubernetes-65\" labeled [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-66 ingress=proxy node \"kubernetes-66\" labeled # 打完标签以后 [root@kubernetes-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 32m v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubernetes-64 kubernetes-65 Ready \u003cnone\u003e 17m v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-65 kubernetes-66 Ready \u003cnone\u003e 4m v1.10.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-66 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.4 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.0 # 国内镜像 jicki/defaultbackend:1.4 jicki/nginx-ingress-controller:0.10.0 # 下载 yaml 文件 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务 # tcp 例子 apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 9000: \"default/tomcat:8080\" # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中 # udp 例子 apiVersion: v1 kind: ConfigMap metadata: name: udp-services namespace: ingress-nginx data: 53: \"kube-system/kube-dns:53\" # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi with-rbac.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 # 导入 yaml 文件 [root@kubernetes-64 ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@kubernetes-64 nginx-ingress]# kubectl apply -f . configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created namespace \"ingress-nginx\" configured serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 66 中 [root@kubernetes-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTA","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:28:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"部署 monitoring k8s 运维相关 ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:29:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:30:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2018-04-23","objectID":"/kubernetes-1.10.1/:31:0","tags":["kubernetes"],"title":"kubernetes 1.10.1","uri":"/kubernetes-1.10.1/"},{"categories":["kubernetes"],"content":"OpenEBS to Kubernetes StorageClass","date":"2018-03-21","objectID":"/openebs-k8s-storageclass/","tags":["kubernetes","openebs"],"title":"OpenEBS to Kubernetes StorageClass","uri":"/openebs-k8s-storageclass/"},{"categories":["kubernetes"],"content":" OpenEBS是一个开源存储平台，为DevOps和容器环境提供持久和集装箱 块存储。 块存储一般用于 Mysql data, redis data, jenkins data, RabbitMQ data, 等数据类的存储 官方 github https://github.com/openebs/openebs OpenEBS to Kubernetes StorageClass ","date":"2018-03-21","objectID":"/openebs-k8s-storageclass/:0:0","tags":["kubernetes","openebs"],"title":"OpenEBS to Kubernetes StorageClass","uri":"/openebs-k8s-storageclass/"},{"categories":["kubernetes"],"content":"部署 OpenEBS 本文 基于 kubernetes 集群中部署这个 存储 # 下载 yaml 文件 wget https://openebs.github.io/charts/openebs-operator.yaml # 最后 StorageClass 部分，可配置容量，等，可配置多个 StorageClass，如下: apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: openebs-standard provisioner: openebs.io/provisioner-iscsi parameters: openebs.io/storage-pool: \"default\" openebs.io/jiva-replica-count: \"2\" openebs.io/volume-monitor: \"true\" openebs.io/capacity: 5G # 按照官方文档只需要直接导入 yaml 既可 kubectl apply -f https://openebs.github.io/charts/openebs-operator.yaml # 查看创建 [root@kubernetes-64 openebs]# kubectl get pod NAME READY STATUS RESTARTS AGE maya-apiserver-5fc675bc6-wl2t7 1/1 Running 0 48s openebs-provisioner-576cbb68d4-bdpwv 1/1 Running 0 48s # 查看 StorageClass [root@kubernetes-64 openebs]# kubectl get StorageClass NAME PROVISIONER AGE openebs-standard openebs.io/provisioner-iscsi 2m ","date":"2018-03-21","objectID":"/openebs-k8s-storageclass/:1:0","tags":["kubernetes","openebs"],"title":"OpenEBS to Kubernetes StorageClass","uri":"/openebs-k8s-storageclass/"},{"categories":["kubernetes"],"content":"测试 应用 这里注意，所有的node 必须安装 iscsi ，因为 openebs 是基于 iscsi 挂载的 [root@kubernetes-64 openebs]# yum install -y iscsi-initiator-utils [root@kubernetes-64 openebs]# vi test-nginx.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim annotations: volume.beta.kubernetes.io/storage-class: openebs-standard spec: accessModes: - ReadWriteOnce resources: requests: storage: 5G --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 1 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: openebs-volume mountPath: \"/usr/share/nginx/html\" volumes: - name: openebs-volume persistentVolumeClaim: claimName: test-claim # 导入配置 [root@kubernetes-64 openebs]# kubectl apply -f test-nginx.yaml deployment.extensions \"nginx-dm\" created # 查看服务 [root@kubernetes-64 openebs]# kubectl get pods NAME READY STATUS RESTARTS AGE maya-apiserver-5fc675bc6-wl2t7 1/1 Running 0 1h nginx-dm-7554876d67-j2kz4 1/1 Running 0 21m openebs-provisioner-576cbb68d4-bdpwv 1/1 Running 0 1h pvc-89c8c04b-2be9-11e8-93d3-44a8420b9988-ctrl-865fbf6bb6-f2n9q 2/2 Running 0 54m pvc-89c8c04b-2be9-11e8-93d3-44a8420b9988-rep-f97764db5-d9pj8 1/1 Running 0 54m pvc-89c8c04b-2be9-11e8-93d3-44a8420b9988-rep-f97764db5-lvzjn 1/1 Running 0 54m # 一个完整的 jenkins 例子 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: jenkins-claim annotations: volume.beta.kubernetes.io/storage-class: openebs-standard spec: accessModes: - ReadWriteOnce resources: requests: storage: 5G --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: jenkins name: jenkins-admin --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: jenkins-admin labels: k8s-app: jenkins roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: jenkins-admin namespace: default --- apiVersion: apps/v1beta1 kind: Deployment metadata: name: jenkins spec: replicas: 1 template: metadata: labels: app: jenkins spec: securityContext: fsGroup: 1000 serviceAccount: \"jenkins-admin\" containers: - name: jenkins image: jenkins/jenkins imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: web protocol: TCP - containerPort: 50000 name: agent protocol: TCP volumeMounts: - name: jenkinshome mountPath: /var/jenkins_home env: - name: JAVA_OPTS value: \"-Xms512m -Xmx1024m -XX:PermSize=512m -XX:MaxPermSize=1024m -Duser.timezone=Asia/Shanghai\" - name: TRY_UPGRADE_IF_NO_MARKER value: \"true\" volumes: - name: jenkinshome persistentVolumeClaim: claimName: jenkins-claim --- kind: Service apiVersion: v1 metadata: labels: app: jenkins name: jenkins spec: ports: - port: 8080 targetPort: 8080 name: web - port: 50000 targetPort: 50000 name: agent selector: app: jenkins --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: jenkins spec: rules: - host: jenkins.jicki.cn http: paths: - path: / backend: serviceName: jenkins servicePort: 8080 # 导入文件 [root@kubernetes-64 openebs]# kubectl apply -f jenkins-openebs.yaml persistentvolumeclaim \"jenkins-claim\" created serviceaccount \"jenkins-admin\" unchanged clusterrolebinding.rbac.authorization.k8s.io \"jenkins-admin\" configured deployment.apps \"jenkins\" created service \"jenkins\" created ingress.extensions \"jenkins\" created # 查看服务 [root@kubernetes-64 openebs]# kubectl get pods NAME READY STATUS RESTARTS AGE jenkins-847bf4f5b-knr8j 1/1 Running 0 21m maya-apiserver-5fc675bc6-zh4r9 1/1 Running 0 1h openebs-provisioner-576cbb68d4-s2dqt 1/1 Running 0 1h pvc-6ea040f0-2c21-11e8-93d3-44a8420b9988-ctrl-79f9fb7854-l7vtw 2/2 Running 0 21m pvc-6ea040f0-2c21-11e8-93d3-44a8420b9988-rep-5c55bb6788-85g56 1/1 Running 0 21m pvc-6ea040f0-2c21-11e8-93d3-44a8420b9988-rep-5c55bb6788-vmr9t 1/1 Running 0 21m # 查看 密码 [root@kubernetes-64 openebs]# kubectl logs jenkins-847bf4f5b-knr8j ","date":"2018-03-21","objectID":"/openebs-k8s-storageclass/:1:1","tags":["kubernetes","openebs"],"title":"OpenEBS to Kubernetes StorageClass","uri":"/openebs-k8s-storageclass/"},{"categories":["kubernetes"],"content":"验证存储 # 在 web ui 里创建一个 job # 查看 pod ，位于 kubernetes-66 中 [root@kubernetes-64 openebs]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE jenkins-847bf4f5b-cfghx 1/1 Running 0 4m 10.254.78.6 kubernetes-66 # 删除 pods [root@kubernetes-64 openebs]# kubectl delete pods/jenkins-847bf4f5b-cfghx pod \"jenkins-847bf4f5b-cfghx\" deleted # 查看重建状态 转移至 kubernetes-65 中 [root@kubernetes-64 openebs]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE jenkins-847bf4f5b-4dtqx 0/1 ContainerCreating 0 6s \u003cnone\u003e kubernetes-65 jenkins-847bf4f5b-cfghx 0/1 Terminating 0 5m 10.254.78.6 kubernetes-66 [root@kubernetes-64 openebs]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE jenkins-847bf4f5b-4dtqx 1/1 Running 0 38s 10.254.126.8 kubernetes-65 # 验证~ jenkins 数据是否存在 ","date":"2018-03-21","objectID":"/openebs-k8s-storageclass/:1:2","tags":["kubernetes","openebs"],"title":"OpenEBS to Kubernetes StorageClass","uri":"/openebs-k8s-storageclass/"},{"categories":["kubernetes"],"content":"kubernetes 1.10.beta4","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":" 更新1.10.beta4 测试版本，测试一些新功能，其次官方更新 ipvs 为 beta kubernetes 1.10.beta4 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:0:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"环境说明 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node, node-66 只做单纯 Node kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:1:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.64 kubernetes-64 172.16.1.65 kubernetes-65 172.16.1.66 kubernetes-66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:2:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:3:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:4:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@kubernetes-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:5:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem 172.16.1.65:/etc/kubernetes/ssl/ scp *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem 172.16.1.66:/etc/kubernetes/ssl/ scp *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm rpm -ivh docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm yum -y install docker-ce-17.03.1.ce # 查看安装 docker version Client: Version: 17.03.1-ce API version: 1.27 Go version: go1.7.5 Git commit: f5ec1e2 Built: Tue Jun 27 02:21:36 2017 OS/Arch: linux/amd64 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:6:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了， 官方 log 提示 etcd 版本为 3.1.12 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:7:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.1.12/etcd-v3.1.12-linux-amd64.tar.gz tar zxvf etcd-v3.1.11-linux-amd64.tar.gz cd etcd-v3.1.11-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:8:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@kubernetes-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:9:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:10:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:11:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:12:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.10.0-beta.4/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:1","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@kubernetes-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:2","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:3","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:4","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@kubernetes-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:5","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' df3b158fbdc425ae2ac70bbef0688921 # 创建 token.csv 文件 cd /opt/ssl vi token.csv df3b158fbdc425ae2ac70bbef0688921,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:6","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # k8s 1.8 开始需要 添加 --authorization-mode=Node # k8s 1.8 开始需要 添加 --admission-control=NodeRestriction # k8s 1.8 开始需要 添加 --audit-policy-file=/etc/kubernetes/audit-policy.yaml # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:7","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:8","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager –cluster-signing-cert-file 与 –cluster-signing-key-file 标签将被删除。 # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:9","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:10","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:11","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:12","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@kubernetes-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@kubernetes-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:13","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:14","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=df3b158fbdc425ae2ac70bbef0688921 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ # 拷贝到所有的机器里 scp bootstrap.kubeconfig 172.16.1.65:/etc/kubernetes/ scp bootstrap.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:15","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 \u003e 配置为 node 本机 IP mkdir /opt/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/opt/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=1 [Install] WantedBy=multi-user.target # 如上配置: kubernetes-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:16","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:17","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@kubernetes-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-Pu4QYp3NAwlC6o8AG8iwdCl52CiqhjiSyrso3335JTs 1m kubelet-bootstrap Pending node-csr-poycCHd7B8YPxc12EBgI3Rwe0wnDJah5uIGvQHzghVY 2m kubelet-bootstrap Pending # 增加 认证 kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:18","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"验证 nodes [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready \u003cnone\u003e 34m v1.10.0-beta.4 kubernetes-65 Ready \u003cnone\u003e 34m v1.10.0-beta.4 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:19","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"一个错误问题 # 密钥文件 这里注意如果 csr 被删除了，请删除如下文件，并重启 kubelet 服务 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key # 查看 csr [root@kubernetes-64 kubernetes]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-4LAZ8jocB-XXRikzDKgKDLcBd0yy-yKyuMWM0WIobTI 3m kubelet-bootstrap Approved # 删除 csr [root@kubernetes-64 kubernetes]# kubectl delete csr/node-csr-4LAZ8jocB-XXRikzDKgKDLcBd0yy-yKyuMWM0WIobTI certificatesigningrequest.certificates.k8s.io \"node-csr-4LAZ8jocB-XXRikzDKgKDLcBd0yy-yKyuMWM0WIobTI\" deleted # 删除文件 rm -rf /root/.kube/* rm -rf /etc/kubernetes/kubelet.kubeconfig rm -rf /etc/kubernetes/ssl/kubelet* rm -rf /opt/kubelet/* # 重新生成配置 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes # 复制过去 cp ~/.kube/config /etc/kubernetes/kubelet.kubeconfig # 重启 kubelet systemctl restart kubelet ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:13:20","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:14:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@kubernetes-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:14:1","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy* /etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:15:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.65:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:15:1","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.10 官方 ipvs 已经是默认的配置 –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm 软件， 在 node 中安装 yum install ipvsadm -y # 创建 kube-proxy 目录 mkdir -p /opt/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.65 \\ --hostname-override=kubernetes-65 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:15:2","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 检查 ipvs [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:15:3","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:16:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:16:1","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:16:2","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 # 创建目录 mkdir /opt/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/opt/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=kubernetes-66 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=1 [Install] WantedBy=multi-user.target # 启动 kubelet 与 kube-proxy systemctl daemon-reload systemctl start kubelet systemctl status kubelet ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:16:3","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service # 创建 kube-proxy 目录 mkdir -p /opt/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/opt/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.66 \\ --hostname-override=kubernetes-66 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl start kube-proxy systemctl status kube-proxy ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:16:4","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"Master 配置 TLS 认证 # 查看 csr 的名称 [root@kubernetes-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-Fla2k7UdFJrN9d8Jjlw588zu0ESUycXHcir1f7bAh5U 14s kubelet-bootstrap Pending node-csr-Pu4QYp3NAwlC6o8AG8iwdCl52CiqhjiSyrso3335JTs 19m kubelet-bootstrap Approved,Issued node-csr-poycCHd7B8YPxc12EBgI3Rwe0wnDJah5uIGvQHzghVY 20m kubelet-bootstrap Approved,Issued # 增加 认证 [root@kubernetes-64 ~]# kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready \u003cnone\u003e 34m v1.10.0-beta.4 kubernetes-65 Ready \u003cnone\u003e 34m v1.10.0-beta.4 kubernetes-66 Ready \u003cnone\u003e 13s v1.10.0-beta.4 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:16:5","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"限制 POD 的调度 由于 master-64 只做 master 不做 pod 调度，所以禁止调度到 master-64中, Pod 的调度是通过 kubelet 服务来启动的，但是不启动 kubelet 的话，节点在 node 里是不可见的。 [root@kubernetes-64 ~]# kubectl cordon kubernetes-64 node \"kubernetes-64\" cordoned [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 35m v1.10.0-beta.4 kubernetes-65 Ready \u003cnone\u003e 35m v1.10.0-beta.4 kubernetes-66 Ready \u003cnone\u003e 58s v1.10.0-beta.4 配置 Flannel 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1eStojia rpm -ivh flannel-0.9.1-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=em1\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:16:6","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 kubernetes-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@kubernetes-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 Flannel 网络的节点 里 curl [root@kubernetes-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:17:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml # vi coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/18 { pods insecure upstream fallthrough in-addr.arpa ip6.arpa } prometheus :9153 proxy . /etc/resolv.conf cache 30 } ... clusterIP: 10.254.0.2 # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:18:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@kubernetes-64 coredns]# kubectl apply -f coredns.yaml serviceaccount \"coredns\" created clusterrole \"system:coredns\" created clusterrolebinding \"system:coredns\" created configmap \"coredns\" created deployment \"coredns\" created service \"coredns\" created ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:19:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@kubernetes-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-6bd7d5dbb5-jh4fj 1/1 Running 0 19s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/coredns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 19s ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:20:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"检查日志 [root@kubernetes-64 coredns]# kubectl logs -n kube-system coredns-6bd7d5dbb5-jh4fj .:53 CoreDNS-1.1.0 linux/amd64, go1.10, c8d91500 2018/03/14 09:58:53 [INFO] CoreDNS-1.1.0 2018/03/14 09:58:53 [INFO] linux/amd64, go1.10, c8d91500 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:21:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 创建的服务 [root@kubernetes-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@kubernetes-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@kubernetes-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:22:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"部署 dashboard \u0026\u0026 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:23:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.8.3 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:24:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:25:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/k8s\\.gcr\\.io/jicki/g' * # 导入文件 [root@kubernetes-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@kubernetes-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-5984fb8cbb-77dl4 1/1 Running 0 3h po/coredns-5984fb8cbb-9hdwt 1/1 Running 0 3h po/kubernetes-dashboard-78bcdc4d64-x6fhq 1/1 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 3h svc/kubernetes-dashboard ClusterIP 10.254.18.143 \u003cnone\u003e 443/TCP 14s ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:26:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:27:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@kubernetes-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 47m v1.10.0-beta.4 kubernetes-65 Ready \u003cnone\u003e 47m v1.10.0-beta.4 kubernetes-66 Ready \u003cnone\u003e 13m v1.10.0-beta.4 # 对 65 与 66 打上 label [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-65 ingress=proxy node \"kubernetes-65\" labeled [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-66 ingress=proxy node \"kubernetes-66\" labeled # 打完标签以后 [root@kubernetes-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 48m v1.10.0-beta.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubernetes-64 kubernetes-65 Ready \u003cnone\u003e 48m v1.10.0-beta.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-65 kubernetes-66 Ready \u003cnone\u003e 13m v1.10.0-beta.4 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-66 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.4 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.12.0 # 国内镜像 jicki/defaultbackend:1.4 jicki/nginx-ingress-controller:0.12.0 # 下载 yaml 文件 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务 # tcp 例子 apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 9000: \"default/tomcat:8080\" # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中 # udp 例子 apiVersion: v1 kind: ConfigMap metadata: name: udp-services namespace: ingress-nginx data: 53: \"kube-system/kube-dns:53\" # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi with-rbac.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 # 导入 yaml 文件 [root@kubernetes-64 ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@kubernetes-64 nginx-ingress]# kubectl apply -f . configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created namespace \"ingress-nginx\" configured serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 66 中 [root@kubernetes-64 ingress]# kubectl get pods -n i","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:28:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"部署 monitoring k8s 运维相关 ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:29:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:30:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2018-03-14","objectID":"/kubernetes-1.10.beta4/:31:0","tags":["kubernetes"],"title":"kubernetes 1.10.beta4","uri":"/kubernetes-1.10.beta4/"},{"categories":["jenkins"],"content":"jenkins build android","date":"2018-03-02","objectID":"/jenkins-android-build/","tags":["jenkins"],"title":"jenkins build android","uri":"/jenkins-android-build/"},{"categories":["jenkins"],"content":" 利用jenkins 对 android 做自动化build apk, 并使用 360 进行加固。 环境配置 ","date":"2018-03-02","objectID":"/jenkins-android-build/:0:0","tags":["jenkins"],"title":"jenkins build android","uri":"/jenkins-android-build/"},{"categories":["jenkins"],"content":"配置 java 环境 # 下载 oracle jdk 1.8 ","date":"2018-03-02","objectID":"/jenkins-android-build/:1:0","tags":["jenkins"],"title":"jenkins build android","uri":"/jenkins-android-build/"},{"categories":["jenkins"],"content":"配置 android sdk # 下载 https://developer.android.com/studio/index.html # 拉倒最下面下载 sdk-tools 如： https://dl.google.com/android/repository/sdk-tools-linux-4333796.zip # 创建目录 mkdir -p /opt/android-sdk cd /opt/android-sdk wget https://dl.google.com/android/repository/sdk-tools-linux-4333796.zip # 创建 env vi /etc/profile # anddrid env export ANDROID_HOME=/opt/android-sdk export PATH=$ANDROID_HOME/tools:$ANDROID_HOME/platform-tools:$PATH # 生效配置 source /etc/profile # 更新 sdk 工具 cd /opt/android-sdk bin/sdkmanager --update bin/sdkmanager --licenses # 执行 打包所需的 版本 tools/android update sdk tools/android update sdk --no-ui --filter build-tools-25.0.2,android-25,extra-android-m2repository ","date":"2018-03-02","objectID":"/jenkins-android-build/:2:0","tags":["jenkins"],"title":"jenkins build android","uri":"/jenkins-android-build/"},{"categories":["jenkins"],"content":"配置 jenkins # 配置一个全局变量 系统管理 --\u003e 系统设置 --\u003e 全局属性 --\u003e 环境变量 --\u003e 新增 键 ANDROID_HOME 值 /opt/android-sdk ","date":"2018-03-02","objectID":"/jenkins-android-build/:3:0","tags":["jenkins"],"title":"jenkins build android","uri":"/jenkins-android-build/"},{"categories":["jenkins"],"content":"添加360 加固 # 这里保存官方文档 以防忘记 http://bbs.360.cn/forum.php?mod=viewthread\u0026tid=6294457 # 下载 Linux 版本的加固 链接：http://pan.baidu.com/s/1jI7OyME （提取码：y3k1） ","date":"2018-03-02","objectID":"/jenkins-android-build/:3:1","tags":["jenkins"],"title":"jenkins build android","uri":"/jenkins-android-build/"},{"categories":["kubernetes"],"content":"kubernetes jenkins","date":"2018-02-08","objectID":"/kubernetes-jenkins/","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":" jenkins 所有的资源全部在 kubernetes 中进行，并配置 kubernetes 云 针对 jenkins slave 做动态资源伸缩，构建时申请 pod, 操作完毕后自动删除 pod 基于 Kubernetes Jenkins slave 动态资源伸缩 pod ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:0:0","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"环境说明 请自行配置 kubernetes 集群, kubernetes 版本为 1.9.1 172.16.1.64 - kubernetes master 172.16.1.65 - kubernetes master or node 172.16.1.66 - kubernetes node ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:1:0","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"先决条件 后端存储, 用于存放 jenkins home 目录，以及 slave 构建依赖包目录 # 配置jenkins 后端存储, 如 nfs, gfs, cephfs 等， 请自行配置，或者参考 之前的文章。 # 基于单独的 nfs 最为后端 文档在这里 https://jicki.cn/2017/01/01/kubernetes-nfs/ ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:2:0","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"创建 jenkins master # 首先创建一个 namespace vi jenkins-namespace.yaml apiVersion: v1 kind: Namespace metadata: name: jenkins # 导入文件 kubectl apply -f jenkins-namespace.yaml namespace \"jenkins\" created # 查看 [root@kubernetes-64 jenkins]# kubectl get namespace NAME STATUS AGE default Active 26d ingress-nginx Active 15d jenkins Active 23s # 创建 jenkins 的 rbac , jenkins 与 api-service 通讯必须要 rabc 认证 vi jenkins-rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: labels: k8s-app: jenkins name: jenkins-admin namespace: jenkins --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: jenkins-admin namespace: jenkins labels: k8s-app: jenkins roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: jenkins-admin namespace: jenkins # 导入文件 [root@kubernetes-64 jenkins]# kubectl apply -f jenkins-rbac.yaml serviceaccount \"jenkins-admin\" created clusterrolebinding \"jenkins-admin\" created # 查看 [root@kubernetes-64 jenkins]# kubectl get serviceaccount -n jenkins NAME SECRETS AGE default 1 3h jenkins-admin 1 1m vi jenkins-deployment.yaml apiVersion: apps/v1beta1 kind: Deployment metadata: name: jenkins namespace: jenkins spec: replicas: 1 strategy: type: RollingUpdate rollingUpdate: maxSurge: 2 maxUnavailable: 0 template: metadata: labels: app: jenkins spec: securityContext: fsGroup: 1000 serviceAccount: \"jenkins-admin\" containers: - name: jenkins image: jenkins/jenkins imagePullPolicy: IfNotPresent ports: - containerPort: 8080 name: web protocol: TCP - containerPort: 50000 name: agent protocol: TCP volumeMounts: - name: jenkinshome mountPath: /var/jenkins_home env: - name: JAVA_OPTS value: \"-Xms512m -Xmx1024m -XX:PermSize=512m -XX:MaxPermSize=1024m -Duser.timezone=Asia/Shanghai\" - name: TRY_UPGRADE_IF_NO_MARKER value: \"true\" volumes: - name: jenkinshome nfs: server: 172.16.1.66 path: \"/opt/data/jenkins\" --- kind: Service apiVersion: v1 metadata: labels: app: jenkins name: jenkins namespace: jenkins spec: ports: - port: 8080 targetPort: 8080 name: web - port: 50000 targetPort: 50000 name: agent selector: app: jenkins --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: jenkins namespace: jenkins spec: rules: - host: jenkins.jicki.cn http: paths: - path: / backend: serviceName: jenkins servicePort: 8080 # 导入 文件 [root@kubernetes-64 jenkins]# kubectl apply -f jenkins-deployment.yaml deployment \"jenkins\" created service \"jenkins\" created ingress \"jenkins\" created # 查看 服务 [root@kubernetes-64 jenkins]# kubectl get pods -n jenkins NAME READY STATUS RESTARTS AGE jenkins-c5f45c84-8vlcp 1/1 Running 0 6m [root@kubernetes-64 jenkins]# [root@kubernetes-64 jenkins]# kubectl get svc -n jenkins NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE jenkins ClusterIP 10.254.47.35 \u003cnone\u003e 8080/TCP,50000/TCP 6m [root@kubernetes-64 jenkins]# [root@kubernetes-64 jenkins]# kubectl get ingress -n jenkins NAME HOSTS ADDRESS PORTS AGE jenkins jenkins.jicki.cn 80 6m # 查看 jenkins 初始密码 [root@kubernetes-64 jenkins]# kubectl logs pods/jenkins-c5f45c84-8vlcp -n jenkins ************************************************************* ************************************************************* ************************************************************* Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation: b2458cfb46724268a4db025c54f172ab This may also be found at: /var/jenkins_home/secrets/initialAdminPassword ************************************************************* ************************************************************* ************************************************************* ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:3:0","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"配置 jenkins # 登录 web ui http://jenkins.jicki.cn # Master 需要安装的插件 1. Pipeline 2. kubernetes plugin 3. Build WIth Parameters (构建 输入参数的插件) 4. Git Parameter Plug-In ( git 分支 构建选择) ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:4:0","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"配置 kubernetes 云 # 点击【系统管理】-【系统设置】-【新增一个云】-【Kubernetes】 ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:5:0","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"基于 pipeline 模式 # 创建一个基于 pipeline 的配置 cloud 字段配置为 kubernetes 云的 name . podTemplate(label: 'jenkins-pod', cloud: 'Jenkins-Cloud') { node('jenkins-pod') { stage('Run shell') { sh 'echo hello world' } } } # 点击立即构建, 控制台输出如下: Started by user 小炒肉 Running in Durability level: MAX_SURVIVABILITY [Pipeline] podTemplate [Pipeline] { [Pipeline] node Still waiting to schedule task jenkins-slave-bkcfk-vpzz0 is offline Running on jenkins-slave-bkcfk-vpzz0 in /home/jenkins/workspace/test-k8s [Pipeline] { [Pipeline] stage [Pipeline] { (Run shell) [Pipeline] sh [test-k8s] Running shell script + echo hello world hello world [Pipeline] } [Pipeline] // stage [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // podTemplate [Pipeline] End of Pipeline Finished: SUCCESS ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:5:1","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"基于 自由风格模式 # 点击【系统管理】-【系统设置】-【新增一个云】-【Kubernetes】- 【 Kubernetes Pod Template 】 - 【 Container Template 】 具体参数 如下： # 添加一个 自动风格的 pj ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:5:2","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"制作 slave 镜像 官方默认的 jenkins-slave 里面不存在任何的工具，依赖，项目环境等，我们需要制作一个属于自己项目的 jenkins-slave 镜像。 官方 jenkins-slave dockerfile 地址 https://github.com/jenkinsci/docker-jnlp-slave 官方 slave https://github.com/jenkinsci/docker-slave # 个人的 oracle-jdk-8 镜像 java -version = 1.8.0_161 docker pull jicki/oracle-jdk:8 # 个人的 gradle 镜像 docker pull jicki/gradle:4.5 # 基于 jicki/gradle:4.5 的一个 jenkins slave docker pull jicki/slave:3.16 # 获取官方的git，主要是 jenkins-slave 脚本文件 git clone https://github.com/jenkinsci/docker-jnlp-slave # 重新编辑 dockerfile 官方默认使用 openjdk-1.8 vi dockerfile FROM jicki/slave:3.16 COPY jenkins-slave /usr/local/bin/jenkins-slave USER root RUN chmod +x /usr/local/bin/jenkins-slave USER jenkins ENTRYPOINT [\"jenkins-slave\"] docker build -t=\"jicki/jenkins-jnlp\" . # 测试自己的镜像 podTemplate(label: 'jicki-jenkins', cloud: 'Jenkins-Cloud', containers: [ containerTemplate( name: 'jnlp', image: 'jicki/jenkins-jnlp', alwaysPullImage: true, args: '${computer.jnlpmac} ${computer.name}'), ], volumes: [ hostPathVolume(mountPath: '/var/run/docker.sock', hostPath: '/var/run/docker.sock'), ],) { node('jicki-jenkins') { stage('Task-1') { stage('show java version') { sh 'java -version' } } } node('jicki-jenkins') { stage('Task-2') { stage('show gradle version') { sh 'gradle -version' } } } } Started by user 小炒肉 Running in Durability level: MAX_SURVIVABILITY [Pipeline] podTemplate [Pipeline] { [Pipeline] node Still waiting to schedule task Waiting for next available executor on jenkins-slave-9v0g5-73tng Running on jenkins-slave-9v0g5-73tng in /home/jenkins/workspace/test-2 [Pipeline] { [Pipeline] stage [Pipeline] { (Task-1) [Pipeline] stage [Pipeline] { (show java version) [Pipeline] sh [test-2] Running shell script + java -version java version \"1.8.0_161\" Java(TM) SE Runtime Environment (build 1.8.0_161-b12) Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode) [Pipeline] } [Pipeline] // stage [Pipeline] } [Pipeline] // stage [Pipeline] } [Pipeline] // node [Pipeline] node Still waiting to schedule task jenkins-slave-9v0g5-nxdf8 is offline Running on jenkins-slave-9v0g5-nxdf8 in /home/jenkins/workspace/test-2 [Pipeline] { [Pipeline] stage [Pipeline] { (Task-2) [Pipeline] stage [Pipeline] { (show gradle version) [Pipeline] sh [test-2] Running shell script + gradle -version ------------------------------------------------------------ Gradle 4.5 ------------------------------------------------------------ Build time: 2018-01-24 17:04:52 UTC Revision: 77d0ec90636f43669dc794ca17ef80dd65457bec Groovy: 2.4.12 Ant: Apache Ant(TM) version 1.9.9 compiled on February 2 2017 JVM: 1.8.0_161 (Oracle Corporation 25.161-b12) OS: Linux 4.4.111-1.el7.elrepo.x86_64 amd64 [Pipeline] } [Pipeline] // stage [Pipeline] } [Pipeline] // stage [Pipeline] } [Pipeline] // node [Pipeline] } [Pipeline] // podTemplate [Pipeline] End of Pipeline Finished: SUCCESS ","date":"2018-02-08","objectID":"/kubernetes-jenkins/:6:0","tags":null,"title":"kubernetes jenkins","uri":"/kubernetes-jenkins/"},{"categories":["kubernetes"],"content":"prometheus monitoring","date":"2018-02-01","objectID":"/prometheus-monitoring/","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"{% raw %} Prometheus Monitoring ","date":"2018-02-01","objectID":"/prometheus-monitoring/:0:0","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"部署 Prometheus ","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:0","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"镜像下载 jicki/prometheus-operator:v0.16.1 jicki/configmap-reload:v0.0.1 jicki/kube-rbac-proxy:v0.2.0 jicki/node-exporter:v0.15.2 jicki/kube-state-metrics:v1.2.0 jicki/addon-resizer:1.0 jicki/monitoring-grafana:4.6.3 jicki/grafana-watcher:v0.0.8 quay.io/prometheus/prometheus:v2.1.0 quay.io/coreos/prometheus-config-reloader:v0.0.2 ","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:1","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"创建 namespaces # vi namespace.yaml apiVersion: v1 kind: Namespace metadata: name: monitoring # 导入 yaml [root@kubernetes-64 monitoring]# kubectl apply -f namespace.yaml namespace \"monitoring\" created ","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:2","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"prometheus-operator # vi prometheus-operator.yaml --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: prometheus-operator namespace: monitoring roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus-operator subjects: - kind: ServiceAccount name: prometheus-operator namespace: monitoring --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: prometheus-operator namespace: monitoring rules: - apiGroups: - extensions resources: - thirdpartyresources verbs: - \"*\" - apiGroups: - apiextensions.k8s.io resources: - customresourcedefinitions verbs: - \"*\" - apiGroups: - monitoring.coreos.com resources: - alertmanagers - prometheuses - servicemonitors verbs: - \"*\" - apiGroups: - apps resources: - statefulsets verbs: [\"*\"] - apiGroups: [\"\"] resources: - configmaps - secrets verbs: [\"*\"] - apiGroups: [\"\"] resources: - pods verbs: [\"list\", \"delete\"] - apiGroups: [\"\"] resources: - services - endpoints verbs: [\"get\", \"create\", \"update\"] - apiGroups: [\"\"] resources: - nodes verbs: [\"list\", \"watch\"] - apiGroups: [\"\"] resources: - namespaces verbs: [\"list\"] --- apiVersion: v1 kind: ServiceAccount metadata: name: prometheus-operator namespace: monitoring --- apiVersion: extensions/v1beta1 kind: Deployment metadata: labels: k8s-app: prometheus-operator name: prometheus-operator namespace: monitoring spec: replicas: 1 template: metadata: labels: k8s-app: prometheus-operator spec: containers: - args: - --kubelet-service=kube-system/kubelet - --config-reloader-image=jicki/configmap-reload:v0.0.1 image: jicki/prometheus-operator:v0.16.1 name: prometheus-operator ports: - containerPort: 8080 name: http resources: limits: cpu: 200m memory: 100Mi requests: cpu: 100m memory: 50Mi securityContext: runAsNonRoot: true runAsUser: 65534 serviceAccountName: prometheus-operator --- apiVersion: v1 kind: Service metadata: name: prometheus-operator namespace: monitoring labels: k8s-app: prometheus-operator spec: type: ClusterIP ports: - name: http port: 8080 targetPort: http protocol: TCP selector: k8s-app: prometheus-operator # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f prometheus-operator.yaml clusterrolebinding \"prometheus-operator\" created clusterrole \"prometheus-operator\" created serviceaccount \"prometheus-operator\" created deployment \"prometheus-operator\" created service \"prometheus-operator\" created [root@kubernetes-64 monitoring]# kubectl get pod -n monitoring NAME READY STATUS RESTARTS AGE prometheus-operator-c544bf4-v6z5z 1/1 Running 0 11s ","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:3","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"node-exporter vi node-exporter.yaml --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: node-exporter namespace: monitoring roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: node-exporter subjects: - kind: ServiceAccount name: node-exporter namespace: monitoring --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: node-exporter namespace: monitoring rules: - apiGroups: [\"authentication.k8s.io\"] resources: - tokenreviews verbs: [\"create\"] - apiGroups: [\"authorization.k8s.io\"] resources: - subjectaccessreviews verbs: [\"create\"] --- apiVersion: v1 kind: ServiceAccount metadata: name: node-exporter namespace: monitoring --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: node-exporter namespace: monitoring spec: updateStrategy: rollingUpdate: maxUnavailable: 1 type: RollingUpdate template: metadata: labels: app: node-exporter name: node-exporter spec: serviceAccountName: node-exporter securityContext: runAsNonRoot: true runAsUser: 65534 hostNetwork: true hostPID: true containers: - image: jicki/node-exporter:v0.15.2 args: - \"--web.listen-address=127.0.0.1:9101\" - \"--path.procfs=/host/proc\" - \"--path.sysfs=/host/sys\" name: node-exporter resources: requests: memory: 30Mi cpu: 100m limits: memory: 50Mi cpu: 200m volumeMounts: - name: proc readOnly: true mountPath: /host/proc - name: sys readOnly: true mountPath: /host/sys - name: kube-rbac-proxy image: jicki/kube-rbac-proxy:v0.2.0 args: - \"--secure-listen-address=:9100\" - \"--upstream=http://127.0.0.1:9101/\" ports: - containerPort: 9100 hostPort: 9100 name: https resources: requests: memory: 20Mi cpu: 10m limits: memory: 40Mi cpu: 20m tolerations: - effect: NoSchedule operator: Exists volumes: - name: proc hostPath: path: /proc - name: sys hostPath: path: /sys --- apiVersion: v1 kind: Service metadata: labels: app: node-exporter k8s-app: node-exporter name: node-exporter namespace: monitoring spec: type: ClusterIP clusterIP: None ports: - name: https port: 9100 protocol: TCP selector: app: node-exporter # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f node-exporter.yaml clusterrolebinding \"node-exporter\" created clusterrole \"node-exporter\" created serviceaccount \"node-exporter\" created daemonset \"node-exporter\" created service \"node-exporter\" created [root@kubernetes-64 monitoring]# kubectl get pods -n monitoring -o wide NAME READY STATUS RESTARTS AGE IP NODE node-exporter-2299v 2/2 Running 0 2m 172.16.1.66 kubernetes-66 node-exporter-2lsfc 2/2 Running 0 2m 172.16.1.64 kubernetes-64 node-exporter-mstjl 2/2 Running 0 2m 172.16.1.65 kubernetes-65 ","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:4","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"kube-state-metrics vi kube-state-metrics.yaml --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: kube-state-metrics namespace: monitoring roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: kube-state-metrics subjects: - kind: ServiceAccount name: kube-state-metrics namespace: monitoring --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: kube-state-metrics namespace: monitoring rules: - apiGroups: [\"\"] resources: - nodes - pods - services - resourcequotas - replicationcontrollers - limitranges - persistentvolumeclaims - persistentvolumes - namespaces - endpoints verbs: [\"list\", \"watch\"] - apiGroups: [\"extensions\"] resources: - daemonsets - deployments - replicasets verbs: [\"list\", \"watch\"] - apiGroups: [\"apps\"] resources: - statefulsets verbs: [\"list\", \"watch\"] - apiGroups: [\"batch\"] resources: - cronjobs - jobs verbs: [\"list\", \"watch\"] - apiGroups: [\"autoscaling\"] resources: - horizontalpodautoscalers verbs: [\"list\", \"watch\"] - apiGroups: [\"authentication.k8s.io\"] resources: - tokenreviews verbs: [\"create\"] - apiGroups: [\"authorization.k8s.io\"] resources: - subjectaccessreviews verbs: [\"create\"] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: kube-state-metrics namespace: monitoring roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: kube-state-metrics-resizer subjects: - kind: ServiceAccount name: kube-state-metrics --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: name: kube-state-metrics-resizer namespace: monitoring rules: - apiGroups: [\"\"] resources: - pods verbs: [\"get\"] - apiGroups: [\"extensions\"] resources: - deployments resourceNames: [\"kube-state-metrics\"] verbs: [\"get\", \"update\"] --- apiVersion: v1 kind: ServiceAccount metadata: name: kube-state-metrics namespace: monitoring --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kube-state-metrics namespace: monitoring spec: replicas: 1 template: metadata: labels: app: kube-state-metrics spec: serviceAccountName: kube-state-metrics securityContext: runAsNonRoot: true runAsUser: 65534 containers: - name: kube-rbac-proxy-main image: jicki/kube-rbac-proxy:v0.2.0 args: - \"--secure-listen-address=:8443\" - \"--upstream=http://127.0.0.1:8081/\" ports: - name: https-main containerPort: 8443 resources: requests: memory: 20Mi cpu: 10m limits: memory: 40Mi cpu: 20m - name: kube-rbac-proxy-self image: jicki/kube-rbac-proxy:v0.2.0 args: - \"--secure-listen-address=:9443\" - \"--upstream=http://127.0.0.1:8082/\" ports: - name: https-self containerPort: 9443 resources: requests: memory: 20Mi cpu: 10m limits: memory: 40Mi cpu: 20m - name: kube-state-metrics image: jicki/kube-state-metrics:v1.2.0 args: - \"--host=127.0.0.1\" - \"--port=8081\" - \"--telemetry-host=127.0.0.1\" - \"--telemetry-port=8082\" - name: addon-resizer image: jicki/addon-resizer:1.0 resources: limits: cpu: 100m memory: 30Mi requests: cpu: 100m memory: 30Mi env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace command: - /pod_nanny - --container=kube-state-metrics - --cpu=100m - --extra-cpu=2m - --memory=150Mi - --extra-memory=30Mi - --threshold=5 - --deployment=kube-state-metrics --- apiVersion: v1 kind: Service metadata: labels: app: kube-state-metrics k8s-app: kube-state-metrics name: kube-state-metrics namespace: monitoring spec: clusterIP: None ports: - name: https-main port: 8443 targetPort: https-main protocol: TCP - name: https-self port: 9443 targetPort: https-self protocol: TCP selector: app: kube-state-metrics # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f kube-state-metrics.yaml clusterrolebinding \"kube-state-metrics\" created clusterrole \"kube-state-metrics\" created rolebinding \"kube-state-metrics\" created role \"kube-state-metrics-resizer\" created serviceaccount \"kube-state-metrics\" created deployment \"kube-state-metrics\" created service \"","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:5","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"prometheus vi prometheus-k8s.yaml --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: prometheus-k8s namespace: monitoring roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: prometheus-k8s namespace: kube-system roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: RoleBinding metadata: name: prometheus-k8s namespace: default roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: prometheus-k8s roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: prometheus-k8s subjects: - kind: ServiceAccount name: prometheus-k8s namespace: monitoring --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: name: prometheus-k8s namespace: monitoring rules: - apiGroups: [\"\"] resources: - nodes - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: - configmaps verbs: [\"get\"] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: name: prometheus-k8s namespace: kube-system rules: - apiGroups: [\"\"] resources: - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: Role metadata: name: prometheus-k8s namespace: default rules: - apiGroups: [\"\"] resources: - services - endpoints - pods verbs: [\"get\", \"list\", \"watch\"] --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: prometheus-k8s rules: - apiGroups: [\"\"] resources: - nodes/metrics verbs: [\"get\"] - nonResourceURLs: [\"/metrics\"] verbs: [\"get\"] --- apiVersion: v1 kind: ConfigMap metadata: name: prometheus-k8s-rules namespace: monitoring labels: role: prometheus-rulefiles prometheus: k8s data: alertmanager.rules.yaml: |+ groups: - name: alertmanager.rules rules: - alert: AlertmanagerConfigInconsistent expr: count_values(\"config_hash\", alertmanager_config_hash) BY (service) / ON(service) GROUP_LEFT() label_replace(prometheus_operator_alertmanager_spec_replicas, \"service\", \"alertmanager-$1\", \"alertmanager\", \"(.*)\") != 1 for: 5m labels: severity: critical annotations: description: The configuration of the instances of the Alertmanager cluster `{{$labels.service}}` are out of sync. - alert: AlertmanagerDownOrMissing expr: label_replace(prometheus_operator_alertmanager_spec_replicas, \"job\", \"alertmanager-$1\", \"alertmanager\", \"(.*)\") / ON(job) GROUP_RIGHT() sum(up) BY (job) != 1 for: 5m labels: severity: warning annotations: description: An unexpected number of Alertmanagers are scraped or Alertmanagers disappeared from discovery. - alert: AlertmanagerFailedReload expr: alertmanager_config_last_reload_successful == 0 for: 10m labels: severity: warning annotations: description: Reloading Alertmanager's configuration has failed for {{ $labels.namespace }}/{{ $labels.pod}}. general.rules.yaml: |+ groups: - name: general.rules rules: - alert: TargetDown expr: 100 * (count(up == 0) BY (job) / count(up) BY (job)) \u003e 10 for: 10m labels: severity: warning annotations: description: '{{ $value }}% of {{ $labels.job }} targets are down.' summary: Targets are down - alert: DeadMansSwitch expr: vector(1) labels: severity: none annotations: description: This is a DeadMansSwitch meant to ensure that the entire Alerting pipeline is functional. summary: Alerting DeadMansSwitch - record: fd_utilization expr: process_open_fds / process_max_fds - alert: FdExhaustionClose expr: predict_linear(fd_utilization[1h], 3600 * 4) \u003e 1 for: 10m labels: severity: warning annotations: description: '{{ $labels.job }","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:6","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"alertmanager # 配置认证 vi alertmanager-secret.yaml apiVersion: v1 kind: Secret metadata: name: alertmanager-main namespace: monitoring data: alertmanager.yaml: Z2xvYmFsOgogIHJlc29sdmVfdGltZW91dDogNW0Kcm91dGU6CiAgZ3JvdXBfYnk6IFsnam9iJ10KICBncm91cF93YWl0OiAzMHMKICBncm91cF9pbnRlcnZhbDogNW0KICByZXBlYXRfaW50ZXJ2YWw6IDEyaAogIHJlY2VpdmVyOiAnbnVsbCcKICByb3V0ZXM6CiAgLSBtYXRjaDoKICAgICAgYWxlcnRuYW1lOiBEZWFkTWFuc1N3aXRjaAogICAgcmVjZWl2ZXI6ICdudWxsJwpyZWNlaXZlcnM6Ci0gbmFtZTogJ251bGwnCg== # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f alertmanager-secret.yaml secret \"alertmanager-main\" created # 查看 [root@kubernetes-64 monitoring]# kubectl get secret -n monitoring NAME TYPE DATA AGE alertmanager-main Opaque 1 28s default-token-t6cdn kubernetes.io/service-account-token 3 4h grafana-credentials Opaque 2 3h kube-state-metrics-token-ctnl6 kubernetes.io/service-account-token 3 3h node-exporter-token-nflwn kubernetes.io/service-account-token 3 3h prometheus-k8s Opaque 2 21m prometheus-k8s-token-2dt66 kubernetes.io/service-account-token 3 21m prometheus-operator-token-k5m47 kubernetes.io/service-account-token 3 3h vi alertmanager.yaml --- apiVersion: monitoring.coreos.com/v1 kind: Alertmanager metadata: name: main namespace: monitoring labels: alertmanager: main spec: replicas: 3 version: v0.13.0 --- apiVersion: v1 kind: Service metadata: labels: alertmanager: main name: alertmanager-main namespace: monitoring spec: ports: - name: web port: 9093 protocol: TCP targetPort: web selector: alertmanager: main --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: alertmanager-ingress namespace: monitoring spec: rules: - host: alertmanager.jicki.cn http: paths: - backend: serviceName: alertmanager-main servicePort: 9093 # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f alertmanager.yaml alertmanager \"main\" created service \"alertmanager-main\" created ingress \"alertmanager-ingress\" created # 查看服务 [root@kubernetes-64 monitoring]# kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE alertmanager-main-0 2/2 Running 0 2m alertmanager-main-1 2/2 Running 0 1m alertmanager-main-2 2/2 Running 0 11s grafana-f8db675bc-5qg9j 2/2 Running 0 3h kube-state-metrics-578778548f-8gf8d 4/4 Running 0 3h node-exporter-9pgx5 2/2 Running 0 3h node-exporter-dz9kp 2/2 Running 0 3h node-exporter-mcd9m 2/2 Running 0 3h prometheus-k8s-0 2/2 Running 0 27m prometheus-k8s-1 2/2 Running 0 27m prometheus-operator-9bf5574-s7k76 1/1 Running 0 3h ","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:7","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"grafana # 创建 认证 (加密方式为 base, YWRtaW4= 等于 admin) vi grafana-credentials.yaml apiVersion: v1 kind: Secret metadata: name: grafana-credentials namespace: monitoring data: user: YWRtaW4= password: YWRtaW4= # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f grafana-credentials.yaml secret \"grafana-credentials\" created # 下载 dashboard yaml 文件 wget https://raw.githubusercontent.com/jicki/kuberneres/master/grafana-dashboards.yaml # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f grafana-dashboards.yaml configmap \"grafana-dashboards-0\" created # vi grafana.yaml --- apiVersion: apps/v1beta1 kind: Deployment metadata: name: grafana namespace: monitoring spec: replicas: 1 template: metadata: labels: app: grafana spec: securityContext: runAsNonRoot: true runAsUser: 65534 containers: - name: grafana image: jicki/monitoring-grafana:4.6.3-non-root env: - name: GF_AUTH_BASIC_ENABLED value: \"true\" - name: GF_AUTH_ANONYMOUS_ENABLED value: \"true\" - name: GF_SECURITY_ADMIN_USER valueFrom: secretKeyRef: name: grafana-credentials key: user - name: GF_SECURITY_ADMIN_PASSWORD valueFrom: secretKeyRef: name: grafana-credentials key: password volumeMounts: - name: grafana-storage mountPath: /data ports: - name: web containerPort: 3000 resources: requests: memory: 100Mi cpu: 100m limits: memory: 200Mi cpu: 200m - name: grafana-watcher image: jicki/grafana-watcher:v0.0.8 args: - '--watch-dir=/var/grafana-dashboards-0' - '--grafana-url=http://localhost:3000' env: - name: GRAFANA_USER valueFrom: secretKeyRef: name: grafana-credentials key: user - name: GRAFANA_PASSWORD valueFrom: secretKeyRef: name: grafana-credentials key: password resources: requests: memory: \"16Mi\" cpu: \"50m\" limits: memory: \"32Mi\" cpu: \"100m\" volumeMounts: - name: grafana-dashboards-0 mountPath: /var/grafana-dashboards-0 volumes: - name: grafana-storage emptyDir: {} - name: grafana-dashboards-0 configMap: name: grafana-dashboards-0 --- apiVersion: v1 kind: Service metadata: name: grafana namespace: monitoring labels: app: grafana spec: ports: - port: 3000 protocol: TCP targetPort: web selector: app: grafana --- apiVersion: extensions/v1beta1 kind: Ingress metadata: name: grafana-ingress namespace: monitoring spec: rules: - host: grafana.jicki.cn http: paths: - backend: serviceName: grafana servicePort: 3000 # 导入 yaml 文件 [root@kubernetes-64 monitoring]# kubectl apply -f grafana.yaml deployment \"grafana\" created service \"grafana\" created ingress \"grafana-ingress\" created # 查看 [root@kubernetes-64 monitoring]# kubectl get pods -n monitoring NAME READY STATUS RESTARTS AGE grafana-f8db675bc-5qg9j 2/2 Running 0 2h kube-state-metrics-578778548f-8gf8d 4/4 Running 0 2h node-exporter-9pgx5 2/2 Running 0 2h node-exporter-dz9kp 2/2 Running 0 2h node-exporter-mcd9m 2/2 Running 0 2h prometheus-operator-9bf5574-s7k76 1/1 Running 0 2h ","date":"2018-02-01","objectID":"/prometheus-monitoring/:1:8","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"页面展示 {% endraw %} ","date":"2018-02-01","objectID":"/prometheus-monitoring/:2:0","tags":null,"title":"prometheus monitoring","uri":"/prometheus-monitoring/"},{"categories":["kubernetes"],"content":"kubernetes 1.9.1","date":"2018-01-23","objectID":"/kubernetes-1.9.1/","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"kubernetes 1.9.1 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:0:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"环境说明 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node, node-66 只做单纯 Node kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:1:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname kubernetes-64: 172.16.1.64 kubernetes-65: 172.16.1.65 kubernetes-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.64 kubernetes-64 172.16.1.65 kubernetes-65 172.16.1.66 kubernetes-66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:2:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:3:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:4:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@kubernetes-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:5:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem 172.16.1.65:/etc/kubernetes/ssl/ scp *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem 172.16.1.66:/etc/kubernetes/ssl/ scp *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm rpm -ivh docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm yum -y install docker-ce-17.03.1.ce # 查看安装 docker version Client: Version: 17.03.1-ce API version: 1.27 Go version: go1.7.5 Git commit: f5ec1e2 Built: Tue Jun 27 02:21:36 2017 OS/Arch: linux/amd64 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:6:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker # 如果报错 请使用 journalctl -f -t docker 和 journalctl -u docker 来定位问题 etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:7:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.2.14/etcd-v3.2.14-linux-amd64.tar.gz tar zxvf etcd-v3.2.14-linux-amd64.tar.gz cd etcd-v3.2.14-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:8:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@kubernetes-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:9:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:10:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:11:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:12:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.9.1/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:1","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@kubernetes-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:2","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:3","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:4","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@kubernetes-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:5","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@kubernetes-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' df3b158fbdc425ae2ac70bbef0688921 # 创建 token.csv 文件 cd /opt/ssl vi token.csv df3b158fbdc425ae2ac70bbef0688921,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:6","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=1 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # k8s 1.8 开始需要 添加 --authorization-mode=Node # k8s 1.8 开始需要 添加 --admission-control=NodeRestriction # k8s 1.8 开始需要 添加 --audit-policy-file=/etc/kubernetes/audit-policy.yaml # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:7","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:8","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-name=kubernetes \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:9","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:10","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=1 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:11","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:12","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@kubernetes-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@kubernetes-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:13","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:14","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=df3b158fbdc425ae2ac70bbef0688921 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:15","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 \u003e 配置为 node 本机 IP mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=kubernetes-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=1 [Install] WantedBy=multi-user.target # 如上配置: kubernetes-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:16","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:17","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@kubernetes-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-Pu4QYp3NAwlC6o8AG8iwdCl52CiqhjiSyrso3335JTs 1m kubelet-bootstrap Pending node-csr-poycCHd7B8YPxc12EBgI3Rwe0wnDJah5uIGvQHzghVY 2m kubelet-bootstrap Pending # 增加 认证 kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:18","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"验证 nodes [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready \u003cnone\u003e 12s v1.9.1 kubernetes Ready \u003cnone\u003e 17s v1.9.1 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 这里注意如果 csr 被删除了，请删除如下文件，并重启 kubelet 服务 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:13:19","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:14:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@kubernetes-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:14:1","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy* /etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy* 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:15:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.65:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.66:/etc/kubernetes/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:15:1","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.9 官方 ipvs 已经 beta , 尝试开启 ipvs 测试一下, 官方 –feature-gates=SupportIPVSProxyMode=false 默认是 false 的， 需要打开 –feature-gates=SupportIPVSProxyMode=true –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm 软件， 在 node 中安装 yum install ipvsadm -y # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.65 \\ --hostname-override=kubernetes-65 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=1 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:15:2","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 检查 ipvs [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:15:3","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:16:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:16:1","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:16:2","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 # 创建目录 mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=kubernetes-66 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=1 [Install] WantedBy=multi-user.target # 启动 kubelet 与 kube-proxy systemctl daemon-reload systemctl start kubelet systemctl status kubelet ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:16:3","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.66 \\ --hostname-override=kubernetes-66 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl start kube-proxy systemctl status kube-proxy ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:16:4","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"Master 配置 TLS 认证 # 查看 csr 的名称 [root@kubernetes-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-Fla2k7UdFJrN9d8Jjlw588zu0ESUycXHcir1f7bAh5U 14s kubelet-bootstrap Pending node-csr-Pu4QYp3NAwlC6o8AG8iwdCl52CiqhjiSyrso3335JTs 19m kubelet-bootstrap Approved,Issued node-csr-poycCHd7B8YPxc12EBgI3Rwe0wnDJah5uIGvQHzghVY 20m kubelet-bootstrap Approved,Issued # 增加 认证 [root@kubernetes-64 ~]# kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready \u003cnone\u003e 17m v1.9.1 kubernetes Ready \u003cnone\u003e 18m v1.9.1 kubernetes-66 Ready \u003cnone\u003e 26s v1.9.1 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:16:5","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"限制 POD 的调度 由于 master-64 只做 master 不做 pod 调度，所以禁止调度到 master-64中, Pod 的调度是通过 kubelet 服务来启动的，但是不启动 kubelet 的话，节点在 node 里是不可见的。 [root@kubernetes-64 ~]# kubectl cordon kubernetes-64 node \"kubernetes-64\" cordoned [root@kubernetes-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 19m v1.9.1 kubernetes Ready \u003cnone\u003e 19m v1.9.1 kubernetes-66 Ready \u003cnone\u003e 1m v1.9.1 配置 Flannel 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1eStojia rpm -ivh flannel-0.9.1-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=em1\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:16:6","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@kubernetes-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 kubernetes-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@kubernetes-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 Flannel 网络的节点 里 curl [root@kubernetes-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@kubernetes-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:17:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"下载镜像 # 官方镜像 coredns/coredns:1.0.4 # 我的镜像 jicki/coredns:1.0.4 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:18:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 wget https://raw.githubusercontent.com/coredns/deployment/master/kubernetes/coredns.yaml.sed mv coredns.yaml.sed coredns.yaml # vi coredns.yaml ... data: Corefile: | .:53 { errors health kubernetes cluster.local 10.254.0.0/18 { pods insecure upstream /etc/resolv.conf fallthrough in-addr.arpa ip6.arpa } ... image: jicki/coredns:1.0.4 ... clusterIP: 10.254.0.2 # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:19:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@kubernetes-64 coredns]# kubectl apply -f coredns.yaml serviceaccount \"coredns\" created clusterrole \"system:coredns\" created clusterrolebinding \"system:coredns\" created configmap \"coredns\" created deployment \"coredns\" created service \"coredns\" created ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:20:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@kubernetes-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-6bd7d5dbb5-jh4fj 1/1 Running 0 19s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/coredns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 19s ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:21:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"检查日志 [root@kubernetes-64 coredns]# kubectl logs -n kube-system coredns-6bd7d5dbb5-jh4fj .:53 CoreDNS-1.0.1 linux/amd64, go1.9.2, 99e163c3 2017/12/20 09:34:24 [INFO] CoreDNS-1.0.1 2017/12/20 09:34:24 [INFO] linux/amd64, go1.9.2, 99e163c3 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:22:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 创建的服务 [root@kubernetes-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@kubernetes-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@kubernetes-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:23:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"部署 dashboard \u0026\u0026 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:24:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.2 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.8.2 ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:25:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:26:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/k8s\\.gcr\\.io/jicki/g' * # 导入文件 [root@kubernetes-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@kubernetes-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-5984fb8cbb-77dl4 1/1 Running 0 3h po/coredns-5984fb8cbb-9hdwt 1/1 Running 0 3h po/kubernetes-dashboard-78bcdc4d64-x6fhq 1/1 Running 0 14s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 3h svc/kubernetes-dashboard ClusterIP 10.254.18.143 \u003cnone\u003e 443/TCP 14s ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:27:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:28:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@kubernetes-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 10d v1.9.1 kubernetes-65 Ready \u003cnone\u003e 10d v1.9.1 kubernetes-66 Ready \u003cnone\u003e 10d v1.9.1 # 对 65 与 66 打上 label [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-65 ingress=proxy node \"kubernetes-65\" labeled [root@kubernetes-64 ingress]# kubectl label nodes kubernetes-66 ingress=proxy node \"kubernetes-66\" labeled # 打完标签以后 [root@kubernetes-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS kubernetes-64 Ready,SchedulingDisabled \u003cnone\u003e 10d v1.9.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=kubernetes-64 kubernetes-65 Ready \u003cnone\u003e 10d v1.9.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-65 kubernetes-66 Ready \u003cnone\u003e 10d v1.9.1 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=kubernetes-66 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.4 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.10.0 # 国内镜像 jicki/defaultbackend:1.4 jicki/nginx-ingress-controller:0.10.0 # 下载 yaml 文件 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # tcp-service 与 udp-service, 由于 ingress 不支持 tcp 与 udp 的转发，所以这里配置了两个基于 tcp 与 udp 的 service ,通过 --tcp-services-configmap 与 --udp-services-configmap 来配置 tcp 与 udp 的转发服务 # tcp 例子 apiVersion: v1 kind: ConfigMap metadata: name: tcp-services namespace: ingress-nginx data: 9000: \"default/tomcat:8080\" # 以上配置， 转发 tomcat:8080 端口 到 ingress 节点的 9000 端口中 # udp 例子 apiVersion: v1 kind: ConfigMap metadata: name: udp-services namespace: ingress-nginx data: 53: \"kube-system/kube-dns:53\" # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi with-rbac.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 这里添加一个 other 端口做为后续tcp转发 ports: - name: http containerPort: 80 - name: https containerPort: 443 - name: other containerPort: 8888 # 导入 yaml 文件 [root@kubernetes-64 ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@kubernetes-64 nginx-ingress]# kubectl apply -f . configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created namespace \"ingress-nginx\" configured serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 66 中 [root@kubernetes-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:29:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:30:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2018-01-23","objectID":"/kubernetes-1.9.1/:31:0","tags":null,"title":"kubernetes 1.9.1","uri":"/kubernetes-1.9.1/"},{"categories":["openshift"],"content":"openshift orgin 3.7","date":"2018-01-09","objectID":"/openshift-3.7.0/","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"Openshift Origin 3 ","date":"2018-01-09","objectID":"/openshift-3.7.0/:0:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"环境说明 类型 主机名 IP 系统 内核 Master ops-master-64 172.16.1.64 CentOS 7.4 Kernel 4.4.x Master ops-master-65 172.16.1.65 CentOS 7.4 Kernel 4.4.x Node ops-node-66 172.16.1.66 CentOS 7.4 Kernel 4.4.x ","date":"2018-01-09","objectID":"/openshift-3.7.0/:1:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"系统要求 类型 CPU 内存 空间 系统 Master 2vCPU 8G 40G CentOS 7.2 以上 Node 1vCPU 8G 20G CentOS 7.2 以上 ","date":"2018-01-09","objectID":"/openshift-3.7.0/:2:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"初始化环境 hostnamectl --static set-hostname hostname ops-master-64: 172.16.1.64 ops-master-65: 172.16.1.65 ops-node-64: 172.16.1.66 # 配置 hostname 通信 vi /etc/hosts # OpenShift hostname 172.16.1.64 ops-master-64 172.16.1.65 ops-master-65 172.16.1.66 ops-node-66 # OpenShift hostname # 启用 NetworkManager systemctl start NetworkManager systemctl enable NetworkManager # 停止, 禁用防火墙 systemctl stop firewalld systemctl diable firewalld # 清除之前安装的 dnsmasq yum remove -y dnsmasq # 清除之前安装的 docker yum remove -y docker-engine-selinux docker-engine docker-ce # 安装 iptables yum -y install iptables iptables-services # 配置 iptables，特别重要的是 ssh 非 22 端口的服务器 iptables-save \u003e /etc/sysconfig/iptables vi /etc/sysconfig/iptables # sample configuration for iptables service # you can edit this manually or use system-config-firewall # please do not ask us to add additional ports/services to this default configuration *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] -A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT -A INPUT -p icmp -j ACCEPT -A INPUT -i lo -j ACCEPT # 需要开放的端口写法 -A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT # 需要开放的端口写法 -A INPUT -j REJECT --reject-with icmp-host-prohibited -A FORWARD -j REJECT --reject-with icmp-host-prohibited COMMIT # 使配置生效 iptables-restore /etc/sysconfig/iptables # 启动 iptables systemctl start iptables systemctl enable iptables # 配置 selinux 安装时默认会打开 vi /etc/selinux/config SELINUX=permissive SELINUXTYPE=targeted ","date":"2018-01-09","objectID":"/openshift-3.7.0/:3:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"安装所需依赖 在所有节点安装所需依赖 yum install -y wget git net-tools bind-utils iptables-services bridge-utils bash-completion python-yaml centos-release-openshift-origin dnsmasq 安装docker ","date":"2018-01-09","objectID":"/openshift-3.7.0/:4:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"yum 安装 docker # 使用官方 yum 原生配置 yum -y install docker ","date":"2018-01-09","objectID":"/openshift-3.7.0/:5:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"更改docker 配置 # 编辑如下配置 vi /etc/sysconfig/docker # OPTIONS 修改为如下配置 OPTIONS='--insecure-registry=10.254.0.0/16 --graph=/opt/docker --registry-mirror=http://b438f72b.m.daocloud.io --log-opt max-size=50m --log-opt max-file=5' # 编辑文件系统 kernel 3.10 设置为 overlay , 4.0 以上设置为 overlay2 vi /etc/sysconfig/docker-storage DOCKER_STORAGE_OPTIONS=\"--storage-driver overlay2\" ","date":"2018-01-09","objectID":"/openshift-3.7.0/:6:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"启动 docker # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker 安装 etcd 集群 ","date":"2018-01-09","objectID":"/openshift-3.7.0/:7:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"安装服务 # yum 安装 etcd yum -y install etcd # 启动服务 systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 安装 ansible # 安装 centos 额外的yum源 yum install -y epel-release # make 缓存 yum clean all \u0026\u0026 yum makecache # 安装 软件 yum install -y python-pip python34 python-netaddr python34-pip ansible pyOpenSSL ","date":"2018-01-09","objectID":"/openshift-3.7.0/:8:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"配置SSH Key 登陆 # 确保本机也可以 ssh 连接，否则下面部署失败 ssh-keygen -t rsa -N \"\" ssh-copy-id -i /root/.ssh/id_rsa.pub 172.16.1.65 ssh-copy-id -i /root/.ssh/id_rsa.pub 172.16.1.66 下载 OpenShift 官方有 github 里有 openshift-ansible https://github.com/openshift/openshift-ansible # 下载最新 releases 版本 cd /opt [root@ops-master-64 opt]# git clone https://github.com/openshift/openshift-ansible 正克隆到 'openshift-ansible'... remote: Counting objects: 86508, done. remote: Compressing objects: 100% (12/12), done. remote: Total 86508 (delta 3), reused 9 (delta 0), pack-reused 86494 接收对象中: 100% (86508/86508), 21.95 MiB | 152.00 KiB/s, done. 处理 delta 中: 100% (52747/52747), done. ","date":"2018-01-09","objectID":"/openshift-3.7.0/:9:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"配置参数 # 目录 openshift-ansible/inventory # 包含如下文件 [root@ops-master-64 inventory]# ls hosts.example hosts.glusterfs.mixed.example hosts.glusterfs.registry-only.example hosts.openstack hosts.glusterfs.external.example hosts.glusterfs.native.example hosts.glusterfs.storage-and-registry.example README.md # 编辑配置文件 hosts.example 里面有完整的参数可供参考 # 新建一个 新的 [root@ops-master-64 inventory]# vi hosts # This is an example of an OpenShift-Ansible host inventory [OSEv3:children] masters nodes etcd # Set variables common for all OSEv3 hosts [OSEv3:vars] # SSH 时使用的用户 ansible_user=root # 日志级别，0 是指记录错误与警告，默认是2 debug_level=0 # 安装版本 origin = 开源版 openshift-enterprise = 企业版 openshift_deployment_type=origin # 安装版本 openshift_release=v3.7 # 跳过如下检测 openshift_disable_check=docker_storage,memory_availability,docker_image_availability # htpasswd auth openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}] # host group for masters [all] ops-master-64 ansible_port=99 ops-master-65 ansible_port=99 ops-node-66 ansible_port=99 [masters] ops-master-64 ops-master-65 [etcd] ops-master-64 ops-master-65 ops-node-66 [nodes] ops-master-64 ops-master-65 ops-node-66 openshift_node_labels=\"{'region': 'primary', 'zone': 'default'}\" 部署 Openshift ","date":"2018-01-09","objectID":"/openshift-3.7.0/:10:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"初始化安装环境 # 首先清理一次环境 cd /opt/openshift-ansible ansible-playbook -i inventory/hosts playbooks/adhoc/uninstall.yml -b -v --private-key=~/.ssh/id_rsa PLAY [lb] **************************************************************************************************************************************************************** skipping: no hosts matched PLAY RECAP *************************************************************************************************************************************************************** ops-master-64 : ok=24 changed=6 unreachable=0 failed=0 ops-master-65 : ok=57 changed=10 unreachable=0 failed=0 ops-node-66 : ok=45 changed=9 unreachable=0 failed=0 ","date":"2018-01-09","objectID":"/openshift-3.7.0/:11:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["openshift"],"content":"开始部署集群 cd /opt/openshift-ansible ansible-playbook -i inventory/hosts playbooks/deploy_cluster.yml -b -v --private-key=~/.ssh/id_rsa ","date":"2018-01-09","objectID":"/openshift-3.7.0/:12:0","tags":null,"title":"openshift origin 3.7","uri":"/openshift-3.7.0/"},{"categories":["kubernetes"],"content":"kubernetes 基础知识","date":"2018-01-01","objectID":"/kubernetes-popularization/","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Kubernetes Kubernetes 是 Google 基于 Borg 开源的容器编排调度，用于管理容器集群自动化部署、扩容以及运维的开源平台。作为云原生计算基金会 CNCF（Cloud Native Computing Foundation）最重要的组件之一（CNCF 另一个毕业项目 Prometheus ），它的目标不仅仅是一个编排系统，而是提供一个规范，可以让你来描述集群的架构，定义服务的最终状态，Kubernetes 可以帮你将系统自动地达到和维持在这个状态，Kubernetes 也可以对容器(Docker)进行集群管理和服务编排（Docker Swarm 类似的功能）,对于大多开发者来说，以容器的方式运行一个程序是一个最基本的需求，跟多的是 Kubernetes 能够提供路由网关、水平扩展、监控、备份、灾难恢复等一系列运维能力。 本文用于科普 Kubernetes 的一些基础知识以及概念。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:0:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Kubernetes 核心组件 Kubernetes 集群由两种角色组成: Master 集群调度节点, 管理集群。(kube-apiserver, kube-controller-manager, kube-scheduler, etcd) Node 引用程序实际运行的工作节点。(kubelet, kube-proxy) ","date":"2018-01-01","objectID":"/kubernetes-popularization/:1:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Master 端 kube-apiserver 集群控制入口, 提供了 HTTP Rest 接口的关键服务进程，是 Kubernetes 里所有资源的增删改查等操作的唯一入口, 也是集群控制的入口。 提供集群管理的 REST API 接口，包括认证授权、数据校验以及集群状态变更等。 提供其他模块之间的数据交互和通信的枢纽（其他模块通过 API Server 查询或修改数据，只有 API Server 才直接操作 etcd） kube-controller-manager 服务运行控制器, 处理常规任务的后台线程 比如故障检测、自动扩展、滚动更新等。kube-controller-manager 由一系列的控制器组成包括如下控制器 Replication Controller Node Controller CronJob Controller Daemon Controller Deployment Controller Endpoint Controller Garbage Collector Namespace Controller Job Controller Pod AutoScaler RelicaSet Service Controller ServiceAccount Controller StatefulSet Controller Volume Controller Resource quota Controller kube-scheduler 负责 Pod 资源调度。监视新创建没有分配到Node的Pod, 为Pod选择一个Node。调度方式: 公平调度 资源高效利用 QoS affinity 和 anti-affinity (约束) 数据本地化（data locality） 内部负载干扰（inter-workload interference） deadlines etcd 用于共享配置和服务发现，存储 Kubernetes 集群所有的网络配置和对象的状态信息。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:1:1","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Node 端 kubelet 负责 Pod 对应的容器的创建，启动等任务，同时与Master节点密切协作, kubelet 提供了一系列PodSpecs集合规范，并确保这些PodSpecs中描述的容器运行正常。 kubelet是主要的节点代理，它会监视已分配给节点的pod, 具体功能 安装 Pod 所需的volume。 下载 Pod 的Secrets。 监控 Pod 中运行的 docker（或experimentally，rkt）容器。 定期执行容器健康检查。 回传 pod 的状态到其他 kubernetes 服务中。 回传 节点 的状态到其他 kubernetes 服务中。 kube-proxy 实现 Kubernetes SVC 的通信与负载均衡机制的重要组件。通过在主机上维护网络规则并执行连接转发来实现Kubernetes服务抽象。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:1:2","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Kubernetes 基本对象与术语 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Pod Pod 是一组紧密关联的容器集合，它们共享 PID、IPC、Network 和 UTS namespace，是 Kubernetes 调度的基本单位。Pod 的设计理念是支持多个容器在一个 Pod 中共享网络和文件系统，可以通过进程间通信和文件共享这种简单高效的方式组合完成服务。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:1","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Label Label 是识别 Kubernetes 对象的标签，以 key/value 的方式附加到对象上（key最长不能超过63字节，value 可以为空，也可以是不超过253字节的字符串）。 Label 不提供唯一性，并且实际上经常是很多对象（如 Pods）都使用相同的 label 来标志具体的应用。 Label 定义好后其他对象可以使用 Label Selector 来选择一组相同 label 的对象（比如 Service 用 label 来选择一组 Pod）。Label Selector 支持以下几种方式: 等于和不等于, 如app=nginx和env!=production 集合，如env in (production, test) 多个label（它们之间是AND关系），如app=nginx, env=test ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:2","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Namespace Namespace 是对一组资源和对象的抽象集合，比如可以用来将系统内部的对象划分为不同的项目组或用户组。常见的 pods, services, deployments 等都是属于某一个 namespace 的（默认是default），而 Node, PersistentVolumes 等则不属于任何 namespace。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:3","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"ReplicaSet ReplicaSet 它的主要作用是确保 Pod 以你指定的副本数运行, 如果有容器异常退出, 会自动创建新的 Pod 来替代, 而异常多出来的容器也会自动回收。 支持集合式的 selector。ReplicaSet 可以独立使用, 但建议使用 Deployment 来自动管理 ReplicaSet, 这样就无需担心跟其他机制的不兼容问题（比如 ReplicaSet 不支持 rolling-update 但 Deployment 支持）, 并且 Deployment 还支持版本记录、回滚、暂停升级等高级特性。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:4","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Deployment Deployment 确保任意时间都有指定数量的 Pod 在运行。如果为某个 Pod 创建了 Deployment 并且指定3个副本，它会创建3个 Pod，并且持续监控它们。如果某个 Pod 不响应，那么 Deployment 会替换它，保持总数为3。如果之前不响应的 Pod 恢复了，现在就有4个 Pod 了，那么 Deployment 会将其中一个终止保持总数为3。如果在运行中将副本总数改为5，Deployment 会立刻启动2个新 Pod，保证总数为5。Deployment 还支持回滚和滚动升级。 创建 Deployment 需要指定: Pod 模板 用于配置 pod 生成的属性和副本。 Label 标签 Deployment 需要监控的 Pod 的标签。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:5","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"StatefulSet StatefulSet 是为了解决有状态服务的问题 (对应 Deployments 和 ReplicaSets是为无状态服务而设计), 其应用场景包括 稳定的持久化存储，即 Pod 重新调度后还是能访问到相同的持久化数据，基于 PVC 来实现。 稳定的网络标志，即 Pod 重新调度后其 PodName 和 HostName 不变，基于 Headless Service（即没有Cluster IP的Service）来实现 有序部署，有序扩展，即 Pod 是有顺序的，在部署或者扩展的时候要依据定义的顺序依次依次进行（即从0到N-1，在下一个Pod运行之前所有之前的 Pod 必须都是 Running 和 Ready 状态），基于 init containers来实现。 有序收缩，有序删除（即从N-1到0） ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:6","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"DaemonSet DaemonSet 保证在每个Node上都运行一个容器副本，常用来部署一些集群的日志、监控或者其他系统管理应用。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:7","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Service Service 是应用服务的抽象，通过 labels 为应用提供负载均衡和服务发现。匹配 labels 的Pod IP 和端口列表组成 endpoints，由 kube-proxy 负责将服务 IP 负载均衡到这些endpoints 上。每个 Service 都会自动分配一个 cluster IP（仅在集群内部可访问的虚拟地址）和 DNS 名，其他容器可以通过该地址或 DNS 来访问服务，而不需要了解后端容器的运行。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:8","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Job Job 负责批量处理短暂的一次性任务 (short lived one-off tasks)，即仅执行一次的任务，它保证批处理任务的一个或多个Pod成功结束。 Kubernetes 支持以下几种 Job 非并行 Job：通常创建一个 Pod 直至其成功结束 固定结束次数的 Job：设置 .spec.completions, 创建多个 Pod，直到 .spec.completions 个 Pod 成功结束 带有工作队列的并行 Job：设置 .spec.Parallelism 但不设置 .spec.completions，当所有Pod结束并且至少一个成功时，Job 就认为是成功 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:9","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"CronJob CronJob 即定时任务，就类似于 Linux 系统的 crontab，在指定的时间周期运行指定的任务。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:10","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Horizontal Pod Autoscaler（HPA） Horizontal Pod Autoscaling 可以根据 CPU 使用率或应用自定义 metrics 自动扩展 Pod 数量（支持replication controller、deployment和replica set）, 从而合理的扩展性能与使用资源。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:11","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"PersistentVolume (PV) PersistentVolume（PV）是集群中由管理员配置的一段网络存储。 它是集群中的资源，就像节点是集群资源一样。 PV是容量插件，如Volumes，但其生命周期独立于使用PV的任何单个pod。 此API对象捕获存储实现的详细信息，包括NFS，iSCSI或特定于云提供程序的存储系统。 PersistentVolume 支持的类型: GCEPersistentDisk AWSElasticBlockStore AzureFile AzureDisk FC (Fibre Channel) Flexvolume Flocker NFS iSCSI RBD (Ceph Block Device) CephFS Cinder (OpenStack block storage) Glusterfs VsphereVolume Quobyte Volumes HostPath Portworx Volumes ScaleIO Volumes StorageOS PersistentVolume 服务状态 Available 资源尚未被claim使用 Bound 卷已经被绑定到claim了 Released claim被删除，卷处于释放状态，但未被集群回收。 Failed 卷自动回收失败 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:12","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"PersistentVolumeClaim（PVC） PersistentVolumeClaim（PVC）是由用户进行存储的请求。 它类似于pod。 Pod消耗节点资源，PVC消耗PV资源。Pod可以请求特定级别的资源（CPU和内存）。声明可以请求特定的大小和访问模式（例如，可以一次读/写或多次只读）。 PVC和PV是一一对应的。 PVC 与 PV 的生命周期 PV是群集中的资源。PVC是对这些资源的请求，并且还充当对资源的检查。 PV和PVC之间的相互作用遵循生命周期：Provisioning ——-\u003e Binding ——–\u003e Using -——\u003e Releasing -——\u003e Recycling Provisioning (准备) 通过集群外的存储系统或者云平台来提供存储持久化支持。 静态提供Static：集群管理员创建多个PV。 它们携带可供集群用户使用的真实存储的详细信息。 它们存在于Kubernetes API中，可用于消费。 动态提供Dynamic：当管理员创建的静态PV都不匹配用户的PersistentVolumeClaim时，集群可能会尝试为PVC动态配置卷。 此配置基于StorageClasses：PVC必须请求一个类，并且管理员必须已创建并配置该类才能进行动态配置。 要求该类的声明有效地为自己禁用动态配置。 Binding (绑定) 用户创建pvc并指定需要的资源和访问模式。在找到可用pv之前，pvc会保持未绑定状态。 Using (使用) 用户可在pod中像volume一样使用pvc。 Releasing (释放) 用户删除pvc来回收存储资源，pv将变成\"released\"状态。由于还保留着之前的数据，这些数据需要根据不同的策略来处理，否则这些存储资源无法被其他pvc使用。 Recycling (回收) pv 可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。 保留策略 - 允许人工处理保留的数据。 删除策略 - 将删除pv和外部关联的存储资源，需要插件支持。 回收策略 - 将执行清除操作，之后可以被新的pvc使用，需要插件支持。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:13","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"StorageClass StorageClass为管理员提供了一种描述他们提供的存储的\"类\"的方法。 不同的类可能映射到服务质量级别，或备份策略，或者由群集管理员确定的任意策略。 Kubernetes本身对于什么类别代表是不言而喻的。 这个概念有时在其他存储系统中称为\"配置文件”。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:14","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Command、Args、Entrypoint、Cmd Command、Args 是属于 kubernetes 的命令 Entrypoint、Cmd 是属于 Docker 的命令 在kubernetes 于 Docker image 中都分别包含命令的情况下: 如果 command 和 args 均没有写, 那么用 Docker image 默认的配置。 如果 command 写了, 但 args 没有写, 那么Docker 默认的配置会被忽略而且仅仅执行kubernetes 的 command（不带任何参数的）。 如果 command 没写, 但 args 写了, 那么Docker 默认配置的 Entrypoint 命令行会被执行, 但是调用的参数是 kubernetes 的 args。 如果 command 和 args 都写了, 那么Docker 默认的配置会被忽略, 而使用 kubernetes 的配置。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:2:15","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Pod 的生命周期 Pod对象自从其创建开始至其终止退出的时间范围称为其生命周期。 创建主容器（main container）为 必需的操作。 初始化容器（init container）。 容器启动后钩子（post start hook）。 容器的存活性探测（liveness probe）。 就绪性探测（readiness probe）。 容器终止前钩子（pre stop hook） 其他Pod的定义操作。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:3:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"初始化容器(init container) 初始化容器（init container）即应用程序的主容器启动之前要运行的容器，常用于为主容器执行一些预置操作，它们具有两 种典型特征。 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成。（注意：如果pod的spec.restartPolicy字段值为 Never，那么运行失败的初始化容器不会被重启。） 每个初始化容器都必须按定义的顺序串行运行。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:3:1","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"pod phase Pod 的 status 属性是一个 PodStatus 对象，拥有一个 phase 字段。它简单描述了 Pod 在其生命周期的阶段。 pod phase 描述 挂起(Pending) kubernetes 通过apiserver创建了pod 资源对象并存入etcd中, 但它尚未被调度完成, 或者仍处于从仓库下载镜像的过程中 运行中(Running) Pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成，至少一个容器正在运行。 成功(Succeeded) Pod中的所有容器都已经成功并且不会被重启。 失败(Failed) Pod中的所有容器都已终止了，并且至少有一个容器是因为失败终止。即容器以非0状态退出或者被系统禁止。 未知(Unknown) ApiServer 无法正常获取到Pod对象的状态信息，通常是由于无法与所在工作节点的kubelet通信所致。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:3:2","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Pod conditions Pod 的 status 属性是一个 PodStatus 对象, 里面包含 PodConditions 数组，代表 Condition 是否通过。 PodCondition 属性描述 字段 描述 lastProbeTime 最后一次探测 Pod Condition 的时间戳。 lastTransitionTime 上次 Condition 从一种状态转换到另一种状态的时间。 message 上次 Condition 状态转换的详细描述。 reason Condition 最后一次转换的原因。 status Condition 状态类型，可以为 True False Unknown type Condition类型(PodScheduled, Ready, Initialized, Unschedulable, ContainersReady) Condition Type 说明 PodScheduled Pod 已被调度到一个节点 Ready Pod 能够提供请求，应该被添加到负载均衡池中以提供服务 Initialized 所有 init containers 成功启动 Unschedulable 调度器不能正常调度容器，例如缺乏资源或其他限制 ContainersReady Pod 中所有容器全部就绪 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:3:3","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Container probes Probe 是在容器上 kubelet 的定期执行的诊断，kubelet 通过调用容器实现的 Handler 来诊断。 Success 容器诊断通过。 Failure 容器诊断失败。 Unknown 诊断失败，因此不应采取任何措施。 Handlers 包含如下三种 ExecAction 在容器内部执行指定的命令，如果命令以状态代码 0 退出，则认为诊断成功。 TCPSocketAction 对指定 IP 和端口的容器执行 TCP 检查，如果端口打开，则认为诊断成功。 HTTPGetAction 对指定 IP + port + path 路径上的容器的执行 HTTP Get 请求。如果响应的状态代码大于或等于 200 且小于 400，则认为诊断成功。 kubelet 可以选择性地对运行中的容器进行两种探测器执行和响应。 livenessProbe 存活性探测, 探测容器是否正在运行，如果活动探测失败，则 kubelet 会杀死容器，并且容器将受其 重启策略 的约束。如果不指定活动探测, 默认状态是 Success。 readinessProbe 就绪性探测, 探测容器是否已准备好为请求提供服务，如果准备情况探测失败，则控制器会从与 Pod 匹配的所有服务的端点中删除 Pod 的 IP 地址。初始化延迟之前的默认准备状态是 Failure, 如果容器未提供准备情况探测，则默认状态为 Success。 以下为 Nginx 应用的两种探针配置示例 apiVersion: apps/v1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 selector: matchLabels: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 name: http # readinessProbe - 检测pod 的 Ready 是否为 true readinessProbe: tcpSocket: port: 80 # 启动后5s 开始检测 initialDelaySeconds: 5 # 检测 间隔为 10s periodSeconds: 10 # livenessProbe - 检测 pod 的 State 是否为 Running livenessProbe: httpGet: path: / port: 80 # 启动后 15s 开始检测 # 检测时间必须在 readinessProbe 之后 initialDelaySeconds: 15 # 检测 间隔为 20s periodSeconds: 20 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:3:4","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Pod RestartPolicy (重启策略) PodSpec 中有一个restartPolicy字段，可能的值为Always、OnFailure和Never。默认为Always。restartPolicy适用于Pod中的所有容器。而且它仅用于控制在同一节点上重新启动Pod对象的相关容器。 首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长依次为10秒、20秒、40秒… 300秒是最大延迟时长。 Pod，一旦绑定到一个节点，Pod对象将永远不会被重新绑定到另一个节点，它要么被重启，要么终止，直到节点发生故障或被删除。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:3:5","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Pod 创建过程 创建一个 pod 的过程 用户通过kubectl或其他API客户端提交了Pod Spec给API Server。 API Server尝试着将Pod对象的相关信息存入etcd中，待写入操作执行完成，API Server即会返回确认信息至客户端。 API Server开始检测etcd中的状态变化。 所有的kubernetes组件均使用watch机制来跟踪检查API Server上的相关的变动。 kube-scheduler（调度器）通过其watcher觉察到API Server创建了新的Pod对象但尚未绑定至任何工作节点。 kube-scheduler（调度器）为Pod对象挑选一个工作节点并将结果信息更新至API Server。 调度结果信息由API Server更新至etcd存储系统，而且API Server也开始反映此Pod对象的调度结果。 Pod被调度到的目标工作节点上的kubelet尝试在当前节点上调用Docker启动容器，并将容器的结果状态返回送至API Server。 API Server将Pod状态信息存入etcd系统中。 在etcd确认写入操作成功完成后，API Server将确认信息发送至相关的kubelet，事件将通过它被接受。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:3:6","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Kubernetes 资源调度与限制 在 Kubernetes 体系中，资源默认是被多租户多应用共享使用的，应用与租户间不可避免地存在资源竞争问题。 在 Kubernetes 中支持分别从 Namespace、Pod 和 Container 三个级别对资源进行管理。 在 Kubernetes 将 cpu 1 Core (核) = 1000m, m 这个单位表示 千分之一核, 2000m 表示 两个完整的核心, 也可以写成2 或者 2.0。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:4:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"ResourceQuota Namespace 级别, 可以通过创建 ResourceQuota 对象对Namespace进行绑定, 提供一个总体资源使用量限制。 可以设置该命名空间中 Pod 可以使用到的计算资源（CPU、内存）、存储资源总量上限。 可以限制该 Namespace 中某种类型对象（如 Pod、RC、Service、Secret、ConfigMap、PVC 等）的总量上限。 ResourceQuota 示例 apiVersion: v1 kind: ResourceQuota metadata: name: quota spec: hard: requests.cpu: \"20\" requests.memory: 30Gi requests.storage: 500Gi requests.ephemeral-storage: 10Gi limits.cpu: \"40\" limits.memory: 60Gi limits.ephemeral-storage: 20Gi pods: \"10\" services: \"5\" replicationcontrollers: \"20\" resourcequotas: \"1\" secrets: \"10\" configmaps: \"10\" persistentvolumeclaims: \"10\" services.nodeports: \"50\" services.loadbalancers: \"10\" requests kubernetes会根据Request的值去查找有足够资源的node来调度此pod, 既超过了 Request 限制的值, Pod 将不会被调度到此node中。 limits 对应资源量的上限, 既最多允许使用这个上限的资源量, 由于cpu是可压缩的, 进程是无法突破上限的, 而memory是不可压缩资源, 当进程试图请求超过limit限制时的memory, 此进程就会被kubernetes杀掉。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:4:1","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"LimitRange LimitRange 对象设置 Namespace 中 Pod 及 Container 的默认资源配额和资源限制。 apiVersion: v1 kind: LimitRange metadata: name: limit spec: limits: - type: Pod max: cpu: \"10\" memory: 100Gi min: cpu: 200m memory: 6Mi maxLimitRequestRatio: cpu: \"2\" memory: \"4\" - type: Container max: cpu: \"2\" memory: 1Gi min: cpu: 100m memory: 3Mi default: cpu: 300m memory: 200Mi defaultRequest: cpu: 200m memory: 100Mi maxLimitRequestRatio: cpu: \"2\" memory: \"4\" - type: PersistentVolumeClaim max: storage: 10Gi min: storage: 5Gi pod 与 Container 以及 pvc 类型可分开定义 LimitRange 分配资源。 limits 字段下面的 default 字段表示每个 Pod 的默认的 limits 配置，所以任何没有分配资源的 limits 的 Pod 都会被自动分配 200Mi limits 的内存和 300m limits 的 CPU。 defaultRequest 字段表示每个 Pod 的默认 requests 配置，所以任何没有分配资源的 requests 的 Pod 都会被自动分配 100Mi requests 的内存和 200m requests 的 CPU。 max 与 min 字段分别限制 type 类型下的 服务最大与最小的限制值。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:4:2","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"ResourceRequests/ResourceLimits 在 Container 级别可以对两种计算资源进行管理 CPU 和 内存。 ResourceRequests 表示容器希望被分配到的可完全保证的资源量，Requests 的值会被提供给 Kubernetes 调度器，以便优化基于资源请求的容器调度。 ResourceLimits 表示容器能用的资源上限，这个上限的值会影响在节点上发生资源竞争时的解决策略。 apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: busybox image: busybox resources: requests: memory: \"100Mi\" cpu: \"200m\" limits: memory: \"200Mi\" cpu: \"250m\" ","date":"2018-01-01","objectID":"/kubernetes-popularization/:4:3","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"kubernetes 与 Cgroup Kubernetes 对内存资源的限制实际上是通过 cgroup 来控制的，cgroup 是容器的一组用来控制内核如何运行进程的相关属性集合。针对内存、CPU 和各种设备都有对应的 cgroup。cgroup 是具有层级的，这意味着每个 cgroup 拥有一个它可以继承属性的父亲，往上一直直到系统启动时创建的 root cgroup。 内存 限制 Kubernetes 通过 cgroup 和 OOM killer 来限制 Pod 的内存资源，当超过内存限制值以后, Kubernetes 会选择好几个进程作为 OOM killer 候选人, 其中最重要的进程是标注为 pause 的进程, 用来为业务容器创建共享的 network 和 namespace, 其 oom_score_adj 值为 -998，可以确保不被杀死。oom_score_adj 值越低就越不容易被杀死, 因为业务容器内pause 之外的所有其他进程的 oom_score_adj 值都相同，所以谁的内存使用量最多，oom_score 值就越高，也就越容易被杀死。 CPU 限制 在 Kubernetes 中设置的 cpu requests 最终会被 cgroup 设置为 cpu.shares 属性的值， cpu limits 会被带宽控制组设置为cpu.cfs_period_us 和 cpu.cfs_quota_us 属性的值。与内存一样，cpu requests 主要用于在调度时通知调度器节点上至少需要多少个 cpu shares 才可以被调度。 cpu requests 与 内存 requests 不同，设置了 cpu requests 会在 cgroup 中设置一个属性，以确保内核会将该数量的 shares 分配给进程。 cpu limits 与 内存 limits 也有所不同。如果容器进程使用的内存资源超过了内存使用限制，那么该进程将会成为 oom-killing 的候选者。但是容器进程基本上永远不能超过设置的 CPU 配额，所以容器永远不会因为尝试使用比分配的更多的 CPU 时间而被驱逐。系统会在调度程序中强制进行 CPU 资源限制，以确保进程不会超过这个限制。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:4:4","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Kubernetes 网络模型 Kubernetes 中每个Pod 都拥有一个独立的IP地址，而且假定所有Pod 都在一个可以直接连通的、扁平的网络空间中，不管是否运行在同一Node上都可以通过Pod的IP来访问。 Kubernetes 中Pod的IP是最小粒度IP。同一个Pod内所有的容器共享一个网络堆栈，该模型称为IP-per-Pod模型。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:5:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Kubernetes 通信 同一个 Node 下, 同一个 pod 内 同一个Pod的容器共享同一个网络命名空间, 它们之间的访问可以用 Pod IP 地址 + 容器端口就可以访问。 同一个 Node 下, 不同的 pod 之间通信 不同的 Node 之间, pod 与 pod 使用 网络组件进行通信 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:5:1","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Kubernetes RBAC 基于角色的访问控制（Role-Based Access Control, 即 “RBAC”）: 使用 “rbac.authorization.k8s.io” API Group 实现授权决策，允许管理员通过 Kubernetes API 动态配置策略。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:6:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"角色 ( ClusterRole 与 Role ) Role（角色） 是一系列权限的集合，例如一个角色可以包含读取 Pod 的权限和列出 Pod 的权限。 Role 只能用来给某个特定 namespace 中的资源作鉴权。 如下是 role 对象的定义的一个 demo 授予 default 这个 namespaces 下的 pods 的权限 kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: default name: demo-role rules: - apiGroups: [\"\"] # 空字符串\"\"表明使用 core API group resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"] 大多数资源由代表其名字的字符串表示，例如 ”pods”，就像它们出现在相关API endpoint 的URL中一样。然而，有一些Kubernetes API还 包含了”子资源”，比如 pod 的 logs。 在Kubernetes中，pod logs endpoint的URL格式为：GET /api/v1/namespaces/{namespace}/pods/{name}/log kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: default name: pod-and-pod-logs-reader rules: - apiGroups: [\"\"] resources: [\"pods\", \"pods/log\"] verbs: [\"get\", \"list\"] 通过 resourceNames 列表，角色可以针对不同种类的请求根据资源名引用资源实例。 当指定了 resourceNames 列表时，不同动作 种类的请求的权限，如使用 ”get”、”delete”、”update”以及”patch”等动词的请求，将被限定到资源列表中所包含的资源实例上。 值得注意的是，如果设置了 resourceNames，则请求所使用的动词不能是 list、watch、create或者delete collection。由于资源名不会出现在 create、list、watch和delete collection 等API请求的URL中，所以这些请求动词不会被设置了resourceNames 的规则所允许，因为规则中的 resourceNames 部分不会匹配这些请求。 如下 如果需要限定一个角色绑定主体只能 ”get” 或者 ”update” 指定的一个 configmap kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: default name: configmap-updater rules: - apiGroups: [\"\"] resources: [\"configmap\"] resourceNames: [\"my-configmap\"] verbs: [\"update\", \"get\"] ClusterRole 对象可以授予与 Role 对象相同的权限，但由于它们属于集群范围对象，也可以使用它们授予对以下几种资源的访问权限 集群范围资源（例如节点，即 node） 非资源类型 endpoint（例如 /healthz） 授权多个 Namespace 如下是 ClusterRole 定义可用于授予用户对某一个 namespace，或者 所有 namespace 的 secret（取决于其绑定方式）的读访问权限 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: # ClusterRole 是集群范围对象，没有 \"namespace\" 区分 name: demo-clusterrole rules: - apiGroups: [\"\"] resources: [\"secrets\"] verbs: [\"get\", \"watch\", \"list\", \"create\", \"delete\"] ","date":"2018-01-01","objectID":"/kubernetes-popularization/:6:1","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"角色绑定 ( ClusterRoleBinding 与 RoleBinding ) RoleBinding 把 Role 或 ClusterRole 中定义的各种权限映射到 User，Service Account 或者 Group，从而让这些用户继承角色在 namespace 中的权限。 如下是 RoleBinding 引用在同一命名空间内定义的Role对象。 # 以下角色绑定定义将允许用户 \"jane\" 从 \"default\" 命名空间中读取pod kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-pods namespace: default subjects: - kind: User name: jane apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: pod-reader apiGroup: rbac.authorization.k8s.io RoleBinding 对象也可以引用一个 ClusterRole 对象用于在 RoleBinding 所在的命名空间内授予用户对所引用的ClusterRole 中定义的命名空间资源的访问权限。这一点允许管理员在整个集群范围内首先定义一组通用的角色，然后再在不同的命名空间中复用这些角色。 如下 RoleBinding 引用的是一个 ClusterRole 对象，但是用户”dave”（即角色绑定主体）还是只能读取”development” 命名空间中的 secret（即RoleBinding所在的命名空间） # 以下角色绑定允许用户\"dave\"读取\"development\"命名空间中的secret。 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-secrets namespace: development # 这里表明仅授权读取\"development\"命名空间中的资源。 subjects: - kind: User name: dave apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io ClusterRoleBinding 让用户继承 ClusterRole 在整个集群中的权限。 如下使用 ClusterRoleBinding 在集群级别和所有命名空间中授予权限。下面示例中所定义的 ClusterRoleBinding 允许在用户组 ”manager” 中的任何用户都可以读取集群中任何命名空间中的 secret 。 # 以下`ClusterRoleBinding`对象允许在用户组\"manager\"中的任何用户都可以读取集群中任何命名空间中的secret。 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: read-secrets-global subjects: - kind: Group name: manager apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: secret-reader apiGroup: rbac.authorization.k8s.io ","date":"2018-01-01","objectID":"/kubernetes-popularization/:6:2","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"默认角色 与 默认角色绑定 API Server 会创建一组默认的 ClusterRole 和 ClusterRoleBinding 对象。这些默认对象中有许多包含 system: 前缀，表明这些资源由 Kubernetes基础组件”拥有”。对这些资源的修改可能导致非功能性集群（non-functional cluster）。 system:node ClusterRole 对象。这个角色定义了 kubelet 的权限。如果这个角色被修改，可能会导致kubelet 无法正常工作。 所有默认的 ClusterRole 和 ClusterRoleBinding 对象都会被标记为 kubernetes.io/bootstrapping=rbac-defaults。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:6:3","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"用户角色 通过命令 kubectl get clusterrole 查看到并不是所有都是以 system:前缀，它们是面向用户的角色。这些角色包含超级用户角色（cluster-admin），即旨在利用 ClusterRoleBinding（cluster-status）在集群范围内授权的角色， 以及那些使用 RoleBinding（admin、edit和view）在特定命名空间中授权的角色。 cluster-admin 超级用户权限，允许对任何资源执行任何操作。在 ClusterRoleBinding 中使用时，可以完全控制集群和所有命名空间中的所有资源。在 RoleBinding 中使用时，可以完全控制 RoleBinding 所在命名空间中的所有资源，包括命名空间自己。 admin 管理员权限，利用 RoleBinding 在某一命名空间内部授予。在 RoleBinding 中使用时，允许针对命名空间内大部分资源的读写访问， 包括在命名空间内创建角色与角色绑定的能力。但不允许对资源配额（resource quota）或者命名空间本身的写访问。 edit 允许对某一个命名空间内大部分对象的读写访问，但不允许查看或者修改角色或者角色绑定。 view 允许对某一个命名空间内大部分对象的只读访问。不允许查看角色或者角色绑定。由于可扩散性等原因，不允许查看 secret 资源。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:6:4","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"API Server 认证机制 通常我们使用客户端 kubectl 来与Kubernetes API Server 交互, 它们之间的接口是REST调用。 为了确保 Kubernetes 集群的安全，API Server 都会对客户端进行身份认证，认证失败则无法调用API。 Pod中访问Kubernetes API Server服务的时候，是以Service方式访问服务名为kubernetes的这个服务，而kubernetes服务又只在HTTPS 443上提供服务，那么如何进行身份认证呢? Service Account Token Kubernetes 有两套账户系统分别是 1. User Account 是给用户使用的, 是全局性的, 主要用于 后端的用户数据库同步。 2. Service Account 是给 Pod 内的进程使用的, 是属于具体的 Namespace 的。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:7:0","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"Service Account 每个Namespace下有一个名为default的默认的Service Account对象，这个Service Account里面有一个名为Tokens的可以当作Volume一样被Mount到Podcast里的Secret,当Pod 启动时，这个Secret会自动被Mount到Pod的指定目录下，用来协助完成Pod中的进程访问API Server时的身份鉴权过程。 kubernetes 开启了 Service Account 会在每个 Namespace 下面都会创建一个默认的 default 的Service Account。 [root@k8s-node-1 ~]# kubectl get sa --all-namespaces |grep default default default 1 13d ingress-nginx default 1 7d16h istio-system default 1 2d20h jicki default 1 2d18h kube-system default 1 13d 每个Service Account 下面都会拥有一个加密过的secret 作为Token。 这个Token 就是 Service Account Token 。这个 Token 才是真正在 API Server 验证(authentication)环节起作用的。 [root@k8s-node-1 ~]# kubectl get sa/default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2020-04-03T10:06:15Z\" name: default namespace: default resourceVersion: \"372\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 23344d2a-08e1-447c-a62e-73b821fcc5ce secrets: - name: default-token-9bcmt 当用户在该Namespace 下创建 pod 的时候都会默认使用这个Service Account 。 [root@k8s-node-1 ~]# kubectl get pods/nginx -o yaml apiVersion: v1 kind: Pod metadata: ..... spec: containers: - image: nginx:alpine imagePullPolicy: IfNotPresent name: nginx resources: {} terminationMessagePath: /dev/termination-log terminationMessagePolicy: File volumeMounts: - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-9bcmt readOnly: true .... volumes: - name: default-token-9bcmt secret: defaultMode: 420 secretName: default-token-9bcmt 查看 secret 的具体内容 ca.crt 这个是API Server的 CA公钥证书, 用于Pod 中的 Process 对 API Server 的服务端数字证书进行校验时使用 Namespace 和 Token 被放到了容器内, 这样容器内就可以通过https的方式通过 SVC - kubernetes 请求访问API Server 。 [root@k8s-node-1 ~]# kubectl get secrets default-token-9bcmt -o yaml apiVersion: v1 data: ca.crt: {base64} namespace: ZGVmYXVsdA== token: {base64} kind: Secret .... type: kubernetes.io/service-account-token Secret Kubernetes 提供了 Secret 来处理敏感信息, 目前Secret的类型有3种 1. Opaque (default): 任意字符串。 2. kubernetes.io/service-account-token: 作用于 Service Account 。 3. kubernetes.io/dockercfg: 作用于Docker registry, 用于下载Docker镜像认证使用。 ","date":"2018-01-01","objectID":"/kubernetes-popularization/:7:1","tags":null,"title":"kubernetes 基础知识","uri":"/kubernetes-popularization/"},{"categories":["kubernetes"],"content":"kubernetes 1.9.0 ipvs coreDNS","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"kubernetes 1.9.0 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:0:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"环境说明 这里配置2个Master 1个node, Master-64 只做 Master, Master-65 既是 Master 也是 Node master-66 只做单纯 Node k8s-master-64: 172.16.1.64 k8s-master-65: 172.16.1.65 k8s-master-66: 172.16.1.66 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:1:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname k8s-master-64: 172.16.1.64 k8s-master-65: 172.16.1.65 k8s-master-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts k8s-master-64: 172.16.1.64 k8s-master-65: 172.16.1.65 k8s-master-66: 172.16.1.66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:2:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:3:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:4:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@k8s-master-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:5:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem 172.16.1.65:/etc/kubernetes/ssl/ scp *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem 172.16.1.66:/etc/kubernetes/ssl/ scp *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 所有服务器预先安装 docker-ce ，官方1.9 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.1 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm rpm -ivh docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch.rpm yum -y install docker-ce-17.03.1.ce # 查看安装 docker version Client: Version: 17.03.1-ce API version: 1.27 Go version: go1.7.5 Git commit: f5ec1e2 Built: Tue Jun 27 02:21:36 2017 OS/Arch: linux/amd64 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:6:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 添加配置 vi /etc/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min Restart=on-abnormal [Install] WantedBy=multi-user.target # 修改其他配置 # 低版本内核， kernel 3.10.x 配置使用 overlay2 vi /etc/docker/daemon.json { \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ] } mkdir -p /etc/systemd/system/docker.service.d/ vi /etc/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --graph=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" vi /etc/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:7:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"安装 etcd 官方地址 https://github.com/coreos/etcd/releases # 下载 二进制文件 wget https://github.com/coreos/etcd/releases/download/v3.2.11/etcd-v3.2.11-linux-amd64.tar.gz tar zxvf etcd-v3.2.11-linux-amd64.tar.gz cd etcd-v3.2.11-linux-amd64 mv etcd etcdctl /usr/bin/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:8:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 etcd 证书这里，默认配置三个，后续如果需要增加，更多的 etcd 节点 这里的认证IP 请多预留几个，以备后续添加能通过认证，不需要重新签发 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@k8s-master-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:9:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 由于 etcd 是最重要的组件，所以 –data-dir 请配置到其他路径中 # 创建 etcd data 目录， 并授权 useradd etcd mkdir -p /opt/etcd chown -R etcd:etcd /opt/etcd # etcd-1 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.64:2380 \\ --listen-peer-urls=https://172.16.1.64:2380 \\ --listen-client-urls=https://172.16.1.64:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.64:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.65:2380 \\ --listen-peer-urls=https://172.16.1.65:2380 \\ --listen-client-urls=https://172.16.1.65:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.65:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /etc/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/opt/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.66:2380 \\ --listen-peer-urls=https://172.16.1.66:2380 \\ --listen-client-urls=https://172.16.1.66:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.66:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380 \\ --initial-cluster-state=new \\ --data-dir=/opt/etcd/ Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:10:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl daemon-reload systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:11:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:12:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.9.0/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl} /usr/local/bin/ scp server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} 172.16.1.65:/usr/local/bin/ scp server/bin/{kube-proxy,kubelet} 172.16.1.66:/usr/local/bin/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:1","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@k8s-master-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:2","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:3","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个。 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:4","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@k8s-master-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:5","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@k8s-master-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' d2d7f3a19490ff667fbe94b0f31f9967 # 创建 token.csv 文件 cd /opt/ssl vi token.csv d2d7f3a19490ff667fbe94b0f31f9967,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 172.16.1.65:/etc/kubernetes/ # 生成高级审核配置文件 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:6","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/18 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # k8s 1.8 添加 --authorization-mode=Node # k8s 1.8 添加 --admission-control=NodeRestriction # k8s 1.8 添加 --audit-policy-file=/etc/kubernetes/audit-policy.yaml # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:7","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:8","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/18 \\ --cluster-cidr=10.254.64.0/18 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:9","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:10","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:11","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:12","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@k8s-master-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@k8s-master-65 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:13","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:14","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=d2d7f3a19490ff667fbe94b0f31f9967 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:15","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 \u003e 配置为 node 本机 IP mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=k8s-master-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=2 [Install] WantedBy=multi-user.target # 如上配置: k8s-master-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:16","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:17","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-UstWvAjJXHrfgNxDgtwl4_xHMBNONvEfAO1f_l-x9cM 15m kubelet-bootstrap Approved,Issued # 增加 认证 kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:18","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"验证 nodes [root@k8s-master-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-65 Ready \u003cnone\u003e 10m v1.9.0 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 这里注意如果 csr 被删除了，请删除如下文件，并重启 kubelet 服务 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:13:19","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:14:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@k8s-master-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:14:1","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 scp kube-proxy*.pem 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy*.pem 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:15:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到需要的 node 端里 scp kube-proxy.kubeconfig 172.16.1.55:/etc/kubernetes/ scp kube-proxy.kubeconfig 172.16.1.56:/etc/kubernetes/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:15:1","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 1.9 官方 ipvs 已经 beta , 尝试开启 ipvs 测试一下. 官方 –feature-gates=SupportIPVSProxyMode=false 默认是 false 需要打开 –feature-gates=SupportIPVSProxyMode=true –masquerade-all 必须添加这项配置，否则 创建 svc 在 ipvs 不会添加规则 打开 ipvs 需要安装 ipvsadm 软件， 在 node 中安装 yum install ipvsadm -y # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.65 \\ --hostname-override=k8s-master-65 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:15:2","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 检查 ipvs [root@k8s-master-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 至此 Master 端 与 Master and Node 端的安装完毕 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:15:3","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理； # 在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 # kubelet、kube-proxy 连接本地的 nginx 代理端口， # 当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:16:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"发布证书 # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:16:1","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.7-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:16:2","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 Kubelet.service 文件 # 创建目录 mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=k8s-master-66 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=2 [Install] WantedBy=multi-user.target # 启动 kubelet 与 kube-proxy systemctl daemon-reload systemctl start kubelet systemctl status kubelet ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:16:3","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 kube-proxy.service # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.66 \\ --hostname-override=k8s-master-66 \\ --cluster-cidr=10.254.64.0/18 \\ --masquerade-all \\ --feature-gates=SupportIPVSProxyMode=true \\ --proxy-mode=ipvs \\ --ipvs-min-sync-period=5s \\ --ipvs-sync-period=5s \\ --ipvs-scheduler=rr \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 systemctl start kube-proxy systemctl status kube-proxy ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:16:4","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"Master 配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-GDfx4Va48hCyWkN7-WjwRFzqcF99zm1R4rj30Q4tcoA 38m kubelet-bootstrap Approved,Issued node-csr-UstWvAjJXHrfgNxDgtwl4_xHMBNONvEfAO1f_l-x9cM 53m kubelet-bootstrap Approved,Issued # 增加 认证 [root@k8s-master-64 ~]# kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve [root@k8s-master-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-64 Ready \u003cnone\u003e 2h v1.9.0 k8s-master-65 Ready \u003cnone\u003e 2h v1.9.0 k8s-master-66 Ready \u003cnone\u003e 2h v1.9.0 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:16:5","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"限制 POD 的调度 由于 master-64 只做 master 不做 pod 调度，所以禁止调度到 master-64中, Pod 的调度是通过 kubelet 服务来启动的，但是不启动 kubelet 的话，节点在 node 里是不可见的。 [root@k8s-master-64 ~]# kubectl cordon k8s-master-64 node \"k8s-master-64\" cordoned [root@k8s-master-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-64 Ready,SchedulingDisabled \u003cnone\u003e 2h v1.9.0 k8s-master-65 Ready \u003cnone\u003e 2h v1.9.0 k8s-master-66 Ready \u003cnone\u003e 2h v1.9.0 配置 Flannel 网络 flannel 网络只部署在 kube-proxy 相关机器 个人 百度盘 下载 https://pan.baidu.com/s/1eStojia rpm -ivh flannel-0.9.1-1.x86_64.rpm # 配置 flannel # 由于我们docker更改了 docker.service.d 的路径 # 所以这里把 flannel.conf 的配置拷贝到 这个目录去 mv /usr/lib/systemd/system/docker.service.d/flannel.conf /etc/systemd/system/docker.service.d # 配置 flannel 网段 etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ set /flannel/network/config \\ '{\"Network\":\"10.254.64.0/18\",\"SubnetLen\":24,\"Backend\":{\"Type\":\"host-gw\"}}' # 修改 flanneld 配置 vi /etc/sysconfig/flanneld # Flanneld configuration options # etcd 地址 FLANNEL_ETCD_ENDPOINTS=\"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" # 配置为上面的路径 flannel/network FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 其他的配置，可查看 flanneld --help,这里添加了 etcd ssl 认证 FLANNEL_OPTIONS=\"-ip-masq=true -etcd-cafile=/etc/kubernetes/ssl/ca.pem -etcd-certfile=/etc/kubernetes/ssl/etcd.pem -etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem -iface=enp2s0f0\" # 启动 flannel systemctl daemon-reload systemctl enable flanneld systemctl start flanneld systemctl status flanneld # 如果报错 请使用 journalctl -f -t flanneld 和 journalctl -u flanneld 来定位问题 # 配置完毕，重启 docker systemctl daemon-reload systemctl restart docker systemctl status docker # 重启 kubelet systemctl daemon-reload systemctl restart kubelet systemctl status kubelet # 验证 网络 ifconfig 查看 docker0 网络 是否已经更改为配置IP网段 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:16:6","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-master-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-84f8f49555-dzpm9 1/1 Running 0 6s 10.254.90.2 k8s-master-65 nginx-dm-84f8f49555-qbnvv 1/1 Running 0 6s 10.254.66.2 k8s-master-66 [root@k8s-master-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.41.39 \u003cnone\u003e 80/TCP 1m # 在 安装了 Flannel 网络的节点 里 curl [root@k8s-master-64 ~]# curl 10.254.51.137 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 查看 ipvs 规则 [root@k8s-master-65 ~]# ipvsadm -L -n IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u003e RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.254.0.1:443 rr persistent 10800 -\u003e 172.16.1.64:6443 Masq 1 0 0 -\u003e 172.16.1.65:6443 Masq 1 0 0 TCP 10.254.41.39:80 rr -\u003e 10.254.66.2:80 Masq 1 0 0 -\u003e 10.254.90.2:80 Masq 1 0 1 配置 CoreDNS 官方 地址 https://coredns.io ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:17:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"下载镜像 # 官方镜像 coredns/coredns:latest # 我的镜像 jicki/coredns:latest ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:18:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"创建 yaml 文件 # vi coredns.yaml apiVersion: v1 kind: ServiceAccount metadata: name: coredns namespace: kube-system labels: kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: Reconcile name: system:coredns rules: - apiGroups: - \"\" resources: - endpoints - services - pods - namespaces verbs: - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" labels: kubernetes.io/bootstrapping: rbac-defaults addonmanager.kubernetes.io/mode: EnsureExists name: system:coredns roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:coredns subjects: - kind: ServiceAccount name: coredns namespace: kube-system --- apiVersion: v1 kind: ConfigMap metadata: name: coredns namespace: kube-system data: Corefile: | .:53 { errors log health kubernetes cluster.local 10.254.0.0/18 proxy . /etc/resolv.conf cache 30 } --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\" spec: replicas: 1 selector: matchLabels: k8s-app: coredns template: metadata: labels: k8s-app: coredns annotations: scheduler.alpha.kubernetes.io/critical-pod: '' scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"CriticalAddonsOnly\", \"operator\":\"Exists\"}]' spec: serviceAccountName: coredns containers: - name: coredns image: jicki/coredns:latest imagePullPolicy: Always args: [ \"-conf\", \"/etc/coredns/Corefile\" ] volumeMounts: - name: config-volume mountPath: /etc/coredns ports: - containerPort: 53 name: dns protocol: UDP - containerPort: 53 name: dns-tcp protocol: TCP livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 60 timeoutSeconds: 5 successThreshold: 1 failureThreshold: 5 dnsPolicy: Default volumes: - name: config-volume configMap: name: coredns items: - key: Corefile path: Corefile --- apiVersion: v1 kind: Service metadata: name: coredns namespace: kube-system labels: k8s-app: coredns kubernetes.io/cluster-service: \"true\" kubernetes.io/name: \"CoreDNS\" spec: selector: k8s-app: coredns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP # 配置说明 # 这里 kubernetes cluster.local 为 创建 svc 的 IP 段 kubernetes cluster.local 10.254.0.0/18 # clusterIP 为 指定 DNS 的 IP clusterIP: 10.254.0.2 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:19:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 导入 [root@k8s-master-64 coredns]# kubectl apply -f coredns.yaml serviceaccount \"coredns\" created clusterrole \"system:coredns\" created clusterrolebinding \"system:coredns\" created configmap \"coredns\" created deployment \"coredns\" created service \"coredns\" created ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:20:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"查看 coredns 服务 [root@k8s-master-64 coredns]# kubectl get pod,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-6bd7d5dbb5-jh4fj 1/1 Running 0 19s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/coredns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 19s ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:21:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"检查日志 [root@k8s-master-64 coredns]# kubectl logs -n kube-system coredns-6bd7d5dbb5-jh4fj .:53 CoreDNS-1.0.1 linux/amd64, go1.9.2, 99e163c3 2017/12/20 09:34:24 [INFO] CoreDNS-1.0.1 2017/12/20 09:34:24 [INFO] linux/amd64, go1.9.2, 99e163c3 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:22:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 创建的服务 [root@k8s-master-64 yaml]# kubectl get pods,svc NAME READY STATUS RESTARTS AGE po/alpine 1/1 Running 0 19s po/nginx-dm-84f8f49555-tmqzm 1/1 Running 0 23s po/nginx-dm-84f8f49555-wdk67 1/1 Running 0 23s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 5h svc/nginx-svc ClusterIP 10.254.40.179 \u003cnone\u003e 80/TCP 23s # 测试 [root@k8s-master-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.40.179 nginx-svc.default.svc.cluster.local [root@k8s-master-64 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.254.0.1 kubernetes.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:23:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"部署 dashboard \u0026\u0026 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:24:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.8.1 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.8.1 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:25:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:26:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 导入文件 [root@k8s-master-64 dashboard]# kubectl apply -f kubernetes-dashboard.yaml secret \"kubernetes-dashboard-certs\" created serviceaccount \"kubernetes-dashboard\" created role \"kubernetes-dashboard-minimal\" created rolebinding \"kubernetes-dashboard-minimal\" created deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created [root@k8s-master-64 ~]# kubectl get pods,svc -n kube-system NAME READY STATUS RESTARTS AGE po/coredns-6bd7d5dbb5-jh4fj 1/1 Running 0 19s po/kubernetes-dashboard-6685778fc9-nl8wt 1/1 Running 0 34s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/coredns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 14m svc/kubernetes-dashboard ClusterIP 10.254.56.83 \u003cnone\u003e 443/TCP 34s # 目前 dashboar 版本，只支持 https 访问 node 里访问 8443 端口 ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:27:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github: https://github.com/kubernetes/ingress-nginx/ ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:28:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@k8s-master-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-64 Ready,SchedulingDisabled \u003cnone\u003e 3h v1.9.0 k8s-master-65 Ready \u003cnone\u003e 3h v1.9.0 k8s-master-66 Ready \u003cnone\u003e 3h v1.9.0 # 对 65 与 66 打上 label [root@k8s-master-64 ingress]# kubectl label nodes k8s-master-65 ingress=proxy node \"k8s-master-65\" labeled [root@k8s-master-64 ingress]# kubectl label nodes k8s-master-66 ingress=proxy node \"k8s-master-66\" labeled # 打完标签以后 [root@k8s-master-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master-64 Ready,SchedulingDisabled \u003cnone\u003e 3h v1.9.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-master-64 k8s-master-65 Ready \u003cnone\u003e 3h v1.9.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-master-65 k8s-master-66 Ready \u003cnone\u003e 3h v1.9.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-master-66 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.4 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0 # 国内镜像 jicki/defaultbackend:1.4 jicki/nginx-ingress-controller:0.9.0 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 vi with-rbac.yaml spec: replicas: 2 .... spec: serviceAccountName: nginx-ingress-serviceaccount hostNetwork: true nodeSelector: ingress: proxy .... # 导入 yaml 文件 [root@k8s-master-64 ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@k8s-master-64 nginx-ingress]# kubectl apply -f . configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created namespace \"ingress-nginx\" configured serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 29 中 [root@k8s-master-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE default-http-backend-76f7d74455-ctmwq 1/1 Running 0 20s 10.254.90.6 k8s-master-65 nginx-ingress-controller-7c7f68fdd4-29t4p 1/1 Running 0 20s 172.16.1.66 k8s-master-66 nginx-ingress-controller-7c7f68fdd4-l58bf 1/1 Running 0 20s 172.16.1.65 k8s-master-65 # 查看我们原有的 svc [root@k8s-master-64 ingress]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 24m nginx-dm-84f8f49555-tmqzm 1/1 Running 0 24m nginx-dm-84f8f49555-wdk67 1/1 Running 0 24m # 创建一个 基于 nginx-dm 的 ingress vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.ji","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:29:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"基础维护 # 当需要对主机进行维护升级时，首先将节点主机设置成不可调度模式： kubectl cordon［nodeid］ # 然后需要将主机上正在运行的容器驱赶到其它可用节点： kubectl drain ［nodeid］ # 给予900秒宽限期优雅的调度 kubectl drain node1.k8s.novalocal --grace-period=120 # 当容器迁移完毕后，运维人员可以对该主机进行操作，配置升级性能参数调优等等。当对主机的维护操作完毕后， 再将主机设置成可调度模式： kubectl uncordon [nodeid] Other ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:30:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2017-12-20","objectID":"/kubernetes-1.9-ipvs/:31:0","tags":null,"title":"kubernetes 1.9.0 ipvs coreDNS","uri":"/kubernetes-1.9-ipvs/"},{"categories":["kubernetes"],"content":"kubernetes 1.8.4 to kubespray","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"kubernetes 1.8.4 to kubespray 官方 github https://github.com/kubernetes-incubator/kubespray 环境准备 ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:0:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"初始化环境 节点 IP 172.16.1.64 node1 172.16.1.65 node2 172.16.1.66 node3 ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:1:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"设置hostname hostnamectl --static set-hostname hostname IP hostname 172.16.1.64 node1 172.16.1.65 node2 172.16.1.66 node3 ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:2:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"配置 hosts vi /etc/hosts IP hostname 172.16.1.64 node1 172.16.1.65 node2 172.16.1.66 node3 ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:3:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"修改内核 vi /etc/sysctl.conf # docker net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:4:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"配置SSH Key 登陆 # 确保本机也可以 ssh 连接，否则下面部署失败 ssh-keygen -t rsa -N \"\" ssh-copy-id -i /root/.ssh/id_rsa.pub 172.16.1.65 ssh-copy-id -i /root/.ssh/id_rsa.pub 172.16.1.66 配置 kubespray ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:5:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"安装基础软件 # 安装 git yum -y install git # 安装 centos 额外的yum源 yum install -y epel-release # make 缓存 yum clean all \u0026\u0026 yum makecache # 安装 软件 yum install -y python-pip python34 python-netaddr python34-pip ansible ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:6:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"下载源码 # git clone git clone https://github.com/kubernetes-incubator/kubespray ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:7:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"下载镜像 老问题，国外镜像被墙, 我已经上传至 docker 官方的 hub 中, 这里需要自行安装 docker . jicki/hyperkube:v1.8.4_coreos.0 jicki/flannel:v0.9.1 jicki/flannel-cni:v0.3.0 jicki/etcd:v3.2.4 jicki/routereflector:v0.4.0 jicki/cluster-proportional-autoscaler-amd64:1.1.1 jicki/k8s-dns-sidecar-amd64:1.14.7 jicki/k8s-dns-kube-dns-amd64:1.14.7 jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.7 jicki/kubernetes-dashboard-init-amd64:v1.0.1 jicki/kubernetes-dashboard-amd64:v1.7.1 jicki/pause-amd64:3.0 images=(cluster-proportional-autoscaler-amd64:1.1.1 k8s-dns-sidecar-amd64:1.14.7 k8s-dns-kube-dns-amd64:1.14.7 k8s-dns-dnsmasq-nanny-amd64:1.14.7 pause-amd64:3.0 hyperkube:v1.8.4_coreos.0 flannel-cni:v0.3.0 flannel:v0.9.1 etcd:v3.2.4 routereflector:v0.4.0 kubernetes-dashboard-init-amd64:v1.0.1 kubernetes-dashboard-amd64:v1.7.1) for imageName in ${images[@]} ; do docker pull jicki/$imageName done ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:8:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"修改配置 cd kubespray/inventory/group_vars vi k8s-cluster.yml # k8s-cluster 为控制一些基础信息的配置文件。 # all.yml 控制一些需要详细配置的信息 # 这里打开 kubelet_load_modules: true # API 负载均衡，否在默认都连接到第一台master (坑爹) loadbalancer_apiserver_localhost: true # 修改 api 密码 vi roles/kubespray-defaults/defaults/main.yaml kube_api_pwd: xxxx # 修改 flannel 网络的模式 默认是 vxlan 修改为 host-gw # 注 host-gw 模式 服务器必须 二层互通 vi roles/network_plugin/flannel/defaults/main.yml flannel_backend_type: \"vxlan\" 修改 flannel_backend_type: \"host-gw\" ## 修改 证书过期时间 -days xxxx vi /opt/kubespray/roles/kubernetes/secrets/files/make-ssl.sh openssl req -x509 -new -nodes -key ca-key.pem -days 10000 -out ca.pem -subj \"/CN=kube-ca\" \u003e /dev/null 2\u003e\u00261 openssl x509 -req -in ${name}.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out ${name}.pem -days 3650 -extensions v3_req -extfile ${CONFIG} \u003e /dev/null 2\u003e\u00261 ## 修改所有 images 的地址为个人的仓库地址 roles/download/defaults/main.yml roles/dnsmasq/templates/dnsmasq-autoscaler.yml.j2 roles/kubernetes-apps/ansible/defaults/main.yml sed -i 's/gcr\\.io\\/google_containers/jicki/g' roles/download/defaults/main.yml sed -i 's/gcr\\.io\\/google_containers/jicki/g' roles/dnsmasq/templates/dnsmasq-autoscaler.yml.j2 sed -i 's/gcr\\.io\\/google_containers/jicki/g' roles/kubernetes-apps/ansible/defaults/main.yml roles/download/defaults/main.yml roles/kubernetes-apps/local_volume_provisioner/defaults/main.yml sed -i 's/quay\\.io\\/coreos/jicki/g' roles/download/defaults/main.yml sed -i 's/quay\\.io\\/calico/jicki/g' roles/download/defaults/main.yml sed -i 's/quay\\.io\\/external_storage/jicki/g' roles/kubernetes-apps/local_volume_provisioner/defaults/main.yml ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:9:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"初始化 cd kubespray CONFIG_FILE=inventory/inventory.cfg python3 contrib/inventory_builder/inventory.py 172.16.1.64 172.16.1.65 172.16.1.66 # 初始化以后会生成一个 inventory.cfg 文件，如果SSH 端口不同，需要打开修改 cd kubespray/inventory vi inventory.cfg [all] node1 ansible_host=172.16.1.64 ansible_port=33 ip=172.16.1.64 node2 ansible_host=172.16.1.65 ansible_port=33 ip=172.16.1.65 node3 ansible_host=172.16.1.66 ansible_port=33 ip=172.16.1.66 [kube-master] node1 node2 [kube-node] node1 node2 node3 [etcd] node1 node2 node3 [k8s-cluster:children] kube-node kube-master [calico-rr] [vault] node1 node2 node3 ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:10:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"部署集群 ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa PLAY RECAP *************************************************************************************************************************************************************** localhost : ok=2 changed=0 unreachable=0 failed=0 node1 : ok=289 changed=24 unreachable=0 failed=0 node2 : ok=242 changed=13 unreachable=0 failed=0 node3 : ok=216 changed=0 unreachable=0 failed=0 Friday 08 December 2017 14:52:54 +0800 (0:00:00.039) 0:05:44.482 ******* # 查看 flannel 模式 FLANNEL_MTU = 1450 是 vxlan FLANNEL_MTU = 1500 是 host-gw [root@node1 kubespray]# cat /var/run/flannel/subnet.env FLANNEL_NETWORK=10.233.64.0/18 FLANNEL_SUBNET=10.233.66.1/24 FLANNEL_MTU=1500 FLANNEL_IPMASQ=true ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:11:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"查看集群 [root@node1 kubespray]# kubectl get nodes NAME STATUS ROLES AGE VERSION node1 Ready master,node 6m v1.8.4+coreos.0 node2 Ready master,node 6m v1.8.4+coreos.0 node3 Ready node 6m v1.8.4+coreos.0 [root@node1 kubespray]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE kube-apiserver-node1 1/1 Running 0 22m kube-apiserver-node2 1/1 Running 0 22m kube-controller-manager-node1 1/1 Running 0 18m kube-controller-manager-node2 1/1 Running 0 18m kube-dns-7ff67768b6-mx28d 3/3 Running 0 17m kube-dns-7ff67768b6-xkjl5 3/3 Running 0 17m kube-flannel-9fmr4 2/2 Running 0 17m kube-flannel-l9w4s 2/2 Running 0 17m kube-flannel-vktd9 2/2 Running 0 17m kube-proxy-node1 1/1 Running 0 22m kube-proxy-node2 1/1 Running 0 22m kube-proxy-node3 1/1 Running 0 22m kube-scheduler-node1 1/1 Running 0 18m kube-scheduler-node2 1/1 Running 0 18m kubedns-autoscaler-dcb9b446b-m87sx 1/1 Running 0 17m kubernetes-dashboard-77c965686c-tq4zw 1/1 Running 0 17m nginx-proxy-node3 1/1 Running 0 21m ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:12:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"测试集群 验证 dns 服务，以及 网络 # 创建一个 deployment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@node1 yaml]# kubectl apply -f nginx-deployment.yaml deployment \"nginx-dm\" created service \"nginx-svc\" created [root@node1 yaml]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-55b58f68b6-7l5qr 1/1 Running 0 27s 10.233.65.3 node2 nginx-dm-55b58f68b6-dkhvl 1/1 Running 0 27s 10.233.64.17 node1 nginx-dm-55b58f68b6-xsvmr 1/1 Running 0 27s 10.233.66.3 node3 [root@node1 yaml]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.233.0.1 \u003cnone\u003e 443/TCP 30m \u003cnone\u003e nginx-svc ClusterIP 10.233.38.166 \u003cnone\u003e 80/TCP 1m name=nginx [root@node1 yaml]# curl 10.233.38.166 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 创建一个 pods apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done [root@node1 yaml]# kubectl apply -f alpine-pods.yaml pod \"alpine\" created [root@node1 yaml]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 19s [root@node1 yaml]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.233.38.166 nginx-svc.default.svc.cluster.local [root@node1 yaml]# kubectl exec -it alpine nslookup kubernetes nslookup: can't resolve '(null)': Name does not resolve Name: kubernetes Address 1: 10.233.0.1 kubernetes.default.svc.cluster.local ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:13:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"部署 ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github https://github.com/kubernetes/ingress-nginx # 下载镜像 jicki/nginx-ingress-controller:0.9.0 jicki/defaultbackend:1.4 # 下载 yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/configmap.yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/tcp-services-configmap.yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/udp-services-configmap.yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # 查看下载 [root@node1 nginx-ingress]# ls -lt 总用量 28 -rw-r--r--. 1 root root 89 12月 8 15:36 udp-services-configmap.yaml -rw-r--r--. 1 root root 1865 12月 8 15:36 with-rbac.yaml -rw-r--r--. 1 root root 63 12月 8 15:36 namespace.yaml -rw-r--r--. 1 root root 2385 12月 8 15:36 rbac.yaml -rw-r--r--. 1 root root 89 12月 8 15:36 tcp-services-configmap.yaml -rw-r--r--. 1 root root 129 12月 8 15:36 configmap.yaml -rw-r--r--. 1 root root 1131 12月 8 15:36 default-backend.yaml # 修改 镜像下载地址 sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 为 node2 node3 打上相同的 label [root@node1 nginx-ingress]# kubectl label nodes node2 ingress=nginx node \"node2\" labeled [root@node1 nginx-ingress]# kubectl label nodes node3 ingress=nginx node \"node3\" labeled # 删除 label kubectl label node node2 ingress- kubectl label node node3 ingress- # 查看 label [root@node1 nginx-ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS node1 Ready master,node 1h v1.8.4+coreos.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=node1,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true node2 Ready master,node 1h v1.8.4+coreos.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node2,node-role.kubernetes.io/master=true,node-role.kubernetes.io/node=true node3 Ready node 59m v1.8.4+coreos.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=node3,node-role.kubernetes.io/node=true # 修改相关配置 vi with-rbac.yaml # 这里配置 replicas: 2 , 上面配置了2个 labels # 修改 yaml 文件 增加 hostNetwork 和 nodeSelector, 第二个 spec 下 增加。 spec: hostNetwork: true serviceAccountName: nginx-ingress-serviceaccount nodeSelector: ingress: nginx # 导入 yaml [root@node1 nginx-ingress]# kubectl apply -f namespace.yaml namespace \"ingress-nginx\" created [root@node1 nginx-ingress]# kubectl apply -f . serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created configmap \"tcp-services\" created configmap \"udp-services\" created deployment \"nginx-ingress-controller\" created configmap \"nginx-configuration\" created deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看服务 [root@node1 nginx-ingress]# kubectl get pods -n ingress-nginx NAME READY STATUS RESTARTS AGE default-http-backend-7f47b7d69b-wdj5j 1/1 Running 0 11s nginx-ingress-controller-c646dfddc-6cw29 1/1 Running 0 11s nginx-ingress-controller-c646dfddc-g9krl 1/1 Running 0 11s # 编写 ingress yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 # 查看服务 [root@node1 yaml]# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 1m [root@node1 yaml]# curl","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:14:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"equalto 错误 # 升级 Jinja2 pip install --upgrade Jinja2 ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:15:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"Stop if swap enabled # 报如下错误: TASK [kubernetes/preinstall : Stop if swap enabled] ********************************************************************************************************************** Wednesday 06 December 2017 15:40:29 +0800 (0:00:00.064) 0:00:10.135 **** fatal: [node1]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false, \"failed\": true } fatal: [node2]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false, ▽ \"failed\": true } fatal: [node3]: FAILED! =\u003e { \"assertion\": \"ansible_swaptotal_mb == 0\", \"changed\": false, \"evaluated_to\": false, \"failed\": true } to retry, use: --limit @/opt/kubespray/cluster.retry ## 解决方法 # 在配置服务器上执行 cd /tmp rm -rf node* # 在每台服务器上执行 swapoff -a # 永久关闭 swap 修改 /etc/fstab 里面 swap 的相关 mount ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:16:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"清理集群 ansible-playbook -i inventory/inventory.cfg reset.yml -b -v --private-key=~/.ssh/id_rsa rm -rf /etc/kubernetes/ rm -rf /var/lib/kubelet rm -rf /var/lib/etcd rm -rf /var/lib/cni rm -rf /usr/local/bin/kubectl rm -rf /opt/cni rm -rf /var/log/containers/ rm -rf /etc/systemd/system/calico-node.service rm -rf /etc/systemd/system/kubelet.service rm -rf /tmp/node* rm -rf /usr/libexec/kubernetes systemctl stop etcd.service systemctl disable etcd.service systemctl stop calico-node.service systemctl disable calico-node.service systemctl stop docker rm -rf /opt/docker systemctl start docker ","date":"2017-12-08","objectID":"/kubernetes-kubespray-1.8.4/:17:0","tags":null,"title":"kubernetes 1.8.4 to kubespray","uri":"/kubernetes-kubespray-1.8.4/"},{"categories":["kubernetes"],"content":"kubernetes 1.8.3","date":"2017-11-17","objectID":"/kubernetes-1.8.3/","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"kubernetes 1.8.3 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:0:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"环境说明 k8s-master-64: 172.16.1.64 k8s-master-65: 172.16.1.65 k8s-master-66: 172.16.1.66 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:1:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname k8s-master-64: 172.16.1.64 k8s-master-65: 172.16.1.65 k8s-master-66: 172.16.1.66 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts k8s-master-64: 172.16.1.64 k8s-master-65: 172.16.1.65 k8s-master-66: 172.16.1.66 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:2:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:3:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:4:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@k8s-master-64 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:5:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl cp ca.csr /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem 172.16.1.65:/etc/kubernetes/ssl/ scp *.csr 172.16.1.65:/etc/kubernetes/ssl/ scp *.pem 172.16.1.66:/etc/kubernetes/ssl/ scp *.csr 172.16.1.66:/etc/kubernetes/ssl/ 安装 docker 所有服务器预先安装 docker-ce ，官方1.8 中提示， 目前 k8s 支持最高 Docker versions 1.11.2, 1.12.6, 1.13.1, and 17.03.2 # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 查看yum 版本 yum list docker-ce.x86_64 --showduplicates |sort -r # 安装指定版本 docker-ce 17.03 被 docker-ce-selinux 依赖, 不能直接yum 安装 docker-ce-selinux wget https://download.docker.com/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm rpm -ivh docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm yum -y install docker-ce-17.03.2.ce # 查看安装 docker version Client: Version: 17.03.2-ce API version: 1.27 Go version: go1.7.5 Git commit: f5ec1e2 Built: Tue Jun 27 02:21:36 2017 OS/Arch: linux/amd64 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:6:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 修改配置 vi /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS $DOCKER_OPTS $DOCKER_DNS_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target # 修改其他配置 mkdir -p /usr/lib/systemd/system/docker.service.d/ vi /usr/lib/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 --graph=/opt/docker --registry-mirror=http://b438f72b.m.daocloud.io --disable-legacy-registry\" vi /usr/lib/systemd/system/docker.service.d/docker-dns.conf # 添加如下 : [Service] Environment=\"DOCKER_DNS_OPTIONS=\\ --dns 10.254.0.2 --dns 114.114.114.114 \\ --dns-search default.svc.cluster.local --dns-search svc.cluster.local \\ --dns-opt ndots:2 --dns-opt timeout:2 --dns-opt attempts:2 \\ # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker etcd 集群 etcd 是k8s集群最重要的组件， etcd 挂了，集群就挂了 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:7:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"安装 etcd rpm -ivh etcd-3.2.9-1.x86_64.rpm ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:8:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@k8s-master-64 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.65:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.66:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:9:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 修改 etcd 配置文件 /etc/etcd/etcd.conf # etcd-1 mv /etc/etcd/etcd.conf /etc/etcd/etcd.conf-bak vi /etc/etcd/etcd.conf # [member] ETCD_NAME=etcd1 ETCD_DATA_DIR=\"/var/lib/etcd/etcd1.etcd\" ETCD_WAL_DIR=\"/var/lib/etcd/wal\" ETCD_SNAPSHOT_COUNT=\"100\" ETCD_HEARTBEAT_INTERVAL=\"100\" ETCD_ELECTION_TIMEOUT=\"1000\" ETCD_LISTEN_PEER_URLS=\"https://172.16.1.64:2380\" ETCD_LISTEN_CLIENT_URLS=\"https://172.16.1.64:2379,http://127.0.0.1:2379\" ETCD_MAX_SNAPSHOTS=\"5\" ETCD_MAX_WALS=\"5\" #ETCD_CORS=\"\" # [cluster] ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.16.1.64:2380\" # if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. \"test=http://...\" ETCD_INITIAL_CLUSTER=\"etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"https://172.16.1.64:2379\" #ETCD_DISCOVERY=\"\" #ETCD_DISCOVERY_SRV=\"\" #ETCD_DISCOVERY_FALLBACK=\"proxy\" #ETCD_DISCOVERY_PROXY=\"\" #ETCD_STRICT_RECONFIG_CHECK=\"false\" #ETCD_AUTO_COMPACTION_RETENTION=\"0\" # [proxy] #ETCD_PROXY=\"off\" #ETCD_PROXY_FAILURE_WAIT=\"5000\" #ETCD_PROXY_REFRESH_INTERVAL=\"30000\" #ETCD_PROXY_DIAL_TIMEOUT=\"1000\" #ETCD_PROXY_WRITE_TIMEOUT=\"5000\" #ETCD_PROXY_READ_TIMEOUT=\"0\" # [security] ETCD_CERT_FILE=\"/etc/kubernetes/ssl/etcd.pem\" ETCD_KEY_FILE=\"/etc/kubernetes/ssl/etcd-key.pem\" ETCD_CLIENT_CERT_AUTH=\"true\" ETCD_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\" ETCD_AUTO_TLS=\"true\" ETCD_PEER_CERT_FILE=\"/etc/kubernetes/ssl/etcd.pem\" ETCD_PEER_KEY_FILE=\"/etc/kubernetes/ssl/etcd-key.pem\" ETCD_PEER_CLIENT_CERT_AUTH=\"true\" ETCD_PEER_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\" ETCD_PEER_AUTO_TLS=\"true\" # [logging] #ETCD_DEBUG=\"false\" # examples for -log-package-levels etcdserver=WARNING,security=DEBUG #ETCD_LOG_PACKAGE_LEVELS=\"\" # etcd-2 mv /etc/etcd/etcd.conf /etc/etcd/etcd.conf-bak vi /etc/etcd/etcd.conf # [member] ETCD_NAME=etcd2 ETCD_DATA_DIR=\"/var/lib/etcd/etcd2.etcd\" ETCD_WAL_DIR=\"/var/lib/etcd/wal\" ETCD_SNAPSHOT_COUNT=\"100\" ETCD_HEARTBEAT_INTERVAL=\"100\" ETCD_ELECTION_TIMEOUT=\"1000\" ETCD_LISTEN_PEER_URLS=\"https://172.16.1.65:2380\" ETCD_LISTEN_CLIENT_URLS=\"https://172.16.1.65:2379,http://127.0.0.1:2379\" ETCD_MAX_SNAPSHOTS=\"5\" ETCD_MAX_WALS=\"5\" #ETCD_CORS=\"\" # [cluster] ETCD_INITIAL_ADVERTISE_PEER_URLS=\"https://172.16.1.65:2380\" # if you use different ETCD_NAME (e.g. test), set ETCD_INITIAL_CLUSTER value for this name, i.e. \"test=http://...\" ETCD_INITIAL_CLUSTER=\"etcd1=https://172.16.1.64:2380,etcd2=https://172.16.1.65:2380,etcd3=https://172.16.1.66:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"https://172.16.1.65:2379\" #ETCD_DISCOVERY=\"\" #ETCD_DISCOVERY_SRV=\"\" #ETCD_DISCOVERY_FALLBACK=\"proxy\" #ETCD_DISCOVERY_PROXY=\"\" #ETCD_STRICT_RECONFIG_CHECK=\"false\" #ETCD_AUTO_COMPACTION_RETENTION=\"0\" # [proxy] #ETCD_PROXY=\"off\" #ETCD_PROXY_FAILURE_WAIT=\"5000\" #ETCD_PROXY_REFRESH_INTERVAL=\"30000\" #ETCD_PROXY_DIAL_TIMEOUT=\"1000\" #ETCD_PROXY_WRITE_TIMEOUT=\"5000\" #ETCD_PROXY_READ_TIMEOUT=\"0\" # [security] ETCD_CERT_FILE=\"/etc/kubernetes/ssl/etcd.pem\" ETCD_KEY_FILE=\"/etc/kubernetes/ssl/etcd-key.pem\" ETCD_CLIENT_CERT_AUTH=\"true\" ETCD_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\" ETCD_AUTO_TLS=\"true\" ETCD_PEER_CERT_FILE=\"/etc/kubernetes/ssl/etcd.pem\" ETCD_PEER_KEY_FILE=\"/etc/kubernetes/ssl/etcd-key.pem\" ETCD_PEER_CLIENT_CERT_AUTH=\"true\" ETCD_PEER_TRUSTED_CA_FILE=\"/etc/kubernetes/ssl/ca.pem\" ETCD_PEER_AUTO_TLS=\"true\" # [logging] #ETCD_DEBUG=\"false\" # examples for -log-package-levels etcdserver=WARNING,security=DEBUG #ETCD_LOG_PACKAGE_LEVELS=\"\" # etcd-3 mv /etc/etcd/etcd.conf /etc/etcd/etcd.conf-bak vi /etc/etcd/etcd.conf # [member] ETCD_NAME=etcd3 ETCD_DATA_DIR=\"/var/lib/etcd/etcd3.etcd\" ETCD_WAL_DIR=\"/var/lib/etcd/wal\" ETCD_SNAPSHOT_COUNT=\"100\" ETCD_HEARTBEAT_INTERVAL=\"100\" ETCD_ELECTION_TIMEOUT=\"1000\" ETCD_LISTEN_PEER_URLS=\"https://172.16.1.66:2380\" ETCD_LISTEN_CLIENT_URLS=\"https://172.16.1.66:2","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:10:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:11:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 35eefb8e7cc93b53 is healthy: got healthy result from https://172.16.1.66:2379 member 4576ff5ed626a66b is healthy: got healthy result from https://172.16.1.64:2379 member bf3bd651ec832339 is healthy: got healthy result from https://172.16.1.65:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 35eefb8e7cc93b53: name=etcd3 peerURLs=https://172.16.1.66:2380 clientURLs=https://172.16.1.66:2379 isLeader=false 4576ff5ed626a66b: name=etcd1 peerURLs=https://172.16.1.64:2380 clientURLs=https://172.16.1.64:2379 isLeader=true bf3bd651ec832339: name=etcd2 peerURLs=https://172.16.1.65:2380 clientURLs=https://172.16.1.65:2379 isLeader=false 配置 Kubernetes 集群 kubectl 安装在所有需要进行操作的机器上 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:12:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"Master and Node Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"安装组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.8.3/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:1","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@k8s-master-64 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.65:/etc/kubernetes/ssl/ scp admin*.pem 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:2","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 生成证书相关的配置文件存储与 /root/.kube 目录中 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:3","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.64\", \"172.16.1.65\", \"172.16.1.66\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.64 和 172.16.1.65 为 Master 的IP，多个Master需要写多个 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:4","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@k8s-master-64 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1261 11月 16 15:12 kubernetes.csr -rw------- 1 root root 1679 11月 16 15:12 kubernetes-key.pem -rw-r--r-- 1 root root 1635 11月 16 15:12 kubernetes.pem -rw-r--r-- 1 root root 475 11月 16 15:12 kubernetes-csr.json # 拷贝到目录 cp kubernetes*.pem /etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.65:/etc/kubernetes/ssl/ scp kubernetes*.pem 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:5","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@k8s-master-64 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' d926457ebfe9b1feaa5957a3d73b8b73 # 创建 token.csv 文件 cd /opt/ssl vi token.csv d59a702004f33c659640bf8dd2717b64,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 172.16.1.65:/etc/kubernetes/ scp token.csv 172.16.1.66:/etc/kubernetes/ # 生成高级审核配置文件 cd /etc/kubernetes cat \u003e\u003e audit-policy.yaml \u003c\u003cEOF # Log all requests at the Metadata level. apiVersion: audit.k8s.io/v1beta1 kind: Policy rules: - level: Metadata EOF # 拷贝 scp audit-policy.yaml 172.16.1.65:/etc/kubernetes/ scp audit-policy.yaml 172.16.1.66:/etc/kubernetes/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:6","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \\ --advertise-address=172.16.1.64 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-policy-file=/etc/kubernetes/audit-policy.yaml \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=0.0.0.0 \\ --secure-port=6443 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=127.0.0.1 \\ --insecure-port=8080 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # k8s 1.8 添加 --authorization-mode=Node # k8s 1.8 添加 --admission-control=NodeRestriction # k8s 1.8 添加 --audit-policy-file=/etc/kubernetes/audit-policy.yaml # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:7","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:8","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-cidr=10.233.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:9","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:10","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=0.0.0.0 \\ --master=http://127.0.0.1:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:11","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:12","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@k8s-master-64 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@k8s-master-2 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@k8s-master-3 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:13","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:14","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=d926457ebfe9b1feaa5957a3d73b8b73 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:15","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 \u003e 配置为 node 本机 IP mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --cgroup-driver=cgroupfs \\ --hostname-override=k8s-master-64 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=2 [Install] WantedBy=multi-user.target # 如上配置: k8s-master-64 本机hostname 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:16","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:17","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-320ITerfRCS4yul0iKNpsYhgnYt9ubVdd1NuuxIhbKI 1m kubelet-bootstrap Pending node-csr-PAaagu_xofoU55gGYWhLzHp4fro3EgEgFq-vHpZeoX8 1m kubelet-bootstrap Pending node-csr-WMtKYQlQRtq5LO8_auYQ8oKT81HL9iIbxzbwYjcqVL0 1m kubelet-bootstrap Pending # 增加 认证 kubectl get csr | grep Pending | awk '{print $1}' | xargs kubectl certificate approve ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:18","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"验证 nodes [root@k8s-master-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-64 Ready \u003cnone\u003e 13s v1.8.3 k8s-master-65 Ready \u003cnone\u003e 13s v1.8.3 k8s-master-66 Ready \u003cnone\u003e 13s v1.8.3 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:13:19","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:14:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@k8s-master-64 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:14:1","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy*.pem /etc/kubernetes/ssl/ scp kube-proxy*.pem 172.16.1.65:/etc/kubernetes/ssl/ scp kube-proxy*.pem 172.16.1.66:/etc/kubernetes/ssl/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:15:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:15:1","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 配置为 各自的 IP # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.64 \\ --hostname-override=k8s-master-64 \\ --cluster-cidr=10.254.0.0/16 \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:15:2","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:15:3","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"Node 端 单 Node 部分 需要部署的组件有 docker calico kubectl kubelet kube-proxy 这几个组件。 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:16:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"安装组件 cd /tmp wget https://dl.k8s.io/v1.8.3/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-proxy,kubelet} /usr/local/bin/ # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:16:1","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kubelet or kube-proxy # kubelet # 首先 创建 kubelet kubeconfig 文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=d926457ebfe9b1feaa5957a3d73b8b73 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ # 创建 kube-proxy kubeconfig 文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:16:2","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.64:6443; server 172.16.1.65:6443; server 172.16.1.66:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 更新权限 chmod +r /etc/nginx/nginx.conf # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 127.0.0.1:6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.5-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy # 重启 Node 的 kubelet 与 kube-proxy systemctl restart kubelet systemctl status kubelet systemctl restart kube-proxy systemctl status kube-proxy ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:16:3","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"Master 配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-64 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-5dohu3QPMPWkxp9HRiNv7fZSXTE3MbxCIdf6G95ehXE 1h kubelet-bootstrap Approved,Issued node-csr-ZFWZ5q5m323w_Iv5eTQclDhfgwaZ0Go-pztEoBfCMJk 22s kubelet-bootstrap Pending node-csr-_yztJwMNeZ9HPlofd0Eiy_4fQLqKCIjU9_IfQ5koDCk 1h kubelet-bootstrap Approved,Issued node-csr-pf-Bb5Iqx6ccvVA67gLVT-G4Zl3Zl5FPUZS4d7V6rk4 2h kubelet-bootstrap Approved,Issued node-csr-u6JUtu1mVbC6sb4bxDuGkZT7Flzehren0OkOlfNp_MA 16s kubelet-bootstrap Pending node-csr-xZZVdiWl1UEnDaRNkp3AHzT_E8FX3A71-5hDgQOW08U 14s kubelet-bootstrap Pending # 增加 认证 [root@k8s-master-64 ~]# kubectl certificate approve NAME [root@k8s-master-64 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-64 Ready \u003cnone\u003e 13s v1.8.3 k8s-master-65 Ready \u003cnone\u003e 13s v1.8.3 k8s-master-66 Ready \u003cnone\u003e 13s v1.8.3 k8s-node-67 Ready \u003cnone\u003e 13s v1.8.3 k8s-node-68 Ready \u003cnone\u003e 13s v1.8.3 k8s-node-69 Ready \u003cnone\u003e 13s v1.8.3 配置 Calico 网络 calico 因为有某些坑，采用 systemd 托管 calico 服务 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:16:4","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"安装 Calico 官网地址 http://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/hosted # 下载 yaml 文件 wget http://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/calico.yaml wget http://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/rbac.yaml # 下载 镜像 # 国外镜像 有墙 quay.io/calico/node:v2.6.0 quay.io/calico/cni:v1.11.0 quay.io/calico/kube-controllers:v1.0.0 # 国内镜像 jicki/node:v2.6.0 jicki/cni:v1.11.0 jicki/kube-controllers:v1.0.0 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:17:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 calico vi calico.yaml # 注意修改如下选项: etcd_endpoints: \"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" etcd_ca: \"/calico-secrets/etcd-ca\" etcd_cert: \"/calico-secrets/etcd-cert\" etcd_key: \"/calico-secrets/etcd-key\" # 这里面要写入 base64 的信息 data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d '\\n') etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d '\\n') etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d '\\n') - name: CALICO_IPV4POOL_CIDR value: \"10.233.0.0/16\" # calico-node DaemonSet 中 # Runs calico/node container on each Kubernetes node. This # container programs network policy and routes on each # host. # 注释掉 移动到 系统 systemd 中运行 #- name: calico-node # image: jicki/node:v2.6.1 # env: # # The location of the Calico etcd cluster. # - name: ETCD_ENDPOINTS # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_endpoints # # Choose the backend to use. # - name: CALICO_NETWORKING_BACKEND # valueFrom: # configMapKeyRef: # name: calico-config # key: calico_backend # # Cluster type to identify the deployment type # - name: CLUSTER_TYPE # value: \"k8s,bgp\" # # Disable file logging so `kubectl logs` works. # - name: CALICO_DISABLE_FILE_LOGGING # value: \"true\" # # Set Felix endpoint to host default action to ACCEPT. # - name: FELIX_DEFAULTENDPOINTTOHOSTACTION # value: \"ACCEPT\" # # Configure the IP Pool from which Pod IPs will be chosen. # - name: CALICO_IPV4POOL_CIDR # value: \"10.233.0.0/16\" # - name: CALICO_IPV4POOL_IPIP # value: \"always\" # # Disable IPv6 on Kubernetes. # - name: FELIX_IPV6SUPPORT # value: \"false\" # # Set Felix logging to \"info\" # - name: FELIX_LOGSEVERITYSCREEN # value: \"info\" # # Set MTU for tunnel device used if ipip is enabled # - name: FELIX_IPINIPMTU # value: \"1440\" # # Location of the CA certificate for etcd. # - name: ETCD_CA_CERT_FILE # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_ca # # Location of the client key for etcd. # - name: ETCD_KEY_FILE # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_key # # Location of the client certificate for etcd. # - name: ETCD_CERT_FILE # valueFrom: # configMapKeyRef: # name: calico-config # key: etcd_cert # # Auto-detect the BGP IP address. # - name: IP # value: \"\" # - name: FELIX_HEALTHENABLED # value: \"true\" # securityContext: # privileged: true # resources: # requests: # cpu: 250m # livenessProbe: # httpGet: # path: /liveness # port: 9099 # periodSeconds: 10 # initialDelaySeconds: 10 # failureThreshold: 6 # readinessProbe: # httpGet: # path: /readiness # port: 9099 # periodSeconds: 10 # volumeMounts: # - mountPath: /lib/modules # name: lib-modules # readOnly: true # - mountPath: /var/run/calico # name: var-run-calico # readOnly: false # - mountPath: /calico-secrets # name: etcd-certs # 注释掉 移动到 系统 systemd 中运行 # This container installs the Calico CNI binaries # and CNI network config file on each node. ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:18:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 [root@k8s-master-64 ~]# kubectl apply -f calico.yaml configmap \"calico-config\" created secret \"calico-etcd-secrets\" created daemonset \"calico-node\" created deployment \"calico-policy-controller\" created serviceaccount \"calico-policy-controller\" created serviceaccount \"calico-node\" created [root@k8s-master-64 ~]# kubectl apply -f rbac.yaml clusterrole \"calico-policy-controller\" created clusterrolebinding \"calico-policy-controller\" created clusterrole \"calico-node\" created clusterrolebinding \"calico-node\" created ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:19:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 calico-node 配置一个 service ，用系统 systemd 启动 # 每个需要安装 calico 网络的都必须配置, 并修改其中的不同的参数。 # 1. NODENAME=k8s-master-64 # 2. IP=172.16.1.64 vi /etc/systemd/system/calico-node.service [Unit] Description=calico node After=docker.service Requires=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \\ -e ETCD_ENDPOINTS=https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379 \\ -e ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/ca.pem \\ -e ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd.pem \\ -e ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd-key.pem \\ -e NODENAME=k8s-master-64 \\ -e IP=172.16.1.64 \\ -e IP6= \\ -e AS= \\ -e CALICO_IPV4POOL_CIDR=10.233.0.0/16 \\ -e CALICO_IPV4POOL_IPIP=always \\ -e CALICO_LIBNETWORK_ENABLED=true \\ -e CALICO_NETWORKING_BACKEND=bird \\ -e CALICO_DISABLE_FILE_LOGGING=true \\ -e FELIX_IPV6SUPPORT=false \\ -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \\ -e FELIX_LOGSEVERITYSCREEN=info \\ -v /etc/kubernetes/ssl/ca.pem:/etc/kubernetes/ssl/ca.pem \\ -v /etc/kubernetes/ssl/etcd.pem:/etc/kubernetes/ssl/etcd.pem \\ -v /etc/kubernetes/ssl/etcd-key.pem:/etc/kubernetes/ssl/etcd-key.pem \\ -v /var/run/calico:/var/run/calico \\ -v /lib/modules:/lib/modules \\ -v /run/docker/plugins:/run/docker/plugins \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /var/log/calico:/var/log/calico \\ jicki/node:v2.6.0 ExecStop=/usr/bin/docker rm -f calico-node Restart=always RestartSec=10 [Install] WantedBy=multi-user.target ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:20:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"启动 calico-node systemctl daemon-reload systemctl start calico-node systemctl enable calico-node systemctl status calico-node ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:21:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 kubelet.service vi /etc/systemd/system/kubelet.service # 增加 如下配置 --network-plugin=cni \\ # 重新加载配置 systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:22:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"验证 Calico [root@k8s-master-64 calico]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-85bd96b9bb-226r2 1/1 Running 0 14m calico-node-4h2rs 1/1 Running 0 14m calico-node-v768t 1/1 Running 0 14m calico-node-xr5kv 1/1 Running 0 14m ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:23:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"安装 Calicoctl cd /usr/local/bin/ wget -c https://github.com/projectcalico/calicoctl/releases/download/v1.6.1/calicoctl chmod +x calicoctl ## 创建 calicoctl 配置文件 # 配置文件， 在 安装了 calico 网络的 机器下 mkdir /etc/calico vi /etc/calico/calicoctl.cfg apiVersion: v1 kind: calicoApiConfig metadata: spec: datastoreType: \"etcdv2\" etcdEndpoints: \"https://172.16.1.64:2379,https://172.16.1.65:2379,https://172.16.1.66:2379\" etcdKeyFile: \"/etc/kubernetes/ssl/etcd-key.pem\" etcdCertFile: \"/etc/kubernetes/ssl/etcd.pem\" etcdCACertFile: \"/etc/kubernetes/ssl/ca.pem\" # 查看 calico 状态 [root@k8s-master-64 ~]# calicoctl node status calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 172.16.1.65 | node-to-node mesh | up | 09:24:50 | Established | | 172.16.1.66 | node-to-node mesh | up | 09:24:51 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:24:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 3 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-master-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-55b58f68b6-6q2lc 1/1 Running 0 2m 10.233.235.65 172.16.1.65 nginx-dm-55b58f68b6-x9hk5 1/1 Running 0 2m 10.233.28.129 172.16.1.64 nginx-dm-55b58f68b6-xbprt 1/1 Running 0 2m 10.233.21.1 172.16.1.66 # 在 node 里 curl [root@k8s-master-64 ~]# curl 10.254.129.54 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 配置 KubeDNS 官方 github yaml 相关 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:25:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"下载镜像 # 官方镜像 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.7 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.7 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.7 # 我的镜像 jicki/k8s-dns-sidecar-amd64:1.14.7 jicki/k8s-dns-kube-dns-amd64:1.14.7 jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.7 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:26:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kube-dns.yaml.base # 修改后缀 mv kube-dns.yaml.base kube-dns.yaml ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:27:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"修改 kube-dns.yaml 1. # clusterIP: __PILLAR__DNS__SERVER__ 修改为我们之前定义的 dns IP 10.254.0.2 2. # 修改 --domain=__PILLAR__DNS__DOMAIN__. 为 我们之前 预定的 domain 名称 --domain=cluster.local. 3. # 修改 --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053 中 domain 为我们之前预定的 --server=/cluster.local./127.0.0.1#10053 4. # 修改 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local., 5. # 修改 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local., ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:28:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 导入 [root@k8s-master-64 kubedns]# kubectl create -f kube-dns.yaml service \"kube-dns\" created serviceaccount \"kube-dns\" created configmap \"kube-dns\" created deployment \"kube-dns\" created ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:29:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@k8s-master-64 kubedns]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-85bd96b9bb-226r2 1/1 Running 0 36m calico-node-4h2rs 1/1 Running 0 36m calico-node-v768t 1/1 Running 0 36m calico-node-xr5kv 1/1 Running 0 36m kube-dns-787dd94b74-6fpk8 3/3 Running 0 24s ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:30:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 dns 自动扩容 # 官方镜像 gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.1.2-r2 # 个人镜像 jicki/cluster-proportional-autoscaler-amd64:1.1.2-r2 # 下载 yaml wget https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns-horizontal-autoscaler/dns-horizontal-autoscaler.yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 导入服务 [root@k8s-master-64 dns]# kubectl apply -f dns-horizontal-autoscaler.yaml serviceaccount \"kube-dns-autoscaler\" created clusterrole \"system:kube-dns-autoscaler\" created clusterrolebinding \"system:kube-dns-autoscaler\" created deployment \"kube-dns-autoscaler\" created # 查看服务 [root@k8s-master-64 dns]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-85bd96b9bb-226r2 1/1 Running 0 47m calico-node-4h2rs 1/1 Running 0 47m calico-node-v768t 1/1 Running 0 47m calico-node-xr5kv 1/1 Running 0 47m kube-dns-787dd94b74-6fpk8 3/3 Running 0 10m kube-dns-787dd94b74-fx6nc 3/3 Running 0 24s kube-dns-autoscaler-856c97456-x6v7j 1/1 Running 0 36s ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:31:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 导入之前的 nginx-dm yaml文件 # 查看 pods [root@k8s-master-64 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-d9d5db9dd-62k92 1/1 Running 0 16s 10.233.108.193 k8s-master-66 nginx-dm-d9d5db9dd-pgsrx 1/1 Running 0 16s 10.233.153.130 k8s-master-65 nginx-dm-d9d5db9dd-s4vtr 1/1 Running 0 16s 10.233.252.3 k8s-master-64 # 查看 svc [root@k8s-master-64 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 2h \u003cnone\u003e nginx-svc ClusterIP 10.254.102.240 \u003cnone\u003e 80/TCP 33s name=nginx # 创建一个 pods 来测试一下 dns apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 pods [root@k8s-master-64 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 7s nginx-dm-d9d5db9dd-62k92 1/1 Running 0 1m nginx-dm-d9d5db9dd-pgsrx 1/1 Running 0 1m nginx-dm-d9d5db9dd-s4vtr 1/1 Running 0 1m # 测试 [root@k8s-master-64 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.102.240 nginx-svc.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:32:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"部署 dashboard \u0026\u0026 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:33:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.3 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.6.3 ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:34:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-controller.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-service.yaml # 因为开启了 RBAC 所以这里需要创建一个 RBAC 认证 vi dashboard-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: dashboard namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: name: dashboard subjects: - kind: ServiceAccount name: dashboard namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:35:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # dashboard-controller.yaml 增加 rbac 授权 # 在第二个 spec 下面 增加 spec: serviceAccountName: dashboard # 导入文件 [root@k8s-master-64 dashboard]# kubectl apply -f . deployment \"kubernetes-dashboard\" created serviceaccount \"dashboard\" created clusterrolebinding \"dashboard\" created service \"kubernetes-dashboard\" created # 查看 svc 与 pod [root@k8s-master-64 ~]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 9m kubernetes-dashboard ClusterIP 10.254.119.100 \u003cnone\u003e 80/TCP 16s ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:36:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:37:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@k8s-master-64 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master-64 Ready \u003cnone\u003e 21h v1.8.3 k8s-master-65 Ready \u003cnone\u003e 21h v1.8.3 k8s-master-66 Ready \u003cnone\u003e 21h v1.8.3 # 对 65 与 66 打上 label [root@k8s-master-64 ingress]# kubectl label nodes k8s-master-65 ingress=proxy node \"172.16.1.65\" labeled [root@k8s-master-64 ingress]# kubectl label nodes k8s-master-66 ingress=proxy node \"172.16.1.66\" labeled # 打完标签以后 [root@k8s-master-64 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master-64 Ready \u003cnone\u003e 21h v1.8.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=k8s-master-64 k8s-master-65 Ready \u003cnone\u003e 21h v1.8.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-master-65 k8s-master-66 Ready \u003cnone\u003e 21h v1.8.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=k8s-master-66 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.4 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.9.0-beta.17 # 国内镜像 jicki/defaultbackend:1.4 jicki/nginx-ingress-controller:0.9.0-beta.17 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 直接导入既可, 这里不需要修改 [root@k8s-master-64 ingress]# kubectl apply -f default-backend.yaml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看服务 [root@k8s-master-64 ingress]# kubectl get deployment -n kube-system default-http-backend NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 36s # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 部署 Ingress Controller 组件 # 下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * sed -i 's/quay\\.io\\/kubernetes-ingress-controller/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 spec: replicas: 2 .... spec: hostNetwork: true serviceAccountName: nginx-ingress-serviceaccount nodeSelector: ingress: proxy .... # 导入 yaml 文件 [root@k8s-master-64 ingress]# kubectl apply -f nginx-ingress-controller.yaml deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 65 与 29 中 [root@k8s-master-64 ingress]# kubectl get pods -n ingress-nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE default-http-backend-7f47b7d69b-5df85 1/1 Running 0 1m 10.233.252.4 k8s-master-64 nginx-ingress-controller-745695d6cf-n74fn 1/1 Running 0 1m 172.16.1.66 k8s-master-66 nginx-ingress-controller-745695d6cf-xrml9 1/1 Running 0 1m 172.16.1.65 k8s-master-65 # 查看我们原有的 svc [root@k8s-master-64 ingress]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 18m kubernetes-dashboard ClusterIP 10.254.119.100 \u003cnone\u003e 80/TCP 9m # 创建 yaml 文件 vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: dashboard-ingress namespace: kube-system spec: rules: - host: dashboard.jicki.cn http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 80 # 导入 yaml [root@k8s-master-64 dashboard]# kubectl apply -f dashboard-ingress.yaml ingress \"dashboard-ingress\" created # 查看 ingress [root@k8s-master-64 dashboard]# kubectl get ingress -n kube-system -o wide NAME HOSTS ADDRESS PORTS AGE dashboard-ingress dashboard.jicki.cn 172.16.1.65,172.16.1.66 80 35s # 测试访问 [root@k8s-master-64 dashboard]# curl -I dashboard","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:38:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2017-11-17","objectID":"/kubernetes-1.8.3/:39:0","tags":null,"title":"kubernetes 1.8.3","uri":"/kubernetes-1.8.3/"},{"categories":["kubernetes"],"content":"Kubernetes Helm Charts","date":"2017-11-08","objectID":"/kubernetes-helm/","tags":null,"title":"Kubernetes Helm Charts","uri":"/kubernetes-helm/"},{"categories":["kubernetes"],"content":"Kubernetes Helm Charts Helm 是由 Deis 发起的一个开源工具，有助于简化部署和管理 Kubernetes 应用。 官方 github 地址 https://github.com/kubernetes/helm ","date":"2017-11-08","objectID":"/kubernetes-helm/:0:0","tags":null,"title":"Kubernetes Helm Charts","uri":"/kubernetes-helm/"},{"categories":["kubernetes"],"content":"Helm 组件 Helm 采用客户端/服务器架构 [ C/S 架构 ] Helm CLI 是 Helm 客户端，可以在本地执行。 Tiller 是服务器端组件，在 Kubernetes 群集上运行，并管理 Kubernetes 应用程序的生命周期。 Chart：一个 Helm 包，其中包含了运行一个应用所需要的镜像、依赖和资源定义等，还可能包含 Kubernetes 集群中的服务定义，类似 Homebrew 中的 formula，APT 的 dpkg 或者 Yum 的 rpm 文件。 Repository 是 Chart 仓库，Helm客户端通过HTTP协议来访问仓库中Chart的索引文件和压缩包。 ","date":"2017-11-08","objectID":"/kubernetes-helm/:1:0","tags":null,"title":"Kubernetes Helm Charts","uri":"/kubernetes-helm/"},{"categories":["kubernetes"],"content":"安装 Helm 查看官方 releases 版本 https://github.com/kubernetes/helm/releases # 下载文件 wget https://storage.googleapis.com/kubernetes-helm/helm-v2.7.0-linux-amd64.tar.gz tar zxvf helm-v2.7.0-linux-amd64.tar.gz mv linux-amd64/helm /usr/local/bin/helm helm help ","date":"2017-11-08","objectID":"/kubernetes-helm/:2:0","tags":null,"title":"Kubernetes Helm Charts","uri":"/kubernetes-helm/"},{"categories":["kubernetes"],"content":"部署 Github Charts repository 官方提示 Charts 可以使用任何提供 YAML 和 tar 文件存储的 http 服务器, 并且可以回答 Get 请求, 可以自建 http 服务器, 使用 Amazon S3 存储, 或者 Github Pages . 官方文档 https://github.com/kubernetes/helm/blob/master/docs/chart_repository.md # 创建 Github Pages 两种方式: 1. 通过配置一个项目来提供其docs/目录的内容 2. 通过配置一个项目来为特定的分支提供服务 # 这里使用 docs/ 的方式。 首先登陆 github.com 创建一个 New repository https://github.com/new 1. 创建一个 名为 charts 为名称的 仓库 2. 然后初始化一下 (git init) # echo \"# My chart to helm\" \u003e README.md # mkdir docs # echo \"hello word!\" \u003e docs/index.html # git init # git add -A # git commit -m \"first commit\" # git remote add origin https://github.com/jicki520/chart.git # git push -u origin master 3. 点击WEB UI 中你的 repo Settings # GitHub Pages # Source 中 选择 master branch /docs folder # 保存以后显示如下 # Your site is ready to be published at https://jicki520.github.io/charts/. # Enforce HTTPS # 尝试访问 https://jicki520.github.io/charts/ 4. 登陆 helm 所在服务器 git clone https://github.com/jicki520/chart.git cd chart helm create mychart helm package mychart mv mychart-0.1.0.tgz docs helm repo index docs --url https://jicki520.github.io/chart git add -A git commit -m 'ADD' git push origin master # 添加一个 package # 官方 github package 地址 https://github.com/kubernetes/charts ","date":"2017-11-08","objectID":"/kubernetes-helm/:3:0","tags":null,"title":"Kubernetes Helm Charts","uri":"/kubernetes-helm/"},{"categories":["kubernetes"],"content":"部署 Tiller 安装 Tiller 到 kubernetes 集群 POD 中。 helm init --upgrade # 初始化配置的时候， Helm 会去 gcr.io 中拉取 tiller 的镜像， 而且会将 \"https://kubernetes-charts.storage.googleapis.com\" 做为 stable repository 地址, 以后下载都会从这里拉取，国内被墙的话，很痛苦，所以我们用自己的。 # helm init -i 指定镜像下载地址，--stable-repo-url 配置缺省 stable repository 地址 helm init --upgrade -i jicki/tiller:v2.7.0 --stable-repo-url https://jicki520.github.io/charts/ # 输出如下信息 tes.oss-cn-hangzhou.aliyuncs.com/charts Creating /root/.helm Creating /root/.helm/repository Creating /root/.helm/repository/cache Creating /root/.helm/repository/local Creating /root/.helm/plugins Creating /root/.helm/starters Creating /root/.helm/cache/archive Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://jicki520.github.io/charts/ Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been upgraded to the current version. Happy Helming! # 查看 kubernetes 服务 kubectl get po -n kube-system |grep tiller tiller-deploy-69bc7b6f75-g9zmh 1/1 Running 0 59s # 卸载 tiller helm reset # 或者 kubectl 删除 deployment ","date":"2017-11-08","objectID":"/kubernetes-helm/:4:0","tags":null,"title":"Kubernetes Helm Charts","uri":"/kubernetes-helm/"},{"categories":["devops"],"content":"DevOps [转载]","date":"2017-10-31","objectID":"/devops/","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":" 本文为 转载文章， 非原创 DevOps DevOps（Development和Operations的组合词）是一种重视“软件开发人员（Dev）”和“IT运维技术人员（Ops）”之间沟通合作的文化、运动或惯例。透过自动化“软件交付”和“架构变更”的流程，来使得构建、测试、发布软件能够更加地快捷、频繁和可靠。 DevOps是一种文化转变，或者说是一个鼓励更好地交流和协作（即团队合作）以便于更快地构建可靠性更高、质量更好的软件的运动。 ","date":"2017-10-31","objectID":"/devops/:0:0","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"DevOps 核心价值 文化观念的改变 + 自动化工具 = 不断适应快速变化的市场 更快速地交付, 响应市场的变化。 更多地关注业务的改进与提升。 ","date":"2017-10-31","objectID":"/devops/:1:0","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"为什么需要DevOps VUCA 概念 V=Volatillity（易变性）是变化的本质和动力，也是由变化驱使和催化产生的 U=Uncertainty（不确定性）缺少预见性,缺乏对意外的预期和对事情的理解和意识 C=Complexity（复杂性）企业为各种力量，各种因素，各种事情所困扰。 A=Ambiguity（模糊性）对现实的模糊，是误解的根源，各种条件和因果关系的混杂。 ","date":"2017-10-31","objectID":"/devops/:2:0","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"产品迭代 我们不管是做互联网还是做游戏，其实最终都是在做产品，做一款用户喜欢的产品。乔布斯有句非常著名的名言：“消费者并不知道自己需要什么，直到我们拿出自己的产品，他们才发现，这是我想要的东西”。所以乔帮主能够在一开始的时候就设计好了产品最终的效果，然后按照零部件一步步迭代生产 现实中的用户其实一开始并不知道自己想要什么，但是直到看到了我们的产品，他才知道自己不想要什么。 即让现实的产品迭代是如此曲折和反复 用户：我平时上下班都是走路，每天都要走五公里，好辛苦，有没有办法帮我设计个工具，解决下我的痛点。 我们思考了下，觉得这个不是很难嘛，可以试下，于是我们讨论 -\u003e 设计 -\u003e 开发 -\u003e 测试 -\u003e 交付给用户了一个滑板。 用户：这个滑板不好操控，可以给我加个扶手吗？ 然后我们按照用户新的需求，生产了个滑板车。 用户：滑板车得滑着走，能不能让我可以骑着走的。 我们继续改进产品，生产了个自行车。 用户：自行车还得登着走，路程远了也很累。 我们又继续优化，把它变成了电瓶车。 用户：电瓶车倒是解决了的需求，不过就是不太安全，能再优化下产品吗？ 经过各种努力我们最后生产出了一辆漂亮的小轿车交付给了用户，终于让用户满意了。 ","date":"2017-10-31","objectID":"/devops/:2:1","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"技术革新 在以前的系统中业务单一、逻辑简单、用户量少，项目团队的规模一般在 10~30人。而现在的系统要面对不同用户的定制化推荐等，互联网连接着人与人、人与物、以及物与物，业务也变得越来越复杂，功能越来越多，如果整个系统耦合在一起，则必定会牵一发而动全身，导致系统维护起来相当困难。 因此IT技术架构也随着系统的复杂化而不断地变化革新，从早期所有服务的All In One发展到现在的微服务架构、从纯手动操作到全自动化流程、从单台物理机到云平台，下图展示了IT技术革新的变化： ","date":"2017-10-31","objectID":"/devops/:2:2","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"DevOps的指导思想 其实DevOps核心思想就是：“快速交付价值，灵活响应变化”。 高效的协作和沟通； 自动化流程和工具； 快速敏捷的开发； 持续交付和部署； 不断学习和创新。 ","date":"2017-10-31","objectID":"/devops/:3:0","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"整体流程图 敏捷管理：一支训练有素的敏捷开发团队是成功实施DevOps的关键。 根据康威定律：软件团队开发的产品是对公司组织架构的反映。 所以根据公司情况调整组织结构是首要条件，它将直接影响到需求、设计和开发阶段的效率、以及沟通的成本。 关于团队的沟通成本的计算公式：沟通成本 = n(n-1)/2，其中n为人数，所以沟通成本将随着组织人员的增加而呈指数级增长。 持续交付部署：实现应用程序的自动化构建、部署、测试和发布。 通过技术工具，把传统的手工操作转变为自动化流程，这不仅有利于提高产品开发、运维部署的效率，还将减少人为因素引起的失误和事故，提早发现问题并及时地解决问题，这样也保证了产品的质量。 IT服务管理：可持续的、高可用的IT服务是保障业务正常的关键要素，它与业务是一个整体。 IT服务管理（ITSM）直接影响产品运营的整个生命周期，传统的IT服务管理（像ITIL）在生产中做的非常好了，但是它对于DevOps来说又显得过于繁琐，所以有必要为DevOps创建一个只关注业务持续性的ITMS，它只需要很少的必要资源来为相应的业务提供服务，ITMS更多地从业务角度考虑了。 注：白话解释下什么是IT服务管理（ITSM），它是传统的“IT管理”转向为“IT服务”为主的一种模式，前者可能更关注具体服务器管理、网络管理和系统软件安装部署等工作；而后者更关注流程的规范化、标准化，明确定义各个流程的目标和范围、成本和效益、运营步骤、关键成功因素和绩效指标、有关人员的责权利，以及各个流程之间的关系等，比如建立线上事故解决流程、服务配置管理流程等； 而光有流程还不够，因为流程主要是IT服务提供方内部使用的，客户对他们并不感兴趣，所以还需将这些流程按需打包成特定的IT服务，然后提供给客户使用，比如在云平台上购买一台虚拟云主机一样。 精益管理：建立一个流水线式的IT服务链，打通开发与运维的鸿沟，实现开发运维一体化的敏捷模式。 精益生产主要来源于丰田生产方式 (TPS)的生产哲学，它以降低浪费、提升整体客户价值而闻名，它主要利用优化自动化流程来提高生产率、降低浪费。所以精益生产的精髓是即时制（JIT）和自动化（Jidoka）。 JIT（Just In time）：JIT用一句话描述就是消耗最少的必要资源，以正确的数量，生产和运送正确的零件。在这种模式下工作，可以最大程度上降低库存，防止过早或者过度生产。大多数公司更倾向用库存来避免潜在的停线风险，而丰田却反其道而行之。通过减少库存“逼迫”对生产中产生的问题做及时且有效的反应。当然JIT这一模式对解决问题的能力是相当大的考验，在能力不足的情况下，会有相当大的断线风险。 Jidoka（Build in quality）：自动化，在丰田TPS系统里，特意给“動”字加上了“人”字旁变成了“働”，换句话说，TPS/精益生产渴望生产的过程控制能像“人”一样智能，在第一时间就异常情况下自动关闭。这种自动停机功能可以防止坏件流入下游，防止机器在错误的生产状态下造成损坏，也可以让人更好的在当前错误状态下进行故障分析。当设备能够做到自动分析故障时，就可以将监管机器的“人”真正解放出来，做到对人力成本的节省。 精益软件开发是精益生产和实践在软件开发领域的应用。精益管理贯穿于整个DevOps阶段，它鼓励主动发现问题，不断的优化流程，从而达到持续交付、快速反馈、降低风险和保障质量的目的。 消除浪费 增强学习 尽量延迟决定 尽快发布 下放权力 嵌入质量 全局优化 ","date":"2017-10-31","objectID":"/devops/:3:1","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"实施DevOps的具体方法 目前大多数IT互联网公司普遍的分层结构，它们一般分为七大部门：产品策划、设计美术、前端工程师、后端工程师、测试工程师、运维\u0026DBA和市场运营等。各部门之间天然的形成了沟通障碍墙，相互之间主要以邮件和会议的形式沟通，效率低下、需求变更困难、很难快速响应市场变化和持续交付高品质的产品。 建立快速敏捷团队 Scrum Master 架构 实现自动化的流程 完整DevOps的Pipeline 提交：工程师将代码在本地测试后，提交到版本控制系统，如 Git代码仓库中。 构建：持续整合系统（如Jenkins CI），在检测到版本控制系统更新时，便自动从Git代码仓库里拉取最新的代码，进行编译、构建。 单元测试：Jenkins完成编译构建后，会自动执行指定的单元测试代码。 部署到测试环境：在完成单元测试后，Jenkins可以将应用程序部署到与生产环境相近的测试环境中进行测试 预生产环境测试：在预生产测试环境里，可以进行一些最后的自动化测试，例如使用Appium自动化测试工具进行测试，以及与实际情况类似的一些测试可由开发人员或客户人员手动进行测试。 部署到生产环境：通过所有测试后，便可以使用灰度更新将最新的版本部署到实际生产环境里。 ","date":"2017-10-31","objectID":"/devops/:4:0","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"DevOps 技术栈 ","date":"2017-10-31","objectID":"/devops/:5:0","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"敏捷管理工具 Trello Teambition Worktile Tower ","date":"2017-10-31","objectID":"/devops/:5:1","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"产品\u0026质量管理 confluence 禅道 Jira Bugzila ","date":"2017-10-31","objectID":"/devops/:5:2","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"代码仓库管理 svn Git ","date":"2017-10-31","objectID":"/devops/:5:3","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"开发流程规范 Git Flow Github Flow Gitlab Flow ","date":"2017-10-31","objectID":"/devops/:5:4","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"自动化构建工具 Gradle Maven SBT ANT ","date":"2017-10-31","objectID":"/devops/:5:5","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"虚拟机与容器化 VMware VirtualBox Vagrant Docker ","date":"2017-10-31","objectID":"/devops/:5:6","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"持续集成（CI）\u0026持续部署（CD） Jenkins Hudson Travis CI CircleCI ","date":"2017-10-31","objectID":"/devops/:5:7","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"自动化测试 Appium Appium是一个移动端的自动化框架，可用于测试原生应用，移动网页应用和混合型应用，且是跨平台的。可用于IOS和Android以及firefox的操作系统。 Selenium Selenium 测试直接在浏览器中运行，就像真实用户所做的一样。Selenium 测试可以在 Windows、Linux 和 Macintosh上的 Internet Explorer、Mozilla 和 Firefox 中运行。 Mock测试 Mock测试就是在测试过程中，对于某些不容易构造或者不容易获取的对象，用一个虚拟的对象来创建以便测试的测试方法。这个虚拟的对象就是Mock对象，Mock对象就是真实对象在调试期间的代替品。Java中的Mock框架常用的有EasyMock和Mockito等。 ","date":"2017-10-31","objectID":"/devops/:5:8","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"自动化运维工具 Ansible Puppet Chef 脚本 ","date":"2017-10-31","objectID":"/devops/:5:9","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["devops"],"content":"监控管理工具 Zabbix Zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级开源解决方案。 ELK Stack日志分析系统 ELK Stack是开源日志处理平台解决方案，背后的商业公司是Elastic。它由日志采集解析工具 Logstash、基于 Lucene 的全文搜索引擎 Elasticsearch、分析可视化平台 Kibana三部分组成。 云监控（如Amazon CloudWatch） Amazon CloudWatch 是一项针对 AWS 云资源和在 AWS 上运行的应用程序进行监控的服务。您可以使用 Amazon CloudWatch 收集和跟踪各项指标、收集和监控日志文件、设置警报以及自动应对 AWS 资源的更改 ","date":"2017-10-31","objectID":"/devops/:5:10","tags":null,"title":"DevOps [转载]","uri":"/devops/"},{"categories":["kubernetes"],"content":"Traefik ingress rbac","date":"2017-10-23","objectID":"/traefik-rbac-ingress/","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"Traefik Ingress 官方文档 https://docs.traefik.io/user-guide/kubernetes/ ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:0:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 # 基于 kubernetes 1.8 rbac wget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-rbac.yaml wget https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik-deployment.yaml ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:1:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"编译 yaml 文件 # 这里只需要编辑 deployment 文件 # 首先来 查看一下 node 标签 kubectl get nodes --show-labels # 我这里增加了 ingress=proxy 标签 到其中两台node 种 # 编辑 deployment 文件 --- apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: kube-system --- kind: Deployment apiVersion: extensions/v1beta1 metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lb spec: replicas: 2 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: serviceAccountName: traefik-ingress-controller hostNetwork: true nodeSelector: ingress: proxy terminationGracePeriodSeconds: 60 containers: - image: traefik name: traefik-ingress-lb ports: - name: web containerPort: 80 hostPort: 80 - name: admin containerPort: 8888 args: - --web - --web.address=:8888 # 修改端口必须配置绑定,否则仍然绑定为 8080 端口 - --kubernetes ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:2:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 kubectl apply -f . serviceaccount \"traefik-ingress-controller\" created deployment \"traefik-ingress-controller\" created service \"traefik-ingress-service\" created clusterrole \"traefik-ingress-controller\" configured clusterrolebinding \"traefik-ingress-controller\" configured ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:3:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"部署 Traefik web ui # 编辑一个 service 服务 vi traefik-web-service.yaml apiVersion: v1 kind: Service metadata: name: traefik-web-ui namespace: kube-system spec: selector: k8s-app: traefik-ingress-lb ports: - port: 80 targetPort: 8888 ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:4:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"编辑一个 Traefik admin-web-ing 文件 vi traefik-ing.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-web-ui namespace: kube-system annotations: kubernetes.io/ingress.class: traefik spec: rules: - host: traefik.jicki.cn http: paths: - backend: serviceName: traefik-web-ui servicePort: 80 ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:5:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"测试其他 ing # 查看 svc kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None \u003cnone\u003e 80/TCP 5d # 编辑 ing apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx servicePort: 80 # 查看 ing kubectl get ing NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 6s # 查看访问 curl nginx.jicki.cn hello word! ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:6:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"测试访问 web ui curl traefik.jicki.cn \u003ca href=\"/dashboard/\"\u003eFound\u003c/a\u003e. ","date":"2017-10-23","objectID":"/traefik-rbac-ingress/:7:0","tags":null,"title":"Traefik ingress rbac","uri":"/traefik-rbac-ingress/"},{"categories":["kubernetes"],"content":"NFS - StorageClass","date":"2017-10-18","objectID":"/nfs-storageclass/","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"基于 StorageClass 的 NFS 动态卷 ","date":"2017-10-18","objectID":"/nfs-storageclass/:0:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"基本信息 172.16.1.64 K8s-Master and node 172.16.1.65 K8s-Master and node 172.16.1.66 K8s-Master and node and NFS Server ","date":"2017-10-18","objectID":"/nfs-storageclass/:1:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"NFS 服务 # 安装 NFS yum -y install nfs-utils rpcbind # k8s 所有节点 安装 NFS 客户端 yum -y install nfs-utils ","date":"2017-10-18","objectID":"/nfs-storageclass/:2:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"配置 NFS 目录与权限 vi /etc/exports 增加 /opt/nfsdata 172.16.1.0/24(rw,sync,no_root_squash) ","date":"2017-10-18","objectID":"/nfs-storageclass/:3:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"启动 NFS 服务 systemctl enable rpcbind.service systemctl enable nfs-server.service systemctl start rpcbind.service systemctl start nfs-server.service # 查看信息 showmount -e 172.16.1.66 Export list for 172.16.1.66: /opt/nfsdata 172.16.1.0/24 ","date":"2017-10-18","objectID":"/nfs-storageclass/:4:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"配置 NFS Client Provisioner # 官网镜像地址 quay.io/external_storage/nfs-client-provisioner:latest # 个人镜像地址 jicki/nfs-client-provisioner:latest # 配置一个 rbac.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: nfs-client-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: nfs-client-provisioner-runner rules: - apiGroups: [\"\"] resources: [\"persistentvolumes\"] verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"] - apiGroups: [\"\"] resources: [\"persistentvolumeclaims\"] verbs: [\"get\", \"list\", \"watch\", \"update\"] - apiGroups: [\"storage.k8s.io\"] resources: [\"storageclasses\"] verbs: [\"get\", \"list\", \"watch\"] - apiGroups: [\"\"] resources: [\"events\"] verbs: [\"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: run-nfs-client-provisioner subjects: - kind: ServiceAccount name: nfs-client-provisioner namespace: default roleRef: kind: ClusterRole name: nfs-client-provisioner-runner apiGroup: rbac.authorization.k8s.io # 配置一个 deployment 服务 kind: Deployment apiVersion: extensions/v1beta1 metadata: name: nfs-client-provisioner spec: replicas: 1 strategy: type: Recreate template: metadata: labels: app: nfs-client-provisioner spec: serviceAccount: nfs-client-provisioner containers: - name: nfs-client-provisioner image: jicki/nfs-client-provisioner:latest volumeMounts: - name: nfs-client-root mountPath: /persistentvolumes env: - name: PROVISIONER_NAME value: fuseim.pri/ifs - name: NFS_SERVER value: 172.16.1.66 - name: NFS_PATH value: /opt/nfsdata volumes: - name: nfs-client-root nfs: server: 172.16.1.66 path: /opt/nfsdata ","date":"2017-10-18","objectID":"/nfs-storageclass/:5:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"创建 服务 kubectl apply -f . serviceaccount \"nfs-client-provisioner\" created clusterrole \"nfs-client-provisioner-runner\" created clusterrolebinding \"run-nfs-client-provisioner\" created deployment \"nfs-client-provisioner\" created # 查看服务 kubectl get pods |grep nfs nfs-client-provisioner-8cdb56f4d-l8vmr 1/1 Running 0 26s ","date":"2017-10-18","objectID":"/nfs-storageclass/:6:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"创建 StorageClass # nfs-storageclass apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: nfs-storage provisioner: fuseim.pri/ifs # fuseim.pri/ifs 是 nfs-client-provisioner 服务中的一个 env # 导入文件 kubectl apply -f nfs-storageclass.yaml storageclass \"nfs-storage\" created # 查看服务 kubectl get storageclass NAME PROVISIONER nfs-storage fuseim.pri/ifs 测试 ","date":"2017-10-18","objectID":"/nfs-storageclass/:7:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"创建一个 nginx StatefulSet # nginx-statefulset apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 volumeClaimTemplates: - metadata: name: html annotations: volume.beta.kubernetes.io/storage-class: \"nfs-storage\" # 这里配置 上面创建的 storageclass 的名称 spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 2Gi template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine volumeMounts: - mountPath: \"/usr/share/nginx/html/\" name: html # 导入nginx-statefulset kubectl apply -f nginx-statefulset.yaml statefulset \"web\" created # 查看服务 kubectl get pods|grep web web-0 1/1 Running 0 1m web-1 1/1 Running 0 1m # 查看 pvc kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE html-web-0 Bound pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 1m html-web-1 Bound pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 1m 验证 测试 挂载 ","date":"2017-10-18","objectID":"/nfs-storageclass/:8:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"缩容 测试 # 查看服务 kubectl get pods|grep web web-0 1/1 Running 0 1m web-1 1/1 Running 0 1m # 查看 nfs-server 中的目录 ll 总用量 8 drwxrwxrwx 2 root root 4096 10月 18 10:19 default-html-web-0-pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 drwxrwxrwx 2 root root 4096 10月 18 10:19 default-html-web-1-pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 # 写入文件到挂载目录中 kubectl exec -it web-1 -- touch /usr/share/nginx/html/jicki.txt kubectl exec -it web-1 -- ls -lt /usr/share/nginx/html total 0 -rw-r--r-- 1 root root 0 Oct 18 02:24 jicki.txt # nfs server 中的文件 ls default-html-web-1-pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2/ jicki.txt # 收缩 pod = 1个 既 缩掉 web-1 kubectl scale statefulset web --replicas=1 statefulset \"web\" scaled # 查看 pod kubectl get pods |grep web web-0 1/1 Running 0 7m # 查看 pvc (发现 web-1 的 pvc 仍然在, 而且是 Bound 状态) kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE html-web-0 Bound pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 8m html-web-1 Bound pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 8m # 恢复 web-1 kubectl scale statefulset web --replicas=2 statefulset \"web\" scaled # 查看服务 kubectl get pods |grep web web-0 1/1 Running 0 13m web-1 1/1 Running 0 22s # 查看pvc kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE html-web-0 Bound pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 13m html-web-1 Bound pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 13m # 查看之前创建的文件 ( 发现恢复了 ) [root@k8s-master-64 yaml]# kubectl exec -it web-1 -- ls -lt /usr/share/nginx/html total 0 -rw-r--r-- 1 root root 0 Oct 18 02:24 jicki.txt ","date":"2017-10-18","objectID":"/nfs-storageclass/:9:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"删除 statefulset 测试 # delete 掉 kubectl delete -f nginx-statefulset.yaml statefulset \"web\" deleted # 查看 statefulset kubectl get statefulset No resources found. # 查看 pvc (发现 仍然 存在) kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE html-web-0 Bound pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 16m html-web-1 Bound pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 16m # nfs server 中的文件 (数据仍然存在) ls default-html-web-1-pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2/ jicki.txt # 重新创建 相同名称的 statefulset kubectl get pods |grep web web-0 1/1 Running 0 3m web-1 1/1 Running 0 3m # 查看 文件，可以看到 文件仍然存在 kubectl exec -it web-1 -- ls -lt /usr/share/nginx/html total 0 -rw-r--r-- 1 root root 0 Oct 18 02:24 jicki.txt ","date":"2017-10-18","objectID":"/nfs-storageclass/:10:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"删除 pvc 测试 # 收缩 replicas = 1 kubectl scale statefulsets web --replicas=1 # 查看 pvc kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE html-web-0 Bound pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 22h html-web-1 Bound pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 2Gi RWO nfs-storage 22h # 删除 pvc kubectl delete pvc/html-web-1 persistentvolumeclaim \"html-web-1\" deleted # nfs server (原目录更改为 archived 开头) archived-default-html-web-1-pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 default-html-web-0-pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 # 重新扩容 replicas = 2 kubectl scale statefulsets web --replicas=2 statefulset \"web\" scaled # nfs server (生成新的目录) archived-default-html-web-1-pvc-bc3478ac-b3aa-11e7-b194-80d4a5d413e2 default-html-web-1-pvc-4a64a8dc-b469-11e7-b194-80d4a5d413e2 default-html-web-0-pvc-bb0c0ada-b3aa-11e7-b194-80d4a5d413e2 ","date":"2017-10-18","objectID":"/nfs-storageclass/:11:0","tags":null,"title":"NFS - StorageClass","uri":"/nfs-storageclass/"},{"categories":["kubernetes"],"content":"kubernetes 1.8.0","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"kubernetes 1.8.0 ipvs 感觉并没有用上，持续更新ing…… 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:0:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"环境说明 k8s-master-25: 172.16.1.25 k8s-master-28: 172.16.1.28 k8s-master-29: 172.16.1.29 k8s-node-30: 172.16.1.30 k8s-node-32: 172.16.1.32 k8s-node-33: 172.16.1.33 k8s-node-34: 172.16.1.34 k8s-node-35: 172.16.1.35 k8s-node-36: 172.16.1.36 k8s-node-42: 172.16.1.42 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:1:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname k8s-master-25: 172.16.1.25 k8s-master-28: 172.16.1.28 k8s-master-29: 172.16.1.29 k8s-node-30: 172.16.1.30 k8s-node-32: 172.16.1.32 k8s-node-33: 172.16.1.33 k8s-node-34: 172.16.1.34 k8s-node-35: 172.16.1.35 k8s-node-36: 172.16.1.36 k8s-node-42: 172.16.1.42 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 172.16.1.25 k8s-master-25 172.16.1.28 k8s-master-28 172.16.1.29 k8s-master-29 172.16.1.30 k8s-node-30 172.16.1.32 k8s-node-32 172.16.1.33 k8s-node-33 172.16.1.34 k8s-node-34 172.16.1.35 k8s-node-35 172.16.1.36 k8s-node-36 172.16.1.42 k8s-node-42 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:2:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:3:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl # config.json 文件 vi config.json { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 vi csr.json { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:4:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@k8s-master-25 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:5:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp *.pem /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp *.pem 172.16.1.28:/etc/kubernetes/ssl/ scp *.pem 172.16.1.29:/etc/kubernetes/ssl/ 安装 docker # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 安装 yum install docker-ce -y ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:6:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 修改配置 vi /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS $DOCKER_OPTS $DOCKER_DNS_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target # 修改其他配置 mkdir -p /usr/lib/systemd/system/docker.service.d/ vi /usr/lib/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) # iptables=false 会使 docker run 的容器无法连网，false 是因为 calico 有一些高级的应用，需要限制容器互通。 # 建议 一般情况 不添加 --iptables=false [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 --graph=/opt/docker --registry-mirror=http://b438f72b.m.daocloud.io --disable-legacy-registry --iptables=false\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker etcd 集群 etcd 是k8s集群的基础组件 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:7:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"安装 etcd yum -y install etcd ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:8:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.25\", \"172.16.1.28\", \"172.16.1.29\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@k8s-master-25 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd*.pem 172.16.1.28:/etc/kubernetes/ssl/ # etcd-3 scp etcd*.pem 172.16.1.29:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:9:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 修改 etcd 启动文件 /usr/lib/systemd/system/etcd.service # etcd-1 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.25:2380 \\ --listen-peer-urls=https://172.16.1.25:2380 \\ --listen-client-urls=https://172.16.1.25:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.25:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.25:2380,etcd2=https://172.16.1.28:2380,etcd3=https://172.16.1.29:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.28:2380 \\ --listen-peer-urls=https://172.16.1.28:2380 \\ --listen-client-urls=https://172.16.1.28:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.28:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.25:2380,etcd2=https://172.16.1.28:2380,etcd3=https://172.16.1.29:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://172.16.1.29:2380 \\ --listen-peer-urls=https://172.16.1.29:2380 \\ --listen-client-urls=https://172.16.1.29:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://172.16.1.29:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://172.16.1.25:2380,etcd2=https://172.16.1.28:2380,etcd3=https://172.16.1.29:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:10:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:11:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://172.16.1.25:2379 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 29262d49176888f5 is healthy: got healthy result from https://172.16.1.29:2379 member d4ba1a2871bfa2b0 is healthy: got healthy result from https://172.16.1.25:2379 member eca58ebdf44f63b6 is healthy: got healthy result from https://172.16.1.28:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://172.16.1.25:2379 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 29262d49176888f5: name=etcd3 peerURLs=https://172.16.1.29:2380 clientURLs=https://172.16.1.29:2379 isLeader=false d4ba1a2871bfa2b0: name=etcd1 peerURLs=https://172.16.1.25:2380 clientURLs=https://172.16.1.25:2379 isLeader=true eca58ebdf44f63b6: name=etcd2 peerURLs=https://172.16.1.28:2380 clientURLs=https://172.16.1.28:2379 isLeader=false 安装 kubectl 工具 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:12:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"Master 端 # 首先安装 kubectl wget https://dl.k8s.io/v1.8.0/kubernetes-client-linux-amd64.tar.gz tar -xzvf kubernetes-client-linux-amd64.tar.gz cp kubernetes/client/bin/* /usr/local/bin/ chmod a+x /usr/local/bin/kube* # 验证安装 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"8\", GitVersion:\"v1.8.0\", GitCommit:\"6e937839ac04a38cac63e6a7a306c5d035fe7b0a\", GitTreeState:\"clean\", BuildDate:\"2017-09-28T22:57:57Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} The connection to the server localhost:8080 was refused - did you specify the right host or port? ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:13:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@k8s-master-25 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 172.16.1.28:/etc/kubernetes/ssl/ scp admin*.pem 172.16.1.29:/etc/kubernetes/ssl/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:14:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 server 配置为 本机IP 各自连接本机的 Api # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://172.16.1.25:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:15:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"kubectl config 文件 # kubeconfig 文件在 如下: /root/.kube 部署 Kubernetes Master 节点 Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:16:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"安装 组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.8.0/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:17:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"172.16.1.25\", \"172.16.1.28\", \"172.16.1.29\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 172.16.1.25 和 172.16.1.28 为 Master 的IP，多个Master需要写多个 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:18:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@k8s-master-25 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1245 7月 4 11:25 kubernetes.csr -rw------- 1 root root 1679 7月 4 11:25 kubernetes-key.pem -rw-r--r-- 1 root root 1619 7月 4 11:25 kubernetes.pem -rw-r--r-- 1 root root 436 7月 4 11:23 kubernetes-csr.json # 拷贝到目录 cp -r kubernetes*.pem /etc/kubernetes/ssl/ scp -r kubernetes*.pem 172.16.1.28:/etc/kubernetes/ssl/ scp -r kubernetes*.pem 172.16.1.29:/etc/kubernetes/ssl/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:19:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@k8s-master-25 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' d59a702004f33c659640bf8dd2717b64 # 创建 token.csv 文件 cd /opt/ssl vi token.csv d59a702004f33c659640bf8dd2717b64,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 172.16.1.28:/etc/kubernetes/ scp token.csv 172.16.1.29:/etc/kubernetes/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:20:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 1.8 新增 (Node) --authorization-mode=Node,RBAC # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --advertise-address=172.16.1.25 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --authorization-mode=Node,RBAC \\ --bind-address=172.16.1.25 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://172.16.1.25:2379,https://172.16.1.28:2379,https://172.16.1.29:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=172.16.1.25 \\ --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --enable-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:21:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:22:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager master 配置为 各自 本地 IP # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://172.16.1.25:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-cidr=10.233.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:23:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:24:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler master 配置为 各自 本地 IP # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://172.16.1.25:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:25:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:26:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@k8s-master-25 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@k8s-master-2 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@k8s-master-3 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:27:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"部署 Master Node 部分 Node 部分 需要部署的组件有 docker calico kubectl kubelet kube-proxy 这几个组件。 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:28:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:29:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 server 配置为 master 本机 IP # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://172.16.1.25:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=d59a702004f33c659640bf8dd2717b64 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:30:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 \u003e 配置为 node 本机 IP mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --address=172.16.1.25 \\ --hostname-override=172.16.1.25 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --fail-swap-on=false \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target # 如上配置: 172.16.1.25 为本机的IP 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:31:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet # 如果报错 请使用 journalctl -f -t kubelet 和 journalctl -u kubelet 来定位问题 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:32:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-25 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-pf-Bb5Iqx6ccvVA67gLVT-G4Zl3Zl5FPUZS4d7V6rk4 1h kubelet-bootstrap Pending # 增加 认证 [root@k8s-master-25 ~]# kubectl certificate approve node-csr-pf-Bb5Iqx6ccvVA67gLVT-G4Zl3Zl5FPUZS4d7V6rk4 certificatesigningrequest \"node-csr-pf-Bb5Iqx6ccvVA67gLVT-G4Zl3Zl5FPUZS4d7V6rk4\" approved [root@k8s-master-25 ~]# ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:33:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"验证 nodes [root@k8s-master-25 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION 172.16.1.25 Ready \u003cnone\u003e 22s v1.8.0 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:34:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:35:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@k8s-master-25 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:36:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy*.pem /etc/kubernetes/ssl/ scp kube-proxy*.pem 172.16.1.28:/etc/kubernetes/ssl/ scp kube-proxy*.pem 172.16.1.29:/etc/kubernetes/ssl/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:37:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 server 配置为各自 本机IP # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://172.16.1.25:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:38:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 配置为 各自的 IP # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=172.16.1.25 \\ --hostname-override=172.16.1.25 \\ --cluster-cidr=10.254.0.0/16 \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:39:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy # 如果报错 请使用 journalctl -f -t kube-proxy 和 journalctl -u kube-proxy 来定位问题 部署 Node 节点 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA cd /tmp wget https://dl.k8s.io/v1.8.0/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-proxy,kubelet} /usr/local/bin/ # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ # kubelet # 首先 创建 kubelet kubeconfig 文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=d59a702004f33c659640bf8dd2717b64 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ # 创建 kube-proxy kubeconfig 文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:40:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 172.16.1.25:6443; server 172.16.1.28:6443; server 172.16.1.29:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.3-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy systemctl status nginx-proxy # 重启 Node 的 kubelet 与 kube-proxy systemctl restart kubelet systemctl status kubelet systemctl restart kube-proxy systemctl status kube-proxy ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:41:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"Master 配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-25 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-5dohu3QPMPWkxp9HRiNv7fZSXTE3MbxCIdf6G95ehXE 1h kubelet-bootstrap Approved,Issued node-csr-8ubThjzvgkN4GSg-uOaYsanjN41cyvFwWCsT411aknY 18s kubelet-bootstrap Pending node-csr-HcwiDOU9cEk8pYrCT0jfFDagPGZC0-myCPynS9Ie0Bg 26s kubelet-bootstrap Pending node-csr-STnSC13Y0MTIMwd2sC1btrQe0ycnS5zP5rNed0DNl2s 23s kubelet-bootstrap Pending node-csr-Sq1S75d6j3cTkvb5L68uv2eqp-ci7NpK0-O-OMcDXWE 20s kubelet-bootstrap Pending node-csr-ZFWZ5q5m323w_Iv5eTQclDhfgwaZ0Go-pztEoBfCMJk 22s kubelet-bootstrap Pending node-csr-_yztJwMNeZ9HPlofd0Eiy_4fQLqKCIjU9_IfQ5koDCk 1h kubelet-bootstrap Approved,Issued node-csr-pf-Bb5Iqx6ccvVA67gLVT-G4Zl3Zl5FPUZS4d7V6rk4 2h kubelet-bootstrap Approved,Issued node-csr-u6JUtu1mVbC6sb4bxDuGkZT7Flzehren0OkOlfNp_MA 16s kubelet-bootstrap Pending node-csr-xZZVdiWl1UEnDaRNkp3AHzT_E8FX3A71-5hDgQOW08U 14s kubelet-bootstrap Pending # 增加 认证 [root@k8s-master-25 ~]# kubectl certificate approve NAME [root@k8s-master-25 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION 172.16.1.25 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.28 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.29 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.30 Ready \u003cnone\u003e 37s v1.8.0 172.16.1.32 Ready \u003cnone\u003e 31s v1.8.0 172.16.1.33 Ready \u003cnone\u003e 22s v1.8.0 172.16.1.34 Ready \u003cnone\u003e 25s v1.8.0 172.16.1.35 Ready \u003cnone\u003e 41s v1.8.0 172.16.1.36 Ready \u003cnone\u003e 17s v1.8.0 172.16.1.42 Ready \u003cnone\u003e 12s v1.8.0 Calico 网络 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:42:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"修改 kubelet.service vi /etc/systemd/system/kubelet.service # 增加 如下配置 --network-plugin=cni \\ # 重新加载配置 systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:43:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"安装 Calico 官网地址 http://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/hosted # 下载 yaml 文件 wget http://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/calico.yaml wget http://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/rbac.yaml # 下载 镜像 # 国外镜像 有墙 quay.io/calico/node:v2.6.0 quay.io/calico/cni:v1.11.0 quay.io/calico/kube-controllers:v1.0.0 # 国内镜像 jicki/node:v2.6.0 jicki/cni:v1.11.0 jicki/kube-controllers:v1.0.0 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:44:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 calico vi calico.yaml # 注意修改如下选项: etcd_endpoints: \"https://172.16.1.25:2379,https://172.16.1.28:2379,https://172.16.1.29:2379\" etcd_ca: \"/calico-secrets/etcd-ca\" etcd_cert: \"/calico-secrets/etcd-cert\" etcd_key: \"/calico-secrets/etcd-key\" # 这里面要写入 base64 的信息 data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d '\\n') etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d '\\n') etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d '\\n') - name: CALICO_IPV4POOL_CIDR value: \"10.233.0.0/16\" ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:45:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 [root@k8s-master-25 ~]# kubectl apply -f calico.yaml configmap \"calico-config\" created secret \"calico-etcd-secrets\" created daemonset \"calico-node\" created deployment \"calico-policy-controller\" created serviceaccount \"calico-policy-controller\" created serviceaccount \"calico-node\" created [root@k8s-master-25 ~]# kubectl apply -f rbac.yaml clusterrole \"calico-policy-controller\" created clusterrolebinding \"calico-policy-controller\" created clusterrole \"calico-node\" created clusterrolebinding \"calico-node\" created ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:46:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"验证 Calico [root@k8s-master-25 calico]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-85bd96b9bb-mxjxg 1/1 Running 0 1m calico-node-25vjm 2/2 Running 0 1m calico-node-4cgxh 2/2 Running 0 1m calico-node-8ztfz 2/2 Running 0 1m calico-node-btdqs 2/2 Running 0 1m calico-node-g9h4l 2/2 Running 0 1m calico-node-kmrjm 2/2 Running 0 1m calico-node-mzsxg 2/2 Running 0 1m calico-node-nrxmh 2/2 Running 0 1m calico-node-wprhw 2/2 Running 0 1m calico-node-xcfvj 2/2 Running 0 1m ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:47:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"安装 Calicoctl cd /usr/local/bin/ wget -c https://github.com/projectcalico/calicoctl/releases/download/v1.3.0/calicoctl chmod +x calicoctl ## 创建 calicoctl 配置文件 # 配置文件， 在 安装了 calico 网络的 机器下 mkdir /etc/calico vi /etc/calico/calicoctl.cfg apiVersion: v1 kind: calicoApiConfig metadata: spec: datastoreType: \"etcdv2\" etcdEndpoints: \"https://172.16.1.25:2379,https://172.16.1.28:2379,https://172.16.1.29:2379\" etcdKeyFile: \"/etc/kubernetes/ssl/etcd-key.pem\" etcdCertFile: \"/etc/kubernetes/ssl/etcd.pem\" etcdCACertFile: \"/etc/kubernetes/ssl/ca.pem\" # 查看 calico 状态 [root@k8s-master-25 ~]# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 172.16.1.36 | node-to-node mesh | up | 07:56:51 | Established | | 172.16.1.42 | node-to-node mesh | up | 07:56:51 | Established | | 172.16.1.34 | node-to-node mesh | up | 07:56:57 | Established | | 172.16.1.35 | node-to-node mesh | up | 07:56:59 | Established | | 172.16.1.33 | node-to-node mesh | up | 07:57:02 | Established | | 172.16.1.28 | node-to-node mesh | up | 07:57:02 | Established | | 172.16.1.29 | node-to-node mesh | up | 07:57:04 | Established | | 172.16.1.32 | node-to-node mesh | up | 07:57:04 | Established | | 172.16.1.30 | node-to-node mesh | up | 07:57:10 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 10 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-master-25 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-55b58f68b6-6phk6 1/1 Running 0 2m 10.233.183.193 172.16.1.32 nginx-dm-55b58f68b6-6q2lc 1/1 Running 0 2m 10.233.235.65 172.16.1.28 nginx-dm-55b58f68b6-9qq2g 1/1 Running 0 2m 10.233.185.65 172.16.1.36 nginx-dm-55b58f68b6-dscgw 1/1 Running 0 2m 10.233.104.65 172.16.1.35 nginx-dm-55b58f68b6-hbfdc 1/1 Running 0 2m 10.233.246.193 172.16.1.33 nginx-dm-55b58f68b6-nkgnq 1/1 Running 0 2m 10.233.209.129 172.16.1.34 nginx-dm-55b58f68b6-tf25w 1/1 Running 0 2m 10.233.14.65 172.16.1.42 nginx-dm-55b58f68b6-x9hk5 1/1 Running 0 2m 10.233.28.129 172.16.1.25 nginx-dm-55b58f68b6-xbprt 1/1 Running 0 2m 10.233.21.1 172.16.1.29 nginx-dm-55b58f68b6-zqmdp 1/1 Running 0 2m 10.233.13.129 172.16.1.30 # 在 node 里 curl [root@k8s-master-25 ~]# curl 10.254.129.54 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 配置 KubeDNS 官方 github yaml 相关 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:48:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"下载镜像 # 官方镜像 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.5 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 # 我的镜像 jicki/k8s-dns-sidecar-amd64:1.14.5 jicki/k8s-dns-kube-dns-amd64:1.14.5 jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.5 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:49:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kube-dns.yaml.base # 修改后缀 mv kube-dns.yaml.base kube-dns.yaml ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:50:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限； [root@k8s-master-25 kubedns]# kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" creationTimestamp: 2017-09-29T04:12:29Z labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-dns resourceVersion: \"78\" selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system%3Akube-dns uid: 688927eb-a4cc-11e7-9f6b-44a8420b9988 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-dns subjects: - kind: ServiceAccount name: kube-dns namespace: kube-system ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:51:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"修改 kube-dns.yaml 1. # clusterIP: __PILLAR__DNS__SERVER__ 修改为我们之前定义的 dns IP 10.254.0.2 2. # 修改 --domain=__PILLAR__DNS__DOMAIN__. 为 我们之前 预定的 domain 名称 --domain=cluster.local. 3. # 修改 --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053 中 domain 为我们之前预定的 --server=/cluster.local./127.0.0.1#10053 4. # 修改 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local., 5. # 修改 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local., ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:52:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 导入 [root@k8s-master-25 kubedns]# kubectl create -f . service \"kube-dns\" created serviceaccount \"kube-dns\" created configmap \"kube-dns\" created deployment \"kube-dns\" created ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:53:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@k8s-master-25 kubedns]# kubectl get all --namespace=kube-system NAME READY STATUS RESTARTS AGE po/calico-node-2dsq4 2/2 Running 0 15h po/calico-node-9ktvk 2/2 Running 0 15h po/calico-node-gwmx5 2/2 Running 0 15h po/calico-policy-controller-458850194-pn65p 1/1 Running 0 15h po/kube-dns-1511229508-jxkvs 3/3 Running 0 4m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 4m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/calico-policy-controller 1 1 1 1 15h deploy/kube-dns 1 1 1 1 4m NAME DESIRED CURRENT READY AGE rs/calico-policy-controller-458850194 1 1 1 15h rs/kube-dns-1511229508 1 1 1 4m ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:54:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 导入之前的 nginx-dm yaml文件 [root@k8s-master-25 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-55b58f68b6-42cmf 1/1 Running 0 49s 10.233.209.130 172.16.1.34 nginx-dm-55b58f68b6-8stgq 1/1 Running 0 49s 10.233.183.194 172.16.1.32 nginx-dm-55b58f68b6-f67fv 1/1 Running 0 49s 10.233.235.66 172.16.1.28 nginx-dm-55b58f68b6-kprjz 1/1 Running 0 49s 10.233.246.194 172.16.1.33 nginx-dm-55b58f68b6-lmfc5 1/1 Running 0 49s 10.233.185.67 172.16.1.36 nginx-dm-55b58f68b6-rpsz5 1/1 Running 0 49s 10.233.104.66 172.16.1.35 nginx-dm-55b58f68b6-vngzm 1/1 Running 0 49s 10.233.21.2 172.16.1.29 nginx-dm-55b58f68b6-x6gq9 1/1 Running 0 49s 10.233.14.66 172.16.1.42 nginx-dm-55b58f68b6-xsccz 1/1 Running 0 49s 10.233.13.130 172.16.1.30 nginx-dm-55b58f68b6-zv8jg 1/1 Running 0 49s 10.233.28.130 172.16.1.25 [root@k8s-master-25 ~]# kubectl get svc -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes ClusterIP 10.254.0.1 \u003cnone\u003e 443/TCP 4h \u003cnone\u003e nginx-svc NodePort 10.254.143.65 \u003cnone\u003e 80:30713/TCP 1m name=nginx # 创建一个 pods 来测试一下 nameserver apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 pods [root@k8s-master-25 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 5s # 测试 [root@k8s-master-25 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.143.65 nginx-svc.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:55:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"部署 dashboard \u0026\u0026 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:56:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.3 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.6.3 ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:57:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-controller.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-service.yaml # 因为开启了 RBAC 所以这里需要创建一个 RBAC 认证 vi dashboard-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: dashboard namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: name: dashboard subjects: - kind: ServiceAccount name: dashboard namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:58:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # dashboard-controller.yaml 增加 rbac 授权 # 在第二个 spec 下面 增加 spec: serviceAccountName: dashboard # 导入文件 [root@k8s-master-25 dashboard]# kubectl apply -f . deployment \"kubernetes-dashboard\" created serviceaccount \"dashboard\" created clusterrolebinding \"dashboard\" created service \"kubernetes-dashboard\" created # 查看 svc 与 pod [root@k8s-master-25 ~]# kubectl get svc -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 9m kubernetes-dashboard ClusterIP 10.254.119.100 \u003cnone\u003e 80/TCP 16s ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:59:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github https://github.com/kubernetes/ingress-nginx ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:60:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@k8s-master-25 ingress]# kubectl get nodes NAME STATUS ROLES AGE VERSION 172.16.1.25 Ready \u003cnone\u003e 2h v1.8.0 172.16.1.28 Ready \u003cnone\u003e 2h v1.8.0 172.16.1.29 Ready \u003cnone\u003e 2h v1.8.0 172.16.1.30 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.32 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.33 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.34 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.35 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.36 Ready \u003cnone\u003e 1h v1.8.0 172.16.1.42 Ready \u003cnone\u003e 1h v1.8.0 # 对 28 与 29 打上 label [root@k8s-master-25 ingress]# kubectl label nodes 172.16.1.28 ingress=proxy node \"172.16.1.28\" labeled [root@k8s-master-25 ingress]# kubectl label nodes 172.16.1.29 ingress=proxy node \"172.16.1.29\" labeled # 打完标签以后 [root@k8s-master-25 ingress]# kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS 172.16.1.25 Ready \u003cnone\u003e 2h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.25 172.16.1.28 Ready \u003cnone\u003e 2h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=172.16.1.28 172.16.1.29 Ready \u003cnone\u003e 2h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=172.16.1.29 172.16.1.30 Ready \u003cnone\u003e 1h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.30 172.16.1.32 Ready \u003cnone\u003e 1h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.32 172.16.1.33 Ready \u003cnone\u003e 1h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.33 172.16.1.34 Ready \u003cnone\u003e 1h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.34 172.16.1.35 Ready \u003cnone\u003e 1h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.35 172.16.1.36 Ready \u003cnone\u003e 1h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.36 172.16.1.42 Ready \u003cnone\u003e 1h v1.8.0 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=172.16.1.42 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.0 gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13 # 国内镜像 jicki/defaultbackend:1.0 jicki/nginx-ingress-controller:0.9.0-beta.13 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/namespace.yaml curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/default-backend.yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 直接导入既可, 这里不需要修改 [root@k8s-master-25 ingress]# kubectl apply -f default-backend.yaml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看服务 [root@k8s-master-25 ingress]# kubectl get deployment -n ingress-nginx default-http-backend NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 36s # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/rbac.yaml # 导入 yaml 文件 [root@k8s-master-25 ingress]# kubectl apply -f rbac.yml namespace \"nginx-ingress\" created serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created # 部署 Ingress Controller 组件 # 下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/with-rbac.yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 spec: replicas: 2 .... spec: hostNetwork: true serviceAccountName: nginx-ingress-serviceaccount nodeSelector: ingres","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:61:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"Master Or Node Update 二进制文件 cd /tmp wget https://dl.k8s.io/v1.8.0/kubernetes-server-linux-amd64.tar.gz systemctl stop kube-apiserver systemctl stop kube-controller-manager systemctl stop kube-scheduler systemctl stop kubelet systemctl stop kube-proxy tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ systemctl start kube-apiserver systemctl start kube-controller-manager systemctl start kube-scheduler systemctl start kubelet systemctl start kube-proxy systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler systemctl status kubelet systemctl status kube-proxy cd .. rm -rf kubernetes* ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:62:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"Node Update 二进制文件 cd /tmp wget https://dl.k8s.io/v1.8.0/kubernetes-server-linux-amd64.tar.gz systemctl stop kubelet systemctl stop kube-proxy tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kubectl,kube-proxy,kubelet} /usr/local/bin/ systemctl start kubelet systemctl start kube-proxy systemctl status kubelet systemctl status kube-proxy cd .. rm -rf kubernetes* ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:63:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"特殊 env # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2017-09-29","objectID":"/kubernetes-1.8.0-up/:64:0","tags":null,"title":"kubernetes 1.8.0","uri":"/kubernetes-1.8.0-up/"},{"categories":["kubernetes"],"content":"Kubernetes Storage Class","date":"2017-09-11","objectID":"/kubernetes-storageclass/","tags":null,"title":"Kubernetes Storage Class","uri":"/kubernetes-storageclass/"},{"categories":["kubernetes"],"content":"Kubernetes Storage Class StorageClass 可以定义多个 StorageClass 对象，并可以分别指定存储插件、设置参数，用于提供不同的存储卷。这样的设计让集群管理员能够在同一个集群内，定义和提供不同类型的、不同参数的卷（相同或者不同的存储系统）。这样的设计还确保了最终用户在无需了解太多的情况下，有能力选择不同的存储选项。本质上是为底层存储提供者描绘了蓝图，以及各种参数。 Ceph RBD StorageClass Ceph RBD 集群部署 这里就略过了，可查看我之前的文章 https://jicki.cn/2017/05/09/kubernetes-ceph-rbd ","date":"2017-09-11","objectID":"/kubernetes-storageclass/:0:0","tags":null,"title":"Kubernetes Storage Class","uri":"/kubernetes-storageclass/"},{"categories":["kubernetes"],"content":"创建 ceph pool # 预先 创建 需要用到的 pool = data [root@ceph-node-1 ~]# ceph osd pool create data 128 # 128 为 pg 数 的计算公式一般为 若少于5个OSD， 设置pg_num为128。 5~10个OSD，设置pg_num为512。 10~50个OSD，设置pg_num为4096。 超过50个OSD，可以参考计算公式。 pg数 = osd 数量 * 100 / pool 复制份数 / pool 数量 # 查看 pool 复制份数, 既 ceph.conf 里设置的 osd_pool_default_size ceph osd dump |grep size|grep rbd # 当 osd pool复制数 pool 数量 变更时，应该重新计算并变更 pg 数 # 变更 pg_num 的时候 应该将 pgp_num 的数量一起变更，否则无法报错 ceph osd pool set rbd pg_num 256 ceph osd pool set rbd pgp_num 256 # 查看 创建的 pool [root@ceph-node-1 ceph-cluster]# ceph osd lspools 0 rbd,1 data, # 查看 pool 具体参数 [root@ceph-node-1 ceph-cluster]# ceph osd dump |grep pool pool 0 'rbd' replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 27 flags hashpspool stripe_width 0 pool 1 'data' replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 128 pgp_num 128 last_change 102 flags hashpspool stripe_width 0 # 查看 pool 下的 image [root@ceph-node-1 ceph-cluster]# rbd -p data list kubernetes-dynamic-pvc-fbd43f54-96a6-11e7-ac97-44a8420b9988 [root@ceph-node-1 ceph-cluster]# rbd -p data info kubernetes-dynamic-pvc-fbd43f54-96a6-11e7-ac97-44a8420b9988 rbd image 'kubernetes-dynamic-pvc-fbd43f54-96a6-11e7-ac97-44a8420b9988': size 51200 MB in 12800 objects order 22 (4096 kB objects) block_name_prefix: rb.0.5f29.74b0dc51 format: 1 ","date":"2017-09-11","objectID":"/kubernetes-storageclass/:1:0","tags":null,"title":"Kubernetes Storage Class","uri":"/kubernetes-storageclass/"},{"categories":["kubernetes"],"content":"创建 secret 认证 # 创建 ceph-secret （如果之前创建过 secret , delete 掉） # 获取 client.admin 的值 [root@ceph-node-1 ceph-cluster]# ceph auth get-key client.admin AQBpUAxZGmDBBxAAVbgeRss9jv39dE0biTE7qQ== # 创建 secret ( type 类型必须为 kubernetes.io/rbd ) kubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='AQBpUAxZGmDBBxAAVbgeRss9jv39dE0biTE7qQ==' --namespace=kube-system kubectl create secret generic ceph-secret --type=\"kubernetes.io/rbd\" --from-literal=key='AQBpUAxZGmDBBxAAVbgeRss9jv39dE0biTE7qQ==' --namespace=default # 查看 状态 [root@k8s-master-25 storageclass]# kubectl get secret -n kube-system |grep ceph ceph-secret kubernetes.io/rbd 1 49s ","date":"2017-09-11","objectID":"/kubernetes-storageclass/:2:0","tags":null,"title":"Kubernetes Storage Class","uri":"/kubernetes-storageclass/"},{"categories":["kubernetes"],"content":"配置 storageclass # 创建一个 storageclass 文件 vi test-storageclass.yaml apiVersion: storage.k8s.io/v1beta1 kind: StorageClass metadata: name: ceph-rbd-test provisioner: kubernetes.io/rbd parameters: monitors: 172.16.1.37:6789,172.16.1.38:6789,172.16.1.39:6789 adminId: admin adminSecretName: ceph-secret adminSecretNamespace: kube-system pool: data userId: admin userSecretName: ceph-secret # 配置说明: (主要是 parameters 字段下) monitors: Ceph Mon 的地址，以,隔开 adminId: Ceph客户端用于创建块设备的用户(不是k8s用户)，默认为 admin 用户； adminSecretNamespace: admin 的 namespaces adminSecret：admin的SecretID pool： RBD的pool存储池 userId: 用于块设备映射的用户ID，默认可以和admin一致 userSecretName： Ceph-Secret的ID # 导入 yaml 文件 [root@k8s-master-25 storageclass]# kubectl apply -f test-storageclass.yaml storageclass \"ceph-rbd-test\" created # 查看 [root@k8s-master-25 storageclass]# kubectl get StorageClass NAME TYPE ceph-rbd-test kubernetes.io/rbd ","date":"2017-09-11","objectID":"/kubernetes-storageclass/:3:0","tags":null,"title":"Kubernetes Storage Class","uri":"/kubernetes-storageclass/"},{"categories":["kubernetes"],"content":"创建 pvc # 创建基于 storageclass 的 pvc vi storageclass-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: storageclass-pvc annotations: volume.beta.kubernetes.io/storage-class: ceph-rbd-test spec: accessModes: - ReadWriteMany resources: requests: storage: 50Gi # 导入 yaml 文件 [root@k8s-master-25 storageclass]# kubectl apply -f storageclass-pvc.yaml persistentvolumeclaim \"storageclass-pvc\" created # 查看 [root@k8s-master-25 storageclass]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE storageclass-pvc Bound pvc-b1faef32-96a4-11e7-bf40-44a8420b9988 50Gi RWO ceph-rbd-test 30m ","date":"2017-09-11","objectID":"/kubernetes-storageclass/:4:0","tags":null,"title":"Kubernetes Storage Class","uri":"/kubernetes-storageclass/"},{"categories":["kubernetes"],"content":"创建测试 StatefulSet storageclass 主要用于 有状态服务 statefulset 的volume 使用 # 创建一个测试 的 StatefulSet # svc 中必须配置 clusterIP: None , 因为 StatefulSet 需要 Headless 服务 apiVersion: apps/v1beta1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 4 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:alpine ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www annotations: volume.beta.kubernetes.io/storage-class: ceph-rbd-test spec: accessModes: [ \"ReadWriteOnce\" ] resources: requests: storage: 10Gi --- apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx ## 查看 pods [root@k8s-master-25 storageclass]# kubectl get pods NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 1m web-1 1/1 Running 0 1m web-2 1/1 Running 0 1m web-3 1/1 Running 0 1m ## 查看自动创建的 pvc [root@k8s-master-25 storageclass]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESSMODES STORAGECLASS AGE www-web-0 Bound pvc-c927b383-96cb-11e7-bf40-44a8420b9988 10Gi RWO ceph-rbd-test 2m www-web-1 Bound pvc-cdfb501e-96cb-11e7-bf40-44a8420b9988 10Gi RWO ceph-rbd-test 1m www-web-2 Bound pvc-d27843f0-96cb-11e7-bf40-44a8420b9988 10Gi RWO ceph-rbd-test 1m www-web-3 Bound pvc-d571ef22-96cb-11e7-bf40-44a8420b9988 10Gi RWO ceph-rbd-test 1m ## 查看 svc [root@k8s-master-25 storageclass]# kubectl get svc -l app=nginx NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx None \u003cnone\u003e 80/TCP 4m ## 测试 dns 以及 Headless 的 dns 服务 [root@k8s-master-25 storageclass]# kubectl exec -it alpine -- nslookup nginx nslookup: can't resolve '(null)': Name does not resolve Name: nginx Address 1: 10.233.104.72 web-2.nginx.default.svc.cluster.local Address 2: 10.233.209.135 web-3.nginx.default.svc.cluster.local Address 3: 10.233.235.68 web-0.nginx.default.svc.cluster.local Address 4: 10.233.246.196 web-1.nginx.default.svc.cluster.local [root@k8s-master-25 storageclass]# kubectl exec -it alpine -- nslookup web-0.nginx nslookup: can't resolve '(null)': Name does not resolve Name: web-0.nginx Address 1: 10.233.235.68 web-0.nginx.default.svc.cluster.local ","date":"2017-09-11","objectID":"/kubernetes-storageclass/:5:0","tags":null,"title":"Kubernetes Storage Class","uri":"/kubernetes-storageclass/"},{"categories":["Linux"],"content":"Open-Falcon 监控","date":"2017-08-18","objectID":"/open-falcon/","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"环境准备 官方文档 https://book.open-falcon.org/zh_0_2 ","date":"2017-08-18","objectID":"/open-falcon/:0:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"安装 golang wget https://storage.googleapis.com/golang/go1.6.4.linux-amd64.tar.gz tar zxvf go1.6.4.linux-amd64.tar.gz mv go /opt/local/ # 增加环境变量 vi /etc/profile # Golang ENV export GOROOT=/opt/local/go export PATH=$PATH:$GOROOT/bin export GOPATH=/opt/local/golang go version go version go1.6.4 linux/amd64 ","date":"2017-08-18","objectID":"/open-falcon/:1:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"安装 Mysql 5.7 # 初始化依赖 yum -y install cmake ncurses ncurses-devel bison bison-devel boost boost-devel # 创建 mysql 用户以及相关目录 /usr/sbin/groupadd mysql /usr/sbin/useradd -g mysql mysql mkdir -p /opt/local/mysql/data mkdir -p /opt/local/mysql/binlog mkdir -p /opt/local/mysql/logs mkdir -p /opt/local/mysql/relaylog mkdir -p /var/lib/mysql mkdir -p /opt/local/mysql/etc # 下载 源码包 wget ftp://ftp.mirrorservice.org/sites/ftp.mysql.com/Downloads/MySQL-5.7/mysql-5.7.19.tar.gz tar zxvf mysql-5.7.19.tar.gz cd mysql-5.7.19 cmake -DCMAKE_INSTALL_PREFIX=\"/opt/local/mysql\" -DDEFAULT_CHARSET=utf8 \\ -DMYSQL_DATADIR=\"/opt/local/mysql/data/\" -DCMAKE_INSTALL_PREFIX=\"/opt/local/mysql\" \\ -DINSTALL_PLUGINDIR=plugin -DWITH_INNOBASE_STORAGE_ENGINE=1 -DDEFAULT_COLLATION=utf8_general_ci \\ -DENABLED_LOCAL_INFILE=1 -DENABLED_PROFILING=1 -DWITH_ZLIB=system \\ -DWITH_EXTRA_CHARSETS=none -DMYSQL_MAINTAINER_MODE=OFF -DEXTRA_CHARSETS=all \\ -DWITH_PERFSCHEMA_STORAGE_ENGINE=1 -DWITH_MYISAM_STORAGE_ENGINE=1 \\ -DDOWNLOAD_BOOST=1 -DWITH_BOOST=/usr/local/boost make -j `cat /proc/cpuinfo | grep processor| wc -l` make install # 创建相关目录，授权 chmod +w /opt/local/mysql chown -R mysql:mysql /opt/local/mysql chmod +w /var/lib/mysql chown -R mysql:mysql /var/lib/mysql cp /opt/local/mysql/support-files/mysql.server /etc/init.d/mysqld chmod 755 /etc/init.d/mysqld echo 'basedir=/opt/local/mysql/' \u003e\u003e /etc/init.d/mysqld echo 'datadir=/opt/local/mysql/data' \u003e\u003e/etc/init.d/mysqld # 创建关联 ln -s /opt/local/mysql/lib/mysql /usr/lib/mysql ln -s /opt/local/mysql/include/mysql /usr/include/mysql ln -s /opt/local/mysql/bin/mysql /usr/bin/mysql ln -s /opt/local/mysql/bin/mysqldump /usr/bin/mysqldump ln -s /opt/local/mysql/bin/myisamchk /usr/bin/myisamchk ln -s /opt/local/mysql/bin/mysqld_safe /usr/bin/mysqld_safe ln -s /tmp/mysql.sock /var/lib/mysql/mysql.sock # 初始化数据库 vi /opt/local/mysql/etc/my.cnf [client] default-character-set=utf8mb4 [mysqld] ########basic settings######## server-id = 1 port = 3306 user = mysql bind_address = 127.0.0.1 autocommit = 1 character_set_server=utf8mb4 collation-server=utf8mb4_unicode_ci skip-character-set-client-handshake init_connect='SET collation_connection = utf8mb4_unicode_ci' init_connect='SET NAMES utf8mb4' skip_name_resolve = 1 max_connections = 800 max_connect_errors = 1000 datadir = /opt/local/mysql/data pid-file = /opt/local/mysql/mysql.pid transaction_isolation = READ-COMMITTED explicit_defaults_for_timestamp = 1 join_buffer_size = 134217728 tmp_table_size = 67108864 tmpdir = /tmp max_allowed_packet = 16777216 sql_mode = \"STRICT_TRANS_TABLES,NO_ENGINE_SUBSTITUTION,NO_ZERO_DATE,NO_ZERO_IN_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER\" interactive_timeout = 1800 wait_timeout = 1800 read_buffer_size = 16777216 read_rnd_buffer_size = 33554432 sort_buffer_size = 33554432 ########log settings######## log_error = /opt/local/mysql/logs/mysqld.log slow_query_log = 1 slow_query_log_file = /opt/local/mysql/logs/slow.log log_queries_not_using_indexes = 1 log_slow_admin_statements = 1 log_slow_slave_statements = 1 log_throttle_queries_not_using_indexes = 10 expire_logs_days = 90 long_query_time = 2 min_examined_row_limit = 100 ########replication settings######## master_info_repository = TABLE relay_log_info_repository = TABLE log_bin = /opt/local/mysql/binlog/mysql-bin sync_binlog = 1 gtid_mode = on enforce_gtid_consistency = 1 log_slave_updates binlog_format = row relay_log = /opt/local/mysql/relaylog/relay-bin relay_log_recovery = 1 binlog_gtid_simple_recovery = 1 slave_skip_errors = ddl_exist_errors ########innodb settings######## innodb_page_size = 16384 innodb_buffer_pool_size = 8G innodb_buffer_pool_instances = 8 innodb_buffer_pool_load_at_startup = 1 innodb_buffer_pool_dump_at_shutdown = 1 innodb_lru_scan_depth = 2000 innodb_lock_wait_timeout = 5 innodb_io_capacity = 4000 innodb_io_capacity_max = 8000 innodb_flush_method = O_DIRECT innodb_file_format = Barracuda innodb_file_format_max = Barracuda innodb_log_group_home_dir = /opt/local/mysql/relaylog innodb_u","date":"2017-08-18","objectID":"/open-falcon/:2:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"安装 redis # 下载 redis wget http://download.redis.io/releases/redis-3.2.10.tar.gz tar zxvf redis-3.2.10.tar.gz cd redis-3.2.10 make make install cd utils ./install_server.sh ...... 输入相关信息 ...... cd /etc/init.d/ mv redis_6379 redis # 创建 目录 mkdir -p /opt/local/redis/{data,logs,conf} cd /opt/local/redis/conf vi redis.conf bind 127.0.0.1 protected-mode yes port 6379 tcp-backlog 2048 timeout 0 tcp-keepalive 300 daemonize yes supervised no pidfile /var/run/redis.pid loglevel notice logfile \"/opt/local/redis/logs/redis.log\" maxmemory 10gb databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename redis_dump.rdb dir /opt/local/redis/data slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 appendonly no appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes # 启动 redis chkconfig redis on service redis start 配置 Open-Falcon ","date":"2017-08-18","objectID":"/open-falcon/:3:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"初始化环境 mkdir -p $GOPATH/src/github.com/open-falcon cd $GOPATH/src/github.com/open-falcon git clone https://github.com/open-falcon/falcon-plus.git ","date":"2017-08-18","objectID":"/open-falcon/:4:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"导入数据库 cd $GOPATH/src/github.com/open-falcon/falcon-plus/scripts/mysql/db_schema/ mysql -u root -p \u003c 1_uic-db-schema.sql mysql -u root -p \u003c 2_portal-db-schema.sql mysql -u root -p \u003c 3_dashboard-db-schema.sql mysql -u root -p \u003c 4_graph-db-schema.sql mysql -u root -p \u003c 5_alarms-db-schema.sql ","date":"2017-08-18","objectID":"/open-falcon/:5:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"编译 程序 cd $GOPATH/src/github.com/open-falcon/falcon-plus/ # 编译所有的模块 make all # 编译指定模块 make agent # 打包 make pack # 创建目录 mkdir /opt/local/open-falcon mv open-falcon-v0.2.1.tar.gz /opt/local/open-falcon cd /opt/local/open-falcon tar zxvf open-falcon-v0.2.1.tar.gz [root@localhost open-falcon]# ls agent aggregator alarm api gateway graph hbs judge nodata open-falcon plugins public transfer ","date":"2017-08-18","objectID":"/open-falcon/:6:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 Transfer transfer是数据转发服务。它接收agent上报的数据，然后按照哈希规则进行数据分片、并将分片后的数据分别push给graph\u0026judge等组件。 cd /opt/local/open-falcon/transfer/config vi cfg.json { \"debug\": true, \"minStep\": 30, \"http\": { \"enabled\": true, \"listen\": \"0.0.0.0:6060\" }, \"rpc\": { \"enabled\": true, \"listen\": \"0.0.0.0:8433\" }, \"socket\": { \"enabled\": false, \"listen\": \"0.0.0.0:4444\", \"timeout\": 3600 }, \"judge\": { \"enabled\": true, \"batch\": 200, \"connTimeout\": 1000, \"callTimeout\": 5000, \"maxConns\": 32, \"maxIdle\": 32, \"replicas\": 500, \"cluster\": { \"judge-00\" : \"127.0.0.1:6080\" } }, \"graph\": { \"enabled\": true, \"batch\": 200, \"connTimeout\": 1000, \"callTimeout\": 5000, \"maxConns\": 32, \"maxIdle\": 32, \"replicas\": 500, \"cluster\": { \"graph-00\" : \"127.0.0.1:6070\" } }, \"tsdb\": { \"enabled\": false, \"batch\": 200, \"connTimeout\": 1000, \"callTimeout\": 5000, \"maxConns\": 32, \"maxIdle\": 32, \"retry\": 3, \"address\": \"127.0.0.1:8088\" } } ","date":"2017-08-18","objectID":"/open-falcon/:7:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 Graph graph是存储绘图数据的组件。graph组件 接收transfer组件推送上来的监控数据，同时处理api组件的查询请求、返回绘图数据。 # 创建数据 目录 mkdir -p /opt/data/6070 cd /opt/local/open-falcon/graph/config vi cfg.json { \"debug\": false, \"http\": { \"enabled\": true, \"listen\": \"0.0.0.0:6071\" }, \"rpc\": { \"enabled\": true, \"listen\": \"0.0.0.0:6070\" }, \"rrd\": { \"storage\": \"/opt/data/6070\" }, \"db\": { \"dsn\": \"root:123456@tcp(127.0.0.1:3306)/graph?loc=Local\u0026parseTime=true\", \"maxIdle\": 4 }, \"callTimeout\": 5000, \"migrate\": { \"enabled\": false, \"concurrency\": 2, \"replicas\": 500, \"cluster\": { \"graph-00\" : \"127.0.0.1:6070\" } } } ","date":"2017-08-18","objectID":"/open-falcon/:8:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 Api 组件 api组件，提供统一的restAPI操作接口。比如：api组件接收查询请求，根据一致性哈希算法去相应的graph实例查询不同metric的数据，然后汇总拿到的数据，最后统一返回给用户。 cd /opt/local/open-falcon/api/config vi cfg.json # 主要修改 \"salt\": \"\" 为加密字串 { \"log_level\": \"debug\", \"db\": { \"falcon_portal\": \"root:123456@tcp(127.0.0.1:3306)/falcon_portal?charset=utf8\u0026parseTime=True\u0026loc=Local\", \"graph\": \"root:123456@tcp(127.0.0.1:3306)/graph?charset=utf8\u0026parseTime=True\u0026loc=Local\", \"uic\": \"root:123456@tcp(127.0.0.1:3306)/uic?charset=utf8\u0026parseTime=True\u0026loc=Local\", \"dashboard\": \"root:123456@tcp(127.0.0.1:3306)/dashboard?charset=utf8\u0026parseTime=True\u0026loc=Local\", \"alarms\": \"root:123456@tcp(127.0.0.1:3306)/alarms?charset=utf8\u0026parseTime=True\u0026loc=Local\", \"db_bug\": true }, \"graphs\": { \"cluster\": { \"graph-00\": \"127.0.0.1:6070\" }, \"max_conns\": 100, \"max_idle\": 100, \"conn_timeout\": 1000, \"call_timeout\": 5000, \"numberOfReplicas\": 500 }, \"metric_list_file\": \"./api/data/metric\", \"web_port\": \"0.0.0.0:8080\", \"access_control\": true, \"signup_disable\": false, \"salt\": \"pleaseinputwhichyouareusingnow\", \"skip_auth\": false, \"default_token\": \"default-token-used-in-server-side\", \"gen_doc\": false, \"gen_doc_path\": \"doc/module.html\" } ","date":"2017-08-18","objectID":"/open-falcon/:9:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"部署 Heartbeat 服务 心跳服务器，所有agent都会连到HBS，每分钟发一次心跳请求。 cd /opt/local/open-falcon/hbs/config vi cfg.json { \"debug\": true, \"database\": \"root:123456@tcp(127.0.0.1:3306)/falcon_portal?loc=Local\u0026parseTime=true\", \"hosts\": \"\", \"maxConns\": 20, \"maxIdle\": 15, \"listen\": \":6030\", \"trustable\": [\"\"], \"http\": { \"enabled\": true, \"listen\": \"0.0.0.0:6031\" } } ","date":"2017-08-18","objectID":"/open-falcon/:10:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"部署 Judge 服务 Judge用于告警判断，agent将数据push给Transfer，Transfer不但会转发给Graph组件来绘图，还会转发给Judge用于判断是否触发告警。 cd /opt/local/open-falcon/judge/config vi cfg.json { \"debug\": true, \"debugHost\": \"nil\", \"remain\": 11, \"http\": { \"enabled\": true, \"listen\": \"0.0.0.0:6081\" }, \"rpc\": { \"enabled\": true, \"listen\": \"0.0.0.0:6080\" }, \"hbs\": { \"servers\": [\"127.0.0.1:6030\"], \"timeout\": 300, \"interval\": 60 }, \"alarm\": { \"enabled\": true, \"minInterval\": 300, \"queuePattern\": \"event:p%v\", \"redis\": { \"dsn\": \"127.0.0.1:6379\", \"maxIdle\": 5, \"connTimeout\": 5000, \"readTimeout\": 5000, \"writeTimeout\": 5000 } } } ","date":"2017-08-18","objectID":"/open-falcon/:11:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"部署一个 mail 服务 发送警告邮件需要部署一个 mail 服务 用于发送邮件, 这里边部署一个简单的 mail-provider 地址 https://github.com/zzlyzq/mail-provider # 下载 依赖模块 cd /opt/local/golang/src/github.com/open-falcon git clone https://github.com/zzlyzq/mail-provider cd /opt/local/golang/src/github.com/open-falcon/mail-provider # 下载依赖 go get ./control build (编译) ./control pack (打包) mkdir /opt/local/open-falcon/mail-provider mv falcon-mail-provider-0.0.1.tar.gz /opt/local/open-falcon/mail-provider cd /opt/local/open-falcon/mail-provider tar zxvf falcon-mail-provider-0.0.1.tar.gz # 修改配置文件 里的 smtp 为自己的地址 # QQ邮箱，请开启 smtp 的功能，在QQ邮箱后台开启 vi cfg.json { \"debug\": true, \"http\": { \"listen\": \"0.0.0.0:4000\", \"token\": \"\" }, \"smtp\": { \"addr\": \"smtp.qq.com:587\", \"username\": \"jicki@qq.com\", \"password\": \"123456\", \"from\": \"jicki@qq.com\" } } # 运行程序 ./control start # 查看日志 ./control tail # 测试, 在测试时 token 暂时设置为空 curl http://127.0.0.1:4000/sender/mail -d \"tos=jicki@qq.com\u0026subject=xx\u0026content=yy\" ","date":"2017-08-18","objectID":"/open-falcon/:12:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"部署一个 微信网关 微信网关 git https://github.com/Yanjunhui/chat cd /opt/local/open-falcon git clone https://github.com/Yanjunhui/chat cd chat/ chmod +x control.sh # 需要修改 配置文件 cat config.conf #http 服务端口 [http] port = 4567 #微信接口信息 [weixin] CorpID = ww6424d33203e90e20 AgentId = 1000002 Secret = FoST_8RQSTjZwH_CN3aQW6UKksjCSI9mizFqD7HKhrw EncodingAESKey = K2M3WMhRHIOH4I1Ww5jxpllGrgY01nvBjUgTvcJEEHX # 启动 ./control.sh start ./control.sh status ## 注意: 要收到 im 报警信息，必须要在 个人用户里面 填写 微信相关资料 微信相关帐号是 登陆微信公众号 --\u003e 通讯里， 里面用户的 帐号 不是个人微信帐号，填写个人帐号，是收不到报警的。 ","date":"2017-08-18","objectID":"/open-falcon/:13:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"部署 Alarm 服务 alarm模块是处理报警event的，judge产生的报警event写入redis，alarm从redis读取处理，并进行不同渠道的发送。 cd /opt/local/open-falcon/alarm/config vi cfg.json { \"log_level\": \"debug\", \"http\": { \"enabled\": true, \"listen\": \"0.0.0.0:9912\" }, \"redis\": { \"addr\": \"127.0.0.1:6379\", \"maxIdle\": 5, \"highQueues\": [ \"event:p0\", \"event:p1\", \"event:p2\" ], \"lowQueues\": [ \"event:p3\", \"event:p4\", \"event:p5\", \"event:p6\" ], \"userIMQueue\": \"/queue/user/im\", \"userSmsQueue\": \"/queue/user/sms\", \"userMailQueue\": \"/queue/user/mail\" }, \"api\": { \"im\": \"http://127.0.0.1:4567/send\", \"sms\": \"http://127.0.0.1:10086/sms\", \"mail\": \"http://127.0.0.1:4000/sender/mail\", \"dashboard\": \"http://127.0.0.1:8081\", \"plus_api\":\"http://127.0.0.1:8080\", \"plus_api_token\": \"default-token-used-in-server-side\" }, \"falcon_portal\": { \"addr\": \"root:123456@tcp(127.0.0.1:3306)/alarms?charset=utf8\u0026loc=Asia%2FChongqing\", \"idle\": 10, \"max\": 100 }, \"worker\": { \"im\": 10, \"sms\": 10, \"mail\": 50 }, \"housekeeper\": { \"event_retention_days\": 7, \"event_delete_batch\": 100 } } ","date":"2017-08-18","objectID":"/open-falcon/:14:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 Nodata 服务 nodata用于检测监控数据的上报异常。nodata和实时报警judge模块协同工作，过程为: 配置了nodata的采集项超时未上报数据，nodata生成一条默认的模拟数据；用户配置相应的报警策略，收到mock数据就产生报警。采集项上报异常检测，作为judge模块的一个必要补充，能够使judge的实时报警功能更加可靠、完善。 cd /opt/local/open-falcon/nodata/config vi cfg.json { \"debug\": true, \"http\": { \"enabled\": true, \"listen\": \"0.0.0.0:6090\" }, \"plus_api\":{ \"connectTimeout\": 500, \"requestTimeout\": 2000, \"addr\": \"http://127.0.0.1:8080\", \"token\": \"default-token-used-in-server-side\" }, \"config\": { \"enabled\": true, \"dsn\": \"root:123456@tcp(127.0.0.1:3306)/falcon_portal?loc=Local\u0026parseTime=true\u0026wait_timeout=604800\", \"maxIdle\": 4 }, \"collector\":{ \"enabled\": true, \"batch\": 200, \"concurrent\": 10 }, \"sender\":{ \"enabled\": true, \"connectTimeout\": 500, \"requestTimeout\": 2000, \"transferAddr\": \"127.0.0.1:6060\", \"batch\": 500 } } ","date":"2017-08-18","objectID":"/open-falcon/:15:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 Aggregator 服务 集群聚合模块。聚合某集群下的所有机器的某个指标的值，提供一种集群视角的监控体验。 cd /opt/local/open-falcon/aggregator/config vi cfg.json { \"debug\": true, \"http\": { \"enabled\": false, \"listen\": \"0.0.0.0:6055\" }, \"database\": { \"addr\": \"root:123456@tcp(127.0.0.1:3306)/falcon_portal?loc=Local\u0026parseTime=true\", \"idle\": 10, \"ids\": [1, -1], \"interval\": 55 }, \"api\": { \"connect_timeout\": 500, \"request_timeout\": 2000, \"plus_api\": \"http://127.0.0.1:8080\", \"plus_api_token\": \"default-token-used-in-server-side\", \"push_api\": \"http://127.0.0.1:1988/v1/push\" } } ","date":"2017-08-18","objectID":"/open-falcon/:16:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"启动所有服务 cd /opt/local/open-falcon ./open-falcon start ./open-falcon check falcon-graph UP 71646 falcon-hbs UP 71658 falcon-judge UP 71670 falcon-transfer UP 71678 falcon-nodata UP 71686 falcon-aggregator UP 71695 falcon-agent UP 71706 falcon-gateway UP 71715 falcon-api UP 71724 falcon-alarm UP 71738 配置 Agent agent用于采集机器负载监控指标，比如cpu.idle、load.1min、disk.io.util等等，每隔60秒push给Transfer。agent与Transfer建立了长连接，数据发送速度比较快，agent提供了一个http接口/v1/push用于接收用户手工push的一些数据，然后通过长连接迅速转发给Transfer。 cd /opt/local/open-falcon/agent/config # Agent 配置文件 { \"debug\": true, \"hostname\": \"\", \"ip\": \"\", \"plugin\": { \"enabled\": false, \"dir\": \"./plugin\", \"git\": \"https://github.com/open-falcon/plugin.git\", \"logs\": \"./logs\" }, \"heartbeat\": { \"enabled\": true, \"addr\": \"127.0.0.1:6030\", \"interval\": 60, \"timeout\": 1000 }, \"transfer\": { \"enabled\": true, \"addrs\": [ \"127.0.0.1:8433\" ], \"interval\": 60, \"timeout\": 1000 }, \"http\": { \"enabled\": false, \"listen\": \":1988\", \"backdoor\": false }, \"collector\": { \"ifacePrefix\": [\"eth\", \"em\"], \"mountPoint\": [] }, \"default_tags\": { }, \"ignore\": { \"cpu.busy\": true, \"df.bytes.free\": true, \"df.bytes.total\": true, \"df.bytes.used\": true, \"df.bytes.used.percent\": true, \"df.inodes.total\": true, \"df.inodes.free\": true, \"df.inodes.used\": true, \"df.inodes.used.percent\": true, \"mem.memtotal\": true, \"mem.memused\": true, \"mem.memused.percent\": true, \"mem.memfree\": true, \"mem.swaptotal\": true, \"mem.swapused\": true, \"mem.swapfree\": true } } # agent 脚本 添加 hostname = IP #!/bin/bash mkdir /opt/local cd /opt/local wget http://172.16.1.100/agent.tar.gz tar zxvf agent.tar.gz rm -rf agent.tar.gz IPADDR=`ifconfig em1|grep inet|grep -v 127.0.0.1|grep -v inet6|awk '{print $2}'|tr -d \"addr:\"` sed -i 's/\\\"hostname\\\"\\:.*$/\\\"hostname\\\"\\: \\\"'$IPADDR'\\\"\\,/g' open-falcon/agent/config/cfg.json cat open-falcon/agent/config/cfg.json cd open-falcon ./open-falcon start agent ./open-falcon check agent 配置前端 dashboard ","date":"2017-08-18","objectID":"/open-falcon/:17:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"初始化依赖 yum install -y python-virtualenv yum install -y python-devel yum install -y openldap-devel yum install -y mysql-devel yum groupinstall \"Development tools\" ","date":"2017-08-18","objectID":"/open-falcon/:18:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"安装配置 dashboard cd /opt/local/open-falcon git clone https://github.com/open-falcon/dashboard.git cd dashboard # 创建 python 独立运行环境 virtualenv ./env # 安装 python 依赖模块 # 执行安装 ./env/bin/pip install -r pip_requirements.txt -i http://mirrors.aliyun.com/pypi/simple/ # 修改配置 # 修改里面的 mysql 配置 rrd/config.py ALARM_DB_PASS ","date":"2017-08-18","objectID":"/open-falcon/:19:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"启动 dashboard # debug 模式 ./env/bin/python wsgi.py # 正常模式 bash control start # 查看日志 bash control tail ","date":"2017-08-18","objectID":"/open-falcon/:20:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"登陆 WEB UI http://172.16.1.100:8081/ 注册 root 帐号 为 admin 帐号 监控报警 ","date":"2017-08-18","objectID":"/open-falcon/:21:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 报警名单 # 首先配置 用户组 dashboard --\u003e Welcome root --\u003e Teams Add+ -- \u003e Create Team # 创建一个 ICT 组 名称： ICT 简介： 运维组 成员： root ","date":"2017-08-18","objectID":"/open-falcon/:22:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 nodata 监控 agent # 创建 HostGrop dashboard --\u003e HostGroups 添加一个名称 dev-server 的 HostGroups 点击 hosts -- \u003e Add Host 添加 dev 相关服务器 # 创建 nodata # 监控 client 的 agent ，如果 agent 掉了， 那么无法上传数据，所以直接配置 模板是不行的， 必须配置 nodata 在 agent 抓不到数据的时候 值为 -1 . dashboard --\u003e nodata Add nodata name: nodata.agent endpoint选择: 机器分组 --- dev-server metric: agent.alive type: GAUGE 周期: 60 数据上报中断时，补发如下值: -1 # 创建 策略配置 dashboard --\u003e Templates 创建一个 名称为 agent-alive 的模板 # 模板基础信息: name: agent-alive 模板策略列表: metric: agent.alive note: 无法连接agent if [all(#3)] \u003c 0 : alarm(); callback(); save 保存 # 模板报警配置: 之前设置的 ICT # Save # 绑定 模板 dashboard --\u003e HostGroups 查找 dev-server 点击 templates 查找 agent-alive 选择 + Bind # 测试 关闭一个 agent 的进程 等待60秒 查看 Alarm-Dashboard 等待收取邮件~ ","date":"2017-08-18","objectID":"/open-falcon/:23:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"配置 系统监控指标 dashboard --\u003e Templates Add 添加一个 系统指标模块 监控指标如下: FAQ ","date":"2017-08-18","objectID":"/open-falcon/:24:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["Linux"],"content":"修改 报警模板 cd open-falcon/falcon-plus/modules/alarm/cron # 报警内容: cat builder.go package cron import ( \"fmt\" \"github.com/open-falcon/falcon-plus/common/model\" \"github.com/open-falcon/falcon-plus/common/utils\" \"github.com/open-falcon/falcon-plus/modules/alarm/g\" ) func BuildCommonSMSContent(event *model.Event) string { return fmt.Sprintf( \"[P%d][%s][%s][][%s %s %s %s %s%s%s][O%d %s]\", event.Priority(), event.Status, event.Endpoint, event.Note(), event.Func(), event.Metric(), utils.SortedTags(event.PushedTags), utils.ReadableFloat(event.LeftValue), event.Operator(), utils.ReadableFloat(event.RightValue()), event.CurrentStep, event.FormattedTime(), ) } func BuildCommonIMContent(event *model.Event) string { return fmt.Sprintf( \"[报警级别: %d][报警状态: %s][报警Host: %s][报警内容: %s][报警时间: %s]\", event.Priority(), event.Status, event.Endpoint, event.Note(), //event.Func(), //event.Metric(), //utils.SortedTags(event.PushedTags), //utils.ReadableFloat(event.LeftValue), //event.Operator(), //utils.ReadableFloat(event.RightValue()), //event.CurrentStep, event.FormattedTime(), ) } func BuildCommonMailContent(event *model.Event) string { link := g.Link(event) return fmt.Sprintf( \"报警状态: %s\\r\\n报警级别: %d\\r\\n报警Host: %s\\r\\n报警事件: %s\\r\\n事件标签: %s\\r\\n报警表达式: %s: %s%s%s\\r\\n报警内容: %s\\r\\n最大报警次数: %d 当前报警次数: %d\\r\\n报警时间: %s\\r\\n报警模板: %s\\r\\n\", event.Status, event.Priority(), event.Endpoint, event.Metric(), utils.SortedTags(event.PushedTags), event.Func(), utils.ReadableFloat(event.LeftValue), event.Operator(), utils.ReadableFloat(event.RightValue()), event.Note(), event.MaxStep(), event.CurrentStep, event.FormattedTime(), link, ) } func GenerateSmsContent(event *model.Event) string { return BuildCommonSMSContent(event) } func GenerateMailContent(event *model.Event) string { return BuildCommonMailContent(event) } func GenerateIMContent(event *model.Event) string { return BuildCommonIMContent(event) } ","date":"2017-08-18","objectID":"/open-falcon/:25:0","tags":null,"title":"Open-Falcon 监控","uri":"/open-falcon/"},{"categories":["kubernetes"],"content":"kubernetes 1.7.3 + calico 多 Master","date":"2017-08-08","objectID":"/kubernetes-1.7.3/","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"kubernetes 1.7.3 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler 我这边配置 既是 master 也是 nodes ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:0:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"环境说明 k8s-master-1: 10.6.0.140 k8s-master-2: 10.6.0.187 k8s-master-3: 10.6.0.188 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:1:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname 10.6.0.140 - k8s-master-1 10.6.0.187 - k8s-master-2 10.6.0.188 - k8s-master-3 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 10.6.0.140 k8s-master-1 10.6.0.187 k8s-master-2 10.6.0.188 k8s-master-3 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:2:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:3:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl /opt/local/cfssl/cfssl print-defaults config \u003e config.json /opt/local/cfssl/cfssl print-defaults csr \u003e csr.json # config.json 文件 { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:4:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@k8s-master-1 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:5:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp * /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp * 10.6.0.187:/etc/kubernetes/ssl/ scp * 10.6.0.188:/etc/kubernetes/ssl/ 安装 docker # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 安装 yum install docker-ce -y ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:6:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 修改配置 vi /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS $DOCKER_OPTS $DOCKER_DNS_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target # 修改其他配置 mkdir -p /usr/lib/systemd/system/docker.service.d/ vi /usr/lib/systemd/system/docker.service.d/docker-options.conf # 添加如下 : (注意 environment 必须在同一行，如果出现换行会无法加载) [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 --graph=/opt/docker --registry-mirror=http://b438f72b.m.daocloud.io --disable-legacy-registry --iptables=false\" # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker etcd 集群 etcd 是k8s集群的基础组件 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:7:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"安装 etcd yum -y install etcd ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:8:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"10.6.0.140\", \"10.6.0.187\", \"10.6.0.188\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@k8s-master-1 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd* 10.6.0.187:/etc/kubernetes/ssl/ # etcd-3 scp etcd* 10.6.0.188:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:9:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 修改 etcd 启动文件 /usr/lib/systemd/system/etcd.service # etcd-1 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.6.0.140:2380 \\ --listen-peer-urls=https://10.6.0.140:2380 \\ --listen-client-urls=https://10.6.0.140:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.6.0.140:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.6.0.140:2380,etcd2=https://10.6.0.187:2380,etcd3=https://10.6.0.188:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.6.0.187:2380 \\ --listen-peer-urls=https://10.6.0.187:2380 \\ --listen-client-urls=https://10.6.0.187:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.6.0.187:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.6.0.140:2380,etcd2=https://10.6.0.187:2380,etcd3=https://10.6.0.188:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.6.0.188:2380 \\ --listen-peer-urls=https://10.6.0.188:2380 \\ --listen-client-urls=https://10.6.0.188:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.6.0.188:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.6.0.140:2380,etcd2=https://10.6.0.187:2380,etcd3=https://10.6.0.188:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:10:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:11:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://10.6.0.140:2379 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 29262d49176888f5 is healthy: got healthy result from https://10.6.0.188:2379 member d4ba1a2871bfa2b0 is healthy: got healthy result from https://10.6.0.140:2379 member eca58ebdf44f63b6 is healthy: got healthy result from https://10.6.0.187:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://10.6.0.140:2379 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 29262d49176888f5: name=etcd3 peerURLs=https://10.6.0.188:2380 clientURLs=https://10.6.0.188:2379 isLeader=false d4ba1a2871bfa2b0: name=etcd1 peerURLs=https://10.6.0.140:2380 clientURLs=https://10.6.0.140:2379 isLeader=true eca58ebdf44f63b6: name=etcd2 peerURLs=https://10.6.0.187:2380 clientURLs=https://10.6.0.187:2379 isLeader=false 安装 kubectl 工具 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:12:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"Master 端 # 首先安装 kubectl wget https://dl.k8s.io/v1.7.3/kubernetes-client-linux-amd64.tar.gz tar -xzvf kubernetes-client-linux-amd64.tar.gz cp kubernetes/client/bin/* /usr/local/bin/ chmod a+x /usr/local/bin/kube* # 验证安装 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.3\", GitCommit:\"2c2fe6e8278a5db2d15a013987b53968c743f2a1\", GitTreeState:\"clean\", BuildDate:\"2017-08-03T07:00:21Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} The connection to the server localhost:8080 was refused - did you specify the right host or port? ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:13:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@k8s-master-1 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ scp admin*.pem 10.6.0.187:/etc/kubernetes/ssl/ scp admin*.pem 10.6.0.188:/etc/kubernetes/ssl/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:14:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 server 配置为 本机IP 各自连接本机的 Api # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:15:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"kubectl config 文件 # kubeconfig 文件在 如下: /root/.kube 部署 Kubernetes Master 节点 Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 kube-scheduler 作用是调度pods分配到那个node里，简单来说就是资源调度。 kube-controller-manager 作用是 对 deployment controller , replication controller, endpoints controller, namespace controller, and serviceaccounts controller等等的循环控制，与kube-apiserver交互。 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:16:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"安装 组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.7.3/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:17:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 cd /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"10.6.0.140\", \"10.6.0.187\", \"10.6.0.188\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 10.6.0.140 和 10.6.0.187 为 Master 的IP，多个Master需要写多个 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:18:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@k8s-master-1 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1245 7月 4 11:25 kubernetes.csr -rw------- 1 root root 1679 7月 4 11:25 kubernetes-key.pem -rw-r--r-- 1 root root 1619 7月 4 11:25 kubernetes.pem -rw-r--r-- 1 root root 436 7月 4 11:23 kubernetes-csr.json # 拷贝到目录 cp -r kubernetes* /etc/kubernetes/ssl/ scp -r kubernetes* 10.6.0.187:/etc/kubernetes/ssl/ scp -r kubernetes* 10.6.0.188:/etc/kubernetes/ssl/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:19:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@k8s-master-1 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 10b459a82af1e16663f25061372fdab4 # 创建 token.csv 文件 cd /opt/ssl vi token.csv 10b459a82af1e16663f25061372fdab4,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ scp token.csv 10.6.0.187:/etc/kubernetes/ scp token.csv 10.6.0.188:/etc/kubernetes/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:20:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 # 配置为 各自的本地 IP vi /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --advertise-address=10.6.0.140 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --authorization-mode=RBAC \\ --bind-address=10.6.0.140 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://10.6.0.140:2379,https://10.6.0.187:2379,https://10.6.0.188:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=10.6.0.140 \\ --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --experimental-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:21:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:22:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager master 配置为 各自 本地 IP # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://10.6.0.140:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-cidr=10.233.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:23:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:24:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler master 配置为 各自 本地 IP # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=Kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://10.6.0.140:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:25:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:26:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@k8s-master-1 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@k8s-master-2 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy {\"health\": \"true\"} etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} [root@k8s-master-3 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:27:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"部署 Master Node 部分 Node 部分 需要部署的组件有 docker calico kubectl kubelet kube-proxy 这几个组件。 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:28:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:29:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 server 配置为 master 本机 IP # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=10b459a82af1e16663f25061372fdab4 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:30:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 \u003e 配置为 node 本机 IP mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --address=10.6.0.140 \\ --hostname-override=10.6.0.140 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --require-kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --max-pods=512 \\ --v=2 ExecStopPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target # 如上配置: 10.6.0.140 为本机的IP 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:31:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:32:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-1 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-LkH2ZX9b2kACKmkNbp9PnK6BYAa5fMeh7nWtrGCipYc 58s kubelet-bootstrap Pending node-csr-cxXvnzvukInZrSXT1EJTFaDzERFsuwsR2hCcgWyYZ2o 1m kubelet-bootstrap Pending node-csr-jcQdD_haTRkPMTXwcHeyjQZUt2lb1S4rDeTgKUeQwgM 1m kubelet-bootstrap Pending # 增加 认证 [root@k8s-master-1 ~]# kubectl certificate approve node-csr-LkH2ZX9b2kACKmkNbp9PnK6BYAa5fMeh7nWtrGCipYc certificatesigningrequest \"node-csr-LkH2ZX9b2kACKmkNbp9PnK6BYAa5fMeh7nWtrGCipYc\" approved [root@k8s-master-1 ~]# [root@k8s-master-1 ~]# kubectl certificate approve node-csr-cxXvnzvukInZrSXT1EJTFaDzERFsuwsR2hCcgWyYZ2o certificatesigningrequest \"node-csr-cxXvnzvukInZrSXT1EJTFaDzERFsuwsR2hCcgWyYZ2o\" approved [root@k8s-master-1 ~]# [root@k8s-master-1 ~]# kubectl certificate approve node-csr-jcQdD_haTRkPMTXwcHeyjQZUt2lb1S4rDeTgKUeQwgM certificatesigningrequest \"node-csr-jcQdD_haTRkPMTXwcHeyjQZUt2lb1S4rDeTgKUeQwgM\" approved ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:33:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"验证 nodes [root@k8s-master-1 ~]# kubectl get nodes NAME STATUS AGE VERSION 10.6.0.140 Ready 27s v1.7.3 10.6.0.187 Ready 20s v1.7.3 10.6.0.188 Ready 37s v1.7.3 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:34:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:35:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@k8s-master-1 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:36:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy*.pem /etc/kubernetes/ssl/ scp kube-proxy*.pem 10.6.0.187:/etc/kubernetes/ssl/ scp kube-proxy*.pem 10.6.0.188:/etc/kubernetes/ssl/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:37:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 server 配置为各自 本机IP # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:38:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 配置为 各自的 IP # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=Kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=10.6.0.140 \\ --hostname-override=10.6.0.140 \\ --cluster-cidr=10.254.0.0/16 \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:39:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy 部署 Node 节点 Node 节点 基于 Nginx 负载 API 做 Master HA # master 之间除 api server 以外其他组件通过 etcd 选举，api server 默认不作处理；在每个 node 上启动一个 nginx，每个 nginx 反向代理所有 api server，node 上 kubelet、kube-proxy 连接本地的 nginx 代理端口，当 nginx 发现无法连接后端时会自动踢掉出问题的 api server，从而实现 api server 的 HA cd /tmp wget https://dl.k8s.io/v1.7.3/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-proxy,kubelet} /usr/local/bin/ # ALL node mkdir -p /etc/kubernetes/ssl/ scp ca.pem kube-proxy.pem kube-proxy-key.pem node-*:/etc/kubernetes/ssl/ # kubelet # 首先 创建 kubelet kubeconfig 文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=10b459a82af1e16663f25061372fdab4 \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ # 创建 kube-proxy kubeconfig 文件 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:40:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 10.6.0.140:6443; server 10.6.0.187:6443; server 10.6.0.188:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.3-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy # 重启 Node 的 kubelet 与 kube-proxy systemctl restart kubelet systemctl status kubelet systemctl restart kube-proxy systemctl status kube-proxy ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:41:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"Master 配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-1 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-LkH2ZX9b2kACKmkNbp9PnK6BYAa5fMeh7nWtrGCipYc 58s kubelet-bootstrap Pending node-csr-cxXvnzvukInZrSXT1EJTFaDzERFsuwsR2hCcgWyYZ2o 1m kubelet-bootstrap Pending node-csr-jcQdD_haTRkPMTXwcHeyjQZUt2lb1S4rDeTgKUeQwgM 1m kubelet-bootstrap Pending # 增加 认证 [root@k8s-master-1 ~]# kubectl certificate approve node-csr-LkH2ZX9b2kACKmkNbp9PnK6BYAa5fMeh7nWtrGCipYc certificatesigningrequest \"node-csr-LkH2ZX9b2kACKmkNbp9PnK6BYAa5fMeh7nWtrGCipYc\" approved [root@k8s-master-1 ~]# [root@k8s-master-1 ~]# kubectl certificate approve node-csr-cxXvnzvukInZrSXT1EJTFaDzERFsuwsR2hCcgWyYZ2o certificatesigningrequest \"node-csr-cxXvnzvukInZrSXT1EJTFaDzERFsuwsR2hCcgWyYZ2o\" approved [root@k8s-master-1 ~]# [root@k8s-master-1 ~]# kubectl certificate approve node-csr-jcQdD_haTRkPMTXwcHeyjQZUt2lb1S4rDeTgKUeQwgM certificatesigningrequest \"node-csr-jcQdD_haTRkPMTXwcHeyjQZUt2lb1S4rDeTgKUeQwgM\" approved Calico 网络 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:42:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"修改 kubelet.service vi /etc/systemd/system/kubelet.service # 增加 如下配置 --network-plugin=cni \\ # 重新加载配置 systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:43:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"修改 kube-proxy.service # 重新加载配置 systemctl daemon-reload systemctl restart kube-proxy.service systemctl status kube-proxy.service ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:44:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"安装 Calico 官网地址 http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/hosted # 下载 yaml 文件 wget http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/calico.yaml wget http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/rbac.yaml # 下载 镜像 # 国外镜像 有墙 quay.io/calico/node:v1.3.0 quay.io/calico/cni:v1.9.1 quay.io/calico/kube-policy-controller:v0.6.0 # 国内镜像 jicki/node:v1.3.0 jicki/cni:v1.9.1 jicki/kube-policy-controller:v0.6.0 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:45:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 calico vi calico.yaml # 注意修改如下选项: etcd_endpoints: \"https://10.6.0.140:2379,https://10.6.0.187:2379,https://10.6.0.188:2379\" etcd_ca: \"/calico-secrets/etcd-ca\" etcd_cert: \"/calico-secrets/etcd-cert\" etcd_key: \"/calico-secrets/etcd-key\" # 这里面要写入 base64 的信息 data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d '\\n') etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d '\\n') etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d '\\n') - name: CALICO_IPV4POOL_CIDR value: \"10.233.0.0/16\" ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:46:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 [root@k8s-master-1 ~]# kubectl apply -f calico.yaml configmap \"calico-config\" created secret \"calico-etcd-secrets\" created daemonset \"calico-node\" created deployment \"calico-policy-controller\" created serviceaccount \"calico-policy-controller\" created serviceaccount \"calico-node\" created [root@k8s-master-1 ~]# kubectl apply -f rbac.yaml clusterrole \"calico-policy-controller\" created clusterrolebinding \"calico-policy-controller\" created clusterrole \"calico-node\" created clusterrolebinding \"calico-node\" created ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:47:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"验证 Calico [root@k8s-master-1 ~]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-node-2dsq4 2/2 Running 0 6m calico-node-9ktvk 2/2 Running 0 6m calico-node-gwmx5 2/2 Running 0 6m calico-policy-controller-458850194-pn65p 1/1 Running 0 6m ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:48:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"安装 Calicoctl cd /usr/local/bin/ wget -c https://github.com/projectcalico/calicoctl/releases/download/v1.3.0/calicoctl chmod +x calicoctl ## 创建 calicoctl 配置文件 # 配置文件， 在 安装了 calico 网络的 机器下 mkdir /etc/calico vi /etc/calico/calicoctl.cfg apiVersion: v1 kind: calicoApiConfig metadata: spec: datastoreType: \"etcdv2\" etcdEndpoints: \"https://10.6.0.140:2379,https://10.6.0.187:2379,https://10.6.0.188:2379\" etcdKeyFile: \"/etc/kubernetes/ssl/etcd-key.pem\" etcdCertFile: \"/etc/kubernetes/ssl/etcd.pem\" etcdCACertFile: \"/etc/kubernetes/ssl/ca.pem\" # 查看 calico 状态 [root@k8s-master-1 ~]# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10.6.0.188 | node-to-node mesh | up | 10:11:59 | Established | | 10.6.0.187 | node-to-node mesh | up | 10:16:32 | Established | +--------------+-------------------+-------+----------+-------------+ IPv6 BGP status No IPv6 peers found. 测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-master-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-2214564181-lxff5 1/1 Running 0 14m nginx-dm-2214564181-qm1bp 1/1 Running 0 14m [root@k8s-master-1 ~]# kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-dm 2 2 2 2 14m [root@k8s-master-1 ~]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.254.0.1 \u003cnone\u003e 443/TCP 4h nginx-svc 10.254.129.54 \u003cnone\u003e 80/TCP 15m # 在 node 里 curl [root@k8s-node-1 ~]# curl 10.254.129.54 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 配置 KubeDNS 官方 github yaml 相关 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:49:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"下载镜像 # 官方镜像 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 # 我的镜像 jicki/k8s-dns-sidecar-amd64:1.14.4 jicki/k8s-dns-kube-dns-amd64:1.14.4 jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.4 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:50:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-cm.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-sa.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-controller.yaml.base curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-svc.yaml.base # 修改后缀 mv kubedns-controller.yaml.base kubedns-controller.yaml mv kubedns-svc.yaml.base kubedns-svc.yaml ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:51:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限； [root@k8s-master-1 kubedns]# kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" creationTimestamp: 2017-07-04T04:15:13Z labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-dns resourceVersion: \"106\" selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindings/system%3Akube-dns uid: 60c1e0e1-606f-11e7-b212-d4ae52d1f0c9 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-dns subjects: - kind: ServiceAccount name: kube-dns namespace: kube-system ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:52:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"修改 kubedns-svc.yaml # kubedns-svc.yaml 中 clusterIP: __PILLAR__DNS__SERVER__ 修改为我们之前定义的 dns IP 10.254.0.2 cat kubedns-svc.yaml apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"KubeDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:53:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"修改 kubedns-controller.yaml 1. # 修改 --domain=__PILLAR__DNS__DOMAIN__. 为 我们之前 预定的 domain 名称 --domain=cluster.local. 2. # 修改 --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053 中 domain 为我们之前预定的 --server=/cluster.local./127.0.0.1#10053 3. # 修改 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local., 4. # 修改 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local., ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:54:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # 导入 [root@k8s-master-1 kubedns]# kubectl create -f . configmap \"kube-dns\" created deployment \"kube-dns\" created serviceaccount \"kube-dns\" created service \"kube-dns\" created ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:55:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@k8s-master-1 kubedns]# kubectl get all --namespace=kube-system NAME READY STATUS RESTARTS AGE po/calico-node-2dsq4 2/2 Running 0 15h po/calico-node-9ktvk 2/2 Running 0 15h po/calico-node-gwmx5 2/2 Running 0 15h po/calico-policy-controller-458850194-pn65p 1/1 Running 0 15h po/kube-dns-1511229508-jxkvs 3/3 Running 0 4m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 4m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/calico-policy-controller 1 1 1 1 15h deploy/kube-dns 1 1 1 1 4m NAME DESIRED CURRENT READY AGE rs/calico-policy-controller-458850194 1 1 1 15h rs/kube-dns-1511229508 1 1 1 4m ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:56:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 导入之前的 nginx-dm yaml文件 [root@k8s-master-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-2214564181-0ctcx 1/1 Running 0 27s 10.233.168.1 10.6.0.188 nginx-dm-2214564181-brz79 1/1 Running 0 3m 10.233.196.2 10.6.0.140 nginx-dm-2214564181-z8whk 1/1 Running 0 3m 10.233.182.65 10.6.0.187 [root@k8s-master-1 ~]# kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.254.0.1 \u003cnone\u003e 443/TCP 16h \u003cnone\u003e nginx-svc 10.254.140.2 \u003cnone\u003e 80/TCP 3m name=nginx # 创建一个 pods 来测试一下 nameserver apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 pods [root@k8s-master-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 5s nginx-dm-2214564181-0ctcx 1/1 Running 0 7m nginx-dm-2214564181-brz79 1/1 Running 0 10m nginx-dm-2214564181-z8whk 1/1 Running 0 10m # 测试 [root@k8s-master-1 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.140.2 nginx-svc.default.svc.cluster.local [root@k8s-master-1 ~]# kubectl exec -it alpine ping nginx-svc PING nginx-svc (10.254.140.2): 56 data bytes 部署 Ingress 与 Dashboard ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:57:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"部署 dashboard \u0026\u0026 heapster 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:58:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.3 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.6.3 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:59:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-controller.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-service.yaml # 因为开启了 RBAC 所以这里需要创建一个 RBAC 认证 vi dashboard-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: dashboard namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: name: dashboard subjects: - kind: ServiceAccount name: dashboard namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:60:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"下载 heapster 镜像 # 官方文件 gcr.io/google_containers/heapster-amd64:v1.3.0 gcr.io/google_containers/heapster-influxdb-amd64:v1.1.1 # 本地文件 jicki/heapster-amd64:v1.3.0 jicki/heapster-influxdb-amd64:v1.1.1 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:61:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"下载 heapster 文件 官方网站 https://github.com/kubernetes/heapster/blob/master/docs/influxdb.md curl -O https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml curl -O https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml curl -O https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:62:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"修改 influxdb 配置 # influxdb.yaml 记录存储data数据 # 如下: volumeMounts: - mountPath: /data name: influxdb-storage volumes: - name: influxdb-storage emptyDir: {} # volumes 请自行修改为 共享存储 或者 本地目录 ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:63:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"导入 yaml # 替换所有的 images sed -i 's/gcr\\.io\\/google_containers/jicki/g' * # dashboard-controller.yaml 增加 rbac 授权 # 在第二个 spec 下面 增加 spec: serviceAccountName: dashboard # 导入文件 [root@k8s-master-1 dashboard]# kubectl apply -f . deployment \"kubernetes-dashboard\" created serviceaccount \"dashboard\" created clusterrolebinding \"dashboard\" created service \"kubernetes-dashboard\" created [root@k8s-master-1 heapster]# kubectl apply -f . clusterrolebinding \"heapster\" created serviceaccount \"heapster\" created deployment \"heapster\" created service \"heapster\" created deployment \"monitoring-influxdb\" created service \"monitoring-influxdb\" created # 查看 svc 与 pod [root@k8s-master-1 ~]# kubectl get svc -n kube-system NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE heapster 10.254.231.18 \u003cnone\u003e 80/TCP 13s kube-dns 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 1h monitoring-influxdb 10.254.240.245 \u003cnone\u003e 8086/TCP 13s ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:64:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress Kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 Kubernetes 服务。 官方 Nginx Ingress github https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:65:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"配置 调度 node # ingress 有多种方式 1. deployment 自由调度 replicas 2. daemonset 全局调度 分配到所有node里 # deployment 自由调度过程中，由于我们需要 约束 controller 调度到指定的 node 中，所以需要对 node 进行 label 标签 # 默认如下: [root@k8s-master-1 ingress]# kubectl get nodes NAME STATUS AGE VERSION 10.6.0.140 Ready 18h v1.7.3 10.6.0.187 Ready 18h v1.7.3 10.6.0.188 Ready 18h v1.7.3 # 对 140 与 187 打上 label [root@k8s-master-1 ingress]# kubectl label nodes 10.6.0.140 ingress=proxy node \"10.6.0.140\" labeled [root@k8s-master-1 ingress]# kubectl label nodes 10.6.0.187 ingress=proxy node \"10.6.0.187\" labeled # 打完标签以后 [root@k8s-master-1 ingress]# kubectl get nodes --show-labels NAME STATUS AGE VERSION LABELS 10.6.0.140 Ready 18h v1.7.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=10.6.0.140 10.6.0.187 Ready 18h v1.7.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,ingress=proxy,kubernetes.io/hostname=10.6.0.187 10.6.0.188 Ready 18h v1.7.3 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/hostname=10.6.0.188 # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.0 gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.11 # 国内镜像 jicki/defaultbackend:1.0 jicki/nginx-ingress-controller:0.9.0-beta.11 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/default-backend.yaml # 直接导入既可, 这里不需要修改 [root@k8s-master-1 ingress]# kubectl apply -f default-backend.yaml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看服务 [root@k8s-master-1 ingress]# kubectl get deployment -n kube-system default-http-backend NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 36s # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/rbac/nginx/nginx-ingress-controller-rbac.yml # 修改 namespace sed -i 's/namespace: nginx-ingress/namespace: kube-system/g' nginx-ingress-controller-rbac.yml # 导入 yaml 文件 [root@k8s-master-1 ingress]# kubectl apply -f nginx-ingress-controller-rbac.yml namespace \"nginx-ingress\" created serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created # 部署 Ingress Controller 组件 # 下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml # 上面 对 两个 node 打了 label 所以配置 replicas: 2 # 修改 yaml 文件 增加 rbac 认证 , hostNetwork 还有 nodeSelector, 第二个 spec 下 增加。 spec: replicas: 2 .... spec: hostNetwork: true serviceAccountName: nginx-ingress-serviceaccount nodeSelector: ingress: proxy .... # 导入 yaml 文件 [root@k8s-master-1 ingress]# kubectl apply -f nginx-ingress-controller.yaml deployment \"nginx-ingress-controller\" created # 查看服务，可以看到这两个 pods 被分别调度到 140 与 187 中 [root@k8s-master-1 ingress]# kubectl get pods -n kube-system -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-ingress-controller-190167013-9j5kd 1/1 Running 0 45s 10.6.0.140 10.6.0.140 nginx-ingress-controller-190167013-n66qd 1/1 Running 0 45s 10.6.0.187 10.6.0.187 # 查看我们原有的 svc [root@k8s-master-1 dashboard]# kubectl get svc -n kube-system kubernetes-dashboard NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard 10.254.243.198 \u003cnone\u003e 80/TCP 21s # 创建 yaml 文件 vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: dashboard-ingress namespace: kube-system spec: rules: - host: dashboard.jicki.cn http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 80 # 导入 yaml [root@k8s-master-1 dashboard]# kubectl apply -f dashboard-ingress.yaml ingress \"dashboard-ingress\" created # 查看 ingress [root@k8s-master-1 dashboard]# kubectl get ingress -n kube-system -o wide NAME HOSTS ADDRESS PORTS AGE dashboard-ingress dashboard.jicki.cn 10.6.0.140,10.6.0.187 80 1h # 测试访问 [root@k8s-maste","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:66:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"Master Or Node Update 二进制文件 cd /tmp wget https://dl.k8s.io/v1.7.5/kubernetes-server-linux-amd64.tar.gz systemctl stop kube-apiserver systemctl stop kube-controller-manager systemctl stop kube-scheduler systemctl stop kubelet systemctl stop kube-proxy tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ systemctl start kube-apiserver systemctl start kube-controller-manager systemctl start kube-scheduler systemctl start kubelet systemctl start kube-proxy systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler systemctl status kubelet systemctl status kube-proxy cd .. rm -rf kubernetes* ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:67:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["kubernetes"],"content":"Node Update 二进制文件 cd /tmp wget https://dl.k8s.io/v1.7.5/kubernetes-server-linux-amd64.tar.gz systemctl stop kubelet systemctl stop kube-proxy tar zxvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kubectl,kube-proxy,kubelet} /usr/local/bin/ systemctl start kubelet systemctl start kube-proxy systemctl status kubelet systemctl status kube-proxy cd .. rm -rf kubernetes* ","date":"2017-08-08","objectID":"/kubernetes-1.7.3/:68:0","tags":null,"title":"kubernetes 1.7.3 + calico 多 Master","uri":"/kubernetes-1.7.3/"},{"categories":["blockchain"],"content":"Ehereum-geth","date":"2017-07-25","objectID":"/ehereum-geth/","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":" 基于 eth 私有链文档 to docker 部署私有链 ","date":"2017-07-25","objectID":"/ehereum-geth/:0:0","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":"创建 创世区块 # 创世区块文件说明 { \"config\": { //区块链的ID，你随便给一个就可以 \"chainId\": 21, //下面三个参数暂时不知道干啥的 \"homesteadBlock\": 0, \"eip155Block\": 0, \"eip158Block\": 0 }, //用来预置账号以及账号的以太币数量，应该也就是所谓的预挖 //前面是钱包地址，后面为 数量， 必须先创建钱包账号 //\"alloc\": { //\"0x0000000000000000000000000000000000000001\": {\"balance\": \"111111111\"}, //\"0x0000000000000000000000000000000000000002\": {\"balance\": \"222222222\"} //} \"alloc\" : {}, //币基地址，也就是默认的钱包地址，因为我没有地址，所以全0，为空 //后面运行Geth后创建新账户时，如果Geth发现没有币基地址，会默认将第一个账户的地址设置为币基地址 //也就是矿工账号 \"coinbase\" : \"0x0000000000000000000000000000000000000000\", //挖矿难度，你可以随便控制. \"difficulty\" : \"0x4000\", //附加信息，随便填个文本或不填也行，类似中本聪在比特币创世块中写的报纸新闻 \"extraData\" : \"\", //gas最高限制，以太坊运行交易，合约等所消耗的gas最高限制，这里设置为最高 \"gasLimit\" : \"0xffffffff\", //64位随机数，用于挖矿，注意他和mixhash的设置需要满足以太坊黄皮书中的要求 //直接用我这个也可以 \"nonce\" : \"0x0000000000000042\", //与nonce共同用于挖矿，注意他和nonce的设置需要满足以太坊黄皮书中的要求 \"mixhash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\", //上一个区块的Hash值，因为是创世块，石头里蹦出来的，没有在它前面的，所以是0 \"parentHash\" : \"0x0000000000000000000000000000000000000000000000000000000000000000\", //创世块的时间戳，这里给0就好 \"timestamp\" : \"0x00\" } ","date":"2017-07-25","objectID":"/ehereum-geth/:1:0","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":"初始化 init 这里geth 环境部署在 docker 中，使用 command 命令初始化 # 首先创建一个目录 mkdir -p /opt/blockchain # 将创世区块文件拷贝到目录下面 如下创世区块文件为 CustomGenesis.json geth --datadir \"/opt/blockchain\" init CustomGenesis.json # 初始化完成生成文件 -rw-r--r-- 1 root root 657 May 11 17:08 CustomGenesis.json drwxr-xr-x 5 root root 4096 Jul 25 14:14 geth srw------- 1 root root 0 Jul 25 09:14 geth.ipc drwx------ 2 root root 4096 May 19 11:56 keystore ","date":"2017-07-25","objectID":"/ehereum-geth/:2:0","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":"预挖币 1. 首先需要创建一个钱包 2. 停止 geth 服务 3. 删除 blockchain 目录下 geth geth.ipc 文件 保留 keystore 目录 以及 CustomGenesis.json 创世文件 4. 修改 CustomGenesis.json 文件 增加 如下： //\"alloc\": { //\"0x0000000000000000000000000000000000000001\": {\"balance\": \"111111111\"}, //\"0x0000000000000000000000000000000000000002\": {\"balance\": \"222222222\"} //} 5. 重新初始化一次 ","date":"2017-07-25","objectID":"/ehereum-geth/:2:1","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":"启动区块服务 在未创建钱包时，不能默认启动挖矿 # 所有权限 geth --datadir \"/opt/blockchain\" --nodiscover --identity \"myetherum\" --miner --minerthreads 1 --rpc --rpcaddr \"0.0.0.0\" --rpcport \"5020\" --rpcapi \"db,eth,net,web3,admin,miner,personal,rpc\" --rpccorsdomain \"*\" --networkid 9527 console # 部分权限 geth --datadir \"/opt/blockchain\" --nodiscover --identity \"myetherum\" --miner --minerthreads 1 --rpc --rpcaddr \"0.0.0.0\" --rpcport \"5020\" --rpcapi \"eth,net,web3,rpc\" --rpccorsdomain \"*\" --networkid 9527 console ","date":"2017-07-25","objectID":"/ehereum-geth/:3:0","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":"操作命令 # 连接终端 geth attach http://127.0.0.1:8545 # 查看节点信息 admin.nodeInfo # 连接其他节点(enode 从 admin.nodeInfo 中查看) admin.addPeer(\"enode://64bd4d3bebab176f59c40ac11d2a24dd2f82663f77eb6bf77843f3f48c2505ac34c8eb1bd8cfd53c1290421faee047feb31459bd4231c068efbdbf25accf744d@172.17.0.4:30303\") # 查看连接信息 admin.peers # 创建钱包 personal.newAccount(\"123456789\") # 启动挖矿 miner.start() # 停止挖矿 miner.stop() # 查看主钱包 币数量 web3.fromWei(eth.getBalance(eth.coinbase), \"ether\") # 查看节点下的钱包账户 eth.accounts # 查看交易所需gas eth.gasPrice ","date":"2017-07-25","objectID":"/ehereum-geth/:4:0","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":"智能合约 在线编辑合约 http://remix.ethereum.org/ # 进入终端 必须有 personal 权限，用于解锁账号 1. 开启挖矿，创建合约必须开启挖矿 2. 需要解锁主钱包账号 personal.unlockAccount(eth.coinbase,'钱包密码') 3. 复制 SOL 文件生成的合约文件到终端。 ","date":"2017-07-25","objectID":"/ehereum-geth/:5:0","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["blockchain"],"content":"docker-compose.yaml docker-compose 直接启动区块 version: '2' services: ethereum: image: ethereum/client-go:alltools-latest hostname: ethereum container_name: ethereum restart: always ports: - \"8545:8545\" volumes: - /opt/blockchain:/opt/blockchain #command: geth --datadir \"/opt/blockchain\" init /opt/blockchain/CustomGenesis.json #command: geth --datadir \"/opt/blockchain\" --identity \"oboetherum\" --mine --minerthreads 1 --rpc --rpcaddr \"0.0.0.0\" --rpcport \"8545\" --rpcapi \"admin,miner,personal,eth,net,web3,rpc\" --rpccor sdomain \"*\" --networkid 62222 command: geth --datadir \"/opt/blockchain\" --identity \"oboetherum\" --mine --minerthreads 1 --rpc --rpcaddr \"0.0.0.0\" --rpcport \"8545\" --rpcapi \"eth,net,web3,rpc\" --rpccorsdomain \"*\" --networki d 62222 ","date":"2017-07-25","objectID":"/ehereum-geth/:6:0","tags":null,"title":"Ehereum-geth","uri":"/ehereum-geth/"},{"categories":["kubernetes"],"content":"kubernetes 1.7.2 + calico 二进制部署","date":"2017-07-25","objectID":"/kubernetes-1.7.2/","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"kubernetes 1.7.2 + Calico 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:0:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"环境说明 k8s-master-1: 10.6.0.140 k8s-master-2: 10.6.0.187 k8s-node-1: 10.6.0.188 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:1:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname 10.6.0.140 - k8s-master-1 10.6.0.187 - k8s-master-2 10.6.0.188 - k8s-node-1 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 10.6.0.140 k8s-master-1 10.6.0.187 k8s-master-2 10.6.0.188 k8s-node-1 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:2:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:3:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl /opt/local/cfssl/cfssl print-defaults config \u003e config.json /opt/local/cfssl/cfssl print-defaults csr \u003e csr.json # config.json 文件 { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:4:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@k8s-master-1 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:5:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp * /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp * 10.6.0.187:/etc/kubernetes/ssl/ scp * 10.6.0.188:/etc/kubernetes/ssl/ etcd 集群 etcd 是k8s集群的基础组件 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:6:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"安装 etcd yum -y install etcd3 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:7:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 etcd 证书 cd /opt/ssl/ vi etcd-csr.json { \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"10.6.0.140\", \"10.6.0.187\", \"10.6.0.188\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } # 生成 etcd 密钥 /opt/local/cfssl/cfssl gencert -ca=/opt/ssl/ca.pem \\ -ca-key=/opt/ssl/ca-key.pem \\ -config=/opt/ssl/config.json \\ -profile=kubernetes etcd-csr.json | /opt/local/cfssl/cfssljson -bare etcd # 查看生成 [root@k8s-master-1 ssl]# ls etcd* etcd.csr etcd-csr.json etcd-key.pem etcd.pem # 拷贝到etcd服务器 # etcd-1 cp etcd*.pem /etc/kubernetes/ssl/ # etcd-2 scp etcd* 10.6.0.187:/etc/kubernetes/ssl/ # etcd-3 scp etcd* 10.6.0.188:/etc/kubernetes/ssl/ # 如果 etcd 非 root 用户，读取证书会提示没权限 chmod 644 /etc/kubernetes/ssl/etcd-key.pem ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:8:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 修改 etcd 启动文件 /usr/lib/systemd/system/etcd.service # etcd-1 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.6.0.140:2380 \\ --listen-peer-urls=https://10.6.0.140:2380 \\ --listen-client-urls=https://10.6.0.140:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.6.0.140:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.6.0.140:2380,etcd2=https://10.6.0.187:2380,etcd3=https://10.6.0.188:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-2 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.6.0.187:2380 \\ --listen-peer-urls=https://10.6.0.187:2380 \\ --listen-client-urls=https://10.6.0.187:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.6.0.187:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.6.0.140:2380,etcd2=https://10.6.0.187:2380,etcd3=https://10.6.0.188:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target # etcd-3 vi /usr/lib/systemd/system/etcd.service [Unit] Description=Etcd Server After=network.target After=network-online.target Wants=network-online.target [Service] Type=notify WorkingDirectory=/var/lib/etcd/ User=etcd # set GOMAXPROCS to number of processors ExecStart=/usr/bin/etcd \\ --name=etcd3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls=https://10.6.0.188:2380 \\ --listen-peer-urls=https://10.6.0.188:2380 \\ --listen-client-urls=https://10.6.0.188:2379,http://127.0.0.1:2379 \\ --advertise-client-urls=https://10.6.0.188:2379 \\ --initial-cluster-token=k8s-etcd-cluster \\ --initial-cluster=etcd1=https://10.6.0.140:2380,etcd2=https://10.6.0.187:2380,etcd3=https://10.6.0.188:2380 \\ --initial-cluster-state=new \\ --data-dir=/var/lib/etcd Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:9:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl enable etcd systemctl start etcd systemctl status etcd # 如果报错 请使用 journalctl -f -t etcd 和 journalctl -u etcd 来定位问题 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:10:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl --endpoints=https://10.6.0.140:2379 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ cluster-health member 29262d49176888f5 is healthy: got healthy result from https://10.6.0.188:2379 member d4ba1a2871bfa2b0 is healthy: got healthy result from https://10.6.0.140:2379 member eca58ebdf44f63b6 is healthy: got healthy result from https://10.6.0.187:2379 cluster is healthy 查看 etcd 集群成员： etcdctl --endpoints=https://10.6.0.140:2379 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --ca-file=/etc/kubernetes/ssl/ca.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ member list 29262d49176888f5: name=etcd3 peerURLs=https://10.6.0.188:2380 clientURLs=https://10.6.0.188:2379 isLeader=false d4ba1a2871bfa2b0: name=etcd1 peerURLs=https://10.6.0.140:2380 clientURLs=https://10.6.0.140:2379 isLeader=true eca58ebdf44f63b6: name=etcd2 peerURLs=https://10.6.0.187:2380 clientURLs=https://10.6.0.187:2379 isLeader=false 安装 docker # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 安装 yum install docker-ce ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:11:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 修改配置 vi /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS $DOCKER_OPTS $DOCKER_DNS_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target # 修改其他配置 cat \u003e\u003e /usr/lib/systemd/system/docker.service.d/docker-options.conf \u003c\u003c EOF [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 --graph=/opt/docker --registry-mirror=http://b438f72b.m.daocloud.io --iptables=false\" EOF # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker systemctl enable docker 安装 kubectl 工具 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:12:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"Master 端 # 首先安装 kubectl wget https://dl.k8s.io/v1.7.2/kubernetes-client-linux-amd64.tar.gz tar -xzvf kubernetes-client-linux-amd64.tar.gz cp kubernetes/client/bin/* /usr/local/bin/ chmod a+x /usr/local/bin/kube* # 验证安装 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.2\", GitCommit:\"922a86cfcd65915a9b2f69f3f193b8907d741d9c\", GitTreeState:\"clean\", BuildDate:\"2017-07-21T08:23:22Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:13:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@k8s-master-1 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:14:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:15:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"分发 kubectl config 文件 # 将上面配置的 kubeconfig 文件分发到其他机器 # 其他服务器创建目录 mkdir /root/.kube scp /root/.kube/config 10.6.0.187:/root/.kube/ scp /root/.kube/config 10.6.0.188:/root/.kube/ 部署 kubernetes Master 节点 Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:16:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"安装 组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.7.2/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:17:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"10.6.0.140\", \"10.6.0.187\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 10.6.0.140, 10.6.0.187 为 Master 的IP， 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:18:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@k8s-master-1 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1245 7月 4 11:25 kubernetes.csr -rw------- 1 root root 1679 7月 4 11:25 kubernetes-key.pem -rw-r--r-- 1 root root 1619 7月 4 11:25 kubernetes.pem -rw-r--r-- 1 root root 436 7月 4 11:23 kubernetes-csr.json # 拷贝到目录 cp -r kubernetes* /etc/kubernetes/ssl/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:19:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@k8s-master-1 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 11849e4f70904706ab3e631e70e6af0d # 创建 token.csv 文件 /opt/ssl vi token.csv 11849e4f70904706ab3e631e70e6af0d,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:20:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 vi /etc/systemd/system/kube-apiserver.service [Unit] Description=kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --advertise-address=10.6.0.140 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --authorization-mode=RBAC \\ --bind-address=10.6.0.140 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://10.6.0.140:2379,https://10.6.0.187:2379,https://10.6.0.188:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=10.6.0.140 \\ --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --experimental-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:21:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:22:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://10.6.0.140:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-cidr=10.233.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:23:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:24:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://10.6.0.140:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:25:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:26:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@k8s-master-1 opt]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} 部署 kubernetes Node 节点 (首先部署 10.6.0.187) Node 节点 需要部署的组件有 docker calico kubectl kubelet kube-proxy 这几个组件。 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:27:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 kubectl wget https://dl.k8s.io/v1.7.2/kubernetes-client-linux-amd64.tar.gz tar -xzvf kubernetes-client-linux-amd64.tar.gz cp kubernetes/client/bin/* /usr/local/bin/ chmod a+x /usr/local/bin/kube* # 验证安装 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.2\", GitCommit:\"922a86cfcd65915a9b2f69f3f193b8907d741d9c\", GitTreeState:\"clean\", BuildDate:\"2017-07-21T08:23:22Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:28:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需在一个node中创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:29:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"下载 二进制文件 cd /tmp wget https://dl.k8s.io/v1.7.2/kubernetes-server-linux-amd64.tar.gz tar zxvf kubernetes-server-linux-amd64.tar.gz cp -r kubernetes/server/bin/{kube-proxy,kubelet} /usr/local/bin/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:30:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=11849e4f70904706ab3e631e70e6af0d \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:31:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --address=10.6.0.187 \\ --hostname-override=10.6.0.187 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --require-kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --v=2 ExecStopPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target # 如上配置: 10.6.0.187 为本机的IP 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:32:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:33:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-2 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-EUE41uO5bofZZ-7GKD_V31oHXsENKFXCkLPy6Dj35Sc 1m kubelet-bootstrap Pending # 增加 认证 kubectl certificate approve node-csr-EUE41uO5bofZZ-7GKD_V31oHXsENKFXCkLPy6Dj35Sc # 提示 certificatesigningrequest \"node-csr-EUE41uO5bofZZ-7GKD_V31oHXsENKFXCkLPy6Dj35Sc\" approved ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:34:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"验证 nodes [root@k8s-master-1 ~]# kubectl get nodes NAME STATUS AGE VERSION 10.6.0.187 Ready 48m v1.7.2 10.6.0.188 Ready 14m v1.7.2 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:35:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:36:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@k8s-master-1 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:37:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy*.pem /etc/kubernetes/ssl/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:38:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"拷贝到Node节点 scp kube-proxy*.pem 10.6.0.187:/etc/kubernetes/ssl/ scp kube-proxy*.pem 10.6.0.188:/etc/kubernetes/ssl/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:39:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:40:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=10.6.0.187 \\ --hostname-override=10.6.0.187 \\ --cluster-cidr=10.254.0.0/16 \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:41:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:42:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"部署其他Node 节点 (第二个节点部署 10.6.0.188) .........省略了....... 参照以上配置 Calico 网络 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:43:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"修改 Node kubelet.service vi /etc/systemd/system/kubelet.service # 增加 如下配置 --network-plugin=cni \\ # 重新加载配置 systemctl daemon-reload systemctl restart kubelet.service systemctl status kubelet.service ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:44:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"修改 Node kube-proxy.service # 重新加载配置 systemctl daemon-reload systemctl restart kube-proxy.service systemctl status kube-proxy.service ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:45:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"安装 Calico 官网地址 http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/hosted # 下载 yaml 文件 wget http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/hosted/calico.yaml wget http://docs.projectcalico.org/v2.3/getting-started/kubernetes/installation/rbac.yaml # 下载 镜像 # 国外镜像 有墙 quay.io/calico/node:v1.3.0 quay.io/calico/cni:v1.9.1 quay.io/calico/kube-policy-controller:v0.6.0 # 国内镜像 jicki/node:v1.3.0 jicki/cni:v1.9.1 jicki/kube-policy-controller:v0.6.0 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:46:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"配置 calico vi calico.yaml # 注意修改如下选项: etcd_endpoints: \"https://10.6.0.140:2379,https://10.6.0.187:2379,https://10.6.0.188:2379\" etcd_ca: \"/calico-secrets/etcd-ca\" etcd_cert: \"/calico-secrets/etcd-cert\" etcd_key: \"/calico-secrets/etcd-key\" # 这里面要写入 base64 的信息 # 分别执行括号内的命令，填写到 etcd-key , etcd-cert, etcd-ca 中，不用括号。 data: etcd-key: (cat /etc/kubernetes/ssl/etcd-key.pem | base64 | tr -d '\\n') etcd-cert: (cat /etc/kubernetes/ssl/etcd.pem | base64 | tr -d '\\n') etcd-ca: (cat /etc/kubernetes/ssl/ca.pem | base64 | tr -d '\\n') - name: CALICO_IPV4POOL_CIDR value: \"10.233.0.0/16\" ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:47:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 [root@k8s-master-1 ~]# kubectl apply -f calico.yaml configmap \"calico-config\" created secret \"calico-etcd-secrets\" created daemonset \"calico-node\" created deployment \"calico-policy-controller\" created serviceaccount \"calico-policy-controller\" created serviceaccount \"calico-node\" created [root@k8s-master-1 ~]# kubectl apply -f rbac.yaml ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:48:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"验证 Calico [root@k8s-master-1 calico]# kubectl get ds -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE calico-node 2 2 2 2 2 \u003cnone\u003e 41s [root@k8s-master-1 calico]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE calico-node-04kd8 2/2 Running 0 1m calico-node-pkbwq 2/2 Running 0 1m calico-policy-controller-4282960220-mcdm7 1/1 Running 0 1m ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:49:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"安装 Calicoctl [root@k8s-master-1 ~]# cd /usr/local/bin/ [root@k8s-master-1 ~]# wget -c https://github.com/projectcalico/calicoctl/releases/download/v1.3.0/calicoctl [root@k8s-master-1 ~]# chmod +x calicoctl [root@k8s-master-1 ~]# calicoctl version Version: v1.3.0 Build date: Git commit: d2babb6 ## 创建 calicoctl 配置文件 # 配置文件， 在 安装了 calico 网络的 机器下 [root@k8s-master-1 ~]# mkdir /etc/calico [root@k8s-master-1 ~]# vi /etc/calico/calicoctl.cfg apiVersion: v1 kind: calicoApiConfig metadata: spec: datastoreType: \"etcdv2\" etcdEndpoints: \"https://10.6.0.140:2379,https://10.6.0.187:2379,https://10.6.0.188:2379\" etcdKeyFile: \"/etc/kubernetes/ssl/etcd-key.pem\" etcdCertFile: \"/etc/kubernetes/ssl/etcd.pem\" etcdCACertFile: \"/etc/kubernetes/ssl/ca.pem\" # 查看 calico 状态 [root@k8s-master-2 ~]# calicoctl node status Calico process is running. IPv4 BGP status +--------------+-------------------+-------+----------+-------------+ | PEER ADDRESS | PEER TYPE | STATE | SINCE | INFO | +--------------+-------------------+-------+----------+-------------+ | 10.6.0.188 | node-to-node mesh | up | 10:05:39 | Established | +--------------+-------------------+-------+----------+-------------+ 测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-master-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-2214564181-lxff5 1/1 Running 0 14m nginx-dm-2214564181-qm1bp 1/1 Running 0 14m [root@k8s-master-1 ~]# kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-dm 2 2 2 2 14m [root@k8s-master-1 ~]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.254.0.1 \u003cnone\u003e 443/TCP 4h nginx-svc 10.254.129.54 \u003cnone\u003e 80/TCP 15m # 在 node 里 curl [root@k8s-node-1 ~]# curl 10.254.129.54 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 配置 KubeDNS 官方 github yaml 相关 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:50:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"下载镜像 # 官方镜像 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 # 我的镜像 jicki/k8s-dns-sidecar-amd64:1.14.4 jicki/k8s-dns-kube-dns-amd64:1.14.4 jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.4 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:51:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-cm.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-sa.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-controller.yaml.base curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-svc.yaml.base # 修改后缀 mv kubedns-controller.yaml.base kubedns-controller.yaml mv kubedns-svc.yaml.base kubedns-svc.yaml ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:52:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限； [root@k8s-master-1 kubedns]# kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" creationTimestamp: 2017-07-04T04:15:13Z labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-dns resourceVersion: \"106\" selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindings/system%3Akube-dns uid: 60c1e0e1-606f-11e7-b212-d4ae52d1f0c9 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-dns subjects: - kind: ServiceAccount name: kube-dns namespace: kube-system ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:53:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"修改 kubedns-svc.yaml # kubedns-svc.yaml 中 clusterIP: __PILLAR__DNS__SERVER__ 修改为我们之前定义的 dns IP 10.254.0.2 cat kubedns-svc.yaml apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"KubeDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:54:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"修改 kubedns-controller.yaml 1. # 修改 --domain=__PILLAR__DNS__DOMAIN__. 为 我们之前 预定的 domain 名称 --domain=cluster.local. 2. # 修改 --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053 中 domain 为我们之前预定的 --server=/cluster.local./127.0.0.1#10053 3. # 修改 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local., 4. # 修改 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local., ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:55:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 [root@k8s-master-1 kubedns]# kubectl create -f . configmap \"kube-dns\" created deployment \"kube-dns\" created serviceaccount \"kube-dns\" created service \"kube-dns\" created ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:56:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@k8s-master-1 kubedns]# kubectl get all --namespace=kube-system NAME READY STATUS RESTARTS AGE po/kube-dns-1511229508-llfgs 3/3 Running 0 1m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 1 1 1 1 1m NAME DESIRED CURRENT READY AGE rs/kube-dns-1511229508 1 1 1 1m ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:57:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"验证 dns 服务 在验证 dns 之前，在 dns 未部署之前创建的 pod 与 deployment 等，都必须删除，重新部署，否则无法解析 # 导入之前的 nginx-dm yaml文件 [root@k8s-master-1 ~]# kubectl get svc nginx-svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc 10.254.79.137 \u003cnone\u003e 80/TCP 29s # 创建一个 pods 来测试一下 nameserver apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 pods [root@k8s-master-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 1m nginx-dm-2214564181-4zjbh 1/1 Running 0 5m nginx-dm-2214564181-tpz8t 1/1 Running 0 5m # 测试 [root@k8s-master-1 ~]# kubectl exec -it alpine ping nginx-svc PING nginx-svc (10.254.207.143): 56 data bytes [root@k8s-master-1 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.207.143 nginx-svc.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:58:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"部署 dashboard 官方 dashboard 的github https://github.com/kubernetes/dashboard ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:59:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.6.1 ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:60:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-controller.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-service.yaml # 因为开启了 RBAC 所以这里需要创建一个 RBAC 认证 vi dashboard-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata: name: dashboard namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1alpha1 metadata: name: dashboard subjects: - kind: ServiceAccount name: dashboard namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: rbac.authorization.k8s.io ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:61:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"导入 yaml # dashboard-controller.yaml 增加 rbac 授权 # 在第二个 spec 下面 增加 spec: serviceAccountName: dashboard # 导入文件 [root@k8s-master-1 dashboard]# kubectl apply -f . deployment \"kubernetes-dashboard\" created serviceaccount \"dashboard\" created clusterrolebinding \"dashboard\" created service \"kubernetes-dashboard\" created # 查看 svc 与 pod [root@k8s-master-1 dashboard]# kubectl get svc -n kube-system kubernetes-dashboard NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard 10.254.167.28 \u003cnone\u003e 80/TCP 31s ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:62:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 kubernetes 服务。 官方 Nginx Ingress github https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.0 gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.11 # 国内镜像 jicki/defaultbackend:1.0 jicki/nginx-ingress-controller:0.9.0-beta.11 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/default-backend.yaml # 直接导入既可, 这里不需要修改 [root@k8s-master-1 ingress]# kubectl apply -f default-backend.yaml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看服务 [root@k8s-master-1 ingress]# kubectl get deployment -n kube-system default-http-backend NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 36s # 部署 Ingress RBAC 认证 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/rbac/nginx/nginx-ingress-controller-rbac.yml # 修改 namespace sed -i 's/namespace: nginx-ingress/namespace: kube-system/g' nginx-ingress-controller-rbac.yml # 导入 yaml 文件 [root@k8s-master-1 ingress]# kubectl apply -f nginx-ingress-controller-rbac.yml namespace \"nginx-ingress\" created serviceaccount \"nginx-ingress-serviceaccount\" created clusterrole \"nginx-ingress-clusterrole\" created role \"nginx-ingress-role\" created rolebinding \"nginx-ingress-role-nisa-binding\" created clusterrolebinding \"nginx-ingress-clusterrole-nisa-binding\" created # 部署 Ingress Controller 组件 # 下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/daemonset/nginx/nginx-ingress-daemonset.yaml # 修改 yaml 文件 增加 rbac 认证 和 hostNetwork , 第二个 spec 下 增加 spec: hostNetwork: true serviceAccountName: nginx-ingress-serviceaccount # 导入 yaml 文件 [root@k8s-master-1 ingress]# kubectl apply -f nginx-ingress-daemonset.yaml daemonset \"nginx-ingress-lb\" created # 查看服务 [root@k8s-master-1 ingress]# kubectl get daemonset -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE nginx-ingress-lb 2 2 2 2 2 \u003cnone\u003e 11s # 创建一个 ingress # 查看我们原有的 svc [root@k8s-master-1 Ingress]# kubectl get svc nginx-svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc 10.254.207.143 \u003cnone\u003e 80/TCP 1d # 创建 yaml 文件 vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 # 导入 yaml [root@k8s-master-1 ingress]# kubectl apply -f nginx-ingress.yaml ingress \"nginx-ingress\" created # 查看 ingress [root@k8s-master-1 Ingress]# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 10.6.0.187,10... 80 24s # 测试访问 [root@k8s-master-1 ingress]# curl -I nginx.jicki.cn HTTP/1.1 200 OK Server: nginx/1.13.2 Date: Thu, 06 Jul 2017 04:21:43 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Wed, 28 Jun 2017 18:27:36 GMT ETag: \"5953f518-264\" Accept-Ranges: bytes # 配置一个 Dashboard Ingress # 查看 dashboard 的 svc [root@k8s-master-1 ingress]# kubectl get svc -n kube-system kubernetes-dashboard NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard 10.254.81.94 \u003cnone\u003e 80/TCP 2h # 编辑一个 yaml 文件 vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: dashboard-ingress namespace: kube-system spec: rules: - host: dashboard.jicki.cn http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 80 # 查看 ingress [root@k8s-master-1 dashboard]# kubectl get ingress -n kube-system NAME HOSTS ADDRESS PORTS AGE dashboard-ingress dashboard.jicki.cn 10.6.0.187,10... 80 1m # 测试访问 [root@k8s-master-1 dashboard]# curl -I dashboard.jicki.cn HTTP/1.1 200 OK Server: nginx/1.13.2 Date: Thu, 06 Jul 2017 06:32:00 GMT Content-Type: text/html; charset=utf-8 Content-Length: 848 Connection: keep-alive Accept-Ranges: bytes Cache-Contr","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:63:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"部署 Master-2 # 下载 二进制 文件 wget https://dl.k8s.io/v1.7.2/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ # 拷贝 Matser-1 的密钥到 Master-2 # 这里我为了方便偷懒，我把所有的密钥都拷贝过去了 [root@k8s-master-1 ~]# cd /etc/kubernetes/ssl [root@k8s-master-1 ssl]# scp -r * 10.6.0.187:/etc/kubernetes/ssl/ # 拷贝 token.csv 文件 [root@k8s-master-1 ~]# cd /etc/kubernetes [root@k8s-master-1 ssl]# scp -r token.csv 10.6.0.187:/etc/kubernetes/ # 配置 Master kube-apiserver vi /etc/systemd/system/kube-apiserver.service [Unit] Description=kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --advertise-address=10.6.0.187 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --authorization-mode=RBAC \\ --bind-address=10.6.0.187 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/etcd.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem \\ --etcd-servers=https://10.6.0.140:2379,https://10.6.0.187:2379,https://10.6.0.188:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=10.6.0.187 \\ --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --experimental-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver # 部署 kube-controller-manager vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://10.6.0.187:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-cidr=10.233.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target # 启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager # 部署 kube-scheduler vi /etc/systemd/system/kube-scheduler.service [Unit] Description=kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://10.6.0.187:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target # 启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler # Master-2 里验证 [root@k8s-master-2 ~]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:64:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"修改 node 配置 # kubelet # 首先 重新创建 kubelet kubeconfig 文件 # 配置集群 (server 这里配置为127.0.0.1 既是 Master 又是 Node 的请配置为 Node IP) kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=11849e4f70904706ab3e631e70e6af0d \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ # 重新创建 kube-proxy kubeconfig 文件 # 配置集群 (server 这里配置为 127.0.0.1 既是 Master 又是 Node 的请配置为 Node IP) kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://127.0.0.1:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:65:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"创建Nginx 代理 在每个 node 都必须创建一个 Nginx 代理， 这里特别注意， 当 Master 也做为 Node 的时候 不需要配置 Nginx-proxy # 创建配置目录 mkdir -p /etc/nginx # 写入代理配置 cat \u003c\u003c EOF \u003e\u003e /etc/nginx/nginx.conf error_log stderr notice; worker_processes auto; events { multi_accept on; use epoll; worker_connections 1024; } stream { upstream kube_apiserver { least_conn; server 10.6.0.140:6443; server 10.6.0.187:6443; } server { listen 0.0.0.0:6443; proxy_pass kube_apiserver; proxy_timeout 10m; proxy_connect_timeout 1s; } } EOF # 配置 Nginx 基于 docker 进程，然后配置 systemd 来启动 cat \u003c\u003c EOF \u003e\u003e /etc/systemd/system/nginx-proxy.service [Unit] Description=kubernetes apiserver docker wrapper Wants=docker.socket After=docker.service [Service] User=root PermissionsStartOnly=true ExecStart=/usr/bin/docker run -p 6443:6443 \\\\ -v /etc/nginx:/etc/nginx \\\\ --name nginx-proxy \\\\ --net=host \\\\ --restart=on-failure:5 \\\\ --memory=512M \\\\ nginx:1.13.3-alpine ExecStartPre=-/usr/bin/docker rm -f nginx-proxy ExecStop=/usr/bin/docker stop nginx-proxy Restart=always RestartSec=15s TimeoutStartSec=30s [Install] WantedBy=multi-user.target EOF # 启动 Nginx systemctl daemon-reload systemctl start nginx-proxy systemctl enable nginx-proxy # 重启 Node 的 kubelet 与 kube-proxy systemctl restart kubelet systemctl status kubelet systemctl restart kube-proxy systemctl status kube-proxy ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:66:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"最后测试一下 # 这里面 10.6.0.187 既是 Master 又是 node # Master-1 [root@k8s-master-1 ~]# kubectl get nodes NAME STATUS AGE VERSION 10.6.0.187 Ready 1d v1.7.2 10.6.0.188 Ready 1d v1.7.2 # Master-2 [root@k8s-master-2 ~]# kubectl get nodes NAME STATUS AGE VERSION 10.6.0.187 Ready 1d v1.7.2 10.6.0.188 Ready 1d v1.7.2 # 最后分别在 Master-1 与 Master-2 都创建 pods deployment 后续需要增加的东西…… # yaml 中的一些 特殊 env env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP ","date":"2017-07-25","objectID":"/kubernetes-1.7.2/:67:0","tags":null,"title":"kubernetes 1.7.2 + calico 二进制部署","uri":"/kubernetes-1.7.2/"},{"categories":["kubernetes"],"content":"kubernetes 1.7.0 + flannel 二进制部署","date":"2017-07-04","objectID":"/kubernetes-1.7.0/","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"kubernetes 1.7.0 + flannel 基于 二进制 文件部署 本地化 kube-apiserver, kube-controller-manager , kube-scheduler ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:0:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"环境说明 k8s-master-1: 10.6.0.140 k8s-master-2: 10.6.0.187 k8s-node-1: 10.6.0.188 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:1:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname 10.6.0.140 - k8s-master-1 10.6.0.187 - k8s-master-2 10.6.0.188 - k8s-node-1 #编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 10.6.0.140 k8s-master-1 10.6.0.187 k8s-master-2 10.6.0.188 k8s-node-1 创建 验证 这里使用 CloudFlare 的 PKI 工具集 cfssl 来生成 Certificate Authority (CA) 证书和秘钥文件。 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:2:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"安装 cfssl mkdir -p /opt/local/cfssl cd /opt/local/cfssl wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 mv cfssl_linux-amd64 cfssl wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 mv cfssljson_linux-amd64 cfssljson wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 mv cfssl-certinfo_linux-amd64 cfssl-certinfo chmod +x * ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:3:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 CA 证书配置 mkdir /opt/ssl cd /opt/ssl /opt/local/cfssl/cfssl print-defaults config \u003e config.json /opt/local/cfssl/cfssl print-defaults csr \u003e csr.json # config.json 文件 { \"signing\": { \"default\": { \"expiry\": \"87600h\" }, \"profiles\": { \"kubernetes\": { \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" } } } } # csr.json 文件 { \"CN\": \"kubernetes\", \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:4:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"生成 CA 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -initca csr.json | /opt/local/cfssl/cfssljson -bare ca [root@k8s-master-1 ssl]# ls -lt 总用量 20 -rw-r--r-- 1 root root 1005 7月 3 17:26 ca.csr -rw------- 1 root root 1675 7月 3 17:26 ca-key.pem -rw-r--r-- 1 root root 1363 7月 3 17:26 ca.pem -rw-r--r-- 1 root root 210 7月 3 17:24 csr.json -rw-r--r-- 1 root root 292 7月 3 17:23 config.json ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:5:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"分发证书 # 创建证书目录 mkdir -p /etc/kubernetes/ssl # 拷贝所有文件到目录下 cp * /etc/kubernetes/ssl # 这里要将文件拷贝到所有的k8s 机器上 scp * 10.6.0.187:/etc/kubernetes/ssl/ scp * 10.6.0.188:/etc/kubernetes/ssl/ etcd 集群 etcd 是k8s集群的基础组件，这里感觉没必要创建双向认证。 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:6:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"安装 etcd yum -y install etcd3 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:7:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"修改 etcd 配置 # etcd-1 # 修改配置文件，/etc/etcd/etcd.conf 需要修改如下参数： mv /etc/etcd/etcd.conf /etc/etcd/etcd.conf-bak vi /etc/etcd/etcd.conf ETCD_NAME=etcd1 ETCD_DATA_DIR=\"/var/lib/etcd/etcd1.etcd\" ETCD_LISTEN_PEER_URLS=\"http://10.6.0.140:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.6.0.140:2379,http://127.0.0.1:2379\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.6.0.140:2380\" ETCD_INITIAL_CLUSTER=\"etcd1=http://10.6.0.140:2380,etcd2=http://10.6.0.187:2380,etcd3=http://10.6.0.188:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.6.0.140:2379\" # etcd-2 # 修改配置文件，/etc/etcd/etcd.conf 需要修改如下参数： mv /etc/etcd/etcd.conf /etc/etcd/etcd.conf-bak vi /etc/etcd/etcd.conf ETCD_NAME=etcd2 ETCD_DATA_DIR=\"/var/lib/etcd/etcd2.etcd\" ETCD_LISTEN_PEER_URLS=\"http://10.6.0.187:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.6.0.187:2379,http://127.0.0.1:2379\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.6.0.187:2380\" ETCD_INITIAL_CLUSTER=\"etcd1=http://10.6.0.140:2380,etcd2=http://10.6.0.187:2380,etcd3=http://10.6.0.188:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.6.0.187:2379\" # etcd-3 # 修改配置文件，/etc/etcd/etcd.conf 需要修改如下参数： mv /etc/etcd/etcd.conf /etc/etcd/etcd.conf-bak vi /etc/etcd/etcd.conf ETCD_NAME=etcd3 ETCD_DATA_DIR=\"/var/lib/etcd/etcd3.etcd\" ETCD_LISTEN_PEER_URLS=\"http://10.6.0.188:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.6.0.188:2379,http://127.0.0.1:2379\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.6.0.188:2380\" ETCD_INITIAL_CLUSTER=\"etcd1=http://10.6.0.140:2380,etcd2=http://10.6.0.187:2380,etcd3=http://10.6.0.188:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.6.0.188:2379\" 修改 etcd 启动文件 /usr/lib/systemd/system/etcd.service sed -i 's/\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\"/\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\" --listen-client-urls=\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\" --advertise-client-urls=\\\\\\\"${ETCD_ADVERTISE_CLIENT_URLS}\\\\\\\" --initial-cluster-token=\\\\\\\"${ETCD_INITIAL_CLUSTER_TOKEN}\\\\\\\" --initial-cluster=\\\\\\\"${ETCD_INITIAL_CLUSTER}\\\\\\\" --initial-cluster-state=\\\\\\\"${ETCD_INITIAL_CLUSTER_STATE}\\\\\\\"/g' /usr/lib/systemd/system/etcd.service ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:8:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"启动 etcd 分别启动 所有节点的 etcd 服务 systemctl enable etcd systemctl start etcd systemctl status etcd ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:9:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"验证 etcd 集群状态 查看 etcd 集群状态： etcdctl cluster-health # 出现 cluster is healthy 表示成功 查看 etcd 集群成员： etcdctl member list 4cccc71dfdfc0646: name=etcd1 peerURLs=http://10.6.0.140:2380 clientURLs=http://10.6.0.140:2379 isLeader=true 5ffd5d99530e9fe6: name=etcd2 peerURLs=http://10.6.0.187:2380 clientURLs=http://10.6.0.187:2379 isLeader=false dcd6fb81996f77c3: name=etcd3 peerURLs=http://10.6.0.188:2380 clientURLs=http://10.6.0.188:2379 isLeader=false Flannel 网络 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:10:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"安装 flannel 这边其实由于内网，就没有使用SSL认证，直接使用了 yum -y install flannel 清除网络中遗留的docker 网络 (docker0, flannel0 等) ifconfig 如果存在 请删除之，以免发生不必要的未知错误 ip link delete docker0 .... ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:11:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 flannel 设置 flannel 所用到的IP段 etcdctl --endpoint http://10.6.0.140:2379 set /flannel/network/config '{\"Network\":\"10.233.0.0/16\",\"SubnetLen\":25,\"Backend\":{\"Type\":\"vxlan\",\"VNI\":1}}' 接下来修改 flannel 配置文件 vim /etc/sysconfig/flanneld # 旧版本: FLANNEL_ETCD=\"http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379\" # 修改为 集群地址 FLANNEL_ETCD_KEY=\"/flannel/network/config\" # 修改为 上面导入配置中的 /flannel/network FLANNEL_OPTIONS=\"--iface=em1\" # 修改为 本机物理网卡的名称 # 新版本: FLANNEL_ETCD=\"http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379\" # 修改为 集群地址 FLANNEL_ETCD_PREFIX=\"/flannel/network\" # 修改为 上面导入配置中的 /flannel/network FLANNEL_OPTIONS=\"--iface=em1\" # 修改为 本机物理网卡的名称 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:12:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"启动 flannel systemctl enable flanneld systemctl start flanneld systemctl status flanneld 安装 docker # 导入 yum 源 # 安装 yum-config-manager yum -y install yum-utils # 导入 yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo # 更新 repo yum makecache # 安装 yum install docker-ce ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:13:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"更改docker 配置 # 修改配置 vi /usr/lib/systemd/system/docker.service [Unit] Description=Docker Application Container Engine Documentation=https://docs.docker.com After=network-online.target firewalld.service Wants=network-online.target [Service] Type=notify ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS $DOCKER_OPTS $DOCKER_DNS_OPTIONS ExecReload=/bin/kill -s HUP $MAINPID LimitNOFILE=infinity LimitNPROC=infinity LimitCORE=infinity TimeoutStartSec=0 Delegate=yes KillMode=process Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target # 修改其他配置 cat \u003e\u003e /usr/lib/systemd/system/docker.service.d/docker-options.conf \u003c\u003c EOF [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 --graph=/opt/docker --registry-mirror=http://b438f72b.m.daocloud.io\" EOF # 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:14:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"查看docker网络 ifconfig docker0: flags=4099\u003cUP,BROADCAST,MULTICAST\u003e mtu 1500 inet 10.233.19.1 netmask 255.255.255.128 broadcast 0.0.0.0 ether 02:42:c1:2c:c5:be txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 em1: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1500 inet 10.6.0.140 netmask 255.255.255.0 broadcast 10.6.0.255 inet6 fe80::d6ae:52ff:fed1:f0c9 prefixlen 64 scopeid 0x20\u003clink\u003e ether d4:ae:52:d1:f0:c9 txqueuelen 1000 (Ethernet) RX packets 16286600 bytes 1741928233 (1.6 GiB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 15841272 bytes 1566357399 (1.4 GiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 flannel.1: flags=4163\u003cUP,BROADCAST,RUNNING,MULTICAST\u003e mtu 1450 inet 10.233.19.0 netmask 255.255.255.255 broadcast 0.0.0.0 inet6 fe80::d9:e2ff:fe46:9cdd prefixlen 64 scopeid 0x20\u003clink\u003e ether 02:d9:e2:46:9c:dd txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 (0.0 B) TX errors 0 dropped 26 overruns 0 carrier 0 collisions 0 安装 kubectl 工具 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:15:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"Master 端 # 首先安装 kubectl wget https://dl.k8s.io/v1.7.0/kubernetes-client-linux-amd64.tar.gz tar -xzvf kubernetes-client-linux-amd64.tar.gz cp kubernetes/client/bin/* /usr/local/bin/ chmod a+x /usr/local/bin/kube* # 验证安装 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.0\", GitCommit:\"d3ada0119e776222f11ec7945e6d860061339aad\", GitTreeState:\"clean\", BuildDate:\"2017-06-29T23:15:59Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:16:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 admin 证书 kubectl 与 kube-apiserver 的安全端口通信，需要为安全通信提供 TLS 证书和秘钥。 cd /opt/ssl/ vi admin-csr.json { \"CN\": \"admin\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"system:masters\", \"OU\": \"System\" } ] } # 生成 admin 证书和私钥 cd /opt/ssl/ /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes admin-csr.json | /opt/local/cfssl/cfssljson -bare admin # 查看生成 [root@k8s-master-1 ssl]# ls admin* admin.csr admin-csr.json admin-key.pem admin.pem cp admin*.pem /etc/kubernetes/ssl/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:17:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 kubectl kubeconfig 文件 # 配置 kubernetes 集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 # 配置 客户端认证 kubectl config set-credentials admin \\ --client-certificate=/etc/kubernetes/ssl/admin.pem \\ --embed-certs=true \\ --client-key=/etc/kubernetes/ssl/admin-key.pem kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin kubectl config use-context kubernetes ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:18:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"分发 kubectl config 文件 # 将上面配置的 kubeconfig 文件分发到其他机器 # 其他服务器创建目录 mkdir /root/.kube scp /root/.kube/config 10.6.0.187:/root/.kube/ scp /root/.kube/config 10.6.0.188:/root/.kube/ 部署 kubernetes Master 节点 Master 需要部署 kube-apiserver , kube-scheduler , kube-controller-manager 这三个组件。 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:19:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"安装 组件 # 从github 上下载版本 cd /tmp wget https://dl.k8s.io/v1.7.0/kubernetes-server-linux-amd64.tar.gz tar -xzvf kubernetes-server-linux-amd64.tar.gz cd kubernetes cp -r server/bin/{kube-apiserver,kube-controller-manager,kube-scheduler,kubectl,kube-proxy,kubelet} /usr/local/bin/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:20:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 kubernetes 证书 /opt/ssl vi kubernetes-csr.json { \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"10.6.0.140\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ## 这里 hosts 字段中 三个 IP 分别为 127.0.0.1 本机， 10.6.0.140 为 Master 的IP， 10.254.0.1 为 kubernetes SVC 的 IP， 一般是 部署网络的第一个IP , 如: 10.254.0.1 ， 在启动完成后，我们使用 kubectl get svc ， 就可以查看到 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:21:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"生成 kubernetes 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes kubernetes-csr.json | /opt/local/cfssl/cfssljson -bare kubernetes # 查看生成 [root@k8s-master-1 ssl]# ls -lt kubernetes* -rw-r--r-- 1 root root 1245 7月 4 11:25 kubernetes.csr -rw------- 1 root root 1679 7月 4 11:25 kubernetes-key.pem -rw-r--r-- 1 root root 1619 7月 4 11:25 kubernetes.pem -rw-r--r-- 1 root root 436 7月 4 11:23 kubernetes-csr.json # 拷贝到目录 cp -r kubernetes* /etc/kubernetes/ssl/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:22:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 kube-apiserver kubelet 首次启动时向 kube-apiserver 发送 TLS Bootstrapping 请求，kube-apiserver 验证 kubelet 请求中的 token 是否与它配置的 token 一致，如果一致则自动为 kubelet生成证书和秘钥。 # 生成 token [root@k8s-master-1 ssl]# head -c 16 /dev/urandom | od -An -t x | tr -d ' ' 11849e4f70904706ab3e631e70e6af0d # 创建 token.csv 文件 /opt/ssl vi token.csv 11849e4f70904706ab3e631e70e6af0d,kubelet-bootstrap,10001,\"system:kubelet-bootstrap\" # 拷贝 cp token.csv /etc/kubernetes/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:23:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 kube-apiserver.service 文件 一、 开启了 RBAC # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 vi /etc/systemd/system/kube-apiserver.service [Unit] Description=kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --advertise-address=10.6.0.140 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --authorization-mode=RBAC \\ --bind-address=10.6.0.140 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\ --etcd-servers=http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=10.6.0.140 \\ --runtime-config=rbac.authorization.k8s.io/v1alpha1 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --experimental-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target 二、 关闭了 RBAC # 自定义 系统 service 文件一般存于 /etc/systemd/system/ 下 vi /etc/systemd/system/kube-apiserver.service [Unit] Description=kubernetes API Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] User=root ExecStart=/usr/local/bin/kube-apiserver \\ --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --advertise-address=10.6.0.140 \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/lib/audit.log \\ --bind-address=10.6.0.140 \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --enable-swagger-ui=true \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\ --etcd-servers=http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379 \\ --event-ttl=1h \\ --kubelet-https=true \\ --insecure-bind-address=10.6.0.140 \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32000 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --experimental-bootstrap-token-auth \\ --token-auth-file=/etc/kubernetes/token.csv \\ --v=2 Restart=on-failure RestartSec=5 Type=notify LimitNOFILE=65536 [Install] WantedBy=multi-user.target # 这里面要注意的是 --service-node-port-range=30000-32000 # 这个地方是 映射外部端口时 的端口范围，随机映射也在这个范围内映射，指定映射端口必须也在这个范围内。 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:24:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"启动 kube-apiserver systemctl daemon-reload systemctl enable kube-apiserver systemctl start kube-apiserver systemctl status kube-apiserver ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:25:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 kube-controller-manager # 创建 kube-controller-manager.service 文件 vi /etc/systemd/system/kube-controller-manager.service [Unit] Description=kubernetes Controller Manager Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=http://10.6.0.140:8080 \\ --allocate-node-cidrs=true \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-cidr=10.233.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:26:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"启动 kube-controller-manager systemctl daemon-reload systemctl enable kube-controller-manager systemctl start kube-controller-manager systemctl status kube-controller-manager ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:27:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 kube-scheduler # 创建 kube-cheduler.service 文件 vi /etc/systemd/system/kube-scheduler.service [Unit] Description=kubernetes Scheduler Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=http://10.6.0.140:8080 \\ --leader-elect=true \\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:28:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"启动 kube-scheduler systemctl daemon-reload systemctl enable kube-scheduler systemctl start kube-scheduler systemctl status kube-scheduler ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:29:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"验证 Master 节点 [root@k8s-master-1 opt]# kubectl get componentstatuses NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy {\"health\": \"true\"} etcd-1 Healthy {\"health\": \"true\"} etcd-2 Healthy {\"health\": \"true\"} 部署 kubernetes Node 节点 (首先部署 10.6.0.187) Node 节点 需要部署的组件有 docker flannel kubectl kubelet kube-proxy 这几个组件。 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:30:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 kubectl wget https://dl.k8s.io/v1.7.0/kubernetes-client-linux-amd64.tar.gz tar -xzvf kubernetes-client-linux-amd64.tar.gz cp kubernetes/client/bin/* /usr/local/bin/ chmod a+x /usr/local/bin/kube* # 验证安装 kubectl version Client Version: version.Info{Major:\"1\", Minor:\"7\", GitVersion:\"v1.7.0\", GitCommit:\"d3ada0119e776222f11ec7945e6d860061339aad\", GitTreeState:\"clean\", BuildDate:\"2017-06-29T23:15:59Z\", GoVersion:\"go1.8.3\", Compiler:\"gc\", Platform:\"linux/amd64\"} ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:31:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 kubelet kubelet 启动时向 kube-apiserver 发送 TLS bootstrapping 请求，需要先将 bootstrap token 文件中的 kubelet-bootstrap 用户赋予 system:node-bootstrapper 角色，然后 kubelet 才有权限创建认证请求(certificatesigningrequests)。 # 先创建认证请求 # user 为 master 中 token.csv 文件里配置的用户 # 只需在一个node中创建一次就可以 kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:32:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"下载 二进制文件 cd /tmp wget https://dl.k8s.io/v1.7.0/kubernetes-server-linux-amd64.tar.gz tar zxvf kubernetes-server-linux-amd64.tar.gz cp -r kubernetes/server/bin/{kube-proxy,kubelet} /usr/local/bin/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:33:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 kubelet kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 \\ --kubeconfig=bootstrap.kubeconfig # 配置客户端认证 kubectl config set-credentials kubelet-bootstrap \\ --token=11849e4f70904706ab3e631e70e6af0d \\ --kubeconfig=bootstrap.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=bootstrap.kubeconfig # 拷贝生成的 bootstrap.kubeconfig 文件 mv bootstrap.kubeconfig /etc/kubernetes/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:34:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 kubelet.service 文件 # 创建 kubelet 目录 mkdir /var/lib/kubelet vi /etc/systemd/system/kubelet.service [Unit] Description=kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=docker.service Requires=docker.service [Service] WorkingDirectory=/var/lib/kubelet ExecStart=/usr/local/bin/kubelet \\ --address=10.6.0.187 \\ --hostname-override=10.6.0.187 \\ --pod-infra-container-image=jicki/pause-amd64:3.0 \\ --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \\ --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \\ --require-kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --cluster_dns=10.254.0.2 \\ --cluster_domain=cluster.local. \\ --hairpin-mode promiscuous-bridge \\ --allow-privileged=true \\ --serialize-image-pulls=false \\ --logtostderr=true \\ --v=2 ExecStopPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT ExecStopPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target # 如上配置: 10.6.0.187 为本机的IP 10.254.0.2 预分配的 dns 地址 cluster.local. 为 kubernetes 集群的 domain jicki/pause-amd64:3.0 这个是 pod 的基础镜像，既 gcr 的 gcr.io/google_containers/pause-amd64:3.0 镜像， 下载下来修改为自己的仓库中的比较快。 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:35:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl daemon-reload systemctl enable kubelet systemctl start kubelet systemctl status kubelet ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:36:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 TLS 认证 # 查看 csr 的名称 [root@k8s-master-2 ~]# kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-EUE41uO5bofZZ-7GKD_V31oHXsENKFXCkLPy6Dj35Sc 1m kubelet-bootstrap Pending # 增加 认证 kubectl certificate approve node-csr-EUE41uO5bofZZ-7GKD_V31oHXsENKFXCkLPy6Dj35Sc # 提示 certificatesigningrequest \"node-csr-EUE41uO5bofZZ-7GKD_V31oHXsENKFXCkLPy6Dj35Sc\" approved ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:37:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"验证 nodes [root@k8s-master-1 ~]# kubectl get nodes NAME STATUS AGE VERSION 10.6.0.187 Ready 33s v1.7.0 # 成功以后会自动生成配置文件与密钥 # 配置文件 ls /etc/kubernetes/kubelet.kubeconfig /etc/kubernetes/kubelet.kubeconfig # 密钥文件 ls /etc/kubernetes/ssl/kubelet* /etc/kubernetes/ssl/kubelet-client.crt /etc/kubernetes/ssl/kubelet.crt /etc/kubernetes/ssl/kubelet-client.key /etc/kubernetes/ssl/kubelet.key ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:38:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"配置 kube-proxy ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:39:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 kube-proxy 证书 # 证书方面由于我们node端没有装 cfssl # 我们回到 master 端 机器 去配置证书，然后拷贝过来 [root@k8s-master-1 ~]# cd /opt/ssl vi kube-proxy-csr.json { \"CN\": \"system:kube-proxy\", \"hosts\": [], \"key\": { \"algo\": \"rsa\", \"size\": 2048 }, \"names\": [ { \"C\": \"CN\", \"ST\": \"ShenZhen\", \"L\": \"ShenZhen\", \"O\": \"k8s\", \"OU\": \"System\" } ] } ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:40:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"生成 kube-proxy 证书和私钥 /opt/local/cfssl/cfssl gencert -ca=/etc/kubernetes/ssl/ca.pem \\ -ca-key=/etc/kubernetes/ssl/ca-key.pem \\ -config=/etc/kubernetes/ssl/config.json \\ -profile=kubernetes kube-proxy-csr.json | /opt/local/cfssl/cfssljson -bare kube-proxy # 查看生成 ls kube-proxy* kube-proxy.csr kube-proxy-csr.json kube-proxy-key.pem kube-proxy.pem # 拷贝到目录 cp kube-proxy*.pem /etc/kubernetes/ssl/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:41:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"拷贝到Node节点 scp kube-proxy*.pem 10.6.0.187:/etc/kubernetes/ssl/ scp kube-proxy*.pem 10.6.0.188:/etc/kubernetes/ssl/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:42:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 kube-proxy kubeconfig 文件 # 配置集群 kubectl config set-cluster kubernetes \\ --certificate-authority=/etc/kubernetes/ssl/ca.pem \\ --embed-certs=true \\ --server=https://10.6.0.140:6443 \\ --kubeconfig=kube-proxy.kubeconfig # 配置客户端认证 kubectl config set-credentials kube-proxy \\ --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \\ --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig # 配置关联 kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig # 配置默认关联 kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig # 拷贝到目录 mv kube-proxy.kubeconfig /etc/kubernetes/ ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:43:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"创建 kube-proxy.service 文件 # 创建 kube-proxy 目录 mkdir -p /var/lib/kube-proxy vi /etc/systemd/system/kube-proxy.service [Unit] Description=kubernetes Kube-Proxy Server Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=network.target [Service] WorkingDirectory=/var/lib/kube-proxy ExecStart=/usr/local/bin/kube-proxy \\ --bind-address=10.6.0.187 \\ --hostname-override=10.6.0.187 \\ --cluster-cidr=10.254.0.0/16 \\ --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \\ --logtostderr=true \\ --v=2 Restart=on-failure RestartSec=5 LimitNOFILE=65536 [Install] WantedBy=multi-user.target ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:44:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"启动 kube-proxy systemctl daemon-reload systemctl enable kube-proxy systemctl start kube-proxy systemctl status kube-proxy ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:45:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"部署其他Node 节点 (第二个节点部署 10.6.0.188) .........省略了....... 参照以上配置 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:46:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"测试集群 # 创建一个 nginx deplyment apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-master-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-2214564181-lxff5 1/1 Running 0 14m nginx-dm-2214564181-qm1bp 1/1 Running 0 14m [root@k8s-master-1 ~]# kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-dm 2 2 2 2 14m [root@k8s-master-1 ~]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.254.0.1 \u003cnone\u003e 443/TCP 4h nginx-svc 10.254.129.54 \u003cnone\u003e 80/TCP 15m # 在 node 里 curl [root@k8s-node-1 ~]# curl 10.254.129.54 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e 配置 KubeDNS 官方 github yaml 相关 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:47:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"下载镜像 # 官方镜像 gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.4 gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.4 gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.4 # 我的镜像 jicki/k8s-dns-sidecar-amd64:1.14.4 jicki/k8s-dns-kube-dns-amd64:1.14.4 jicki/k8s-dns-dnsmasq-nanny-amd64:1.14.4 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:48:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-cm.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-sa.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-controller.yaml.base curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/kubedns-svc.yaml.base # 修改后缀 mv kubedns-controller.yaml.base kubedns-controller.yaml mv kubedns-svc.yaml.base kubedns-svc.yaml ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:49:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"系统预定义的 RoleBinding 预定义的 RoleBinding system:kube-dns 将 kube-system 命名空间的 kube-dns ServiceAccount 与 system:kube-dns Role 绑定， 该 Role 具有访问 kube-apiserver DNS 相关 API 的权限； [root@k8s-master-1 kubedns]# kubectl get clusterrolebindings system:kube-dns -o yaml apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: annotations: rbac.authorization.kubernetes.io/autoupdate: \"true\" creationTimestamp: 2017-07-04T04:15:13Z labels: kubernetes.io/bootstrapping: rbac-defaults name: system:kube-dns resourceVersion: \"106\" selfLink: /apis/rbac.authorization.k8s.io/v1beta1/clusterrolebindings/system%3Akube-dns uid: 60c1e0e1-606f-11e7-b212-d4ae52d1f0c9 roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: system:kube-dns subjects: - kind: ServiceAccount name: kube-dns namespace: kube-system ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:50:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"修改 kubedns-svc.yaml # kubedns-svc.yaml 中 clusterIP: __PILLAR__DNS__SERVER__ 修改为我们之前定义的 dns IP 10.254.0.2 cat kubedns-svc.yaml apiVersion: v1 kind: Service metadata: name: kube-dns namespace: kube-system labels: k8s-app: kube-dns kubernetes.io/cluster-service: \"true\" addonmanager.kubernetes.io/mode: Reconcile kubernetes.io/name: \"KubeDNS\" spec: selector: k8s-app: kube-dns clusterIP: 10.254.0.2 ports: - name: dns port: 53 protocol: UDP - name: dns-tcp port: 53 protocol: TCP ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:51:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"修改 kubedns-controller.yaml 1. # 修改 --domain=__PILLAR__DNS__DOMAIN__. 为 我们之前 预定的 domain 名称 --domain=cluster.local. 2. # 修改 --server=/__PILLAR__DNS__DOMAIN__/127.0.0.1#10053 中 domain 为我们之前预定的 --server=/cluster.local./127.0.0.1#10053 3. # 修改 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local., 4. # 修改 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.__PILLAR__DNS__DOMAIN__, 中的 domain 为我们之前预定的 --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local., ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:52:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 [root@k8s-master-1 kubedns]# kubectl create -f . configmap \"kube-dns\" created deployment \"kube-dns\" created serviceaccount \"kube-dns\" created service \"kube-dns\" created ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:53:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"查看 kubedns 服务 [root@k8s-master-1 kubedns]# kubectl get all --namespace=kube-system NAME READY STATUS RESTARTS AGE po/kube-dns-1511229508-llfgs 3/3 Running 0 1m NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/kube-dns 10.254.0.2 \u003cnone\u003e 53/UDP,53/TCP 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/kube-dns 1 1 1 1 1m NAME DESIRED CURRENT READY AGE rs/kube-dns-1511229508 1 1 1 1m ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:54:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"验证 dns 服务 # 导入之前的 nginx-dm yaml文件 [root@k8s-master-1 ~]# kubectl get svc nginx-svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc 10.254.79.137 \u003cnone\u003e 80/TCP 29s # 创建一个 pods 来测试一下 nameserver apiVersion: v1 kind: Pod metadata: name: alpine spec: containers: - name: alpine image: alpine command: - sh - -c - while true; do sleep 1; done # 查看 pods [root@k8s-master-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE alpine 1/1 Running 0 1m nginx-dm-2214564181-4zjbh 1/1 Running 0 5m nginx-dm-2214564181-tpz8t 1/1 Running 0 5m # 测试 [root@k8s-master-1 ~]# kubectl exec -it alpine ping nginx-svc PING nginx-svc (10.254.207.143): 56 data bytes [root@k8s-master-1 ~]# kubectl exec -it alpine nslookup nginx-svc nslookup: can't resolve '(null)': Name does not resolve Name: nginx-svc Address 1: 10.254.207.143 nginx-svc.default.svc.cluster.local 部署 Ingress 与 Dashboard ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:55:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"部署 dashboard 官方 dashboard 的github https://github.com/kubernetes/dashboard 这里注意，以下部署的应用为 api-service 关闭了 RBAC 的， 在开启了 RBAC 的情况下，无论是 dashboard 与 nginx ingress 都需要修改，默认是有问题的。 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:56:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"下载 dashboard 镜像 # 官方镜像 gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.1 # 国内镜像 jicki/kubernetes-dashboard-amd64:v1.6.1 ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:57:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-controller.yaml curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dashboard/dashboard-service.yaml ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:58:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"导入 yaml [root@k8s-master-1 dashboard]# kubectl apply -f . deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created # 查看 svc 与 pod [root@k8s-master-1 dashboard]# kubectl get svc -n kube-system kubernetes-dashboard NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard 10.254.167.28 \u003cnone\u003e 80/TCP 31s ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:59:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["kubernetes"],"content":"部署 Nginx Ingress kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 Nginx Haproxy 等负载均衡工具来暴露 kubernetes 服务。 官方 Nginx Ingress github https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx # 下载镜像 # 官方镜像 gcr.io/google_containers/defaultbackend:1.0 gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.10 # 国内镜像 jicki/defaultbackend:1.0 jicki/nginx-ingress-controller:0.9.0-beta.10 # 部署 Nginx backend , Nginx backend 用于统一转发 没有的域名 到指定页面。 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/default-backend.yaml # 直接导入既可, 这里不需要修改 [root@k8s-master-1 ingress]# kubectl apply -f default-backend.yml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看服务 [root@k8s-master-1 ingress]# kubectl get deployment -n kube-system default-http-backend NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 36s # 部署 Ingress Controller 组件 # 下载 yaml 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/daemonset/nginx/nginx-ingress-daemonset.yaml # 导入 yaml 文件 [root@k8s-master-1 ingress]# kubectl apply -f nginx-ingress-daemonset.yaml daemonset \"nginx-ingress-lb\" created # 查看服务 [root@k8s-master-1 ingress]# kubectl get daemonset -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE-SELECTOR AGE nginx-ingress-lb 2 2 2 2 2 \u003cnone\u003e 11s # 创建一个 ingress # 查看我们原有的 svc [root@k8s-master-1 Ingress]# kubectl get svc nginx-svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-svc 10.254.207.143 \u003cnone\u003e 80/TCP 1d # 创建 yaml 文件 vi nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-svc servicePort: 80 # 导入 yaml [root@k8s-master-1 ingress]# kubectl apply -f nginx-ingress.yaml ingress \"nginx-ingress\" created # 查看 ingress [root@k8s-master-1 Ingress]# kubectl get ingress NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 10.6.0.187,10... 80 24s # 测试访问 [root@k8s-master-1 ingress]# curl -I nginx.jicki.cn HTTP/1.1 200 OK Server: nginx/1.13.2 Date: Thu, 06 Jul 2017 04:21:43 GMT Content-Type: text/html Content-Length: 612 Connection: keep-alive Last-Modified: Wed, 28 Jun 2017 18:27:36 GMT ETag: \"5953f518-264\" Accept-Ranges: bytes # 配置一个 Dashboard Ingress # 查看 dashboard 的 svc [root@k8s-master-1 ingress]# kubectl get svc -n kube-system kubernetes-dashboard NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard 10.254.81.94 \u003cnone\u003e 80/TCP 2h # 编辑一个 yaml 文件 vi dashboard-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: dashboard-ingress namespace: kube-system spec: rules: - host: dashboard.jicki.cn http: paths: - backend: serviceName: kubernetes-dashboard servicePort: 80 # 查看 ingress [root@k8s-master-1 dashboard]# kubectl get ingress -n kube-system NAME HOSTS ADDRESS PORTS AGE dashboard-ingress dashboard.jicki.cn 10.6.0.187,10... 80 1m # 测试访问 [root@k8s-master-1 dashboard]# curl -I dashboard.jicki.cn HTTP/1.1 200 OK Server: nginx/1.13.2 Date: Thu, 06 Jul 2017 06:32:00 GMT Content-Type: text/html; charset=utf-8 Content-Length: 848 Connection: keep-alive Accept-Ranges: bytes Cache-Control: no-store Last-Modified: Tue, 16 May 2017 12:53:01 GMT ","date":"2017-07-04","objectID":"/kubernetes-1.7.0/:60:0","tags":null,"title":"kubernetes 1.7.0 + flannel 二进制部署","uri":"/kubernetes-1.7.0/"},{"categories":["jenkins"],"content":"jenkins gradle docker 持续集成","date":"2017-06-06","objectID":"/jenkins-gradle-build/","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"部署 jenkins 基于 jenkins 持续集成 自动打包 构建镜像 更新版本 ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:0:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"初始化环境 # 安装 open-jdk yum -y install java # 安装 kubectl yum install -y kubectl # 拷贝 master 证书到 本机 证书文件一般在 /etc/kubernetes/ssl 下 需要拷贝的证书有 ca.pem admin.pem admin-key.pem # 执行如下脚本 vi master.sh #!/bin/bash KUBE_API_SERVER=\"https://master-api\" CERT_DIR=${CERT_DIR-\".\"} kubectl config set-cluster default-cluster --server=${KUBE_API_SERVER} \\ --certificate-authority=${CERT_DIR}/ca.pem kubectl config set-credentials default-admin \\ --certificate-authority=${CERT_DIR}/ca.pem \\ --client-key=${CERT_DIR}/admin-key.pem \\ --client-certificate=${CERT_DIR}/admin.pem kubectl config set-context default-system --cluster=default-cluster --user=default-admin kubectl config use-context default-system # 注意修改 master api 地址 # 这里一定要切换 jenkins 用户，否则执行报错 su jenkins sh master.sh # 安装 docker 这里略过了 # 配置 jenkins 的 docker 权限 vim /etc/sudoers ## Allow root to run any commands anywhere root ALL=(ALL) ALL jenkins ALL=(ALL) ALL # 加到 root 组里 usermod -aG docker jenkins # 这里要注意，如果你之前安装了jenkins 修改以后要重启jenkins ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:1:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"安装 jenkins # 导入 jenkins yum 源 wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat-stable/jenkins.repo # 导入 key rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key # 安装 jenkins yum -y install jenkins ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:2:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"配置 jenkins # 修改 默认目录 vi /etc/sysconfig/jenkins JENKINS_HOME=\"/opt/jenkins\" # JVM 调优 (4G内存限制2G内存) JENKINS_JAVA_OPTIONS=\"-Djava.awt.headless=true -Xms1024m -Xmx2048m -XX:PermSize=512m -XX:MaxPermSize=1024m\" # 创建 目录 mkdir -p /opt/jenkins # 授权 chown -R jenkins:jenkins /opt/jenkins ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:3:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"启动 服务 systemctl start jenkins chkconfig jenkins on ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:4:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"访问测试 http://myip:8080/ # 查看初始化密码 cat /opt/jenkins/secrets/initialAdminPassword 8e082d8cd85e4207a148bf2429e32f59 ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:5:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"配置 用户密钥 # 打开 jenkins 用户 bash vi /etc/passwd jenkins 用户 /var/lib/jenkins 修改为 /home/jenkins /bin/false 修改为 /bin/bash # 创建 home 目录 mkdir /home/jenkins # 授权 chown -R jenkins:jenkins /home/jenkins # 切换用户 su jenkins # 生成key ssh-keygen -t rsa -b 4096 -C \"jenkins@git\" # 查看 公钥 cat /home/jenkins/.ssh/id_rsa.pub ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:6:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"添加 Credentials # 登陆 web ui --\u003e Credentials --\u003e System --\u003e Global Credentials --\u003e Add Credentials Kind: SSH Username with private key Scope: Global(Jenkins, nodes, items, all child tiems, etc) Username: jenkins-git Private Key: Form the Jenkins master ~/.ssh Passphrase: ID: Decription: jenins-git ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:7:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"添加 插件 # 登陆 web ui --\u003e 系统管理 --\u003e 插件管理 1. Pipeline 2. Gradle Plugin 3. Git plugin 4. Build WIth Parameters (构建 输入参数的插件) 5. Email Extension Plugin (邮件 发送 插件) 6. Multiple SCMs Plugin (多 git 版本库 同时构建) 7. Git Parameter Plug-In ( git 分支 构建选择) 8. description setter plugin ( 配置 Build History 显示具体信息 ) 9. user build vars plugin ( 显示 构建用户名 而非 id ) ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:8:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"配置 全局组件 # 登陆 web ui --\u003e 系统管理 --\u003e Global Tool Configuration JDK: (新增JDK) JDK: orace-jdk-8 JAVA_HOME: /usr/java/jdk1.8.0_131/ Git: Name: git-1.8.3 Path to Git executable: /usr/bin/git Gradle: Name: gradle-2.5 GRADLE_HOME: /opt/gradle Docker: Name: docker-1.13.1 Installation root: /opt/docker (docker info |grep Root) ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:9:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"配置插件 # 登陆 web ui --\u003e 系统管理 --\u003e 系统设置 Extended E-mail Notification: (没有标注 表示为空) # SMTP server: xxxx.mail.com # Default Content Type: HTML(text/html) # Default Subject: Default Subject = 构建通知:$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS! # Default Content: \u003cb style=\"font-size:12px\"\u003e(本邮件是程序自动下发的，请勿回复，\u003cspan style=\"color:red\"\u003e请相关人员fix it,重新提交到git 构建\u003c/span\u003e)\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e项目名称：$PROJECT_NAME\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建编号：$BUILD_NUMBER\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003eGIT版本号：${GIT_REVISION}\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建状态：$BUILD_STATUS\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e触发原因：${CAUSE}\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建日志地址：\u003ca href=\"${BUILD_URL}console\"\u003e${BUILD_URL}console\u003c/a\u003e\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建地址：\u003ca href=\"$BUILD_URL\"\u003e$BUILD_URL\u003c/a\u003e\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e变更集:${JELLY_SCRIPT,template=\"html\"}\u003cbr\u003e\u003c/b\u003e\u003chr\u003e Default Triggers: Failure - Any Success ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:10:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"创建项目 # 创建一个 自由风格 项目 Enter an item name: java test General: 参数化构建过程 --\u003e 添加参数 --\u003e String Parameter: # 这里定义一个 参数，用来选择打包的项目，因为我这边一个项目下有多个war包，这里我是用 gradle 构建项目的，其他工具可以自行处理。 名字: models 默认值:(这里取settings.gradle 中的 include, 如下只是测试，只做参考) \"java_1\",\"java_2\" 描述: 填写需要打包的项目 格式为： \"项目名称\" 多个项目以, 号隔开 如： \"java_1\" 或者 \"java_1\",\"java_2\" 参数化构建过程 --\u003e 添加参数 --\u003e Git Parameter: Name: git_tag Parameter Type: Branch or Tag Default Value: origin/master # 源码管理 ( 这里需要填写上面参数化构建 Git Parameter 名称) Git: Branches to build Branch Specitfier (blank for 'any'): $git_tag ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:11:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"Build Environment # 添加 user 的 var #勾选如下: Set jenkins user build variables # 变量说明如下: This plugin is used to set user build variables: BUILD_USER -- full name of user started build, BUILD_USER_FIRST_NAME -- first name of user started build, BUILD_USER_LAST_NAME -- last name of user started build, BUILD_USER_ID -- id of user started build. BUILD_USER_EMAIL -- email of user started build. ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:12:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["jenkins"],"content":"构建 1. 选择一个 Execute shell Command: (这里是设置选择构建项目，因为有时候一个git版本库下有多个项目，需要分开来构建) sed -i '/include*/d' ${WORKSPACE}/settings.gradle echo \"include $models\" \u003e\u003e ${WORKSPACE}/settings.gradle 2. 选择Invoke Gradle Script Gradle Version: gradle-2.5 Tasks: clean war --stacktrace --debug 3. Execute Shell Command: mv ${WORKSPACE}/java_*/build/libs/*.war /opt/jenkins/war/ ls -lt /opt/jenkins/war/ 4. Execute Shell Command: #!/bin/sh set -e war_home=/opt/jenkins/war date=`date +%y%m%d%M` cd $war_home rm -rf dockerfile for war in $(ls);do cat \u003e dockerfile \u003c\u003c EOF FROM service/tomcat add $war /opt/htdocs/webapp/ EOF echo \"\u003e\u003e\u003e cat dockerfile \u003c\u003c\u003c\" cat dockerfile img_name=\"$war|sed -e 's/\\.war//g;s/_/-/g'\" image_name=$(eval echo $img_name) # echo $image_name docker build -t=\"job/$image_name:$date\" . docker push job/$image_name:$date done docker images |grep \"$date\" 5. Execute Shell Command: rm -rf /opt/jenkins/war/* 构建后操作 (Post-build Actions) # 构建后操作 (这里只写需要更改的) Editable Email Notification: Content Type: HTML (text/html) # 点击下面的 Advanced Settings.. Triggers: Failure - Any 高级... Recipient List: jicki@qq.com Success 高级... Recipient List: jicki@qq.com # 添加 Set build description Regular expression: 留空 Description: 本次发布由 \u003cspan style=\"color:#E53333;\"\u003e\u003cstrong\u003e$BUILD_USER\u003c/strong\u003e\u003c/span\u003e 发起，Git 分支\u003cspan style=\"color:#E53333;\"\u003e\u003cstrong\u003e $git_tag \u003c/strong\u003e\u003c/span\u003e ","date":"2017-06-06","objectID":"/jenkins-gradle-build/:13:0","tags":null,"title":"jenkins gradle docker 持续集成","uri":"/jenkins-gradle-build/"},{"categories":["kubernetes"],"content":"kargo kubernetes 1.6.4","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"1、初始化环境 kargo update k8s 1.6.4 ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:0:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"1.1、环境： 节点 IP 角色 node-1 10.6.0.52 Master node-2 10.6.0.53 Master node-3 10.6.0.55 Node node-4 10.6.0.56 Node ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:1:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"1.2、配置SSH Key 登陆 # 确保本机也可以 ssh 连接，否则下面部署失败 ssh-keygen -t rsa -N \"\" ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.52 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.53 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.55 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.56 2、获取 Kargo Kargo 官方github https://github.com/kubernetes-incubator/kargo ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:2:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"2.1、安装基础软件 Kargo 是基于 ansible 统一部署，所以必须安装 ansible # 安装 centos 额外的yum源 yum install -y epel-release # 安装 软件 yum install -y python-pip python34 python-netaddr python34-pip ansible # 如果 报 no test named 'equalto' ，需要升级 Jinja2 pip install --upgrade Jinja2 ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:3:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"2.2、获取源码 git clone https://github.com/kubernetes-incubator/kargo ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:4:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"2.3、编辑配置文件 cd kargo vim inventory/group_vars/k8s-cluster.yml 这里主要修改一些 网段，密码 等信息 ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:5:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"2.4、生成集群配置文件 cd kargo CONFIG_FILE=inventory/inventory.cfg python3 contrib/inventory_builder/inventory.py 10.6.0.52 10.6.0.53 10.6.0.55 10.6.0.56 # 输入如下： DEBUG: Adding group all DEBUG: Adding group kube-master DEBUG: Adding group kube-node DEBUG: Adding group etcd DEBUG: Adding group k8s-cluster:children DEBUG: Adding group calico-rr DEBUG: adding host node1 to group all DEBUG: adding host node2 to group all DEBUG: adding host node3 to group all DEBUG: adding host node4 to group all DEBUG: adding host kube-node to group k8s-cluster:children DEBUG: adding host kube-master to group k8s-cluster:children DEBUG: adding host node1 to group etcd DEBUG: adding host node2 to group etcd DEBUG: adding host node3 to group etcd DEBUG: adding host node1 to group kube-master DEBUG: adding host node2 to group kube-master DEBUG: adding host node1 to group kube-node DEBUG: adding host node2 to group kube-node DEBUG: adding host node3 to group kube-node DEBUG: adding host node4 to group kube-node # 生成的配置文件在当前目录，既 kargo/inventory 目录下 inventory.cfg # 配置文件如下(默认配置双master，可自行修改)： # SSH 非 22 端口 添加 ansible_port=xxx [all] node1 ansible_host=10.6.0.52 ansible_port=33 ip=10.6.0.52 node2 ansible_host=10.6.0.53 ansible_port=33 ip=10.6.0.53 node3 ansible_host=10.6.0.55 ansible_port=33 ip=10.6.0.55 node4 ansible_host=10.6.0.56 ansible_port=33 ip=10.6.0.56 [kube-master] node1 node2 [kube-node] node1 node2 node3 node4 [etcd] node1 node2 node3 [k8s-cluster:children] kube-node kube-master [calico-rr] # 1.6.4 镜像下载 http://pan.baidu.com/s/1nvUc5mx ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:6:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"2.5、部署集群 # 执行如下命令，请确保SSH KEY 登陆, 端口一致 ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa # 升级的命令，没有确认 ansible-playbook upgrade-cluster.yml -b -i inventory/inventory.cfg -e kube_version=v1.6.4 -v --private-key=~/.ssh/id_rsa ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:7:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"2.6、测试 # 两个 master 中使用 kubectl get nodes [root@k8s-node-1 ~]# kubectl get nodes NAME STATUS AGE VERSION k8s-node-1 Ready 16m v1.6.4+coreos.0 k8s-node-2 Ready 20m v1.6.4+coreos.0 k8s-node-3 Ready 16m v1.6.4+coreos.0 k8s-node-4 Ready 16m v1.6.4+coreos.0 [root@k8s-node-2 ~]# kubectl get nodes NAME STATUS AGE VERSION k8s-node-1 Ready 11m v1.6.4+coreos.0 k8s-node-2 Ready 16m v1.6.4+coreos.0 k8s-node-3 Ready 11m v1.6.4+coreos.0 k8s-node-4 Ready 11m v1.6.4+coreos.0 [root@k8s-node-1 ~]# kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE dnsmasq-411420702-z0gkx 1/1 Running 0 16m dnsmasq-autoscaler-1155841093-1hxdl 1/1 Running 0 16m elasticsearch-logging-v1-kgt1t 1/1 Running 0 15m elasticsearch-logging-v1-vm4bd 1/1 Running 0 15m fluentd-es-v1.22-6gql6 1/1 Running 0 15m fluentd-es-v1.22-8zkjh 1/1 Running 0 15m fluentd-es-v1.22-cjskv 1/1 Running 0 15m fluentd-es-v1.22-j4857 1/1 Running 0 15m kibana-logging-2924323056-x3vjk 1/1 Running 0 15m kube-apiserver-k8s-node-1 1/1 Running 0 15m kube-apiserver-k8s-node-2 1/1 Running 0 20m kube-controller-manager-k8s-node-1 1/1 Running 0 16m kube-controller-manager-k8s-node-2 1/1 Running 0 21m kube-proxy-k8s-node-1 1/1 Running 0 16m kube-proxy-k8s-node-2 1/1 Running 0 21m kube-proxy-k8s-node-3 1/1 Running 0 16m kube-proxy-k8s-node-4 1/1 Running 0 16m kube-scheduler-k8s-node-1 1/1 Running 0 16m kube-scheduler-k8s-node-2 1/1 Running 0 21m kubedns-3830354952-pfl7n 3/3 Running 4 16m kubedns-autoscaler-54374881-64x6d 1/1 Running 0 16m nginx-proxy-k8s-node-3 1/1 Running 0 16m nginx-proxy-k8s-node-4 1/1 Running 0 16m [root@k8s-node-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE netchecker-agent-3x3sj 1/1 Running 0 16m netchecker-agent-ggxs2 1/1 Running 0 16m netchecker-agent-hostnet-45k84 1/1 Running 0 16m netchecker-agent-hostnet-kwvc8 1/1 Running 0 16m netchecker-agent-hostnet-pwm77 1/1 Running 0 16m netchecker-agent-hostnet-z4gmq 1/1 Running 0 16m netchecker-agent-q3291 1/1 Running 0 16m netchecker-agent-qtml6 1/1 Running 0 16m netchecker-server 1/1 Running 0 16m # 配置一个 nginx deplyment 与 nginx service apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 apiVersion: v1 kind: Service metadata: name: nginx-dm spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx.yaml deployment \"nginx-dm\" created service \"nginx-dm\" created [root@k8s-node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-4194680597-0h071 1/1 Running 0 9m 10.233.75.8 k8s-node-4 nginx-dm-4194680597-dzcf3 1/1 Running 0 9m 10.233.76.124 k8s-node-3 [root@k8s-node-1 ~]# kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.233.0.1 \u003cnone\u003e 443/TCP 39m \u003cnone\u003e netchecker-service 10.233.0.126 \u003cnodes\u003e 8081:31081/TCP 33m app=netchecker-server nginx-dm 10.233.56.138 \u003cnone\u003e 80/TCP 10m name=nginx # 部署一个 curl 的 pods 用来测试 内部通信 apiVersion: v1 kind: Pod metadata: name: curl spec: containers: - name: curl image: radial/busyboxplus:curl command: - sh - -c - while true; do sleep 1; done # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f curl.yaml pod \"curl\" created [root@k8s-node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE curl 1/1 Running 0 2m 10.233.75.22 k8s-node-4 # 测试 curl --\u003e nginx-svc [root@k8s-node-1 ~]# kubectl exec -it curl curl nginx-dm \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial supp","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:8:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"2.7、部署一个 Nginx Ingress kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 nginx haproxy 等负载均衡工具来暴露 kubernetes 服务。 # 首先 部署一个 http-backend, 用于统一转发 没有的域名 到指定页面。 # 官方 nginx ingress 库 https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx # 下载官方的 nginx backend 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/default-backend.yaml # 直接导入既可 [root@k8s-node-1 ~]# kubectl apply -f default-backend.yaml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看 deployment 与 service [root@k8s-node-1 ~]# kubectl get deployment --namespace=kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 33m [root@k8s-node-1 ~]# kubectl get svc --namespace=kube-system NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE default-http-backend 10.233.20.232 \u003cnone\u003e 80/TCP 33m # 部署 Ingress Controller 组件 # 下载 官方 nginx-ingress-controller 的yaml文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml # 编辑 yaml 文件，打开 hostNetwork: true , 将端口绑定到宿主机中 # 这里面deployment 默认只启动了一个pods, 这里可以修改 kind: Deployment 为 kind: DaemonSet 并注释掉 replicas # 或者 修改 replicas: 1 为 N vi nginx-ingress-controller.yaml 将 hostNetwork: true 前面的注释去掉 # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx-ingress-controller.yaml deployment \"nginx-ingress-controller\" created # 查看 deployment 或者 daemonsets [root@k8s-node-1 yaml]# kubectl get deployment --namespace=kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-ingress-controller 1 1 1 1 31s [root@k8s-node-1 yaml]# kubectl get daemonsets --namespace=kube-system NAME DESIRED CURRENT READY NODE-SELECTOR AGE nginx-ingress-controller 4 4 4 \u003cnone\u003e 1m # 最后开始 部署 Ingress # 这里请先看看官方 ingress 的 yaml 写法 # https://kubernetes.io/docs/user-guide/ingress/ # 我们使用 之前创建的 nginx-dm service，我们来写一个 ingress # 首先查看一下 svc [root@k8s-node-1 yaml]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.233.0.1 \u003cnone\u003e 443/TCP 1d netchecker-service 10.233.0.126 \u003cnodes\u003e 8081:31081/TCP 1d nginx-dm 10.233.56.138 \u003cnone\u003e 80/TCP 1d zookeeper-1 10.233.25.46 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d zookeeper-2 10.233.49.4 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d zookeeper-3 10.233.50.206 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d # 创建 yaml 文件， 这里特别注意，如果 svc 在 kube-system 下 # 必须在 metadata: 下面添加 namespace: kube-system 指定命名空间 vim nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-dm servicePort: 80 # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx-ingress.yaml ingress \"nginx-ingress\" created # 查看一下 创建的 ingress [root@k8s-node-1 ~]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 17s # 这里显示 ADDRESS 为 空 实际上 所有 master 与 nodes 都绑定了 # 将域名解析到 任何一个 IP 上都可以。 # 下面访问 http://nginx.jicki.cn/ # 这里注意，Ingresses 只做简单的端口转发。 维护 FAQ # 卸载 cd kargo ansible-playbook -i inventory/inventory.cfg reset.yml -b -v --private-key=~/.ssh/id_rsa # 增加节点 # 首先编辑 inventory/inventory.cfg 增加一个节点 例：node5 [kube-node] node1 node2 node3 node4 node5 # 执行命令 使用 --limit 参数 ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa --limit node5 # 报错1 # hostname 的问题 # 部署 kargo 必须配置 hostname 否则 多 master 会出现 无法创建 api 等 pods # 如果 执行了 ansible-playbook 之前没改 hostname 必须删除 /tmp 下的 node[N] # 否则更改 /etc/hosts 失败 # 报错 2 TASK [vault : check_vault | Set fact about the Vault cluster's initialization state] *** Monday 10 April 2017 17:47:42 +0800 (0:00:00.088) 0:01:22.030 ********** fatal: [node1]: FAILED! =\u003e {\"failed\": true, \"msg\": \"'dict object' has no attribute 'vault'\"} fatal: [node3]: FAILED! =\u003e {\"failed\": true, \"msg\": \"'dict object' has no attribute 'vault'\"} # 解决方案 升级 ansible =\u003e 2.2.1.0 ","date":"2017-06-06","objectID":"/kargo-k8s-1.6.4/:9:0","tags":null,"title":"kargo kubernetes 1.6.4","uri":"/kargo-k8s-1.6.4/"},{"categories":["kubernetes"],"content":"kubernetes Grafana2","date":"2017-05-11","objectID":"/kubernetes-grafana2/","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":" kubernetes 基于 Grafana2 监控 ","date":"2017-05-11","objectID":"/kubernetes-grafana2/:0:0","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":"下载 yaml 文件 mkdir grafana curl -O https://github.com/jicki/kuberneres/blob/master/grafana/grafana-deployment.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/grafana/grafana-service.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/grafana/heapster-deployment.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/grafana/heapster-service.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/grafana/influxdb-deployment.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/grafana/influxdb-service.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/grafana/monitoring2-namespace.yaml ","date":"2017-05-11","objectID":"/kubernetes-grafana2/:1:0","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":"编辑 yaml 文件 cd grafana # 这里只需要编辑 influxdb-deployment.yaml # 这里面的 volume 需要修改为自己的配置 ","date":"2017-05-11","objectID":"/kubernetes-grafana2/:2:0","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":"下载 镜像 # 提前下载镜像，因为被墙，你懂得 http://pan.baidu.com/s/1kViIjaR ","date":"2017-05-11","objectID":"/kubernetes-grafana2/:3:0","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":"导入 yaml 文件 kubectl apply -f grafana ","date":"2017-05-11","objectID":"/kubernetes-grafana2/:4:0","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":"查看导入 kubectl get all --namespace=monitoring NAME READY STATUS RESTARTS AGE po/grafana-434744905-ffzd6 1/1 Running 0 2h po/heapster-1110581374-93kwc 1/1 Running 0 2h po/influxdb-149151442-f890s 1/1 Running 0 2h NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE svc/grafana2 10.233.17.138 \u003cnodes\u003e 3002:30002/TCP 2h svc/heapster 10.233.43.80 \u003cnone\u003e 80/TCP 2h svc/influxdb-svc 10.233.39.147 \u003cnone\u003e 8086/TCP 2h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deploy/grafana 1 1 1 1 2h deploy/heapster 1 1 1 1 2h deploy/influxdb 1 1 1 1 2h NAME DESIRED CURRENT READY AGE rs/grafana-434744905 1 1 1 2h rs/heapster-1110581374 1 1 1 2h rs/influxdb-149151442 1 1 1 2h ","date":"2017-05-11","objectID":"/kubernetes-grafana2/:5:0","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":"测试 http://node-ip:30002/ ","date":"2017-05-11","objectID":"/kubernetes-grafana2/:6:0","tags":null,"title":"kubernetes Grafana2","uri":"/kubernetes-grafana2/"},{"categories":["kubernetes"],"content":"kubernetes EFK","date":"2017-05-10","objectID":"/kubernetes-efk/","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"kubernetes EFK ","date":"2017-05-10","objectID":"/kubernetes-efk/:0:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"初始化环境 # 增加max_map_count echo 'vm.max_map_count=262144' \u003e\u003e /etc/sysctl.conf sysctl -p ","date":"2017-05-10","objectID":"/kubernetes-efk/:1:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"配置 namespace vi logging-namespace.yaml --- apiVersion: v1 kind: Namespace metadata: name: logging ","date":"2017-05-10","objectID":"/kubernetes-efk/:2:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"配置 elasticsearch # 增加 elasticsearch-deployment.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/efk/elasticsearch-deployment.yaml # 这里需要按需修改 # JVM 配置 \"-Xms4g -Xmx4g\" # volume 配置 volumes: - name: es-volume persistentVolumeClaim: claimName: efk-claim # 增加 elasticsearch-service.yaml 文件 curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/efk/elasticsearch-service.yaml ","date":"2017-05-10","objectID":"/kubernetes-efk/:3:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"配置 fluentd # 增加 fluentd-daemonset.yaml # td-agent 这一段 配置输入，请按需配置 curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/efk/fluentd-daemonset.yaml ","date":"2017-05-10","objectID":"/kubernetes-efk/:4:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"配置 kibana # 增加 kibana-deployment.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/efk/kibana-deployment.yaml # 增加 kibana-service.yaml curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/efk/kibana-service.yaml ","date":"2017-05-10","objectID":"/kubernetes-efk/:5:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"导入 yaml # 导入 kubectl apply -f logging-namespace.yaml kubectl apply -f fluentd-daemonset.yaml kubectl apply -f elasticsearch-deployment.yaml kubectl apply -f elasticsearch-service.yaml kubectl apply -f kibana-deployment.yaml kubectl apply -f kibana-service.yaml # 查看导入 kubectl get all --namespace=logging -o wide ","date":"2017-05-10","objectID":"/kubernetes-efk/:6:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"测试 # elasticsearch 管理端口为 30200 http://node-ip:30200/ # kibana 访问页面 30601 http://node-ip:30601 # 1. 选择 @timestamps 点击 create # 2. curl -O https://raw.githubusercontent.com/jicki/kuberneres/master/efk/dashboards/elk-v1.json # 3. 选择 management \u003e Saved Object \u003e Import \u003e elk-v1.json # 4. 选择 Dashboard 勾选 ELK 并点击 ELK，等待加载。 ","date":"2017-05-10","objectID":"/kubernetes-efk/:7:0","tags":null,"title":"kubernetes EFK","uri":"/kubernetes-efk/"},{"categories":["kubernetes"],"content":"kubernetes Ceph RBD","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":" 使用 Ceph RBD 做为 kubernetes 后端存储 Ceph 安装部署 由于这里我们使用 RBD 所以我们使用到的组件为 Ceph.mon, Ceph.osd, 这两个组件就可以了。 Ceph.mds 为 cephfs 所需组件 ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:0:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"部署环境 # Ceph.Mon = 2n+1 个 ，3个的情况下只能掉线一个，如果同时2个掉线 # 集群会出现无法仲裁，集群会一直等待 Ceph.Mon 恢复超过半数。 172.16.1.37 Ceph.admin + Ceph.Mon 172.16.1.38 Ceph.Mon + Ceph.osd 172.16.1.39 Ceph.Mon + Ceph.osd 172.16.1.40 Ceph.osd ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:1:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"初始化环境 # 1. ceph 对时间要求很严格， 一定要同步所有的服务器时间 # 2. 配置 hosts cat \u003c\u003c \"EOF\" \u003e\u003e /etc/hosts # --- Ceph Hosts ----- 172.16.1.37 ceph-node-1 172.16.1.38 ceph-node-2 172.16.1.39 ceph-node-3 172.16.1.40 ceph-node-4 # --- Ceph Hosts ----- EOF # 3. 配置 hostname hostnamectl --static set-hostname ceph-node-1 hostnamectl --static set-hostname ceph-node-2 hostnamectl --static set-hostname ceph-node-3 hostnamectl --static set-hostname ceph-node-4 # 4. Ceph.admin 节点 配置无密码ssh ssh-keygen ssh-copy-id ceph-node-1 -p33 ssh-copy-id ceph-node-2 -p33 ssh-copy-id ceph-node-3 -p33 ssh-copy-id ceph-node-4 -p33 Now try logging into the machine, with: \"ssh -p '33' 'ceph-node-2'\" # 5. 增加 ~/.ssh/config 文件, 否则 ceph-deploy 创建集群 报端口错误 [root@ceph-node-1 ~]# vi ~/.ssh/config Host ceph-node-1 Hostname ceph-node-1 Port 33 Host ceph-node-2 Hostname ceph-node-2 Port 33 Host ceph-node-3 Hostname ceph-node-3 Port 33 Host ceph-node-4 Hostname ceph-node-4 Port 33 ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:2:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"安装 Ceph-deploy # 管理节点 安装 ceph-deploy 管理工具 # 配置 官方 的 Ceph 源 rpm --import https://download.ceph.com/keys/release.asc rpm -Uvh --replacepkgs https://download.ceph.com/rpm-jewel/el7/noarch/ceph-release-1-0.el7.noarch.rpm # 安装 epel 源 rpm -Uvh http://mirrors.ustc.edu.cn/centos/7/extras/x86_64/Packages/epel-release-7-9.noarch.rpm [root@ceph-node-1 ~]# yum makecache [root@ceph-node-1 ~]# yum -y install ceph-deploy ntpdate ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:3:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"创建 Ceph-Mon # 创建集群目录，用于存放配置文件，证书等信息 mkdir -p /opt/ceph-cluster cd /opt/ceph-cluster/ # 创建ceph-mon 节点 [root@ceph-node-1 ceph-cluster]# ceph-deploy new ceph-node-1 ceph-node-2 ceph-node-3 Monitor initial members are ['ceph-node-1', 'ceph-node-2', 'ceph-node-3'] # 查看配置文件 [root@ceph-node-1 ceph-cluster]# cat ceph.conf [global] fsid = 211a5d26-0377-4d1c-8555-c4793cef83c1 mon_initial_members = ceph-node-1, ceph-node-2, ceph-node-3 mon_host = 172.16.1.37,172.16.1.38,172.16.1.39 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx # 修改 osd 的副本数，既数据保存N份。 echo 'osd_pool_default_size = 2' \u003e\u003e ./ceph.conf # 注: 如果文件系统为 ext4 请添加 echo 'osd max object name len = 256' \u003e\u003e ./ceph.conf echo 'osd max object namespace len = 64' \u003e\u003e ./ceph.conf ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:4:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"安装 Ceph # 可通过 ceph-deploy 安装，也可以登陆node 本地安装 # 命令: ceph-deploy install {ceph-node}[{ceph-node} ...] [root@ceph-node-1 ceph-cluster]# ceph-deploy install ceph-node-1 ceph-node-2 ceph-node-3 ceph-node-4 # 使用 cpeh-deploy 安装，会一直使用官方源下载，超级慢 # 如果出现超时，请自行在每个服务器中使用 yum 安装 # 替换 ceph 源 为 163 源 sed -i 's/download\\.ceph\\.com/mirrors\\.163\\.com\\/ceph/g' /etc/yum.repos.d/ceph.repo yum -y install ceph ceph-radosgw # 检测 安装 [root@ceph-node-1 ceph-cluster]# ceph --version ceph version 10.2.7 (50e863e0f4bc8f4b9e31156de690d765af245185) ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:5:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"初始化 ceph-mon 节点 # 请务必在 ceph-cluster 目录下 [root@ceph-node-1 ceph-cluster]# ceph-deploy mon create-initial [ceph_deploy.gatherkeys][INFO ] Storing ceph.client.admin.keyring [ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mds.keyring [ceph_deploy.gatherkeys][INFO ] keyring 'ceph.mon.keyring' already exists [ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-osd.keyring [ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-rgw.keyring [ceph_deploy.gatherkeys][INFO ] Destroy temp directory /tmp/tmpNfXVDA ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:6:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"初始化 ceph.osd 节点 # 首先创建 存储空间, 如果使用分区，可略过 [root@ceph-node-2 ~]# mkdir -p /opt/ceph-osd-1 \u0026\u0026 chown ceph:ceph /opt/ceph-osd-1 [root@ceph-node-3 ~]# mkdir -p /opt/ceph-osd-2 \u0026\u0026 chown ceph:ceph /opt/ceph-osd-2 [root@ceph-node-4 ~]# mkdir -p /opt/ceph-osd-3 \u0026\u0026 chown ceph:ceph /opt/ceph-osd-3 # 启动 osd [root@ceph-node-1 ceph-cluster]# ceph-deploy osd prepare ceph-node-2:/opt/ceph-osd-1 ceph-node-3:/opt/ceph-osd-2 ceph-node-4:/opt/ceph-osd-3 # 激活 所有 osd 节点 [root@ceph-node-1 ceph-cluster]# ceph-deploy osd activate ceph-node-2:/opt/ceph-osd-1 ceph-node-3:/opt/ceph-osd-2 ceph-node-4:/opt/ceph-osd-3 # 把管理节点的配置文件与keyring同步至其它节点 [root@ceph-node-1 ceph-cluster]# ceph-deploy admin ceph-node-1 ceph-node-2 ceph-node-3 ceph-node-4 # 没有出现 红色的 ERROR # 查看状态 [root@ceph-node-1 ceph-cluster]# ceph -s cluster 7ab7e076-24e8-4236-bd4e-88675aab7365 health HEALTH_OK monmap e1: 3 mons at {ceph-node-1=172.16.1.37:6789/0,ceph-node-2=172.16.1.38:6789/0,ceph-node-3=172.16.1.39:6789/0} election epoch 6, quorum 0,1,2 ceph-node-1,ceph-node-2,ceph-node-3 osdmap e15: 3 osds: 3 up, 3 in flags sortbitwise,require_jewel_osds pgmap v24: 64 pgs, 1 pools, 0 bytes data, 0 objects 15628 MB used, 8957 GB / 9452 GB avail 64 active+clean # 查看 osd tree [root@ceph-node-1 ceph-cluster]# ceph osd tree ID WEIGHT TYPE NAME UP/DOWN REWEIGHT PRIMARY-AFFINITY -1 9.23126 root default -2 3.07709 host ceph-node-2 0 3.07709 osd.0 up 1.00000 1.00000 -3 3.07709 host ceph-node-3 1 3.07709 osd.1 up 1.00000 1.00000 -4 3.07709 host ceph-node-4 2 3.07709 osd.2 up 1.00000 1.00000 # 设置所有开机启动 systemctl enable ceph-osd.target systemctl enable ceph-mon.target # 重启系统以后重启 osd #查看处于down 的 osd ceph osd tree # 登陆所在node 启动ceph-osd进程 [id 为 tree 查看的 id] systemctl start ceph-osd@id kubernetes Volume Ceph RBD 官方 RDB 的文件 https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/rbd ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:7:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"k8s 安装 ceph-common # 1. 在所有的 k8s node 里面安装 ceph-common yum -y install ceph-common # 2. 拷贝 ceph.conf 与 ceph.client.admin.keyring 拷贝到 /etc/ceph/ 目录下 # 3. 配置 kubelet 增加ceph参数 # 这里最好是在创建 k8s 集群的时候就添加好 vim /usr/local/bin/kubelet # 增加 如下 -v /etc/ceph:/etc/ceph:ro \\ -v /sbin/modprobe:/sbin/modprobe:ro \\ -v /usr/sbin/modprobe:/usr/sbin/modprobe:ro \\ -v /lib/modules:/lib/modules:ro \\ # 重启 kubelet systemctl restart kubelet.service # 否则创建 pod 的时候 报错 with: rbd: failed to modprobe rbd error:exit status 1 ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:8:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"修改 ceph 配置 # 修改配置 # rbd image有4个 features，layering, exclusive-lock, object-map, fast-diff, deep-flatten 因为目前内核仅支持layering，修改默认配置。 [root@ceph-node-1 ceph-cluster]# echo 'rbd_default_features = 1' \u003e\u003e ./ceph.conf # 验证一下 [root@ceph-node-1 ceph-cluster]# ceph --show-config|grep rbd|grep features rbd_default_features = 1 # 把管理节点的配置文件与keyring同步至其它节点 [root@ceph-node-1 ceph-cluster]# ceph-deploy --overwrite-conf admin ceph-node-1 ceph-node-2 ceph-node-3 ceph-node-4 ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:9:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"创建 ceph-secret # 获取 client.admin 的值 [root@ceph-node-1 ceph-cluster]# ceph auth get-key client.admin AQBpUAxZGmDBBxAAVbgeRss9jv39dE0biTE7qQ== # 转换成 base64 编码 [root@ceph-node-1 ceph-cluster]# echo \"AQBpUAxZGmDBBxAAVbgeRss9jv39dE0biTE7qQ==\"|base64 QVFCcFVBeFpHbURCQnhBQVZiZ2VSc3M5anYzOWRFMGJpVEU3cVE9PQo= # 创建ceph-secret.yaml文件 apiVersion: v1 kind: Secret metadata: name: ceph-secret data: key: QVFCcFVBeFpHbURCQnhBQVZiZ2VSc3M5anYzOWRFMGJpVEU3cVE9PQo= # 导入 yaml 文件 [root@k8s-node-28 cephrbd]# kubectl apply -f ceph-secret.yaml secret \"ceph-secret\" created # 查看 状态 [root@k8s-node-28 cephrbd]# kubectl get secret |grep ceph ceph-secret Opaque 1 53s ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:10:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"创建 ceph image # 创建一个 1G 的 image [root@ceph-node-1 ceph-cluster]# rbd create test-image -s 1G # 查看 image [root@ceph-node-1 ceph-cluster]# rbd list test-image [root@ceph-node-1 ceph-cluster]# rbd info test-image rbd image 'test-image': size 1024 MB in 256 objects order 22 (4096 kB objects) block_name_prefix: rbd_data.373b2ae8944a format: 2 features: layering flags: # 这里 format 为 2 , 如果旧系统 不支持 format 2 ，可将 format 设置为 1 。 ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:11:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"创建一个 pv # 创建 test-pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: test-pv spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce rbd: monitors: - 172.16.1.37:6789 - 172.16.1.38:6789 - 172.16.1.38:6789 pool: rbd image: test-image user: admin secretRef: name: ceph-secret fsType: ext4 readOnly: false persistentVolumeReclaimPolicy: Recycle # 导入 yaml 文件 [root@k8s-node-28 cephrbd]# kubectl apply -f test-pv.yaml persistentvolume \"test-pv\" created # 查看 pv [root@k8s-node-28 cephrbd]# kubectl get pv |grep test-pv test-pv 1Gi RWO Recycle Available 33s ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:12:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"创建一个 pvc # vi test-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata: name: test-claim spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi # 导入 yaml 文件 [root@k8s-node-28 cephrbd]# kubectl apply -f test-pvc.yaml persistentvolumeclaim \"test-claim\" created # 查看 pvc [root@k8s-node-28 cephrbd]# kubectl get pvc |grep test test-claim Bound test-pv 1Gi RWO 31s ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:13:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"创建一个 deployment vi nginx-deplyment.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 1 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: ceph-rbd-volume mountPath: \"/usr/share/nginx/html\" volumes: - name: ceph-rbd-volume persistentVolumeClaim: claimName: test-claim # 导入 yaml 文件 [root@k8s-node-28 cephrbd]# kubectl apply -f nginx-deplyment.yaml deployment \"nginx-dm\" configured # 查看 pods [root@k8s-node-28 cephrbd]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-1697116629-j4m1g 1/1 Running 0 8m ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:14:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"测试 # 查看 分区文件 [root@k8s-node-28 cephrbd]# kubectl exec -it nginx-dm-1697116629-j4m1g -- df -h Filesystem Size Used Available Use% Mounted on overlay 115.2G 4.9G 104.4G 4% / tmpfs 62.8G 0 62.8G 0% /dev tmpfs 62.8G 0 62.8G 0% /sys/fs/cgroup /dev/mapper/centos-root 115.2G 4.9G 104.4G 4% /dev/termination-log /dev/mapper/centos-root 115.2G 4.9G 104.4G 4% /etc/resolv.conf /dev/mapper/centos-root 115.2G 4.9G 104.4G 4% /etc/hostname /dev/mapper/centos-root 115.2G 4.9G 104.4G 4% /etc/hosts shm 64.0M 0 64.0M 0% /dev/shm /dev/rbd0 975.9M 1.3M 907.4M 0% /usr/share/nginx/html tmpfs 62.8G 12.0K 62.8G 0% /var/run/secrets/kubernetes.io/serviceaccount tmpfs 62.8G 0 62.8G 0% /proc/kcore tmpfs 62.8G 0 62.8G 0% /proc/timer_list tmpfs 62.8G 0 62.8G 0% /proc/timer_stats tmpfs 62.8G 0 62.8G 0% /proc/sched_debug tmpfs 62.8G 0 62.8G 0% /sys/firmware ## 写入测试 [root@k8s-node-28 cephrbd]# kubectl exec -it nginx-dm-1697116629-j4m1g -- touch /usr/share/nginx/html/jicki.html # 删除 pod [root@k8s-node-28 cephrbd]# kubectl delete pod/nginx-dm-1697116629-j4m1g pod \"nginx-dm-1697116629-j4m1g\" deleted # 新的 pod [root@k8s-node-28 cephrbd]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-dm-1697116629-v2n52 1/1 Running 0 11s # 查看文件 [root@k8s-node-28 cephrbd]# kubectl exec -it nginx-dm-1697116629-v2n52 -- ls -lt /usr/share/nginx/html total 16 -rw-r--r-- 1 root root 0 May 9 07:53 jicki.html FAQ and Error ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:15:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"FAQ 1 # ceph 挂载关于 namespace != default 的情况下 1. 需要配置 secret = namespace kubectl get secret 可以看到只存在 default 的 认证 2. 需要配置 pv = namespace ，虽然pv 是全局的，但是最好还是配置一下 kubectl get pv --namespace= 3. 需要配置 pvc = namespace 。 kubectl get pvc --namespace= # 确认以上3点，否则在挂载namespace 的时候会报错 ceph timeout expired waiting for volumes to attach/mount for pod ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:16:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"Error 1 # 删除 osd 如果创建出错，请删除 osd # 使用 ceph osd tree 查看出错的 osd.id ceph osd out id 提出集群 # 登陆所在 服务器 停止服务 systemctl stop ceph-osd@id # 然后再执行 crush remove 删除 tree ceph osd crush remove osd.id # 最后 执行 auth del 和 rm 删除 ceph auth del osd.id ceph osd rm id # 在运行 tree 已经不存在了 # 如果使用分区挂载 还需要 登陆 osd 节点中 umount 分区 # 使用 lsblk 查看挂载 # 如: /dev/sdb 被挂载 umount /dev/sdb ceph-disk zap /dev/sdb ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:17:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"Error 2 # 如果暴力删除了 osd 然后重建了 osd ceph -s 报错 stale+active+undersized+degraded+remapped # 执行 ceph health detail 查看所有报错的信息 # 执行 ceph pg \u003cpgid\u003e query 查看具体信息 # 如果执行 query 报错 # 使用 ceph pg force_create_pg \u003cpgid\u003e 覆盖错误 # 如果 datail 有很多 pgid 出错 使用 for 循环去跑 for pg in `ceph health detail | grep \"stale+active+undersized+degraded\" | awk '{print $2}' | sort | uniq`; do ceph pg force_create_pg $pg done # 如果仍然不能解决问题，那么请使用暴力方法 # 重建ceph 集群 删除集群所有文件,重新部署 systemctl stop ceph-osd.target systemctl stop ceph-mon.target rm -rf /etc/ceph/* rm -rf /var/log/ceph/* rm -rf /var/lib/ceph/* # osd 清除挂载 目录的里的所有内容 # 卸载 ceph yum -y remove ceph ceph-radosgw # 重新安装 yum -y install ceph ceph-radosgw # 在 osd 节点 创建目录 mkdir -p /var/lib/ceph/osd/ ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:18:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"无法删除 rbd image # 删除报错 [root@ceph-node-1 ~]# rbd rm test-image 2017-07-07 11:41:39.733710 7f7a55898d80 -1 librbd: image has watchers - not removing Removing image: 0% complete...failed. rbd: error: image still has watchers This means the image is still open or the client using it crashed. Try again after closing/unmapping it or waiting 30s for the crashed client to timeout. # 查看 状态 与 客户端连接 [root@ceph-node-1 ~]# rbd status test-image Watchers: watcher=172.16.1.29:0/1324761697 client.14212 cookie=1 watcher=172.16.1.42:0/3859462470 client.14226 cookie=1 # 登陆所在 服务器 29 [root@k8s-node-29 ~]# rbd showmapped id pool image snap device 0 rbd test-image - /dev/rbd0 # 执行 rbd unmap [root@k8s-node-29 ~]# rbd unmap /dev/rbd0 # 登陆所在 服务器 42 [root@k8s-node-42 ~]# rbd showmapped id pool image snap device 0 rbd test-image - /dev/rbd0 # 执行 rbd unmap [root@k8s-node-42 ~]# rbd unmap /dev/rbd0 # 最后执行 rbd rm [root@ceph-node-1 ~]# rbd rm test-image Removing image: 100% complete...done. ","date":"2017-05-09","objectID":"/kubernetes-ceph-rbd/:19:0","tags":null,"title":"kubernetes Ceph RBD","uri":"/kubernetes-ceph-rbd/"},{"categories":["kubernetes"],"content":"kubernetes glusterfs","date":"2017-04-21","objectID":"/kubernetes-glusterfs/","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"kubernetes glusterfs ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:0:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"安装 glusterfs # 先安装 gluster 源 yum install centos-release-gluster -y # 安装 glusterfs 组件 yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma glusterfs-geo-replication glusterfs-devel ## 创建 glusterfs 目录 mkdir /opt/glusterd ## 修改 glusterd 目录 sed -i 's/var\\/lib/opt/g' /etc/glusterfs/glusterd.vol # 启动 glusterfs systemctl start glusterd.service # 设置开机启动 systemctl enable glusterd.service #查看状态 systemctl status glusterd.service ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:1:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"配置 glusterfs # 配置 hosts vi /etc/hosts gluster-1 10.6.0.52 gluster-2 10.6.0.53 gluster-3 10.6.0.55 gluster-4 10.6.0.56 # 开放端口 iptables -I INPUT -p tcp --dport 24007 -j ACCEPT # 创建存储目录 mkdir /opt/gfs_data # 添加节点到 集群 # 执行操作的本机不需要probe 本机 [root@gluster-1 ~]# gluster peer probe gluster-2 gluster peer probe gluster-3 gluster peer probe gluster-4 # 查看集群状态 gluster peer status ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:2:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"配置 volume # 创建 分布卷 gluster volume create dht-volume transport tcp gluster-1:/opt/gfs_data gluster-2:/opt/gfs_data # 查看volume状态 gluster volume info ... Type: Distribute ... # 启动 分布卷 gluster volume start dht-volume # 创建 复制卷 gluster volume create afr-volume replica 2 transport tcp gluster-1:/opt/afr_data gluster-2:/opt/afr_data # 查看volume状态 gluster volume info ... Type: Replicate ... # 启动 复制卷 gluster volume start afr-volume # 创建 条带卷 gluster volume create str-volume stripe 2 transport tcp gluster-1:/opt/str_data gluster-2:/opt/str_data # 查看volume状态 gluster volume info ... Type: Stripe ... # 启动 条带卷 gluster volume start str-volume # 上面三种基本模式，可以互相组合 # 这里我们使用 组合 分布式复制卷 需要最少4台服务器 replica 必须为倍数 gluster volume create k8s-volume replica 2 transport tcp gluster-1:/opt/gfs_data gluster-2:/opt/gfs_data gluster-3:/opt/gfs_data gluster-4:/opt/gfs_data # 查看 volume 状态 gluster volume info Volume Name: k8s-volume Type: Distributed-Replicate Volume ID: 981c41fa-bbe1-4a36-a1e2-9f76de1dc8f1 Status: Created Snapshot Count: 0 Number of Bricks: 2 x 2 = 4 Transport-type: tcp Bricks: Brick1: gluster-1:/opt/gfs_data Brick2: gluster-2:/opt/gfs_data Brick3: gluster-3:/opt/gfs_data Brick4: gluster-4:/opt/gfs_data Options Reconfigured: transport.address-family: inet nfs.disable: on # 启动 k8s-volume gluster volume start k8s-volume ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:3:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"gluster 调优 # 开启 指定 volume 的配额 gluster volume quota k8s-volume enable # 限制 指定 volume 的配额 gluster volume quota k8s-volume limit-usage / 5TB # 设置 cache 大小, 默认32MB gluster volume set k8s-volume performance.cache-size 4GB # 设置 io 线程, 太大会导致进程崩溃 gluster volume set k8s-volume performance.io-thread-count 16 # 设置 网络检测时间, 默认42s gluster volume set k8s-volume network.ping-timeout 10 # 设置 目录索引的自动愈合进程 gluster volume set k8s-volume cluster.self-heal-daemon on # 设置 自动愈合的检测间隔, 默认600s gluster volume set k8s-volume cluster.heal-timeout 300 # 设置 写缓冲区的大小, 默认1M gluster volume set k8s-volume performance.write-behind-window-size 1024MB kubernetes 配置 glusterfs 官方的文档 https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/glusterfs ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:4:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"kubernetes 安装客户端 # 在所有 k8s node 中安装 glusterfs 客户端 yum install -y glusterfs glusterfs-fuse # 配置 hosts vi /etc/hosts gluster-1 10.6.0.52 gluster-2 10.6.0.53 gluster-3 10.6.0.55 gluster-4 10.6.0.56 ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:5:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"配置 endpoints curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-endpoints.json # 修改 endpoints.json ，配置 glusters 集群节点ip # 每一个 addresses 为一个 ip 组 { \"addresses\": [ { \"ip\": \"10.6.0.52\" } ], \"ports\": [ { \"port\": 1 } ] }, # 导入 glusterfs-endpoints.json kubectl apply -f glusterfs-endpoints.json # 查看 endpoints 信息 kubectl get ep ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:6:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"配置 service curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-service.json # service.json 不需要配置，里面查找的是 enpointes 的名称与端口，端口默认配置为 1 # 导入 glusterfs-service.json kubectl apply -f glusterfs-service.json # 查看 service 信息 kubectl get svc ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:7:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"创建 测试 pod curl -O https://raw.githubusercontent.com/kubernetes/kubernetes/master/examples/volumes/glusterfs/glusterfs-pod.json # 编辑 glusterfs-pod.json # 修改 volumes 下的 path 为上面创建的 volume 名称 \"path\": \"k8s-volume\" # 导入 glusterfs-pod.json kubectl apply -f glusterfs-pod.json # 查看 pods 状态 kubectl get pods NAME READY STATUS RESTARTS AGE glusterfs 1/1 Running 0 1m # 查看 pods 所在 node kubectl describe pods/glusterfs # 登陆 node 物理机 使用 df 可查看 挂载目录 ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:8:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"配置 pv PersistentVolume（pv）和 PersistentVolumeClaim（pvc）是k8s提供的两种API资源，用于抽象存储细节。管理员关注于如何通过pv提供存储功能而无需关注用户如何使用，同样的用户只需要挂载pvc到容器中而不需要关注存储卷采用何种技术实现。 pvc和pv的关系与pod和node关系类似，前者消耗后者的资源。pvc可以向pv申请指定大小的存储资源并设置访问模式。 pv 属性 storage 容量 读写属性 分别为 - ReadWriteOnce：单个节点读写 , ReadOnlyMany：多节点只读 , ReadWriteMany：多节点读写 。 vi glusterfs-pv.yaml --- apiVersion: v1 kind: PersistentVolume metadata: name: gluster-dev-volume spec: capacity: storage: 8Gi accessModes: - ReadWriteMany glusterfs: endpoints: \"glusterfs-cluster\" path: \"k8s-volume\" readOnly: false --- # 导入 pv kubectl apply -f glusterfs-pv.yaml # 查看 pv kubectl get pv pvc 属性 访问属性 与 pv 相同 容量，向pv申请的容量 \u003c= pv 总容量 ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:9:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"配置 pvc --- kind: PersistentVolumeClaim apiVersion: v1 metadata: name: glusterfs-nginx spec: accessModes: - ReadWriteMany resources: requests: storage: 8Gi --- # 导入 pvc kubectl apply -f glusterfs-pvc.yaml # 查看 pvc kubectl get pv # STATUS 为 Bound ， VOLUME 为 pv name ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:10:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"创建 nginx deployment 挂载 volume vi nginx-deployment.yaml --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: gluster-dev-volume mountPath: \"/usr/share/nginx/html\" volumes: - name: gluster-dev-volume persistentVolumeClaim: claimName: glusterfs-nginx --- # 导入 deployment kubectl apply -f nginx-deployment.yaml # 查看 deployment kubectl get pods |grep nginx-dm nginx-dm-2784556780-cnjdw 1/1 Running 0 8m nginx-dm-2784556780-pt8vf 1/1 Running 0 8m # 查看 挂载 kubectl exec -it nginx-dm-2784556780-cnjdw -- df -h|grep k8s-volume # 创建文件 测试 kubectl exec -it nginx-dm-2784556780-cnjdw -- touch /usr/share/nginx/html/index.html kubectl exec -it nginx-dm-2784556780-pt8vf -- ls -lt /usr/share/nginx/html/index.html # 验证 glusterfs # 因为我们使用 分布式复制卷，所以可以看到2个节点中有文件 [root@gluster-1 ~] ls /opt/gfs_data/ [root@gluster-2 ~] ls /opt/gfs_data/ [root@gluster-3 ~] ls /opt/gfs_data/ [root@gluster-4 ~] ls /opt/gfs_data/ ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:11:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"FAQ 问题 # 在使用 pv 与 pvc 的过程中遇到问题 # 第一次创建 pv 与 pvc 的时候 状态都是OK的 # 当删除 pvc 以后，再创建 pvc 状态一直 pending 无论如何都不正常 # 查看日志 报 no persistent volumes available for this claim and no storage class is set # 这里我们可以跳过 pv 与 pvc 只需要创建一个 ep 就可以 # 挂载的时候我们直接在 yaml 目录下 写 ep 就行 # 如下为 deployment 的 yaml 文件 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: gluster-efk-volume mountPath: \"/usr/share/nginx/html\" volumes: - name: gluster-efk-volume glusterfs: endpoints: glusterfs-cluster path: efk-volume readOnly: False ","date":"2017-04-21","objectID":"/kubernetes-glusterfs/:12:0","tags":null,"title":"kubernetes glusterfs","uri":"/kubernetes-glusterfs/"},{"categories":["kubernetes"],"content":"kargo kubernetes 1.6.1","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"1、初始化环境 ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:0:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"1.1、环境： 节点 IP 角色 node-1 10.6.0.52 Master node-2 10.6.0.53 Master node-3 10.6.0.55 Node node-4 10.6.0.56 Node ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:1:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"1.2、配置SSH Key 登陆 # 确保本机也可以 ssh 连接，否则下面部署失败 ssh-keygen -t rsa -N \"\" ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.52 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.53 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.55 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.56 2、获取 Kargo Kargo 官方github https://github.com/kubernetes-incubator/kargo ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:2:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"2.1、安装基础软件 Kargo 是基于 ansible 统一部署，所以必须安装 ansible # 安装 centos 额外的yum源 yum install -y epel-release # 安装 软件 yum install -y python-pip python34 python-netaddr python34-pip ansible # 如果 报 no test named 'equalto' ，需要升级 Jinja2 pip install --upgrade Jinja2 ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:3:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"2.2、获取源码 git clone https://github.com/kubernetes-incubator/kargo ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:4:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"2.3、编辑配置文件 cd kargo vim inventory/group_vars/k8s-cluster.yml # 配置文件如下: # 其中{ { 与 } } 实际为 \"{+{ }+}\" 因为博客 jellky 的原因 他会解析\"{+{ }+}\" 并自动去掉，所以我在里面加了空格。 # 请自行使用 sed 替换 # Valid bootstrap options (required): ubuntu, coreos, centos, none bootstrap_os: centos etcd_data_dir: /var/lib/etcd bin_dir: /usr/local/bin kube_config_dir: /etc/kubernetes kube_script_dir: \"{ { bin_dir } }/kubernetes-scripts\" kube_manifest_dir: \"{ { kube_config_dir } }/manifests\" system_namespace: kube-system kube_log_dir: \"/var/log/kubernetes\" kube_cert_dir: \"{ { kube_config_dir } }/ssl\" kube_token_dir: \"{ { kube_config_dir } }/tokens\" kube_users_dir: \"{ { kube_config_dir } }/users\" kube_api_anonymous_auth: false kube_version: v1.6.1 local_release_dir: \"/tmp/releases\" retry_stagger: 5 kube_cert_group: kube-cert kube_log_level: 2 kube_api_pwd: \"jicki\" kube_users: kube: pass: \"{ { kube_api_pwd } }\" role: admin root: pass: \"{ { kube_api_pwd } }\" role: admin kube_network_plugin: calico enable_network_policy: true kube_service_addresses: 10.233.0.0/18 kube_pods_subnet: 10.233.64.0/18 kube_network_node_prefix: 24 kube_apiserver_ip: \"{ { kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') } }\" kube_apiserver_port: 6443 # (https) kube_apiserver_insecure_port: 8080 # (http) cluster_name: cluster.local ndots: 2 dns_mode: dnsmasq_kubedns resolvconf_mode: docker_dns deploy_netchecker: true skydns_server: \"{ { kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') } }\" dns_server: \"{ { kube_service_addresses|ipaddr('net')|ipaddr(2)|ipaddr('address') } }\" dns_domain: \"{ { cluster_name } }\" docker_daemon_graph: \"/opt/docker\" docker_options: \"--insecure-registry={ { kube_service_addresses } } --graph={ { docker_daemon_graph } }\" docker_bin_dir: \"/usr/bin\" etcd_deployment_type: docker kubelet_deployment_type: docker cert_management: script vault_deployment_type: docker k8s_image_pull_policy: IfNotPresent efk_enabled: true helm_enabled: false ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:5:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"2.4、生成集群配置文件 cd kargo CONFIG_FILE=inventory/inventory.cfg python3 contrib/inventory_builder/inventory.py 10.6.0.52 10.6.0.53 10.6.0.55 10.6.0.56 # 输入如下： DEBUG: Adding group all DEBUG: Adding group kube-master DEBUG: Adding group kube-node DEBUG: Adding group etcd DEBUG: Adding group k8s-cluster:children DEBUG: Adding group calico-rr DEBUG: adding host node1 to group all DEBUG: adding host node2 to group all DEBUG: adding host node3 to group all DEBUG: adding host node4 to group all DEBUG: adding host kube-node to group k8s-cluster:children DEBUG: adding host kube-master to group k8s-cluster:children DEBUG: adding host node1 to group etcd DEBUG: adding host node2 to group etcd DEBUG: adding host node3 to group etcd DEBUG: adding host node1 to group kube-master DEBUG: adding host node2 to group kube-master DEBUG: adding host node1 to group kube-node DEBUG: adding host node2 to group kube-node DEBUG: adding host node3 to group kube-node DEBUG: adding host node4 to group kube-node # 生成的配置文件在当前目录，既 kargo/inventory 目录下 inventory.cfg # 配置文件如下(默认配置双master，可自行修改)： # SSH 非 22 端口 添加 ansible_port=xxx [all] node1 ansible_host=10.6.0.52 ansible_port=33 ip=10.6.0.52 node2 ansible_host=10.6.0.53 ansible_port=33 ip=10.6.0.53 node3 ansible_host=10.6.0.55 ansible_port=33 ip=10.6.0.55 node4 ansible_host=10.6.0.56 ansible_port=33 ip=10.6.0.56 [kube-master] node1 node2 [kube-node] node1 node2 node3 node4 [etcd] node1 node2 node3 [k8s-cluster:children] kube-node kube-master [calico-rr] # 1.6.1 镜像下载 http://pan.baidu.com/s/1c1qke6 ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:6:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"2.5、部署集群 # 执行如下命令，请确保SSH KEY 登陆, 端口一致 ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa ","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:7:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"2.6、测试 # 两个 master 中使用 kubectl get nodes [root@k8s-node-1 ~]# kubectl get nodes NAME STATUS AGE k8s-node-1 Ready 16m k8s-node-2 Ready 20m k8s-node-3 Ready 16m k8s-node-4 Ready 16m [root@k8s-node-2 ~]# kubectl get nodes NAME STATUS AGE k8s-node-1 Ready 11m k8s-node-2 Ready 16m k8s-node-3 Ready 11m k8s-node-4 Ready 11m [root@k8s-node-1 ~]# kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE dnsmasq-411420702-z0gkx 1/1 Running 0 16m dnsmasq-autoscaler-1155841093-1hxdl 1/1 Running 0 16m elasticsearch-logging-v1-kgt1t 1/1 Running 0 15m elasticsearch-logging-v1-vm4bd 1/1 Running 0 15m fluentd-es-v1.22-6gql6 1/1 Running 0 15m fluentd-es-v1.22-8zkjh 1/1 Running 0 15m fluentd-es-v1.22-cjskv 1/1 Running 0 15m fluentd-es-v1.22-j4857 1/1 Running 0 15m kibana-logging-2924323056-x3vjk 1/1 Running 0 15m kube-apiserver-k8s-node-1 1/1 Running 0 15m kube-apiserver-k8s-node-2 1/1 Running 0 20m kube-controller-manager-k8s-node-1 1/1 Running 0 16m kube-controller-manager-k8s-node-2 1/1 Running 0 21m kube-proxy-k8s-node-1 1/1 Running 0 16m kube-proxy-k8s-node-2 1/1 Running 0 21m kube-proxy-k8s-node-3 1/1 Running 0 16m kube-proxy-k8s-node-4 1/1 Running 0 16m kube-scheduler-k8s-node-1 1/1 Running 0 16m kube-scheduler-k8s-node-2 1/1 Running 0 21m kubedns-3830354952-pfl7n 3/3 Running 4 16m kubedns-autoscaler-54374881-64x6d 1/1 Running 0 16m nginx-proxy-k8s-node-3 1/1 Running 0 16m nginx-proxy-k8s-node-4 1/1 Running 0 16m [root@k8s-node-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE netchecker-agent-3x3sj 1/1 Running 0 16m netchecker-agent-ggxs2 1/1 Running 0 16m netchecker-agent-hostnet-45k84 1/1 Running 0 16m netchecker-agent-hostnet-kwvc8 1/1 Running 0 16m netchecker-agent-hostnet-pwm77 1/1 Running 0 16m netchecker-agent-hostnet-z4gmq 1/1 Running 0 16m netchecker-agent-q3291 1/1 Running 0 16m netchecker-agent-qtml6 1/1 Running 0 16m netchecker-server 1/1 Running 0 16m # 配置一个 nginx deplyment 与 nginx service apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 apiVersion: v1 kind: Service metadata: name: nginx-dm spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx.yaml deployment \"nginx-dm\" created service \"nginx-dm\" created [root@k8s-node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-4194680597-0h071 1/1 Running 0 9m 10.233.75.8 k8s-node-4 nginx-dm-4194680597-dzcf3 1/1 Running 0 9m 10.233.76.124 k8s-node-3 [root@k8s-node-1 ~]# kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.233.0.1 \u003cnone\u003e 443/TCP 39m \u003cnone\u003e netchecker-service 10.233.0.126 \u003cnodes\u003e 8081:31081/TCP 33m app=netchecker-server nginx-dm 10.233.56.138 \u003cnone\u003e 80/TCP 10m name=nginx # 部署一个 curl 的 pods 用来测试 内部通信 apiVersion: v1 kind: Pod metadata: name: curl spec: containers: - name: curl image: radial/busyboxplus:curl command: - sh - -c - while true; do sleep 1; done # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f curl.yaml pod \"curl\" created [root@k8s-node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE curl 1/1 Running 0 2m 10.233.75.22 k8s-node-4 # 测试 curl --\u003e nginx-svc [root@k8s-node-1 ~]# kubectl exec -it curl curl nginx-dm \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e \u003cstyle\u003e body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; } \u003c/style\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003eWelcome to nginx!\u003c/h1\u003e \u003cp\u003eIf you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u003c/p\u003e \u003cp\u003eFor online documentation and support please refer to \u003ca href=\"http://nginx.org/\"\u003enginx.org\u003c/a\u003e.\u003cbr/\u003e Commercial support is available at \u003ca href=\"http://nginx.com/\"\u003enginx.com\u003c/a\u003e.\u003c/p\u003e \u003cp\u003e\u003cem\u003eThank you for using nginx.\u003c/em\u003e\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e # 创建一个 zk 集群 zk-de","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:8:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["kubernetes"],"content":"2.7、部署一个 Nginx Ingress kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 nginx haproxy 等负载均衡工具来暴露 kubernetes 服务。 # 首先 部署一个 http-backend, 用于统一转发 没有的域名 到指定页面。 # 官方 nginx ingress 库 https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx # 下载 docker images docker pull jicki/defaultbackend:1.0 docker tag jicki/defaultbackend:1.0 gcr.io/google_containers/defaultbackend:1.0 docker rmi jicki/defaultbackend:1.0 # 下载官方的 nginx backend 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/default-backend.yaml # 直接导入既可 [root@k8s-node-1 ~]# kubectl apply -f default-backend.yaml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看 deployment 与 service [root@k8s-node-1 ~]# kubectl get deployment --namespace=kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 33m [root@k8s-node-1 ~]# kubectl get svc --namespace=kube-system NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE default-http-backend 10.233.20.232 \u003cnone\u003e 80/TCP 33m # 部署 Ingress Controller 组件 # 下载 docker 镜像 docker pull jicki/nginx-ingress-controller:0.9.0-beta.3 docker tag jicki/nginx-ingress-controller:0.9.0-beta.3 gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3 docker rmi jicki/nginx-ingress-controller:0.9.0-beta.3 # 下载 官方 nginx-ingress-controller 的yaml文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml # 编辑 yaml 文件，打开 hostNetwork: true , 将端口绑定到宿主机中 # 这里面deployment 默认只启动了一个pods, 这里可以修改 kind: Deployment 为 kind: DaemonSet 并注释掉 replicas # 或者 修改 replicas: 1 为 N vi nginx-ingress-controller.yaml 将 hostNetwork: true 前面的注释去掉 # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx-ingress-controller.yaml deployment \"nginx-ingress-controller\" created # 查看 deployment 或者 daemonsets [root@k8s-node-1 yaml]# kubectl get deployment --namespace=kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-ingress-controller 1 1 1 1 31s [root@k8s-node-1 yaml]# kubectl get daemonsets --namespace=kube-system NAME DESIRED CURRENT READY NODE-SELECTOR AGE nginx-ingress-controller 4 4 4 \u003cnone\u003e 1m # 最后开始 部署 Ingress # 这里请先看看官方 ingress 的 yaml 写法 # https://kubernetes.io/docs/user-guide/ingress/ # 我们使用 之前创建的 nginx-dm service，我们来写一个 ingress # 首先查看一下 svc [root@k8s-node-1 yaml]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.233.0.1 \u003cnone\u003e 443/TCP 1d netchecker-service 10.233.0.126 \u003cnodes\u003e 8081:31081/TCP 1d nginx-dm 10.233.56.138 \u003cnone\u003e 80/TCP 1d zookeeper-1 10.233.25.46 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d zookeeper-2 10.233.49.4 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d zookeeper-3 10.233.50.206 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d # 创建 yaml 文件， 这里特别注意，如果 svc 在 kube-system 下 # 必须在 metadata: 下面添加 namespace: kube-system 指定命名空间 vim nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-dm servicePort: 80 # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx-ingress.yaml ingress \"nginx-ingress\" created # 查看一下 创建的 ingress [root@k8s-node-1 ~]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 17s # 这里显示 ADDRESS 为 空 实际上 所有 master 与 nodes 都绑定了 # 将域名解析到 任何一个 IP 上都可以。 # 下面访问 http://nginx.jicki.cn/ # 这里注意，Ingresses 只做简单的端口转发。 维护 FAQ # 卸载 cd kargo ansible-playbook -i inventory/inventory.cfg reset.yml -b -v --private-key=~/.ssh/id_rsa # 增加节点 # 首先编辑 inventory/inventory.cfg 增加一个节点 例：node5 [kube-node] node1 node2 node3 node4 node5 # 执行命令 使用 --limit 参数 ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa --limit node5 # 报错1 # hostname 的问题 # 部署 kargo 必须配置 hostname 否则 多 master 会出现 无法创建 api 等 pods # 如果 执行了 ansible-playbook 之前没改 hostname 必须删除 /tmp 下的 node[N] # 否则更改 /etc/hosts 失败 # 报错 2 TASK [vault : check_vault | Set fact about the Vault cluster's initialization state] **","date":"2017-04-11","objectID":"/kargo-k8s-1.6.1/:9:0","tags":null,"title":"kargo kubernetes 1.6.1","uri":"/kargo-k8s-1.6.1/"},{"categories":["codis"],"content":"Codis 3.1","date":"2017-04-06","objectID":"/codis-redis/","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":" 官方 配置文档, 使用说明 https://github.com/CodisLabs/codis/blob/release3.2/doc/tutorial_zh.md 环境配置 ","date":"2017-04-06","objectID":"/codis-redis/:0:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"安装 git yum install -y git autoconf ","date":"2017-04-06","objectID":"/codis-redis/:1:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"安装 golang，(使用 1.5.x 版本) wget https://storage.googleapis.com/golang/go1.5.4.linux-amd64.tar.gz tar zxvf go1.5.4.linux-amd64.tar.gz mkdir /opt/local mv go /opt/local/ ","date":"2017-04-06","objectID":"/codis-redis/:2:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"配置环境变量 vi /etc/profile # 添加如下: # golang ENV export GOROOT=/opt/local/go export PATH=$PATH:$GOROOT/bin export GOPATH=/opt/local/codis # 立即生效 source /etc/profile # 检查版本 go version go version go1.5.4 linux/amd64 echo $GOPATH /opt/local/codis 安装 Codis ","date":"2017-04-06","objectID":"/codis-redis/:3:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"下载 Codis 源码 mkdir -p $GOPATH/src/github.com/CodisLabs cd $GOPATH/src/github.com/CodisLabs git clone https://github.com/CodisLabs/codis.git -b release3.1 ","date":"2017-04-06","objectID":"/codis-redis/:4:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"编译 Codis 源码 cd $GOPATH/src/github.com/CodisLabs/codis make make -j -C extern/redis-3.2.4/ # 输入如下信息： go build -i -o bin/codis-dashboard ./cmd/dashboard go build -i -o bin/codis-proxy ./cmd/proxy go build -i -o bin/codis-admin ./cmd/admin go build -i -o bin/codis-ha ./cmd/ha go build -i -o bin/codis-fe ./cmd/fe # 查看 安装以后的版本 cat bin/version version = 2017-03-08 14:07:13 +0800 @b1919d11593dfd1f47a2461837233dfc8fc78002 @3.1.5-26-gb1919d1 compile = 2017-04-05 18:13:46 +0800 by go version go1.5.4 linux/amd64 # 复制文件，方便管理 mkdir -p /opt/local/codis/{bin,logs,data}/ cp -rf $GOPATH/src/github.com/CodisLabs/codis/bin/* /opt/local/codis/bin cp -rf $GOPATH/src/github.com/CodisLabs/codis/config /opt/local/codis/ 配置 Codis ","date":"2017-04-06","objectID":"/codis-redis/:5:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"Codis Dashboard cd /opt/local/codis/config/ vim dashboard.toml # 修改配置文件 ################################################## # # # Codis-Dashboard # # # ################################################## # Set Coordinator, only accept \"zookeeper\" \u0026 \"etcd\" \u0026 \"filesystem\". coordinator_name = \"zookeeper\" coordinator_addr = \"127.0.0.1:2181\" # Set Codis Product Name/Auth. product_name = \"codis-demo\" product_auth = \"\" # Set bind address for admin(rpc), tcp only. admin_addr = \"0.0.0.0:18080\" # Set configs for redis sentinel. sentinel_quorum = 2 sentinel_parallel_syncs = 1 sentinel_down_after = \"30s\" sentinel_failover_timeout = \"5m\" sentinel_notification_script = \"\" sentinel_client_reconfig_script = \"\" # 启动 Dashboard nohup /opt/local/codis/bin/codis-dashboard --ncpu=4 --config=/opt/local/codis/config/dashboard.toml \\ --log=/opt/local/codis/logs/dashboard.log --log-level=WARN \u0026 ","date":"2017-04-06","objectID":"/codis-redis/:6:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"Codis Proxy cd /opt/local/codis/config/ vim proxy.toml # 修改配置文件 ################################################## # # # Codis-Proxy # # # ################################################## # Set Codis Product Name/Auth. product_name = \"codis-demo\" product_auth = \"\" # Set bind address for admin(rpc), tcp only. admin_addr = \"0.0.0.0:11080\" # Set bind address for proxy, proto_type can be \"tcp\", \"tcp4\", \"tcp6\", \"unix\" or \"unixpacket\". proto_type = \"tcp4\" proxy_addr = \"0.0.0.0:19000\" # Set jodis address \u0026 session timeout, only accept \"zookeeper\" \u0026 \"etcd\". jodis_name = \"zookeeper\" jodis_addr = \"127.0.0.1:2181\" jodis_timeout = \"20s\" jodis_compatible = false # Set datacenter of proxy. proxy_datacenter = \"\" # Set max number of alive sessions. proxy_max_clients = 1000 # Set max offheap memory size. (0 to disable) proxy_max_offheap_size = \"1024mb\" # Set heap placeholder to reduce GC frequency. proxy_heap_placeholder = \"256mb\" # Proxy will ping backend redis (and clear 'MASTERDOWN' state) in a predefined interval. (0 to disable) backend_ping_period = \"5s\" # Set backend recv buffer size \u0026 timeout. backend_recv_bufsize = \"128kb\" backend_recv_timeout = \"30s\" # Set backend send buffer \u0026 timeout. backend_send_bufsize = \"128kb\" backend_send_timeout = \"30s\" # Set backend pipeline buffer size. backend_max_pipeline = 1024 # Set backend never read replica groups, default is false backend_primary_only = false # Set backend parallel connections per server backend_primary_parallel = 1 backend_replica_parallel = 1 # Set backend tcp keepalive period. (0 to disable) backend_keepalive_period = \"75s\" # If there is no request from client for a long time, the connection will be closed. (0 to disable) # Set session recv buffer size \u0026 timeout. session_recv_bufsize = \"128kb\" session_recv_timeout = \"30m\" # Set session send buffer size \u0026 timeout. session_send_bufsize = \"64kb\" session_send_timeout = \"30s\" # Make sure this is higher than the max number of requests for each pipeline request, or your client may be blocked. # Set session pipeline buffer size. session_max_pipeline = 1024 # Set session tcp keepalive period. (0 to disable) session_keepalive_period = \"75s\" # Set session to be sensitive to failures. Default is false, instead of closing socket, proxy will send an error response to client. session_break_on_failure = false # Set metrics server (such as http://localhost:28000), proxy will report json formatted metrics to specified server in a predefined period. metrics_report_server = \"\" metrics_report_period = \"1s\" # Set influxdb server (such as http://localhost:8086), proxy will report metrics to influxdb. metrics_report_influxdb_server = \"\" metrics_report_influxdb_period = \"1s\" metrics_report_influxdb_username = \"\" metrics_report_influxdb_password = \"\" metrics_report_influxdb_database = \"\" # 启动 codis-proxy nohup /opt/local/codis/bin/codis-proxy --ncpu=4 --config=/opt/local/codis/config/proxy.toml \\ --log=/opt/local/codis/logs/proxy.log --log-level=WARN \u0026 # 日志输出如下： proxy waiting online ... # 必须添加到集群中，才正常。 # 可使用 Codis-fe 界面添加 或者使用 Codis-admin 添加 ","date":"2017-04-06","objectID":"/codis-redis/:7:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"Codis-Server # 启动 server 与 启动 redis 方法相同 nohup /opt/local/codis/bin/codis-server /opt/local/codis/config/redis.conf \u0026 # 启动 Codis-Server 以后需要使用 Codis-fe 或者 Codis-admin 添加到集群 # 配置文件 bind 127.0.0.1 protected-mode yes port 6379 tcp-backlog 2048 timeout 0 tcp-keepalive 300 daemonize no supervised no pidfile /var/run/redis_6379.pid loglevel notice logfile \"/opt/local/codis/logs/redis.log\" maxmemory 10gb databases 16 save 900 1 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes rdbcompression yes rdbchecksum yes dbfilename redis_dump.rdb dir /opt/local/codis/data slave-serve-stale-data yes slave-read-only yes repl-diskless-sync no repl-diskless-sync-delay 5 repl-disable-tcp-nodelay no slave-priority 100 appendonly no appendfilename \"appendonly.aof\" appendfsync everysec no-appendfsync-on-rewrite no auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb aof-load-truncated yes lua-time-limit 5000 slowlog-log-slower-than 10000 slowlog-max-len 128 latency-monitor-threshold 0 notify-keyspace-events \"\" hash-max-ziplist-entries 512 hash-max-ziplist-value 64 list-max-ziplist-size -2 list-compress-depth 0 set-max-intset-entries 512 zset-max-ziplist-entries 128 zset-max-ziplist-value 64 hll-sparse-max-bytes 3000 activerehashing yes client-output-buffer-limit normal 0 0 0 client-output-buffer-limit slave 256mb 64mb 60 client-output-buffer-limit pubsub 32mb 8mb 60 hz 10 aof-rewrite-incremental-fsync yes ","date":"2017-04-06","objectID":"/codis-redis/:8:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"Codis-FE # 启动 codis-fe nohup /opt/local/codis/bin/codis-fe --ncpu=4 --log=/opt/local/codis/logs/fe.log --log-level=WARN --zookeeper=127.0.0.1:2181 --listen=127.0.0.1:8080 \u0026 ","date":"2017-04-06","objectID":"/codis-redis/:9:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["codis"],"content":"Codis-admin # 添加 codis-proxy /opt/local/codis/bin/codis-admin --dashboard=127.0.0.1:18080 --create-proxy -x 127.0.0.1:11080 # 日志输入如下： jodis create node /jodis/codis-demo/proxy-15fba3007f3c6ee0887749681cb82307 proxy is working ... # 修复异常退出的 Codis-dashboard # dashboard 非正常退出，或者 kill -9 时使用 /opt/local/codis/bin/codis-admin --remove-lock --product=codis-demo --zookeeper=127.0.0.1:2181 # 修复异常退出的 Codis-proxy # proxy 非正常退出，或者 kill -9 时使用 /opt/local/bin/codis-admin --dashboard=127.0.0.1:18080 --remove-proxy --addr=127.0.0.1:11080 --force ","date":"2017-04-06","objectID":"/codis-redis/:10:0","tags":null,"title":"Codis 3.1","uri":"/codis-redis/"},{"categories":["kubernetes"],"content":"kargo kubernetes 1.5.4","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":" 基于Kargo 快速部署多master 的 kubernetes 集群 1、初始化环境 ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:0:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"1.1、环境： 节点 IP 角色 node-1 10.6.0.52 Master node-2 10.6.0.53 Master node-3 10.6.0.55 Node node-4 10.6.0.56 Node ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:1:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"1.2、配置SSH Key 登陆 # 确保本机也可以 ssh 连接，否则下面部署失败 ssh-keygen -t rsa -N \"\" ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.52 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.53 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.55 ssh-copy-id -i /root/.ssh/id_rsa.pub 10.6.0.56 2、获取 Kargo Kargo 官方github https://github.com/kubernetes-incubator/kargo ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:2:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"2.1、安装基础软件 Kargo 是基于 ansible 统一部署，所以必须安装 ansible # 安装 centos 额外的yum源 yum install -y epel-release # 安装 软件 yum install -y python-pip python34 python-netaddr python34-pip ansible # 升级 Jinja2 否则报错 no test named 'equalto' pip install --upgrade Jinja2 # k8s 1.5.4镜像, 可提前下载，安装docker 以后导入 # 百度网盘地址 http://pan.baidu.com/s/1jH5105S # karogo 2.2.0 k8s 1.5.5镜像 http://pan.baidu.com/s/1pKCdOoV ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:3:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"2.2、获取源码 git clone https://github.com/kubernetes-incubator/kargo ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:4:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"2.3、编辑配置文件 cd kargo vim inventory/group_vars/k8s-cluster.yml # 配置文件如下: # 其中{ { 与 } } 实际为 \"{+{ }+}\" 因为博客 jellky 的原因 他会解析\"{+{ }+}\" 并自动去掉，所以我在里面加了空格。 # 请自行使用 sed 替换 # 启动集群的基础系统 bootstrap_os: centos # etcd 数据存放位置 etcd_data_dir: /var/lib/etcd # 二进制文件将要安装的位置 bin_dir: /usr/local/bin # kubernetes 配置文件存放目录以及命名空间 kube_config_dir: /etc/kubernetes kube_script_dir: \"{ { bin_dir } }/kubernetes-scripts\" kube_manifest_dir: \"{ { kube_config_dir } }/manifests\" system_namespace: kube-system # 日志存放位置 kube_log_dir: \"/var/log/kubernetes\" # kubernetes 证书存放位置 kube_cert_dir: \"{ { kube_config_dir } }/ssl\" # token存放位置 kube_token_dir: \"{ { kube_config_dir } }/tokens\" # basic auth 认证文件存放位置 kube_users_dir: \"{ { kube_config_dir } }/users\" # 关闭匿名授权 kube_api_anonymous_auth: false ## 使用的 kubernetes 版本 kube_version: v1.5.4 # 安装过程中缓存文件下载位置(最少 1G) local_release_dir: \"/tmp/releases\" # 重试次数，比如下载失败等情况 retry_stagger: 5 # 证书组 kube_cert_group: kube-cert # 集群日志等级 kube_log_level: 2 # HTTP 下 api server 密码及用户 kube_api_pwd: \"test123\" kube_users: kube: pass: \"{ { kube_api_pwd } }\" role: admin root: pass: \"{ { kube_api_pwd } }\" role: admin # 网络 CNI 组件 (calico, weave or flannel) kube_network_plugin: calico # 服务地址分配 kube_service_addresses: 10.233.0.0/18 # pod 地址分配 kube_pods_subnet: 10.233.64.0/18 # 网络节点大小分配 kube_network_node_prefix: 24 # api server 监听地址及端口 kube_apiserver_ip: \"{ { kube_service_addresses|ipaddr('net')|ipaddr(1)|ipaddr('address') } }\" kube_apiserver_port: 6443 # (https) kube_apiserver_insecure_port: 8080 # (http) # 默认 dns 后缀 cluster_name: cluster.local # Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods ndots: 2 # DNS 组件 dnsmasq_kubedns/kubedns dns_mode: dnsmasq_kubedns # Can be docker_dns, host_resolvconf or none resolvconf_mode: docker_dns # 部署 netchecker 来检测 DNS 和 HTTP 状态 deploy_netchecker: true # skydns service IP 配置 skydns_server: \"{ { kube_service_addresses|ipaddr('net')|ipaddr(3)|ipaddr('address') } }\" dns_server: \"{ { kube_service_addresses|ipaddr('net')|ipaddr(2)|ipaddr('address') } }\" dns_domain: \"{ { cluster_name } }\" # docker 存储目录 docker_daemon_graph: \"/var/lib/docker\" # docker 的额外配置参数，默认会在 /etc/systemd/system/docker.service.d/ 创建相关配置，如果节点已经安装了 docker，并且做了自己的配置，比如启用的 device mapper ，那么要删除/更改这里，防止冲突导致 docker 无法启动 docker_options: \"--insecure-registry={ { kube_service_addresses } } --graph={ { docker_daemon_graph } } --iptables=false\" docker_bin_dir: \"/usr/bin\" # 组件部署方式 # Settings for containerized control plane (etcd/kubelet/secrets) etcd_deployment_type: docker kubelet_deployment_type: docker cert_management: script vault_deployment_type: docker # 这里修改为 读取本地的,否则会自动去下载 k8s_image_pull_policy: IfNotPresent # Monitoring apps for k8s efk_enabled: true ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:5:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"2.4、生成集群配置文件 cd kargo CONFIG_FILE=inventory/inventory.cfg python3 contrib/inventory_builder/inventory.py 10.6.0.52 10.6.0.53 10.6.0.55 10.6.0.56 # 输入如下： DEBUG: Adding group all DEBUG: Adding group kube-master DEBUG: Adding group kube-node DEBUG: Adding group etcd DEBUG: Adding group k8s-cluster:children DEBUG: Adding group calico-rr DEBUG: adding host node1 to group all DEBUG: adding host node2 to group all DEBUG: adding host node3 to group all DEBUG: adding host node4 to group all DEBUG: adding host kube-node to group k8s-cluster:children DEBUG: adding host kube-master to group k8s-cluster:children DEBUG: adding host node1 to group etcd DEBUG: adding host node2 to group etcd DEBUG: adding host node3 to group etcd DEBUG: adding host node1 to group kube-master DEBUG: adding host node2 to group kube-master DEBUG: adding host node1 to group kube-node DEBUG: adding host node2 to group kube-node DEBUG: adding host node3 to group kube-node DEBUG: adding host node4 to group kube-node # 生成的配置文件在当前目录，既 kargo/inventory 目录下 inventory.cfg # 配置文件如下(默认配置双master，可自行修改)： # SSH 非 22 端口 添加 ansible_port=xxx [all] node1 ansible_host=10.6.0.52 ansible_port=33 ip=10.6.0.52 node2 ansible_host=10.6.0.53 ansible_port=33 ip=10.6.0.53 node3 ansible_host=10.6.0.55 ansible_port=33 ip=10.6.0.55 node4 ansible_host=10.6.0.56 ansible_port=33 ip=10.6.0.56 [kube-master] node1 node2 [kube-node] node1 node2 node3 node4 [etcd] node1 node2 node3 [k8s-cluster:children] kube-node kube-master [calico-rr] # 这里修改一下 下载 镜像 的版本 # 我在使用的时候 官方已经更新到 1.5.4 了， kargo 里的还是 1.5.3 版本的 cd kargo vi roles/download/defaults/main.yml # 修改里面的download 版本 # 或者很快 kargo 已经更新 1.5.4版本 ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:6:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"2.5、部署集群 # 执行如下命令，请确保SSH KEY 登陆, 端口一致 ansible-playbook -i inventory/inventory.cfg cluster.yml -b -v --private-key=~/.ssh/id_rsa ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:7:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"2.6、测试 # 两个 master 中使用 kubectl get nodes [root@k8s-node-1 ~]# kubectl get nodes NAME STATUS AGE k8s-node-1 Ready 16m k8s-node-2 Ready 20m k8s-node-3 Ready 16m k8s-node-4 Ready 16m [root@k8s-node-2 ~]# kubectl get nodes NAME STATUS AGE k8s-node-1 Ready 11m k8s-node-2 Ready 16m k8s-node-3 Ready 11m k8s-node-4 Ready 11m [root@k8s-node-1 ~]# kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE dnsmasq-411420702-z0gkx 1/1 Running 0 16m dnsmasq-autoscaler-1155841093-1hxdl 1/1 Running 0 16m elasticsearch-logging-v1-kgt1t 1/1 Running 0 15m elasticsearch-logging-v1-vm4bd 1/1 Running 0 15m fluentd-es-v1.22-6gql6 1/1 Running 0 15m fluentd-es-v1.22-8zkjh 1/1 Running 0 15m fluentd-es-v1.22-cjskv 1/1 Running 0 15m fluentd-es-v1.22-j4857 1/1 Running 0 15m kibana-logging-2924323056-x3vjk 1/1 Running 0 15m kube-apiserver-k8s-node-1 1/1 Running 0 15m kube-apiserver-k8s-node-2 1/1 Running 0 20m kube-controller-manager-k8s-node-1 1/1 Running 0 16m kube-controller-manager-k8s-node-2 1/1 Running 0 21m kube-proxy-k8s-node-1 1/1 Running 0 16m kube-proxy-k8s-node-2 1/1 Running 0 21m kube-proxy-k8s-node-3 1/1 Running 0 16m kube-proxy-k8s-node-4 1/1 Running 0 16m kube-scheduler-k8s-node-1 1/1 Running 0 16m kube-scheduler-k8s-node-2 1/1 Running 0 21m kubedns-3830354952-pfl7n 3/3 Running 4 16m kubedns-autoscaler-54374881-64x6d 1/1 Running 0 16m nginx-proxy-k8s-node-3 1/1 Running 0 16m nginx-proxy-k8s-node-4 1/1 Running 0 16m [root@k8s-node-1 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE netchecker-agent-3x3sj 1/1 Running 0 16m netchecker-agent-ggxs2 1/1 Running 0 16m netchecker-agent-hostnet-45k84 1/1 Running 0 16m netchecker-agent-hostnet-kwvc8 1/1 Running 0 16m netchecker-agent-hostnet-pwm77 1/1 Running 0 16m netchecker-agent-hostnet-z4gmq 1/1 Running 0 16m netchecker-agent-q3291 1/1 Running 0 16m netchecker-agent-qtml6 1/1 Running 0 16m netchecker-server 1/1 Running 0 16m # 配置一个 nginx deplyment 与 nginx service apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-dm spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx.yaml deployment \"nginx-dm\" created service \"nginx-dm\" created [root@k8s-node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-dm-4194680597-0h071 1/1 Running 0 9m 10.233.75.8 k8s-node-4 nginx-dm-4194680597-dzcf3 1/1 Running 0 9m 10.233.76.124 k8s-node-3 [root@k8s-node-1 ~]# kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.233.0.1 \u003cnone\u003e 443/TCP 39m \u003cnone\u003e netchecker-service 10.233.0.126 \u003cnodes\u003e 8081:31081/TCP 33m app=netchecker-server nginx-dm 10.233.56.138 \u003cnone\u003e 80/TCP 10m name=nginx # 部署一个 curl 的 pods 用来测试 内部通信 apiVersion: v1 kind: Pod metadata: name: curl spec: containers: - name: curl image: radial/busyboxplus:curl command: - sh - -c - while true; do sleep 1; done # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f curl.yaml pod \"curl\" created [root@k8s-node-1 ~]# kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE curl 1/1 Running 0 2m 10.233.75.22 k8s-node-4 # 测试 curl --\u003e nginx-svc [root@k8s-node-1 ~]# kubectl exec -it curl curl nginx-dm Welcome to nginx! # 创建一个 zk 集群 zk-deplyment 和 service apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-1 spec: replicas: 1 template: metadata: labels: name: zookeeper-1 spec: containers: - name: zookeeper-1 image: jicki/zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: NODES value: \"0.0.0.0,zookeeper-2,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-2 spec: replicas: 1 template: metadata: labels: name: zookeeper-2 spec: containers: - name: zookeeper-2 image: ji","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:8:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"2.7、部署一个 Nginx Ingress kubernetes 暴露服务的方式目前只有三种：LoadBlancer Service、NodePort Service、Ingress； 什么是 Ingress ? Ingress 就是利用 nginx haproxy 等负载均衡工具来暴露 kubernetes 服务。 # 首先 部署一个 http-backend, 用于统一转发 没有的域名 到指定页面。 # 官方 nginx ingress 库 https://github.com/kubernetes/ingress/tree/master/examples/deployment/nginx # 下载 docker images docker pull jicki/defaultbackend:1.0 docker tag jicki/defaultbackend:1.0 gcr.io/google_containers/defaultbackend:1.0 docker rmi jicki/defaultbackend:1.0 # 下载官方的 nginx backend 文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/default-backend.yaml # 直接导入既可 [root@k8s-node-1 ~]# kubectl apply -f default-backend.yaml deployment \"default-http-backend\" created service \"default-http-backend\" created # 查看 deployment 与 service [root@k8s-node-1 ~]# kubectl get deployment --namespace=kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default-http-backend 1 1 1 1 33m [root@k8s-node-1 ~]# kubectl get svc --namespace=kube-system NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE default-http-backend 10.233.20.232 \u003cnone\u003e 80/TCP 33m # 部署 Ingress Controller 组件 # 下载 docker 镜像 docker pull jicki/nginx-ingress-controller:0.9.0-beta.3 docker tag jicki/nginx-ingress-controller:0.9.0-beta.3 gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3 docker rmi jicki/nginx-ingress-controller:0.9.0-beta.3 # 下载 官方 nginx-ingress-controller 的yaml文件 curl -O https://raw.githubusercontent.com/kubernetes/ingress/master/examples/deployment/nginx/nginx-ingress-controller.yaml # 编辑 yaml 文件，打开 hostNetwork: true , 将端口绑定到宿主机中 # 这里面deployment 默认只启动了一个pods, 这里可以修改 kind: Deployment 为 kind: DaemonSet 并注释掉 replicas # 或者 修改 replicas: 1 为 N vi nginx-ingress-controller.yaml 将 hostNetwork: true 前面的注释去掉 # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx-ingress-controller.yaml deployment \"nginx-ingress-controller\" created # 查看 deployment 或者 daemonsets [root@k8s-node-1 yaml]# kubectl get deployment --namespace=kube-system NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-ingress-controller 1 1 1 1 31s [root@k8s-node-1 yaml]# kubectl get daemonsets --namespace=kube-system NAME DESIRED CURRENT READY NODE-SELECTOR AGE nginx-ingress-controller 4 4 4 \u003cnone\u003e 1m # 最后开始 部署 Ingress # 这里请先看看官方 ingress 的 yaml 写法 # https://kubernetes.io/docs/user-guide/ingress/ # 我们使用 之前创建的 nginx-dm service，我们来写一个 ingress # 首先查看一下 svc [root@k8s-node-1 yaml]# kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.233.0.1 \u003cnone\u003e 443/TCP 1d netchecker-service 10.233.0.126 \u003cnodes\u003e 8081:31081/TCP 1d nginx-dm 10.233.56.138 \u003cnone\u003e 80/TCP 1d zookeeper-1 10.233.25.46 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d zookeeper-2 10.233.49.4 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d zookeeper-3 10.233.50.206 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 1d # 创建 yaml 文件， 这里特别注意，如果 svc 在 kube-system 下 # 必须在 metadata: 下面添加 namespace: kube-system 指定命名空间 vim nginx-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: nginx.jicki.cn http: paths: - backend: serviceName: nginx-dm servicePort: 80 # 导入 yaml 文件 [root@k8s-node-1 ~]# kubectl apply -f nginx-ingress.yaml ingress \"nginx-ingress\" created # 查看一下 创建的 ingress [root@k8s-node-1 ~]# kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE nginx-ingress nginx.jicki.cn 80 17s # 这里显示 ADDRESS 为 空 实际上 所有 master 与 nodes 都绑定了 # 将域名解析到 任何一个 IP 上都可以。 # 下面访问 http://nginx.jicki.cn/ # 这里注意，Ingresses 只做简单的端口转发。 维护 FAQ # 卸载 cd kargo ansible-playbook -i inventory/inventory.cfg reset.yml -b -v --private-key=~/.ssh/id_rsa # hostname 的问题 # 部署 kargo 必须配置 hostname 否则 多 master 会出现 无法创建 api 等 pods # 如果 执行了 ansible-playbook 之前没改 hostname 必须删除 /tmp 下的 node[N] # 否则更改 /etc/hosts 失败 ","date":"2017-03-13","objectID":"/kargo-kubernetes-1.5.4/:9:0","tags":null,"title":"kargo kubernetes 1.5.4","uri":"/kargo-kubernetes-1.5.4/"},{"categories":["kubernetes"],"content":"kubernetes 1.5.3","date":"2017-03-03","objectID":"/kubernetes-1.5.3/","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":" kubernetes update 1.5.3 1 初始化环境 ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:0:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"1.1 环境： 节点 IP node-1 10.6.0.140 node-2 10.6.0.187 node-3 10.6.0.188 ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:1:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"1.2 设置hostname hostnamectl --static set-hostname hostname IP hostname 10.6.0.140 k8s-node-1 10.6.0.187 k8s-node-2 10.6.0.188 k8s-node-3 ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:2:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"1.3 配置 hosts vi /etc/hosts IP hostname 10.6.0.140 k8s-node-1 10.6.0.187 k8s-node-2 10.6.0.188 k8s-node-3 2.0 部署 kubernetes master ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:3:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.1 添加yum # 使用我朋友的 yum 源，嘿嘿 cat \u003c\u003cEOF\u003e /etc/yum.repos.d/kubernetes.repo [mritdrepo] name=Mritd Repository baseurl=https://yum.mritd.me/centos/7/x86_64 enabled=1 gpgcheck=1 gpgkey=https://cdn.mritd.me/keys/rpm.public.key EOF yum makecache yum install -y socat kubelet kubeadm kubectl kubernetes-cni ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:4:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.2 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:5:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.3 下载镜像 images=(kube-proxy-amd64:v1.5.3 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.3 kube-controller-manager-amd64:v1.5.3 kube-apiserver-amd64:v1.5.3 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.1 dnsmasq-metrics-amd64:1.0) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done # 如果速度很慢，可配置一下加速 docker 启动文件 增加 --registry-mirror=http://b438f72b.m.daocloud.io ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:6:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.4 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:7:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.5 创建集群 kubeadm init --api-advertise-addresses=10.6.0.140 \\ --use-kubernetes-version v1.5.3 \\ --pod-network-cidr 10.244.0.0/16 [kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters. [preflight] Running pre-flight checks [preflight] Starting the kubelet service [init] Using kubernetes version: v1.5.3 [tokens] Generated token: \"c53ef2.d257d49589d634f0\" [certificates] Generated Certificate Authority key and certificate. [certificates] Generated API Server key and certificate [certificates] Generated Service Account signing keys [certificates] Created keys and certificates in \"/etc/kubernetes/pki\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\" [apiclient] Created API client, waiting for the control plane to become ready [apiclient] All control plane components are healthy after 15.299235 seconds [apiclient] Waiting for at least one node to register and become ready [apiclient] First node is ready after 1.002937 seconds [apiclient] Creating a test deployment [apiclient] Test deployment succeeded [token-discovery] Created the kube-discovery deployment, waiting for it to become ready [token-discovery] kube-discovery is ready after 2.502881 seconds [addons] Created essential addon: kube-proxy [addons] Created essential addon: kube-dns Your kubernetes master has initialized successfully! You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: http://kubernetes.io/docs/admin/addons/ You can now join any number of machines by running the following on each node: kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:8:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.6 记录 token You can now join any number of machines by running the following on each node: kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 # 如果忘记了 token 使用如下 命令可获取 kubectl -n kube-system get secret clusterinfo -o yaml | grep token-map | awk '{print $2}' | base64 --decode | sed \"s|{||g;s|}||g;s|:|.|g;s/\\\"//g;\" | xargs echo ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:9:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.7 配置网络 # 建议先下载镜像，否则容易下载不到 docker pull quay.io/coreos/flannel:v0.7.0-amd64 # 或者这样 docker pull jicki/flannel:v0.7.0-amd64 docker tag jicki/flannel:v0.7.0-amd64 quay.io/coreos/flannel:v0.7.0-amd64 docker rmi jicki/flannel:v0.7.0-amd64 # http://kubernetes.io/docs/admin/addons/ 这里有多种网络模式，选择一种 # 这里选择 Flannel 选择 Flannel init 时必须配置 --pod-network-cidr kubectl create -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:10:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"2.8 检查 kubelet 状态 systemctl status kubelet 3.0 部署 kubernetes node ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:11:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"3.1 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:12:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"3.2 下载镜像 images=(kube-proxy-amd64:v1.5.3 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.3 kube-controller-manager-amd64:v1.5.3 kube-apiserver-amd64:v1.5.3 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.1 dnsmasq-metrics-amd64:1.0) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:13:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"3.3 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:14:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"3.4 加入集群 kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on the master to see this machine join. ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:15:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"3.5 查看集群状态 [root@k8s-node-1 ~]#kubectl get node NAME STATUS AGE k8s-node-1 Ready,master 27m k8s-node-2 Ready 6s k8s-node-3 Ready 9s ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:16:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"3.6 查看服务状态 [root@k8s-node-1 ~]#kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system dummy-2088944543-qrp68 1/1 Running 1 1h kube-system kube-apiserver-k8s-node-1 1/1 Running 2 1h kube-system kube-controller-manager-k8s-node-1 1/1 Running 2 1h kube-system kube-discovery-1769846148-g2lpc 1/1 Running 1 1h kube-system kube-dns-2924299975-xbhv4 4/4 Running 3 1h kube-system kube-flannel-ds-39g5n 2/2 Running 2 1h kube-system kube-flannel-ds-dwc82 2/2 Running 2 1h kube-system kube-flannel-ds-qpkm0 2/2 Running 2 1h kube-system kube-proxy-16c50 1/1 Running 2 1h kube-system kube-proxy-5rkc8 1/1 Running 2 1h kube-system kube-proxy-xwrq0 1/1 Running 2 1h kube-system kube-scheduler-k8s-node-1 1/1 Running 2 1h 4.0 设置 kubernetes ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:17:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"4.1 其他主机控制集群 # 备份master节点的 配置文件 /etc/kubernetes/admin.conf # 保存至 其他电脑, 通过执行配置文件控制集群 kubectl --kubeconfig ./admin.conf get nodes ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:18:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"4.2 配置dashboard #下载 yaml 文件, 直接导入会去官方拉取images curl -O https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml #编辑 yaml 文件 imagePullPolicy: Always 修改为 imagePullPolicy: IfNotPresent kubectl create -f ./kubernetes-dashboard.yaml deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created # 查看 NodePort ，既外网访问端口 kubectl describe svc kubernetes-dashboard --namespace=kube-system NodePort: \u003cunset\u003e 31736/TCP # 访问 dashboard http://10.6.0.140:31736 5.0 kubernetes 应用部署 ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:19:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"5.1 部署一个 nginx rc 编写 一个 nginx yaml apiVersion: v1 kind: ReplicationController metadata: name: nginx-rc spec: replicas: 2 selector: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 [root@k8s-node-1 ~]#kubectl get rc NAME DESIRED CURRENT READY AGE nginx-rc 2 2 2 2m [root@k8s-node-1 ~]#kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-rc-2s8k9 1/1 Running 0 10m 10.32.0.3 k8s-node-1 nginx-rc-s16cm 1/1 Running 0 10m 10.40.0.1 k8s-node-2 编写一个 nginx service 让集群内部容器可以访问 (ClusterIp) apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-node-1 ~]#kubectl create -f nginx-svc.yaml service \"nginx-svc\" created [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29s name=nginx 编写一个 curl 的pods apiVersion: v1 kind: Pod metadata: name: curl spec: containers: - name: curl image: radial/busyboxplus:curl command: - sh - -c - while true; do sleep 1; done # 测试pods 内部通信 [root@k8s-node-1 ~]#kubectl exec curl curl nginx # 在任何node节点中，可使用ip访问 [root@k8s-node-2 ~]# curl 10.6.164.79 [root@k8s-node-3 ~]# curl 10.6.164.79 编写一个 nginx service 让外部可以访问 (NodePort) apiVersion: v1 kind: Service metadata: name: nginx-svc-node spec: ports: - port: 80 targetPort: 80 protocol: TCP type: NodePort selector: name: nginx [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29m name=nginx nginx-svc-node 10.12.95.227 \u003cnodes\u003e 80/TCP 17s name=nginx [root@k8s-node-1 ~]#kubectl describe svc nginx-svc-node |grep NodePort Type: NodePort NodePort: \u003cunset\u003e 32669/TCP # 使用 ALL node节点物理IP + 端口访问 http://10.6.0.140:32669 http://10.6.0.187:32669 http://10.6.0.188:32669 ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:20:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"5.2 部署一个 zookeeper 集群 编写 一个 zookeeper-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-1 spec: replicas: 1 template: metadata: labels: name: zookeeper-1 spec: containers: - name: zookeeper-1 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: NODES value: \"0.0.0.0,zookeeper-2,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-2 spec: replicas: 1 template: metadata: labels: name: zookeeper-2 spec: containers: - name: zookeeper-2 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: NODES value: \"zookeeper-1,0.0.0.0,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-3 spec: replicas: 1 template: metadata: labels: name: zookeeper-3 spec: containers: - name: zookeeper-3 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: NODES value: \"zookeeper-1,zookeeper-2,0.0.0.0\" ports: - containerPort: 2181 --- apiVersion: v1 kind: Service metadata: name: zookeeper-1 labels: name: zookeeper-1 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-1 --- apiVersion: v1 kind: Service metadata: name: zookeeper-2 labels: name: zookeeper-2 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-2 --- apiVersion: v1 kind: Service metadata: name: zookeeper-3 labels: name: zookeeper-3 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-3 [root@k8s-node-1 ~]#kubectl create -f zookeeper-cluster.yaml --record [root@k8s-node-1 ~]#kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE zookeeper-1-2149121414-cfyt4 1/1 Running 0 51m 10.32.0.3 k8s-node-2 zookeeper-2-2653289864-0bxee 1/1 Running 0 51m 10.40.0.1 k8s-node-3 zookeeper-3-3158769034-5csqy 1/1 Running 0 51m 10.40.0.2 k8s-node-3 [root@k8s-node-1 ~]#kubectl get deployment -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE zookeeper-1 1 1 1 1 51m zookeeper-2 1 1 1 1 51m zookeeper-3 1 1 1 1 51m [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR zookeeper-1 10.8.111.19 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-1 zookeeper-2 10.6.10.124 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-2 zookeeper-3 10.0.146.143 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-3 ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:21:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"5.3 部署一个 kafka 集群 编写 一个 kafka-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-1 spec: replicas: 1 template: metadata: labels: name: kafka-1 spec: containers: - name: kafka-1 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-2 spec: replicas: 1 template: metadata: labels: name: kafka-2 spec: containers: - name: kafka-2 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-3 spec: replicas: 1 template: metadata: labels: name: kafka-3 spec: containers: - name: kafka-3 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: v1 kind: Service metadata: name: kafka-1 labels: name: kafka-1 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-1 --- apiVersion: v1 kind: Service metadata: name: kafka-2 labels: name: kafka-2 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-2 --- apiVersion: v1 kind: Service metadata: name: kafka-3 labels: name: kafka-3 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-3 FAQ: ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:22:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["kubernetes"],"content":"kube-discovery error failed to create \"kube-discovery\" deployment [deployments.extensions \"kube-discovery\" already exists] kubeadm reset kubeadm init ","date":"2017-03-03","objectID":"/kubernetes-1.5.3/:23:0","tags":null,"title":"kubernetes 1.5.3","uri":"/kubernetes-1.5.3/"},{"categories":["Linux"],"content":"Ubuntu Desktop","date":"2017-02-15","objectID":"/ubuntu-desktop/","tags":null,"title":"Ubuntu Desktop","uri":"/ubuntu-desktop/"},{"categories":["Linux"],"content":" Ubuntu 桌面版本折腾 生命不止 折腾不止 Ubuntu Desktop 16.04 ","date":"2017-02-15","objectID":"/ubuntu-desktop/:0:0","tags":null,"title":"Ubuntu Desktop","uri":"/ubuntu-desktop/"},{"categories":["Linux"],"content":"折腾完以后的界面 ","date":"2017-02-15","objectID":"/ubuntu-desktop/:1:0","tags":null,"title":"Ubuntu Desktop","uri":"/ubuntu-desktop/"},{"categories":["kubernetes"],"content":"kubernetes 1.5.2","date":"2017-02-13","objectID":"/kubernetes-1.5.2/","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":" kubernetes update 1.5.2 and docker 1.13.1 update default driver the overlay2 1 初始化环境 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:0:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"1.1 环境： 节点 IP node-1 10.6.0.140 node-2 10.6.0.187 node-3 10.6.0.188 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:1:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"1.2 设置hostname hostnamectl --static set-hostname hostname IP hostname 10.6.0.140 k8s-node-1 10.6.0.187 k8s-node-2 10.6.0.188 k8s-node-3 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:2:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"1.3 配置 hosts vi /etc/hosts IP hostname 10.6.0.140 k8s-node-1 10.6.0.187 k8s-node-2 10.6.0.188 k8s-node-3 2.0 部署 kubernetes master ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:3:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.1 添加yum # 使用我朋友的 yum 源，嘿嘿 cat \u003e /etc/yum.repos.d/kubernetes.repo \u003c\u003c EOF [mritdrepo] name=Mritd Repository baseurl=https://yumrepo.b0.upaiyun.com/centos/7/x86_64 enabled=1 gpgcheck=1 gpgkey=https://mritd.b0.upaiyun.com/keys/rpm.public.key EOF yum makecache yum install -y socat kubelet kubeadm kubectl kubernetes-cni ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:4:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.2 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:5:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.3 下载镜像 images=(kube-proxy-amd64:v1.5.2 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.2 kube-controller-manager-amd64:v1.5.2 kube-apiserver-amd64:v1.5.2 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0 dnsmasq-metrics-amd64:1.0) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done # 如果速度很慢，可配置一下加速 docker 启动文件 增加 --registry-mirror=http://b438f72b.m.daocloud.io ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:6:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.4 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:7:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.5 创建集群 kubeadm init --api-advertise-addresses=10.6.0.140 \\ --use-kubernetes-version v1.5.2 \\ --pod-network-cidr 10.244.0.0/16 [kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters. [preflight] Running pre-flight checks [preflight] Starting the kubelet service [init] Using kubernetes version: v1.5.2 [tokens] Generated token: \"c53ef2.d257d49589d634f0\" [certificates] Generated Certificate Authority key and certificate. [certificates] Generated API Server key and certificate [certificates] Generated Service Account signing keys [certificates] Created keys and certificates in \"/etc/kubernetes/pki\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\" [apiclient] Created API client, waiting for the control plane to become ready [apiclient] All control plane components are healthy after 15.299235 seconds [apiclient] Waiting for at least one node to register and become ready [apiclient] First node is ready after 1.002937 seconds [apiclient] Creating a test deployment [apiclient] Test deployment succeeded [token-discovery] Created the kube-discovery deployment, waiting for it to become ready [token-discovery] kube-discovery is ready after 2.502881 seconds [addons] Created essential addon: kube-proxy [addons] Created essential addon: kube-dns Your kubernetes master has initialized successfully! You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: http://kubernetes.io/docs/admin/addons/ You can now join any number of machines by running the following on each node: kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:8:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.6 记录 token You can now join any number of machines by running the following on each node: kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:9:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.7 配置网络 # 建议先下载镜像，否则容易下载不到 docker pull quay.io/coreos/flannel:v0.7.0-amd64 # 或者这样 docker pull jicki/flannel:v0.7.0-amd64 docker tag jicki/flannel:v0.7.0-amd64 quay.io/coreos/flannel:v0.7.0-amd64 docker rmi jicki/flannel:v0.7.0-amd64 # http://kubernetes.io/docs/admin/addons/ 这里有多种网络模式，选择一种 # 这里选择 Flannel 选择 Flannel init 时必须配置 --pod-network-cidr kubectl create -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:10:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"2.8 检查 kubelet 状态 systemctl status kubelet 3.0 部署 kubernetes node ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:11:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"3.1 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:12:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"3.2 下载镜像 images=(kube-proxy-amd64:v1.5.2 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.2 kube-controller-manager-amd64:v1.5.2 kube-apiserver-amd64:v1.5.2 etcd-amd64:3.0.14-k ubeadm kube-dnsmasq-amd64:1.4 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0 dnsmasq-metrics-amd64:1.0) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:13:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"3.3 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:14:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"3.4 加入集群 kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on the master to see this machine join. ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:15:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"3.5 查看集群状态 [root@k8s-node-1 ~]#kubectl get node NAME STATUS AGE k8s-node-1 Ready,master 27m k8s-node-2 Ready 6s k8s-node-3 Ready 9s ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:16:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"3.6 查看服务状态 [root@k8s-node-1 ~]#kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system dummy-2088944543-qrp68 1/1 Running 1 1h kube-system kube-apiserver-k8s-node-1 1/1 Running 2 1h kube-system kube-controller-manager-k8s-node-1 1/1 Running 2 1h kube-system kube-discovery-1769846148-g2lpc 1/1 Running 1 1h kube-system kube-dns-2924299975-xbhv4 4/4 Running 3 1h kube-system kube-flannel-ds-39g5n 2/2 Running 2 1h kube-system kube-flannel-ds-dwc82 2/2 Running 2 1h kube-system kube-flannel-ds-qpkm0 2/2 Running 2 1h kube-system kube-proxy-16c50 1/1 Running 2 1h kube-system kube-proxy-5rkc8 1/1 Running 2 1h kube-system kube-proxy-xwrq0 1/1 Running 2 1h kube-system kube-scheduler-k8s-node-1 1/1 Running 2 1h 4.0 设置 kubernetes ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:17:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"4.1 其他主机控制集群 # 备份master节点的 配置文件 /etc/kubernetes/admin.conf # 保存至 其他电脑, 通过执行配置文件控制集群 kubectl --kubeconfig ./admin.conf get nodes ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:18:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"4.2 配置dashboard #下载 yaml 文件, 直接导入会去官方拉取images curl -O https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml #编辑 yaml 文件 vi kubernetes-dashboard.yaml image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.0 修改为 image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0 imagePullPolicy: Always 修改为 imagePullPolicy: IfNotPresent kubectl create -f ./kubernetes-dashboard.yaml deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created # 查看 NodePort ，既外网访问端口 kubectl describe svc kubernetes-dashboard --namespace=kube-system NodePort: \u003cunset\u003e 31736/TCP # 访问 dashboard http://10.6.0.140:31736 5.0 kubernetes 应用部署 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:19:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"5.1 部署一个 nginx rc 编写 一个 nginx yaml apiVersion: v1 kind: ReplicationController metadata: name: nginx-rc spec: replicas: 2 selector: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 [root@k8s-node-1 ~]#kubectl get rc NAME DESIRED CURRENT READY AGE nginx-rc 2 2 2 2m [root@k8s-node-1 ~]#kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-rc-2s8k9 1/1 Running 0 10m 10.32.0.3 k8s-node-1 nginx-rc-s16cm 1/1 Running 0 10m 10.40.0.1 k8s-node-2 编写一个 nginx service 让集群内部容器可以访问 (ClusterIp) apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-node-1 ~]#kubectl create -f nginx-svc.yaml service \"nginx-svc\" created [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29s name=nginx 编写一个 curl 的pods apiVersion: v1 kind: Pod metadata: name: curl spec: containers: - name: curl image: radial/busyboxplus:curl command: - sh - -c - while true; do sleep 1; done # 测试pods 内部通信 [root@k8s-node-1 ~]#kubectl exec curl curl nginx # 在任何node节点中，可使用ip访问 [root@k8s-node-2 ~]# curl 10.6.164.79 [root@k8s-node-3 ~]# curl 10.6.164.79 编写一个 nginx service 让外部可以访问 (NodePort) apiVersion: v1 kind: Service metadata: name: nginx-svc-node spec: ports: - port: 80 targetPort: 80 protocol: TCP type: NodePort selector: name: nginx [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29m name=nginx nginx-svc-node 10.12.95.227 \u003cnodes\u003e 80/TCP 17s name=nginx [root@k8s-node-1 ~]#kubectl describe svc nginx-svc-node |grep NodePort Type: NodePort NodePort: \u003cunset\u003e 32669/TCP # 使用 ALL node节点物理IP + 端口访问 http://10.6.0.140:32669 http://10.6.0.187:32669 http://10.6.0.188:32669 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:20:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"5.2 部署一个 zookeeper 集群 编写 一个 zookeeper-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-1 spec: replicas: 1 template: metadata: labels: name: zookeeper-1 spec: containers: - name: zookeeper-1 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: NODES value: \"0.0.0.0,zookeeper-2,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-2 spec: replicas: 1 template: metadata: labels: name: zookeeper-2 spec: containers: - name: zookeeper-2 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: NODES value: \"zookeeper-1,0.0.0.0,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-3 spec: replicas: 1 template: metadata: labels: name: zookeeper-3 spec: containers: - name: zookeeper-3 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: NODES value: \"zookeeper-1,zookeeper-2,0.0.0.0\" ports: - containerPort: 2181 --- apiVersion: v1 kind: Service metadata: name: zookeeper-1 labels: name: zookeeper-1 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-1 --- apiVersion: v1 kind: Service metadata: name: zookeeper-2 labels: name: zookeeper-2 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-2 --- apiVersion: v1 kind: Service metadata: name: zookeeper-3 labels: name: zookeeper-3 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-3 [root@k8s-node-1 ~]#kubectl create -f zookeeper-cluster.yaml --record [root@k8s-node-1 ~]#kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE zookeeper-1-2149121414-cfyt4 1/1 Running 0 51m 10.32.0.3 k8s-node-2 zookeeper-2-2653289864-0bxee 1/1 Running 0 51m 10.40.0.1 k8s-node-3 zookeeper-3-3158769034-5csqy 1/1 Running 0 51m 10.40.0.2 k8s-node-3 [root@k8s-node-1 ~]#kubectl get deployment -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE zookeeper-1 1 1 1 1 51m zookeeper-2 1 1 1 1 51m zookeeper-3 1 1 1 1 51m [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR zookeeper-1 10.8.111.19 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-1 zookeeper-2 10.6.10.124 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-2 zookeeper-3 10.0.146.143 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-3 ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:21:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"5.3 部署一个 kafka 集群 编写 一个 kafka-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-1 spec: replicas: 1 template: metadata: labels: name: kafka-1 spec: containers: - name: kafka-1 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-2 spec: replicas: 1 template: metadata: labels: name: kafka-2 spec: containers: - name: kafka-2 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-3 spec: replicas: 1 template: metadata: labels: name: kafka-3 spec: containers: - name: kafka-3 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: v1 kind: Service metadata: name: kafka-1 labels: name: kafka-1 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-1 --- apiVersion: v1 kind: Service metadata: name: kafka-2 labels: name: kafka-2 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-2 --- apiVersion: v1 kind: Service metadata: name: kafka-3 labels: name: kafka-3 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-3 FAQ: ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:22:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["kubernetes"],"content":"kube-discovery error failed to create \"kube-discovery\" deployment [deployments.extensions \"kube-discovery\" already exists] kubeadm reset kubeadm init ","date":"2017-02-13","objectID":"/kubernetes-1.5.2/:23:0","tags":null,"title":"kubernetes 1.5.2","uri":"/kubernetes-1.5.2/"},{"categories":["ssl"],"content":"配置 Let’s Encrypt SSL https 证书","date":"2017-01-24","objectID":"/let-encrypt-https-ssl/","tags":null,"title":"配置 Let’s Encrypt SSL https 证书","uri":"/let-encrypt-https-ssl/"},{"categories":["ssl"],"content":" 配置 Let’s Encrypt SSL https 证书 一、安装acme.sh curl https://get.acme.sh | sh #安装成功以后目录如下 /root/.acme.sh/ 二、通过验证DNS签发证书 # GoDaddy 的域名 https://developer.godaddy.com/keys/ # 配置 两个变量 export GD_Key=\"sdfsdfsdfljlbjkljlkjsdfoiwje\" export GD_Secret=\"asdfsdafdsfdsfdsfdsfdsafd\" 三、生成DNS TXT记录 acme.sh --issue --dns dns_gd -d jicki.cn # 如果报错 可使用 acme.sh --issue --dns -d jicki.cn # 会提示，手动创建一条 txt 记录 Add the following TXT record: Domain: '_acme-challenge.jicki.cn' TXT value: 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx' Please be aware that you prepend _acme-challenge. before your domain so the resulting subdomain will be: _acme-challenge.jicki.cn Please add the TXT records to the domains, and retry again. # 等待解析完成之后, 重新生成证书: acme.sh --renew -d jicki.cn Renew: 'jicki.cn' Single domain='jicki.cn' Getting domain auth token for each domain Verifying:jicki.cn Success Verify finished, start to sign. Cert success. # 如果解析未完成会提示 jicki.cn:Verify error:DNS problem: query timed out looking up CAA for jicki.cn 四、配置证书 acme.sh --installcert -d jicki.cn \\ --keypath /etc/nginx/ssl/jicki.cn.key \\ --fullchainpath /etc/nginx/ssl/jicki.cn.cer # 提示 Installing key to:/etc/nginx/ssl/jicki.cn.key Installing full chain to:/etc/nginx/ssl/jicki.cn.cer 配置nginx # 修改 nginx.conf 配置 增加 ssl server { listen 80; listen [::]:80; server_name jicki.cn; # Redirect all HTTP requests to HTTPS with a 301 Moved Permanently response. return 301 https://$host$request_uri; } server { listen 443; server_name www.jicki.cn jicki.cn; if ($http_user_agent ~ ApacheBench|WebBench|Jmeter){ return 403; } ### Begin of SSL config ssl on; ssl_certificate /etc/nginx/ssl/jicki.cn.cer; ssl_certificate_key /etc/nginx/ssl/jicki.cn.key; ssl_session_timeout 1d; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_prefer_server_ciphers on; ssl_ciphers EECDH+CHACHA20:EECDH+CHACHA20-draft:EECDH+AES128:RSA+AES128:EECDH+AES256:RSA+AES256:EECDH+3DES:RSA+3DES:!MD5; ssl_session_cache shared:SSL:50m; ssl_session_tickets on; ssl_stapling on; ssl_stapling_verify on; resolver 114.114.114.114 valid=300s; resolver_timeout 10s; ### End of SSL config location / { proxy_buffer_size 64k; proxy_buffers 32 32k; proxy_busy_buffers_size 256k; proxy_pass http://jekyll; proxy_cache one; } } FAQ: ","date":"2017-01-24","objectID":"/let-encrypt-https-ssl/:0:0","tags":null,"title":"配置 Let’s Encrypt SSL https 证书","uri":"/let-encrypt-https-ssl/"},{"categories":["ssl"],"content":"官方文档 https://github.com/Neilpang/acme.sh/wiki/%E8%AF%B4%E6%98%8E ","date":"2017-01-24","objectID":"/let-encrypt-https-ssl/:1:0","tags":null,"title":"配置 Let’s Encrypt SSL https 证书","uri":"/let-encrypt-https-ssl/"},{"categories":["ssl"],"content":"关于证书自动更新 官方提示 目前证书在 60 天以后会自动更新, 你无需任何操作. 今后有可能会缩短这个时间, 不过都是自动的, 你不用关心. ","date":"2017-01-24","objectID":"/let-encrypt-https-ssl/:2:0","tags":null,"title":"配置 Let’s Encrypt SSL https 证书","uri":"/let-encrypt-https-ssl/"},{"categories":["docker"],"content":"docker 1.13 swarm 集群","date":"2017-01-20","objectID":"/docker-1.13-swarm/","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["docker"],"content":"1.13 Changelog #简单介绍如下几点: # XFS 默认文件系统更新为 overlay2 Server Version: 1.13.0 Storage Driver: overlay Backing Filesystem: xfs # 添加 service 容器互通 ping 支持 # service 创建 添加 hostname docker service create --hostname # 正式支持 docker stack # 支持docker-compose version 3 docker stack deploy 安装 docker 1.13 wget -qO- https://get.docker.com/ | sh #修改 配置 sed -i 's/dockerd/dockerd --graph=\\/opt\\/docker\\/images/g' /lib/systemd/system/docker.service systemctl daemon-reload systemctl restart docker.service systemctl enable docker.service 创建 swarm ","date":"2017-01-20","objectID":"/docker-1.13-swarm/:0:0","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["docker"],"content":"Swarm init [root@swarm-node-1 ~]#docker swarm init --advertise-addr 10.6.0.140 Swarm initialized: current node (hf0h3qnlsicf5x6bltubjxgtg) is now a manager. To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-26a59cyiceqq93w79abuyuaifkr2pxbsj41k9gas0ttxfrcw4x-an3tg2qodlh8noc8fx4zwniic \\ 10.6.0.140:2377 To add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions. ","date":"2017-01-20","objectID":"/docker-1.13-swarm/:1:0","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["docker"],"content":"Swarm join [root@swarm-node-2 ~]#docker swarm join \\ \u003e --token SWMTKN-1-26a59cyiceqq93w79abuyuaifkr2pxbsj41k9gas0ttxfrcw4x-an3tg2qodlh8noc8fx4zwniic \\ \u003e 10.6.0.140:2377 This node joined a swarm as a worker. [root@swarm-node-3 ~]#docker swarm join \\ \u003e --token SWMTKN-1-26a59cyiceqq93w79abuyuaifkr2pxbsj41k9gas0ttxfrcw4x-an3tg2qodlh8noc8fx4zwniic \\ \u003e 10.6.0.140:2377 This node joined a swarm as a worker. [root@swarm-node-1 ~]#docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS e5b6fq2gfv3fex37q06974kg6 swarm-node-2 Ready Active hf0h3qnlsicf5x6bltubjxgtg * swarm-node-1 Ready Active Leader zfniuw09nktpqdyswulnxemqt swarm-node-3 Ready Active ","date":"2017-01-20","objectID":"/docker-1.13-swarm/:2:0","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["docker"],"content":"overlay network # 创建一个 overlay 网络 [root@swarm-node-1 ~]#docker network create --driver overlay --opt encrypted --subnet=10.0.9.0/24 my-net vxdbvrbvxcpep6sk6mtfy3nlp [root@swarm-node-1 ~]#docker network ls NETWORK ID NAME DRIVER SCOPE vxdbvrbvxcpe my-net overlay swarm ","date":"2017-01-20","objectID":"/docker-1.13-swarm/:3:0","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["docker"],"content":"docker service create docker service create --replicas 3 --name my-nginx --network my-net --endpoint-mode dnsrr nginx:alpine ","date":"2017-01-20","objectID":"/docker-1.13-swarm/:4:0","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["docker"],"content":"docker stack deploy [root@swarm-node-1 ~]#docker-compose version docker-compose version 1.10.0, build 4bd6f1a docker-py version: 2.0.2 CPython version: 2.7.5 OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013 # 编写一个 docker-compose.yaml version: '3' networks: network-pro: external: name: procdution services: nginx-1: image: nginx:alpine networks: network-pro: aliases: - nginx deploy: replicas: 3 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure delay: 5s max_attempts: 5 window: 120s hostname: nginx-1 container_name: nginx-1 ports: - \"80:80\" volumes: - /opt/upload/nginx-1/logs:/var/log/nginx/ # 创建 service docker stack deploy -c docker-compose.yaml nginx Creating service nginx_nginx-1 ","date":"2017-01-20","objectID":"/docker-1.13-swarm/:5:0","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["docker"],"content":"创建一个zk 集群 # 编写一个 zk.yaml version: '3' networks: network-pro: external: name: procdution services: zookeeper-1: image: zk:alpine networks: network-pro: aliases: - zookeeper-1 deploy: update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure delay: 5s max_attempts: 5 window: 120s hostname: zookeeper-1 container_name: zookeeper-1 environment: - NODE_ID=1 - NODES=zookeeper-1,zookeeper-2,zookeeper-3 zookeeper-2: image: zk:alpine networks: network-pro: aliases: - zookeeper-2 deploy: update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure delay: 5s max_attempts: 5 window: 120s hostname: zookeeper-2 container_name: zookeeper-2 environment: - NODE_ID=2 - NODES=zookeeper-1,zookeeper-2,zookeeper-3 zookeeper-3: image: zk:alpine networks: network-pro: aliases: - zookeeper-3 deploy: update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure delay: 5s max_attempts: 5 window: 120s hostname: zookeeper-3 container_name: zookeeper-3 environment: - NODE_ID=3 - NODES=zookeeper-1,zookeeper-2,zookeeper-3 # 创建 service [root@swarm-node-1 ~]#docker stack deploy -c zk.yaml zk [root@swarm-node-1 ~]#docker stack ls NAME SERVICES nginx 2 zk 3 [root@swarm-node-1 ~]#docker service ls ID NAME MODE REPLICAS IMAGE 0ivc88rl1krj nginx_nginx-2 replicated 1/1 nginx:alpine luysdxxywxpg zk_zookeeper-3 replicated 1/1 zk:alpine mgi1acc4nz2p nginx_nginx-1 replicated 1/1 nginx:alpine wtkwruzhzg0w zk_zookeeper-1 replicated 1/1 zk:alpine ztlj74u7ir7f zk_zookeeper-2 replicated 1/1 zk:alpine ","date":"2017-01-20","objectID":"/docker-1.13-swarm/:6:0","tags":null,"title":"docker 1.13 swarm 集群","uri":"/docker-1.13-swarm/"},{"categories":["kubernetes"],"content":"kubernetes nfs","date":"2017-01-01","objectID":"/kubernetes-nfs/","tags":null,"title":"kubernetes nfs","uri":"/kubernetes-nfs/"},{"categories":["kubernetes"],"content":"安装 NFS 服务 # 安装 NFS yum -y install nfs-utils rpcbind # k8s 所有节点 安装 NFS 客户端 yum -y install nfs-utils ","date":"2017-01-01","objectID":"/kubernetes-nfs/:1:0","tags":null,"title":"kubernetes nfs","uri":"/kubernetes-nfs/"},{"categories":["kubernetes"],"content":"配置 NFS vi /etc/exports 增加 /opt/data/logs 172.16.1.0/24(rw,sync,no_root_squash) /opt/data/gogs 172.16.1.0/24(rw,sync,no_root_squash) ","date":"2017-01-01","objectID":"/kubernetes-nfs/:2:0","tags":null,"title":"kubernetes nfs","uri":"/kubernetes-nfs/"},{"categories":["kubernetes"],"content":"启动 NFS systemctl enable rpcbind.service systemctl enable nfs-server.service systemctl start rpcbind.service systemctl start nfs-server.service # 查看启动情况 rpcinfo -p # 查看配置列表 showmount -e 172.16.1.20 ","date":"2017-01-01","objectID":"/kubernetes-nfs/:3:0","tags":null,"title":"kubernetes nfs","uri":"/kubernetes-nfs/"},{"categories":["kubernetes"],"content":"k8s deployment 挂载 配置 volumeMounts: - name: logs mountPath: /opt/local/nginx/logs readOnly: false volumes: - name: logs nfs: server: 172.16.1.20 path: /opt/data/logs # 完整挂载信息 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: gogs spec: replicas: 1 template: metadata: labels: name: gogs spec: containers: - name: gogs image: gogs/gogs imagePullPolicy: IfNotPresent ports: - name: web containerPort: 3000 - name: ssh containerPort: 22 resources: limits: cpu: 500m memory: 1024Mi volumeMounts: - name: data mountPath: /data readOnly: false - name: localtime mountPath: /etc/localtime readOnly: true volumes: - name: data nfs: server: 172.16.1.20 path: /opt/data/gogs - name: localtime hostPath: path: /etc/localtime ","date":"2017-01-01","objectID":"/kubernetes-nfs/:4:0","tags":null,"title":"kubernetes nfs","uri":"/kubernetes-nfs/"},{"categories":["docker"],"content":"traefik 负载均衡","date":"2016-12-26","objectID":"/traefik-load-balancing/","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":" Traefik 是一款开源的反向代理与负载均衡工具。它最大的优点是能够与常见的微服务系统直接整合，可以实现自动化动态配置。目前支持Docker, Swarm, Mesos/Marathon, Mesos, kubernetes, Consul, Etcd, Zookeeper, BoltDB, Rest API等等后端模型 Traefik 简介 traefik 官网 https://docs.traefik.io/ github https://github.com/containous/traefik traefik 与 kubernetes ","date":"2016-12-26","objectID":"/traefik-load-balancing/:0:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.1 kubernetes 环境 [root@k8s-node-1 ~]#kubectl get nodes NAME STATUS AGE k8s-node-1 Ready,master 5d k8s-node-2 Ready 5d k8s-node-3 Ready 5d ","date":"2016-12-26","objectID":"/traefik-load-balancing/:1:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.2 下载 traefik-dm # 定义 node-2 node-3 标签。 kubectl label nodes k8s-node-2 role=traefik-lb kubectl label nodes k8s-node-3 role=traefik-lb # 查看此标签的 nodes [root@k8s-node-1 ~]#kubectl get nodes -l role=traefik-lb NAME STATUS AGE k8s-node-2 Ready 5d k8s-node-3 Ready 5d # 下载yaml 文件 curl -O https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/traefik.yaml # yaml 可修改 replicas = node 个数 # 增加 restartPolicy: Always # 修改 web ui 端口 containerPort: 8080 修改为 containerPort: 8888 # args: 下面增加 --web.address=:8888 # 增加 nodeSelector: # role: traefik-lb # 修改后的文件如下： apiVersion: v1 kind: Deployment apiVersion: extensions/v1beta1 metadata: name: traefik-ingress-controller namespace: kube-system labels: k8s-app: traefik-ingress-lb spec: replicas: 2 selector: matchLabels: k8s-app: traefik-ingress-lb template: metadata: labels: k8s-app: traefik-ingress-lb name: traefik-ingress-lb spec: terminationGracePeriodSeconds: 60 hostNetwork: true restartPolicy: Always nodeSelector: role: traefik-lb containers: - image: traefik name: traefik-ingress-lb resources: limits: cpu: 200m memory: 30Mi requests: cpu: 100m memory: 20Mi ports: - name: http containerPort: 80 hostPort: 80 - name: admin containerPort: 8888 args: - --web - --web.address=:8888 - --kubernetes # 创建 traefik deployment [root@k8s-node-1 ~]#kubectl create -f traefik.yaml deployment \"traefik-ingress-controller\" created # 查看 deployment [root@k8s-node-1 ~]#kubectl get deploy --all-namespaces NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system traefik-ingress-controller 1 1 1 1 2m ","date":"2016-12-26","objectID":"/traefik-load-balancing/:2:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.3 部署一个 nginx 应用 # 创建一个 nginx yaml 文件 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-dm spec: replicas: 2 template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 --- apiVersion: v1 kind: Service metadata: name: nginx-dm spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx # 创建 deploy 与 svc [root@k8s-node-1 ~]#kubectl create -f nginx.yaml # 查看 nginx-dm [root@k8s-node-1 ~]#kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.96.0.1 \u003cnone\u003e 443/TCP 5d nginx-dm 10.105.76.250 \u003cnone\u003e 80/TCP 34m [root@k8s-node-1 ~]#kubectl get deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-dm 2 2 2 2 35m ","date":"2016-12-26","objectID":"/traefik-load-balancing/:3:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.4 创建 traefik ingress # 创建 traefik-ingress yaml 文件 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: traefik-ingress spec: rules: - host: traefik.test.jicki.cn http: paths: - path: / backend: serviceName: nginx-dm servicePort: 80 # 创建 ingress [root@k8s-node-1 ~]#kubectl create -f traefik-ingress.yaml # 查看 ing [root@k8s-node-1 ~]#kubectl get ing NAME HOSTS ADDRESS PORTS AGE traefik-ingress traefik.test.jicki.cn 80 32m ","date":"2016-12-26","objectID":"/traefik-load-balancing/:4:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.5 创建 traefik web ui # 下载 web ui yaml 文件 curl -O https://raw.githubusercontent.com/containous/traefik/master/examples/k8s/ui.yaml [root@k8s-node-1 ~]#kubectl create -f ui.yaml ","date":"2016-12-26","objectID":"/traefik-load-balancing/:5:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.6 访问 nginx 应用 [root@k8s-node-1 ~]#curl -I traefik.test.jicki.cn HTTP/1.1 200 OK Accept-Ranges: bytes Content-Length: 612 Content-Type: text/html Date: Mon, 26 Dec 2016 09:18:33 GMT Etag: \"585011f4-264\" Last-Modified: Tue, 13 Dec 2016 15:21:24 GMT Server: nginx/1.11.7 ","date":"2016-12-26","objectID":"/traefik-load-balancing/:6:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.7 访问 traefik web ui http://10.6.0.187:8888 http://10.6.0.188:8888 ","date":"2016-12-26","objectID":"/traefik-load-balancing/:7:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["docker"],"content":"1.8 心跳检测 ","date":"2016-12-26","objectID":"/traefik-load-balancing/:8:0","tags":null,"title":"traefik 负载均衡","uri":"/traefik-load-balancing/"},{"categories":["kubernetes"],"content":"kubernetes 1.5.1","date":"2016-12-13","objectID":"/kubernetes-1.5.0/","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":" kubernetes 1.5.1 , 配置文档 1 初始化环境 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:0:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"1.1 环境： 节点 IP node-1 10.6.0.140 node-2 10.6.0.187 node-3 10.6.0.188 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:1:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"1.2 设置hostname hostnamectl --static set-hostname hostname IP hostname 10.6.0.140 k8s-node-1 10.6.0.187 k8s-node-2 10.6.0.188 k8s-node-3 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:2:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"1.3 配置 hosts vi /etc/hosts IP hostname 10.6.0.140 k8s-node-1 10.6.0.187 k8s-node-2 10.6.0.188 k8s-node-3 2.0 部署 kubernetes master ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:3:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.1 添加yum # 使用我朋友的 yum 源，嘿嘿 cat \u003e /etc/yum.repos.d/kubernetes.repo \u003c\u003c EOF [mritdrepo] name=Mritd Repository baseurl=https://yumrepo.b0.upaiyun.com/centos/7/x86_64 enabled=1 gpgcheck=1 gpgkey=https://mritd.b0.upaiyun.com/keys/rpm.public.key EOF yum makecache yum install -y socat kubelet kubeadm kubectl kubernetes-cni ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:4:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.2 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:5:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.3 安装 etcd 集群 yum -y install etcd # 创建etcd data 目录 mkdir -p /opt/etcd/data chown -R etcd:etcd /opt/etcd/ # 修改配置文件，/etc/etcd/etcd.conf 需要修改如下参数： ETCD_NAME=etcd1 ETCD_DATA_DIR=\"/opt/etcd/data/etcd1.etcd\" ETCD_LISTEN_PEER_URLS=\"http://10.6.0.140:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.6.0.140:2379,http://127.0.0.1:2379\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.6.0.140:2380\" ETCD_INITIAL_CLUSTER=\"etcd1=http://10.6.0.140:2380,etcd2=http://10.6.0.187:2380,etcd3=http://10.6.0.188:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.6.0.140:2379\" # 修改 etcd 启动文件 sed -i 's/\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\"/\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\" --listen-client-urls=\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\" --advertise-client-urls=\\\\\\\"${ETCD_ADVERTISE_CLIENT_URLS}\\\\\\\" --initial-cluster-token=\\\\\\\"${ETCD_INITIAL_CLUSTER_TOKEN}\\\\\\\" --initial-cluster=\\\\\\\"${ETCD_INITIAL_CLUSTER}\\\\\\\" --initial-cluster-state=\\\\\\\"${ETCD_INITIAL_CLUSTER_STATE}\\\\\\\"/g' /usr/lib/systemd/system/etcd.service # 启动 etcd systemctl enable etcd systemctl start etcd systemctl status etcd # 查看集群状态 etcdctl cluster-health ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:6:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.4 下载镜像 images=(kube-proxy-amd64:v1.5.1 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.1 kube-controller-manager-amd64:v1.5.1 kube-apiserver-amd64:v1.5.1 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0 dnsmasq-metrics-amd64:1.0) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done # 如果速度很慢，可配置一下加速 docker 启动文件 增加 --registry-mirror=\"http://b438f72b.m.daocloud.io\" ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:7:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.5 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:8:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.6 创建集群 kubeadm init --api-advertise-addresses=10.6.0.140 \\ --external-etcd-endpoints=http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379 \\ --use-kubernetes-version v1.5.1 \\ --pod-network-cidr 10.244.0.0/16 Flag --external-etcd-endpoints has been deprecated, this flag will be removed when componentconfig exists [kubeadm] WARNING: kubeadm is in alpha, please do not use it for production clusters. [preflight] Running pre-flight checks [preflight] Starting the kubelet service [init] Using kubernetes version: v1.5.1 [tokens] Generated token: \"c53ef2.d257d49589d634f0\" [certificates] Generated Certificate Authority key and certificate. [certificates] Generated API Server key and certificate [certificates] Generated Service Account signing keys [certificates] Created keys and certificates in \"/etc/kubernetes/pki\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\" [kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\" [apiclient] Created API client, waiting for the control plane to become ready [apiclient] All control plane components are healthy after 15.299235 seconds [apiclient] Waiting for at least one node to register and become ready [apiclient] First node is ready after 1.002937 seconds [apiclient] Creating a test deployment [apiclient] Test deployment succeeded [token-discovery] Created the kube-discovery deployment, waiting for it to become ready [token-discovery] kube-discovery is ready after 2.502881 seconds [addons] Created essential addon: kube-proxy [addons] Created essential addon: kube-dns Your kubernetes master has initialized successfully! You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: http://kubernetes.io/docs/admin/addons/ You can now join any number of machines by running the following on each node: kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:9:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.7 记录 token You can now join any number of machines by running the following on each node: kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:10:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.8 配置网络 # 建议先下载镜像，否则容易下载不到 docker pull quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-amd64 # 或者这样 docker pull jicki/flannel-git:v0.6.1-28-g5dde68d-amd64 docker tag jicki/flannel-git:v0.6.1-28-g5dde68d-amd64 quay.io/coreos/flannel-git:v0.6.1-28-g5dde68d-amd64 docker rmi jicki/flannel-git:v0.6.1-28-g5dde68d-amd64 # http://kubernetes.io/docs/admin/addons/ 这里有多种网络模式，选择一种 # 这里选择 Flannel 选择 Flannel init 时必须配置 --pod-network-cidr kubectl create -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:11:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"2.9 检查 kubelet 状态 systemctl status kubelet 3.0 部署 kubernetes node ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:12:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"3.1 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:13:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"3.2 下载镜像 images=(kube-proxy-amd64:v1.5.1 kube-discovery-amd64:1.0 kubedns-amd64:1.9 kube-scheduler-amd64:v1.5.1 kube-controller-manager-amd64:v1.5.1 kube-apiserver-amd64:v1.5.1 etcd-amd64:3.0.14-kubeadm kube-dnsmasq-amd64:1.4 exechealthz-amd64:1.2 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.5.0 dnsmasq-metrics-amd64:1.0) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:14:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"3.3 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:15:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"3.4 加入集群 kubeadm join --token=c53ef2.d257d49589d634f0 10.6.0.140 Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on the master to see this machine join. ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:16:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"3.5 查看集群状态 [root@k8s-node-1 ~]#kubectl get node NAME STATUS AGE k8s-node-1 Ready,master 27m k8s-node-2 Ready 6s k8s-node-3 Ready 9s ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:17:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"3.6 查看服务状态 [root@k8s-node-1 ~]#kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system dummy-2088944543-qrp68 1/1 Running 1 1h kube-system kube-apiserver-k8s-node-1 1/1 Running 2 1h kube-system kube-controller-manager-k8s-node-1 1/1 Running 2 1h kube-system kube-discovery-1769846148-g2lpc 1/1 Running 1 1h kube-system kube-dns-2924299975-xbhv4 4/4 Running 3 1h kube-system kube-flannel-ds-39g5n 2/2 Running 2 1h kube-system kube-flannel-ds-dwc82 2/2 Running 2 1h kube-system kube-flannel-ds-qpkm0 2/2 Running 2 1h kube-system kube-proxy-16c50 1/1 Running 2 1h kube-system kube-proxy-5rkc8 1/1 Running 2 1h kube-system kube-proxy-xwrq0 1/1 Running 2 1h kube-system kube-scheduler-k8s-node-1 1/1 Running 2 1h 4.0 设置 kubernetes ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:18:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"4.1 其他主机控制集群 # 备份master节点的 配置文件 /etc/kubernetes/admin.conf # 保存至 其他电脑, 通过执行配置文件控制集群 kubectl --kubeconfig ./admin.conf get nodes ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:19:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"4.2 配置dashboard #下载 yaml 文件, 直接导入会去官方拉取images curl -O https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml #编辑 yaml 文件 vi kubernetes-dashboard.yaml image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.0 修改为 image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0 imagePullPolicy: Always 修改为 imagePullPolicy: IfNotPresent kubectl create -f ./kubernetes-dashboard.yaml deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created # 查看 NodePort ，既外网访问端口 kubectl describe svc kubernetes-dashboard --namespace=kube-system NodePort: \u003cunset\u003e 31736/TCP # 访问 dashboard http://10.6.0.140:31736 5.0 kubernetes 应用部署 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:20:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"5.1 部署一个 nginx rc 编写 一个 nginx yaml apiVersion: v1 kind: ReplicationController metadata: name: nginx-rc spec: replicas: 2 selector: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 80 [root@k8s-node-1 ~]#kubectl get rc NAME DESIRED CURRENT READY AGE nginx-rc 2 2 2 2m [root@k8s-node-1 ~]#kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-rc-2s8k9 1/1 Running 0 10m 10.32.0.3 k8s-node-1 nginx-rc-s16cm 1/1 Running 0 10m 10.40.0.1 k8s-node-2 编写一个 nginx service 让集群内部容器可以访问 (ClusterIp) apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-node-1 ~]#kubectl create -f nginx-svc.yaml service \"nginx-svc\" created [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29s name=nginx 编写一个 curl 的pods apiVersion: v1 kind: Pod metadata: name: curl spec: containers: - name: curl image: radial/busyboxplus:curl command: - sh - -c - while true; do sleep 1; done # 测试pods 内部通信 [root@k8s-node-1 ~]#kubectl exec curl curl nginx # 在任何node节点中，可使用ip访问 [root@k8s-node-2 ~]# curl 10.6.164.79 [root@k8s-node-3 ~]# curl 10.6.164.79 编写一个 nginx service 让外部可以访问 (NodePort) apiVersion: v1 kind: Service metadata: name: nginx-svc-node spec: ports: - port: 80 targetPort: 80 protocol: TCP type: NodePort selector: name: nginx [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29m name=nginx nginx-svc-node 10.12.95.227 \u003cnodes\u003e 80/TCP 17s name=nginx [root@k8s-node-1 ~]#kubectl describe svc nginx-svc-node |grep NodePort Type: NodePort NodePort: \u003cunset\u003e 32669/TCP # 使用 ALL node节点物理IP + 端口访问 http://10.6.0.140:32669 http://10.6.0.187:32669 http://10.6.0.188:32669 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:21:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"5.2 部署一个 zookeeper 集群 编写 一个 zookeeper-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-1 spec: replicas: 1 template: metadata: labels: name: zookeeper-1 spec: containers: - name: zookeeper-1 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: NODES value: \"0.0.0.0,zookeeper-2,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-2 spec: replicas: 1 template: metadata: labels: name: zookeeper-2 spec: containers: - name: zookeeper-2 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: NODES value: \"zookeeper-1,0.0.0.0,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-3 spec: replicas: 1 template: metadata: labels: name: zookeeper-3 spec: containers: - name: zookeeper-3 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: NODES value: \"zookeeper-1,zookeeper-2,0.0.0.0\" ports: - containerPort: 2181 --- apiVersion: v1 kind: Service metadata: name: zookeeper-1 labels: name: zookeeper-1 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-1 --- apiVersion: v1 kind: Service metadata: name: zookeeper-2 labels: name: zookeeper-2 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-2 --- apiVersion: v1 kind: Service metadata: name: zookeeper-3 labels: name: zookeeper-3 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-3 [root@k8s-node-1 ~]#kubectl create -f zookeeper-cluster.yaml --record [root@k8s-node-1 ~]#kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE zookeeper-1-2149121414-cfyt4 1/1 Running 0 51m 10.32.0.3 k8s-node-2 zookeeper-2-2653289864-0bxee 1/1 Running 0 51m 10.40.0.1 k8s-node-3 zookeeper-3-3158769034-5csqy 1/1 Running 0 51m 10.40.0.2 k8s-node-3 [root@k8s-node-1 ~]#kubectl get deployment -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE zookeeper-1 1 1 1 1 51m zookeeper-2 1 1 1 1 51m zookeeper-3 1 1 1 1 51m [root@k8s-node-1 ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR zookeeper-1 10.8.111.19 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-1 zookeeper-2 10.6.10.124 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-2 zookeeper-3 10.0.146.143 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-3 ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:22:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"5.3 部署一个 kafka 集群 编写 一个 kafka-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-1 spec: replicas: 1 template: metadata: labels: name: kafka-1 spec: containers: - name: kafka-1 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-2 spec: replicas: 1 template: metadata: labels: name: kafka-2 spec: containers: - name: kafka-2 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-3 spec: replicas: 1 template: metadata: labels: name: kafka-3 spec: containers: - name: kafka-3 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: v1 kind: Service metadata: name: kafka-1 labels: name: kafka-1 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-1 --- apiVersion: v1 kind: Service metadata: name: kafka-2 labels: name: kafka-2 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-2 --- apiVersion: v1 kind: Service metadata: name: kafka-3 labels: name: kafka-3 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-3 FAQ: ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:23:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["kubernetes"],"content":"kube-discovery error failed to create \"kube-discovery\" deployment [deployments.extensions \"kube-discovery\" already exists] kubeadm reset kubeadm init ","date":"2016-12-13","objectID":"/kubernetes-1.5.0/:24:0","tags":null,"title":"kubernetes 1.5.1","uri":"/kubernetes-1.5.0/"},{"categories":["python"],"content":"Python 学习之路","date":"2016-12-01","objectID":"/python-learning/","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":" Python 学习之路，Python 学习笔记 Python Logging 日志模块 ","date":"2016-12-01","objectID":"/python-learning/:0:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"屏幕与文件输出 import logging # 创建一个 logging 对象, TEST-LOG 为定义这个LOG的 name [ %(name)s ] logger = logging.getLogger('TEST-LOG') # 设置日志 级别为 DEBUG, 全局日志 级别 全局级别 优先级高 logger.setLevel(logging.DEBUG) # 创建一个 用于 屏幕输出 的 StreamHandeler ch = logging.StreamHandler() # 设置日志 级别为 DEBUG ch.setLevel(logging.DEBUG) # 创建一个 用于 文件输出 的 FileHandler, 并输出到 access.log 文件 fh = logging.FileHandler(\"access.log\") # 设置日志 级别为 WARNING fh.setLevel(logging.WARNING) # 创建一个 日志格式 Formatter ( formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') # 让 屏幕输出，与 文件输出 都按照 formatter 这个格式生成 ch.setFormatter(formatter) fh.setFormatter(formatter) # 讲 StreamHandeler 与 FileHandler 添加到 logger 这个对象中。 logger.addHandler(ch) logger.addHandler(fh) # 'application' code logger.debug('debug message') logger.info('info message') logger.warn('warn message') logger.error('error message') logger.critical('critical message') ","date":"2016-12-01","objectID":"/python-learning/:1:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"日志文件 轮转 import logging from logging.handlers import RotatingFileHandler #定义一个RotatingFileHandler，最多备份5个日志文件，每个日志文件最大10M Rthandler = RotatingFileHandler('myapp.log', maxBytes=10*1024*1024,backupCount=5) Rthandler.setLevel(logging.INFO) formatter = logging.Formatter('%(name)-12s: %(levelname)-8s %(message)s') Rthandler.setFormatter(formatter) logging.getLogger('').addHandler(Rthandler) ","date":"2016-12-01","objectID":"/python-learning/:2:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"handle方法 logging.StreamHandler: 日志输出到流，可以是sys.stderr、sys.stdout或者文件 logging.FileHandler: 日志输出到文件 日志回滚方式，实际使用时用RotatingFileHandler和TimedRotatingFileHandler logging.handlers.BaseRotatingHandler logging.handlers.RotatingFileHandler logging.handlers.TimedRotatingFileHandler logging.handlers.SocketHandler: 远程输出日志到TCP/IP sockets logging.handlers.DatagramHandler: 远程输出日志到UDP sockets logging.handlers.SMTPHandler: 远程输出日志到邮件地址 logging.handlers.SysLogHandler: 日志输出到syslog logging.handlers.NTEventLogHandler: 远程输出日志到Windows NT/2000/XP的事件日志 logging.handlers.MemoryHandler: 日志输出到内存中的制定buffer logging.handlers.HTTPHandler: 通过\"GET\"或\"POST\"远程输出到HTTP服务器 Python 面相对象编程 ","date":"2016-12-01","objectID":"/python-learning/:3:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"类的 实例化 # 把一个抽象的类变成一个具体的对象的过程， 叫实例化。 class Role(object): def __init__(self,name,role,weapon,life): self.name = name self.role = role self.weapon = weapon self.life = life def buy_weapon(self,weapon): self.weapon = weapon print(\"[%s] buy weapon [%s]\" %(self.name,weapon)) def info(self): print(''' ------个人信息------- 姓名：%s 身份：%s 武器：%s 生命：%s --------------------- ''' %(self.name,self.role,self.weapon,self.life)) p1 = Role('jicki','警察','手枪',100) p2 = Role('luck','匪徒','机关枪',100) p1.info() p2.info() p1.buy_weapon('AK47') p2.buy_weapon('狙击枪') p1.info() p2.info() 打印结果： ------个人信息------- 姓名：jicki 身份：警察 武器：手枪 生命：100 --------------------- ------个人信息------- 姓名：luck 身份：匪徒 武器：机关枪 生命：100 --------------------- [jicki] buy weapon [AK47] [luck] buy weapon [狙击枪] ------个人信息------- 姓名：jicki 身份：警察 武器：AK47 生命：100 --------------------- ------个人信息------- 姓名：luck 身份：匪徒 武器：狙击枪 生命：100 --------------------- ","date":"2016-12-01","objectID":"/python-learning/:4:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"类的继承 class ShoolMember(object): def __init__(self,name,age,sex): self.name = name self.age = age self.sex = sex self.enroll() def enroll(self): print(\"[%s] 加入了学校\" %self.name) def tell(self): # 所有类 公有方法 print(\"大家好，我的名字叫[%s]\" %self.name) class Teacher(ShoolMember): #继承了 ShoolMember 类 def __init__(self,name,age,sex,coures,salary): # 重写 ShoolMember 父类的构造方法__init__ super(Teacher,self).__init__(name,age,sex) # 新式类 的继承，继承了父类的__init__ self.coures = coures self.salary = salary def teaching(self): # Tenacher 类 私有方法 print(\"[%s] 是教 [%s] 课程\" %(self.name,self.coures)) class Student(ShoolMember): #继承 ShoolMember 类 def __init__(self,name,age,sex,coures,tuition): # 重写 ShoolMember 父类的构造方法__init__ super(Student,self).__init__(name,age,sex) # 新式类 的继承，继承了父类的__init__ self.coures = coures self.tuition = tuition def pay_tuition(self): # Student 类 私有方法 print(\"[%s] 学习 [%s] 课程交了 [%s] 学费\" %(self.name,self.coures,self.tuition)) # 实例化 t1 = Teacher(\"alax\",22,'F','Python',10000) t2 = Teacher('wupeiqi',23,'F','Python',10000) s1 = Student('jicki',22,'F','Python',6000) t1.teaching() # 调用 Teacher 私有方法 s1.pay_tuition() # 调用 Student 私有方法 t1.tell() # 调用共用 方法 s1.tell() # 调用共用 方法 输出结果： [alax] 加入了学校 [wupeiqi] 加入了学校 [jicki] 加入了学校 [alax] 是教 [Python] 课程 [jicki] 学习 [Python] 课程交了 [6000] 学费 大家好，我的名字叫[alax] 大家好，我的名字叫[jicki] ","date":"2016-12-01","objectID":"/python-learning/:5:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"多态 Python 多态： 多态性 允许子类类型的指针赋值给父类类型的指针。 多态的作用是什么？ 我们知道，封装可以隐藏实现细节，使的代码模块化； 继承可以扩展已存在的代码模块（类）； 封装与继承的目的都是为了 代码重用。 而多态则是为了实现 接口重用，多态的作用就是为了类的继承和派生的时候，保证能使用类的成员中任意类的实例的某一属性时的正确调用。 Python 是不支持多态的，可以使用如下方法实现： class Animal(object): def __init__(self,name): self.name = name def talk(self): raise NotImplementedError(\"Subclass must implement\") class Cat(Animal): def talk(self): return 'miao, miao !' class Dog(Animal): def talk(self): return 'wang wang !' def animal_talk(obj): print(obj.talk()) d = Dog(\"WangCai\") c = Cat(\"BoSi\") animal_talk(c) animal_talk(d) 输出结果： miao, miao ! wang wang ! Python 反射 import sys class WebServer(object): def __init__(self,host,port): self.host = host self.port = port def start(self): print('service start.....') def stop(self): print('service stop......') def restart(self): self.stop() self.start() if __name__ == '__main__': server = WebServer('localhost',9999) if hasattr(server,sys.argv[1]): #判断server这个objcet 中是否包含 sys.argv[1] ，既然用户输入的这个 方法 fun = getattr(server,sys.argv[1]) #获取server 中这个objcet 的 sys.argv[1] ，既用户输入的这个 方法 fun() # 加上() 调用 Python Socket ","date":"2016-12-01","objectID":"/python-learning/:6:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"服务端例子 import socket ip_port = ('127.0.0.1', 9999) # IP 与 端口，在一个 元组里面 sk = socket.socket() # socket 默认是 tcp 协议 sk.bind(ip_port) # 绑定 IP 与 端口 sk.listen(5) # 服务端最大连接数 while True: print('等待连接....') conn,addr = sk.accept() #accpet 返回两个变量 conn 是客户端连接过来时创建的实例。addr 是客户端的IP地址。 print('客户段连接IP ', addr) #当客户端连接过来时，打印 客户端的IP地址。 client_data = conn.recv(1024) # client_data 这个变量等于 客户端发送的数据 (1024) = 1K 。 print(str(client_data,'utf8')) # 打印客户端发送过来的信息，因为是中文所以python 3.0 中要使用 str 来声明 utf8 conn.sendall(bytes('服务端发来消息...','utf8')) # 发送这条信息到客户端,python 3.0 要使用 bytes 来发送, 并声明 utf8 conn.close() ","date":"2016-12-01","objectID":"/python-learning/:7:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"客户端例子 import socket ip_port = ('127.0.0.1', 9999) # IP 与 端口，在一个 元组里面 sk = socket.socket() # socket 默认是 tcp 协议 sk.connect(ip_port) # 连接服务端 IP 与 端口 sk.sendall(bytes('客户端发来消息....','utf8')) # 发送这条信息到服务端,python 3.0 要使用 bytes 来发送, 并声明 utf8 server_reply = sk.recv(1024) # server_reply 这个变量等于 客户端发送的数据 (1024) = 1K 。 print(str(server_reply,'utf8')) # 打印服务端发送过来的信息，因为是中文所以python 3.0 中要使用 str 来声明 utf8 sk.close() # 关闭连接 Python 异常处理 try: pass except Exception as e: pass ","date":"2016-12-01","objectID":"/python-learning/:8:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"常用的异常列表 AttributeError 试图访问一个对象没有的树形，比如foo.x，但是foo没有属性x IOError 输入/输出异常；基本上是无法打开文件 ImportError 无法引入模块或包；基本上是路径问题或名称错误 IndentationError 语法错误（的子类） ；代码没有正确对齐 IndexError 下标索引超出序列边界，比如当x只有三个元素，却试图访问x[5] KeyError 试图访问字典里不存在的键 KeyboardInterrupt Ctrl+C被按下 NameError 使用一个还未被赋予对象的变量 SyntaxError Python代码非法，代码不能编译(个人认为这是语法错误，写错了） TypeError 传入对象类型与要求的不符合 UnboundLocalError 试图访问一个还未被设置的局部变量，基本上是由于另有一个同名的全局变量， 导致你以为正在访问它 ValueError 传入一个调用者不期望的值，即使值的类型是正确的 ","date":"2016-12-01","objectID":"/python-learning/:9:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"抓住特定错误 s1 = input('\u003e\u003e\u003e') try: int(s1) except KeyError as e: print('键错误') except ValueError as e: print('Value 错误') except Exception as e: print('错误: ', e ) ","date":"2016-12-01","objectID":"/python-learning/:10:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"异常在程序中的应用 try: # 主代码块 pass except KeyError as e: #出现KeyError时，执行如下程序 pass else: # 主代码块执行完，执行该块 pass finally: # 无论异常与否，都会执行该块 pass ","date":"2016-12-01","objectID":"/python-learning/:11:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"主动触发异常 try: raise Exception('抛出异常') except Exception as e: print(e) ","date":"2016-12-01","objectID":"/python-learning/:12:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"自定义异常 class MyException(Exception): #创建一个类，继承 Exception def __init__(self, msg): self.message = msg def __str__(self): return self.message try: raise MyException('我的异常') #主动触发MyException except MyException as e: print(e) ","date":"2016-12-01","objectID":"/python-learning/:13:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"异常中的断言 class MyException(Exception): #创建一个类，继承 Exception def __init__(self, msg): self.message = msg def __str__(self): return self.message a = 1 #定义一个变量 try: assert a == 2 # 断言 a 是否 等于 2 ,不等于就抛出 AssertionError 这个错误 except MyException as e: print(e) Python 线程的调用 Python 启动线程有2种方法，分别为 直接调用 与 继承调用。 ","date":"2016-12-01","objectID":"/python-learning/:14:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"直接调用 import threading import time def sayhi(num): print('运行线程号为： %s' %num) time.sleep(3) if __name__ == '__main__': t1 = threading.Thread(target=sayhi,args=(1,)) #创建一个线程 t1 t2 = threading.Thread(target=sayhi,args=(2,)) #创建一个线程 t2 t1.start() #运行线程t1 t2.start() #运行线程t2 print(t1.getName()) #打印线程t1名称 print(t2.getName()) #打印线程t2名称 t1.join() #join 等待线程t1运行完毕再往下走 t2.join() #join 等待线程t2运行完毕再往下走 print(\"运行完毕\") ","date":"2016-12-01","objectID":"/python-learning/:15:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"直接调用，多线程 import threading import time def sayhi(num): print('运行线程号为： %s' %num) time.sleep(3) if __name__ == '__main__': t_list = [] for i in range(10): # 循环创建 10个 线程。 t = threading.Thread(target=sayhi,args=[i,]) t.start() t_list.append(t) # 将创建的进程 写入 t_list 这个列表中。 for i in t_list: # 循环列表，让所有线程，join ,执行完毕。 i.join() print(\"运行完毕\") ","date":"2016-12-01","objectID":"/python-learning/:16:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"继承的方式 import threading import time class MyThread(threading.Thread): #定义一个类，继承 threading.Thread 这个父类。 def __init__(self,num): threading.Thread.__init__(self) # 重写父类的构造函数。 self.num = num def run(self): #定义每个线程要运行的函数 print(\"线程运行号:%s\" %self.num) time.sleep(3) if __name__ == '__main__': t1 = MyThread(1) t2 = MyThread(2) t1.start() t2.start() ","date":"2016-12-01","objectID":"/python-learning/:17:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"继承方式，增加join import threading import time class MyThread(threading.Thread): #定义一个类，继承 threading.Thread 这个父类。 def __init__(self,num): threading.Thread.__init__(self) # 重写父类的构造函数。 self.num = num def run(self): #定义每个线程要运行的函数 print(\"线程运行号:%s\" %self.num) time.sleep(3) print('线程 %s 运行完毕' %self.num) if __name__ == '__main__': t1 = MyThread(1) t2 = MyThread(2) t1.start() t2.start() t1.join() t2.join() ","date":"2016-12-01","objectID":"/python-learning/:18:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"多线程 Events # Event 对象用于线程间通信，它是由线程设置的信号标志，如果信号标志位真，则其他线程等待直到信号接触。 # event = threading.Event() 生成实例 # event.wait() 等待设定，既 线程信号标签为 假, 阻塞。 # event.set() 设置标签，既 线程信号标签为 真，通行。 # event.clear() 清除设定。 # event.isSet() 判断是否有设定。 import threading import time def light(): if not event.isSet(): event.set() count = 0 while True: if count \u003c 10: print('\\033[42;1m --绿灯-- \\033[0m') elif count \u003c 13: print('\\033[43;1m --黄灯-- \\033[0m') elif count \u003c 20: if event.is_set(): event.clear() print('\\033[41;1m --红灯-- \\033[0m') else: count = 0 event.set() print('\\033[42;1m --绿灯-- \\033[0m') time.sleep(1) count += 1 def car(n): while 1: time.sleep(1) if event.isSet(): print(\"汽车 [%s] 通过红绿灯 \" %n) else: print(\"汽车 [%s] 等待红绿灯 \" %n) event.wait() if __name__ == '__main__': event = threading.Event() Light = threading.Thread(target=light) Light.start() for i in range(3): t = threading.Thread(target=car,args=(i,)) t.start() ","date":"2016-12-01","objectID":"/python-learning/:19:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"多进程(multiprocessing) # 并发 执行 一个例子: from multiprocessing import Process import time def f(name): time.sleep(2) print(\"hello\", name) def f2(name): time.sleep(2) print(\"hello\", name) if __name__ == \"__main__\": p1 = Process(target=f, args=('bob1',)) p2 = Process(target=f, args=('bob2',)) p3 = Process(target=f2, args=('pop1',)) p4 = Process(target=f2, args=('pop2',)) p5 = Process(target=f2, args=('pop3',)) p1.start() p2.start() p3.start() p4.start() p5.start() p5.join() Queue 队列 ","date":"2016-12-01","objectID":"/python-learning/:20:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"多进程间通讯 Queues # 不同进程间内存是不共享的,要想实现进程间数据交换，可使用 Queues from multiprocessing import Process,Queue def f(q): q.put([1,None, 'hello']) q.put([2,None, 'word']) if __name__ == '__main__': q = Queue() p = Process(target=f, args=(q,)) p2 = Process(target=f, args=(q,)) p.start() p2.start() print('p-1 get :' , q.get()) print('p-2 get :' , q.get()) print('p2-1 get :' , q.get()) print('p2-3 get :' , q.get()) p.join() ","date":"2016-12-01","objectID":"/python-learning/:21:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"生产者消费者模型 import queue,threading,time q = queue.Queue() def consumer(n): while True: print(\"消费者-[ %s]， 消费了 [ %s]\" % (n,q.get())) time.sleep(1) q.task_done() def profucer(n): count = 1 while True: print(\"生产者-[ %s] 生产了 [ %s] 个\" %(n,count)) q.put(count) q.join() print(\"已经全部消费完毕\") c1 = threading.Thread(target=consumer,args=[\"消费者1\",]) c2 = threading.Thread(target=consumer,args=[\"消费者2\",]) c3 = threading.Thread(target=consumer,args=[\"消费者3\",]) p1 = threading.Thread(target=profucer,args=[\"生产者1\",]) p2 = threading.Thread(target=profucer,args=[\"生产者2\",]) p3 = threading.Thread(target=profucer,args=[\"生产者3\",]) c1.start() c2.start() c3.start() p1.start() p2.start() p3.start() ","date":"2016-12-01","objectID":"/python-learning/:22:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"协程 # gevent 模块 import gevent def foo(): print(\"运行foo\") gevent.sleep(1) print(\"再次运行foo\") def b1(): print(\"运行b1\") gevent.sleep(1) print(\"再次运行b1\") def c1(): print(\"运行c1\") gevent.sleep(1) print(\"再次运行c1\") gevent.joinall([ gevent.spawn(foo), gevent.spawn(b1), gevent.spawn(c1) ]) ","date":"2016-12-01","objectID":"/python-learning/:23:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"异步IO模型 select poll Epoll ","date":"2016-12-01","objectID":"/python-learning/:24:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"Python - Mysql python 操作 mysql # MySQLdb 模块 只能在 Python 2.7 中使用 Python3 以后不支持 # python 3.x 请使用 pymysql 模块 import pymysql conn = pymysql.connect(host='127.0.0.1', user='jicki', passwd='123456',db='jicki') cur = conn.cursor() cur.execute(\"INSERT INTO user (username,password,encrypt,valid) VALUES ('raid','123456','1','2')\") cur.execute(\"SELECT * FROM user\") # fetchone() 取一条数据 # fetchamany(3) 取指定条数数据 # fetchall() 取所有数据 for r in cur.fetchall(): print(r) cur.close() # conn.rollback() # 回滚 conn.commit() # 写入数据库 conn.close() # 利用列表 与 executemany 批量插入数据 import pymysql li = [ ('jicki', '1234', '2', '3'), ('xiao', '3333','4','5'), ('dada', '4444', '6', '7'), ] conn = pymysql.connect(host='127.0.0.1', user='jicki', passwd='123456',db='jicki') cur = conn.cursor() cur.executemany(\"INSERT INTO user(username,password,encrypt,valid) VALUES(%s,%s,%s,%s)\", li) cur.execute(\"SELECT * FROM user\") for r in cur.fetchall(): print(r) cur.close() conn.commit() conn.close() ","date":"2016-12-01","objectID":"/python-learning/:25:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"事件驱动 事件驱动 分为二部分，注册事件 与 触发事件 ","date":"2016-12-01","objectID":"/python-learning/:26:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"Python Redis 使用 redis 模块 pip install redis # 简单的操作 import redis r = redis.Redis(host='127.0.0.1',port=6379) r.set(\"name\", \"jicki\") print(r.get(\"name\")) # 连接池 的方式连接 import redis pool = redis.ConnectionPool(host=\"127.0.0.1\",port=6379) r = redis.Redis(connection_pool=pool) r.set(\"name\", \"jicki\") print(r.get(\"name\")) # set 操作的参数 ex 过期时间 (秒) px 过期时间 (毫秒) nx 为 True 时 只有 键值 不存在时 才可 set xx 为 True 时 只有 键值 存在时 才可 set # 例: r.set(\"name\", \"jicki\", ex=3) r.set(\"name\", \"jicki\", px=100) r.set(\"name\", \"jicki\", nx=True) r.set(\"name\", \"jicki\", xx=True) # Hash 操作 import redis pool = redis.ConnectionPool(host=\"127.0.0.1\",port=6379) r = redis.Redis(connection_pool=pool) r.hmset(\"info\", {'fo1':'o1','fo2':'o2', 'fo3':'o3'}) print(r.hget(\"info\",'fo1')) print(r.hgetall(\"info\")) ","date":"2016-12-01","objectID":"/python-learning/:27:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"redis 订阅 与 发布 # 创建一个类 import redis redis_pool = redis.ConnectionPool(host=\"127.0.0.1\", port=6379) # 创建一个连接池 class RedisHelper(object): def __init__(self): self.__conn = redis.Redis(connection_pool=redis_pool) # 配置连接池 self.chan_sub = 'fm109' # 订阅频道号 self.chan_pub = 'fm109' # 发布频道号 def public(self,msg): # 定义了一个发布的函数 self.__conn.publish(self.chan_pub,msg) # 发布消息 return True def subscribe(self): # 定义一个 订阅 的函数 pub = self.__conn.pubsub() # 生成一个 订阅的实例 pub.subscribe(self.chan_sub) # 选择订阅频道 pub.parse_response() # 接收发布的消息 return pub # 订阅端 from redis_helper import RedisHelper obj = RedisHelper() # 实例化 redis_sub = obj.subscribe() # 订阅者调用 subscribe 这个函数 while True: # 循环接收 发布者 发送的消息 msg = redis_sub.parse_response() # 收听 订阅频道 print(msg) # 打印消息 # 发布端 from redis_helper import RedisHelper obj = RedisHelper() # 实例化 while True: msg = input(\"请输入发布的信息: \").strip() if len(msg) == 0: continue else: redis_pub = obj.public(msg) ","date":"2016-12-01","objectID":"/python-learning/:28:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"Python - RabbitMQ # Python 使用pika 模块操作 RabbitMQ # RabbitMQ 发送端 (生产者) import pika # 创建认证 credentials = pika.PlainCredentials('jicki', '123456') # 定义 rabbitmq 的连接 conn = pika.BlockingConnection(pika.ConnectionParameters('127.0.0.1',5672, '/', credentials)) # 生成一个管道 channel = conn.channel() # 创建一个队列 名 (durable=True 设置为持久化队列) channel.queue_declare(queue='hello', durable=True) # 发送消息 channel.basic_publish(exchange='', routing_key='hello', body='hello jicki ') print(\" 消息 发送成功\" ) # 关闭连接 conn.close() # RabbitMQ 接收端 (消费者) import pika # 创建认证 credentials = pika.PlainCredentials('jicki', '123456') # 定义 rabbitmq 的连接 conn = pika.BlockingConnection(pika.ConnectionParameters('127.0.0.1',5672, '/', credentials)) # 生成一个管道 channel = conn.channel() # 创建一个队列 名 (durable=True 设置为持久化队列) channel.queue_declare(queue='hello', durable=True) def callback(ch, method, properties, body): print(\"返回信息 %r\" %body) # 设置同时只处理一个任务(负载均衡, 如果不设置，会轮询) channel.basic_qos(prefetch_count=1) # 声明队列信息 channel.basic_consume(callback, queue='hello', no_ack=False) print('等待队列消息') # 开始接收任务 channel.start_consuming() ","date":"2016-12-01","objectID":"/python-learning/:29:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"RabbitMQ 发布与订阅 发布端代码 import pika # 创建认证 credentials = pika.PlainCredentials('jicki', '123456') # 定义 rabbitmq 的连接 conn = pika.BlockingConnection(pika.ConnectionParameters('10.6.0.188',5672, '/', credentials)) # 生成一个管道 channel = conn.channel() # 创建一个 exchange channel.exchange_declare(exchange='logs',type='fanout') # 发布的消息 message = ''.join(sys.argv[1:]) or 'Hello jicki' # 往 exchange 发送消息 channel.basic_publish(exchange='logs', routing_key='', body=message) print('消息 [ %r] 发送成功' %message) conn.close() 订阅端代码 import pika # 创建认证 credentials = pika.PlainCredentials('jicki', '123456') # 定义 rabbitmq 的连接 conn = pika.BlockingConnection(pika.ConnectionParameters('10.6.0.188',5672, '/', credentials)) # 生成一个管道 channel = conn.channel() # 声明这个 exchange channel.exchange_declare(exchange='logs', type='fanout') # 声明一个 queue, 不需要指定 queue 名 # rabbitmq 会随机生成一个 queue , 当消费者断开后，queue 会自动删除 result = channel.queue_declare(exclusive=True) queue_name = result.method.queue # 将这个 queue 与 exchange 绑定 channel.queue_bind(exchange='logs', queue=queue_name) print(\"等待消息.....\") def callbak(ch, method, properties, body): print(\" [ %r]\" %body) # 声明队列信息 channel.basic_consume(callbak, queue=queue_name, no_ack=True) # 开始接受消息 channel.start_consuming() ","date":"2016-12-01","objectID":"/python-learning/:30:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["python"],"content":"RabbitMQ 带过滤的订阅与发布 topic pub 端 import pika import sys # 创建认证 credentials = pika.PlainCredentials('jicki', '123456') # 定义 rabbitmq 的连接 conn = pika.BlockingConnection(pika.ConnectionParameters('10.6.0.188',5672, '/', credentials)) # 生成一个管道 channel = conn.channel() channel.exchange_declare(exchange='topic_logs', type='topic') routing_key = sys.argv[1] if len(sys.argv) \u003e 1 else 'anonymous.info' message = ''.join(sys.argv[2:]) or 'hello jicki' channel.basic_publish(exchange= 'topic_logs', routing_key=routing_key, body=message) print('发送 [ %r:%r]' %(routing_key, message)) conn.close() topic sub 端 import pika import sys # 创建认证 credentials = pika.PlainCredentials('jicki', '123456') # 定义 rabbitmq 的连接 conn = pika.BlockingConnection(pika.ConnectionParameters('10.6.0.188',5672, '/', credentials)) # 生成一个管道 channel = conn.channel() channel.exchange_declare(exchange='topic_logs', type='topic') result = channel.queue_declare(exclusive=True) queue_name = result.method.queue binding_keys = sys.argv[1:] if not binding_keys: sys.stderr.write(\"%s[binding_keys] \\n\" % sys.argv[0]) sys.exit(1) for binding_keys in binding_keys: channel.queue_bind(exchange='topic_logs', queue=queue_name, routing_key=binding_keys) print('等待消息.......') def callback(ch, method, properties, body): print('消息 [ %r:%r] 发送成功' % (method.routing_key, body)) channel.basic_consume(callback, queue=queue_name, no_ack=True) channel.start_consuming() ","date":"2016-12-01","objectID":"/python-learning/:31:0","tags":null,"title":"Python 学习之路","uri":"/python-learning/"},{"categories":["kubernetes"],"content":"kubernetes 实战应用部署","date":"2016-11-04","objectID":"/kubernetes-application/","tags":null,"title":"kubernetes 实战应用部署","uri":"/kubernetes-application/"},{"categories":["kubernetes"],"content":" kubernetes 一些应用的部署， yaml 文件编写 1 kubernetes 应用部署 ","date":"2016-11-04","objectID":"/kubernetes-application/:0:0","tags":null,"title":"kubernetes 实战应用部署","uri":"/kubernetes-application/"},{"categories":["kubernetes"],"content":"1.1 部署一个 nginx rc 编写 一个 nginx yaml apiVersion: v1 kind: ReplicationController metadata: name: nginx-rc spec: replicas: 2 selector: name: nginx template: metadata: labels: name: nginx spec: containers: - name: nginx image: nginx imagePullPolicy: IfNotPresent ports: - containerPort: 80 [root@k8s-master ~]#kubectl get rc NAME DESIRED CURRENT READY AGE nginx-rc 2 2 2 2m [root@k8s-master ~]#kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE nginx-rc-2s8k9 1/1 Running 0 10m 10.32.0.3 k8s-node-1 nginx-rc-s16cm 1/1 Running 0 10m 10.40.0.1 k8s-node-2 编写一个 nginx service 让集群内部容器可以访问 (ClusterIp) apiVersion: v1 kind: Service metadata: name: nginx-svc spec: ports: - port: 80 targetPort: 80 protocol: TCP selector: name: nginx [root@k8s-master ~]#kubectl create -f nginx-svc.yaml service \"nginx-svc\" created [root@k8s-master ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29s name=nginx 编写一个 curl 的pods apiVersion: v1 kind: Pod metadata: name: curl spec: containers: - name: curl image: radial/busyboxplus:curl command: - sh - -c - while true; do sleep 1; done # 测试pods 内部通信 [root@k8s-master ~]#kubectl exec curl curl nginx # 在任何node节点中，可使用ip访问 [root@k8s-node-1 ~]# curl 10.6.164.79 [root@k8s-node-2 ~]# curl 10.6.164.79 编写一个 nginx service 让外部可以访问 (NodePort) apiVersion: v1 kind: Service metadata: name: nginx-svc-node spec: ports: - port: 80 targetPort: 80 protocol: TCP type: NodePort selector: name: nginx [root@k8s-master ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR kubernetes 10.0.0.1 \u003cnone\u003e 443/TCP 2d \u003cnone\u003e nginx-svc 10.6.164.79 \u003cnone\u003e 80/TCP 29m name=nginx nginx-svc-node 10.12.95.227 \u003cnodes\u003e 80/TCP 17s name=nginx [root@k8s-master ~]#kubectl describe svc nginx-svc-node |grep NodePort Type: NodePort NodePort: \u003cunset\u003e 32669/TCP # 使用 ALL node节点物理IP + 端口访问 http://localhost:32669 ","date":"2016-11-04","objectID":"/kubernetes-application/:1:0","tags":null,"title":"kubernetes 实战应用部署","uri":"/kubernetes-application/"},{"categories":["kubernetes"],"content":"1.2 部署一个 zookeeper 集群 编写 一个 zookeeper-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-1 spec: replicas: 1 template: metadata: labels: name: zookeeper-1 spec: containers: - name: zookeeper-1 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: NODES value: \"0.0.0.0,zookeeper-2,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-2 spec: replicas: 1 template: metadata: labels: name: zookeeper-2 spec: containers: - name: zookeeper-2 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: NODES value: \"zookeeper-1,0.0.0.0,zookeeper-3\" ports: - containerPort: 2181 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: zookeeper-3 spec: replicas: 1 template: metadata: labels: name: zookeeper-3 spec: containers: - name: zookeeper-3 image: zk:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: NODES value: \"zookeeper-1,zookeeper-2,0.0.0.0\" ports: - containerPort: 2181 --- apiVersion: v1 kind: Service metadata: name: zookeeper-1 labels: name: zookeeper-1 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-1 --- apiVersion: v1 kind: Service metadata: name: zookeeper-2 labels: name: zookeeper-2 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-2 --- apiVersion: v1 kind: Service metadata: name: zookeeper-3 labels: name: zookeeper-3 spec: ports: - name: client port: 2181 protocol: TCP - name: followers port: 2888 protocol: TCP - name: election port: 3888 protocol: TCP selector: name: zookeeper-3 [root@k8s-master ~]#kubectl create -f zookeeper-cluster.yaml --record [root@k8s-master ~]#kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE zookeeper-1-2149121414-cfyt4 1/1 Running 0 51m 10.32.0.3 k8s-node-1 zookeeper-2-2653289864-0bxee 1/1 Running 0 51m 10.40.0.1 k8s-node-2 zookeeper-3-3158769034-5csqy 1/1 Running 0 51m 10.40.0.2 k8s-node-2 [root@k8s-master ~]#kubectl get deployment -o wide NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE zookeeper-1 1 1 1 1 51m zookeeper-2 1 1 1 1 51m zookeeper-3 1 1 1 1 51m [root@k8s-master ~]#kubectl get svc -o wide NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR zookeeper-1 10.8.111.19 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-1 zookeeper-2 10.6.10.124 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-2 zookeeper-3 10.0.146.143 \u003cnone\u003e 2181/TCP,2888/TCP,3888/TCP 51m name=zookeeper-3 ","date":"2016-11-04","objectID":"/kubernetes-application/:2:0","tags":null,"title":"kubernetes 实战应用部署","uri":"/kubernetes-application/"},{"categories":["kubernetes"],"content":"1.3 部署一个 kafka 集群 编写 一个 kafka-cluster.yaml apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-1 spec: replicas: 1 template: metadata: labels: name: kafka-1 spec: containers: - name: kafka-1 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"1\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-2 spec: replicas: 1 template: metadata: labels: name: kafka-2 spec: containers: - name: kafka-2 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"2\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: extensions/v1beta1 kind: Deployment metadata: name: kafka-deployment-3 spec: replicas: 1 template: metadata: labels: name: kafka-3 spec: containers: - name: kafka-3 image: kafka:alpine imagePullPolicy: IfNotPresent env: - name: NODE_ID value: \"3\" - name: ZK_NODES value: \"zookeeper-1,zookeeper-2,zookeeper-3\" ports: - containerPort: 9092 --- apiVersion: v1 kind: Service metadata: name: kafka-1 labels: name: kafka-1 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-1 --- apiVersion: v1 kind: Service metadata: name: kafka-2 labels: name: kafka-2 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-2 --- apiVersion: v1 kind: Service metadata: name: kafka-3 labels: name: kafka-3 spec: ports: - name: client port: 9092 protocol: TCP selector: name: kafka-3 ","date":"2016-11-04","objectID":"/kubernetes-application/:3:0","tags":null,"title":"kubernetes 实战应用部署","uri":"/kubernetes-application/"},{"categories":["kubernetes"],"content":"kubernetes 1.4.5","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":" 更新 kubernetes 1.4.5 , 配置文档 1 初始化环境 ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:0:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"1.1 环境： 节点 IP node-1 10.6.0.140 node-2 10.6.0.187 node-3 10.6.0.188 ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:1:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"1.2 设置hostname hostnamectl --static set-hostname hostname IP hostname 10.6.0.140 k8s-master 10.6.0.187 k8s-node-1 10.6.0.188 k8s-node-2 ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:2:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"1.3 配置 hosts vi /etc/hosts IP hostname 10.6.0.140 k8s-master 10.6.0.187 k8s-node-1 10.6.0.188 k8s-node-2 2 部署 kubernetes master ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:3:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"2.1 添加yum ------------------------------------------------------ cat \u003c\u003cEOF\u003e /etc/yum.repos.d/k8s.repo [kubelet] name=kubelet baseurl=http://files.rm-rf.ca/rpms/kubelet/ enabled=1 gpgcheck=0 EOF ------------------------------------------------------ yum makecache yum install -y socat kubelet kubeadm kubectl kubernetes-cni ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:4:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"2.2 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:5:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"2.3 下载镜像 images=(kube-proxy-amd64:v1.4.5 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.5 kube-controller-manager-amd64:v1.4.5 kube-apiserver-amd64:v1.4.5 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:6:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"2.4 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:7:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"2.5 创建集群 kubeadm init --api-advertise-addresses=10.6.0.140 --use-kubernetes-version v1.4.5 ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:8:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"2.6 记录 token kubernetes master initialised successfully! You can now join any number of machines by running the following on each node: kubeadm join --token=cf9fd4.7a477d4299305b93 10.6.0.140 ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:9:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"2.7 检查 kubelet 状态 systemctl status kubelet 3 部署 kubernetes node ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:10:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"3.1 安装docker wget -qO- https://get.docker.com/ | sh systemctl enable docker systemctl start docker ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:11:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"3.2 下载镜像 images=(kube-proxy-amd64:v1.4.5 kube-discovery-amd64:1.0 kubedns-amd64:1.7 kube-scheduler-amd64:v1.4.5 kube-controller-manager-amd64:v1.4.5 kube-apiserver-amd64:v1.4.5 etcd-amd64:2.2.5 kube-dnsmasq-amd64:1.3 exechealthz-amd64:1.1 pause-amd64:3.0 kubernetes-dashboard-amd64:v1.4.1) for imageName in ${images[@]} ; do docker pull jicki/$imageName docker tag jicki/$imageName gcr.io/google_containers/$imageName docker rmi jicki/$imageName done ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:12:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"3.3 启动 kubernetes systemctl enable kubelet systemctl start kubelet ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:13:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"3.4 加入集群 kubeadm join --token=cf9fd4.7a477d4299305b93 10.6.0.140 Node join complete: * Certificate signing request sent to master and response received. * Kubelet informed of new secure connection details. Run 'kubectl get nodes' on the master to see this machine join. ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:14:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"3.5 查看集群状态 [root@k8s-master ~]#kubectl get nodes NAME STATUS AGE k8s-master Ready 8m k8s-node-1 Ready 1m k8s-node-2 Ready 1m 4 设置 kubernetes ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:15:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"4.1 配置 POD 网络 kubectl apply -f https://git.io/weave-kube daemonset \"weave-net\" created ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:16:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"4.2 查看系统服务状态 # kube-dns 必须配置完网络才能 Running [root@k8s-master ~]#kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system dummy-2088944543-io6ca 1/1 Running 0 22m kube-system etcd-k8s-master 1/1 Running 0 22m kube-system kube-apiserver-k8s-master 1/1 Running 0 22m kube-system kube-controller-manager-k8s-master 1/1 Running 0 20m kube-system kube-discovery-982812725-rm6ut 1/1 Running 0 22m kube-system kube-dns-2247936740-htw22 3/3 Running 0 21m kube-system kube-proxy-amd64-lo0hr 1/1 Running 0 15m kube-system kube-proxy-amd64-t3qpn 1/1 Running 0 15m kube-system kube-proxy-amd64-wwj2z 1/1 Running 0 21m kube-system kube-scheduler-k8s-master 1/1 Running 0 21m kube-system weave-net-6k3ha 2/2 Running 0 11m kube-system weave-net-auf0c 2/2 Running 0 11m kube-system weave-net-bxj6d 2/2 Running 0 11m ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:17:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"4.3 配置 flannel 网络 # 必须使用 --pod-network-cidr 指定网段(10.244.0.0/16 是因为 kube-flannel.yml 文件里面写了这个网段) kubeadm init --api-advertise-addresses 10.6.0.140 --use-kubernetes-version v1.4.5 --pod-network-cidr 10.244.0.0/16 # 导入yml文件 kubectl create -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:18:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"4.4 其他主机控制集群 # 备份master节点的 配置文件 /etc/kubernetes/admin.conf # 保存至 其他电脑, 通过执行配置文件控制集群 kubectl --kubeconfig ./admin.conf get nodes ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:19:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"4.5 配置dashboard #下载 yaml 文件, 直接导入会去官方拉取images curl -O https://rawgit.com/kubernetes/dashboard/master/src/deploy/kubernetes-dashboard.yaml #编辑 yaml 文件 vi kubernetes-dashboard.yaml image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.0 修改为 image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.4.1 imagePullPolicy: Always 修改为 imagePullPolicy: IfNotPresent kubectl create -f ./kubernetes-dashboard.yaml deployment \"kubernetes-dashboard\" created service \"kubernetes-dashboard\" created # 查看 NodePort ，既外网访问端口 kubectl describe svc kubernetes-dashboard --namespace=kube-system NodePort: \u003cunset\u003e 31736/TCP # 访问 dashboard http://localhost:31736 FAQ: ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:20:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["kubernetes"],"content":"kube-discovery error failed to create \"kube-discovery\" deployment [deployments.extensions \"kube-discovery\" already exists] systemctl stop kubelet; docker rm -f -v $(docker ps -q); find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v; rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd; systemctl start kubelet kubeadm init ","date":"2016-10-31","objectID":"/kubernetes-update-1.4.5/:21:0","tags":null,"title":"kubernetes 1.4.5","uri":"/kubernetes-update-1.4.5/"},{"categories":["docker"],"content":"docker 1.12 swarm 集群","date":"2016-10-25","objectID":"/docker-swarm-1.12/","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"docker 1.12 swarm ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:0:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"新特性 docker swarm：集群管理，子命令有init, join, leave, update docker service：服务创建，子命令有create, inspect, update, remove, tasks docker node：节点管理，子命令有accept, promote, demote, inspect, update, tasks, ls, rm docker stack/deploy：试验特性，用于多应用部署， 类似与 docker-compose 中的特性。 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:1:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"安装 docker wget -qO- https://get.docker.com/ | sh [root@swarm-manager ~]# docker version Client: Version: 1.12.1 API version: 1.24 Go version: go1.6.3 Git commit: 23cf638 Built: OS/Arch: linux/amd64 Server: Version: 1.12.1 API version: 1.24 Go version: go1.6.3 Git commit: 23cf638 Built: OS/Arch: linux/amd64 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:2:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"btrfs 文件系统 修改 docker 默认的存储系统 为 btrfs 官方文档 https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/ 前提: 系统 必须支持 btrfs (cat /proc/filesystems | grep btrfs) 需要 一块未使用的分区或者未使用的硬盘 PS: 如果系统正在使用，没有未使用的分区或者硬盘，可使用虚拟方式创建 #选择一个分区，如 /opt mkdir /opt/btrfsimg #创建一个 100G 的 img 文件 dd if=/dev/zero of=/opt/btrfsimg/btrfs.img bs=1024 count=102400000 #格式化为 btrfs mkfs.btrfs -L 'btrfs' /opt/btrfsimg/btrfs.img #查看类型 blkid /opt/btrfsimg/btrfs.img #停止 docker systemctl stop docker.service #清空 /var/lib/docker rm -rf /var/lib/docker/* #挂载 btrfs.img mount -o loop /opt/btrfsimg/btrfs.img /var/lib/docker #将挂载加入 fstab echo \"/opt/btrfsimg/btrfs.img /var/lib/docker btrfs defaults 0 0\" \u003e\u003e /etc/fstab #挂载 mount -a #docker 启动项 增加 --storage-driver=btrfs sed -i 's/dockerd/dockerd --storage-driver=btrfs --insecure-registry 172.16.1.26:5000/g' /lib/systemd/system/docker.service #重启 docker systemctl daemon-reload systemctl start docker.service #docekr info 查看 Storage Driver: btrfs Build Version: Btrfs v3.19.1 swarm 命令 我们首先来看看 1.12 中 新特性里面的 内置 swarm 命令 （swarmkit采用raft协议构建集群） [root@swarm-manager ~]# docker swarm --help Usage: docker swarm COMMAND Manage Docker Swarm Options: --help Print usage Commands: init Initialize a swarm join Join a swarm as a node and/or manager join-token Manage join tokens update Update the swarm leave Leave a swarm Run 'docker swarm COMMAND --help' for more information on a command. ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:3:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"init 初始化 [root@swarm-manager ~]# docker swarm init --advertise-addr 10.6.0.140 Swarm initialized: current node (2jqcgbfehfibna79brhc2mwns) is now a manager. To add a worker to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-5rq3nqvkbk4j4mlyfiykei02qczg9ajoct9u4ig7u6f7bn8kxi-94zjynj4723hwv91oakqh3cda \\ 10.6.0.140:2377 To add a manager to this swarm, run the following command: docker swarm join \\ --token SWMTKN-1-5rq3nqvkbk4j4mlyfiykei02qczg9ajoct9u4ig7u6f7bn8kxi-7c0h0rv5rg1ufx1j2ztvk0rw4 \\ 10.6.0.140:2377 执行 docker node ls 可以查看 Swarm 的集群情况 （只能在 manager 中执行） [root@swarm-manager ~]#docker node ls ID NAME MEMBERSHIP STATUS AVAILABILITY MANAGER STATUS 0vwpni05mew2j84i6gjet44iu * swarm-manager Accepted Ready Active Leader [root@swarm-manager ~]# netstat -lan|grep 2377 可以看到 群集开放了一这个 2377 的端口。 默认绑定 0.0.0.0:2377 ，当然我们也可以使用 docker swarm init –listen-addr : 进行绑定ip 2377 这个端口是用于 Swarm 中 node 节点加入使使用的。 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:4:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"join 加入 Swarm [root@swarm-node-1 ~]#docker swarm join --token SWMTKN-1-5rq3nqvkbk4j4mlyfiykei02qczg9ajoct9u4ig7u6f7bn8kxi-94zjynj4723hwv91oakqh3cda 10.6.0.140:2377 This node joined a Swarm as a worker. 这里在 node-1 里面执行了 join 命令，加入了 10.6.0.140 这个 manager 这个 Swarm 集群里 [root@swarm-manager ~]#docker node ls ID NAME MEMBERSHIP STATUS AVAILABILITY MANAGER STATUS 0vwpni05mew2j84i6gjet44iu * swarm-manager Accepted Ready Active Leader 4mqsmp0gzlqeicit98ce8wh2q swarm-node-1 Accepted Ready Active 这里可以看到 node-1 已经加入到 swarm 的集群里面来了。 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:5:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"update 命令 [root@swarm-manager ~]#docker swarm update Swarm updated. ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:6:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"leave 命令 退出集群 [root@swarm-node-1 ~]#docker swarm leave 10.6.0.140:2377 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:7:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"inspect 命令 查询 Swarm 集群 的整体信息。 （只能在 manager 中执行） [root@swarm-manager ~]#docker swarm inspect [ { \"ID\": \"c052zw5ll0ugw08shg2xf7ajp\", \"Version\": { \"Index\": 11 }, \"CreatedAt\": \"2016-06-23T02:09:18.935434519Z\", \"UpdatedAt\": \"2016-06-23T02:09:19.155114277Z\", \"Spec\": { \"Name\": \"default\", \"AcceptancePolicy\": { \"Policies\": [ { \"Role\": \"worker\", \"Autoaccept\": true }, { \"Role\": \"manager\", \"Autoaccept\": false } ] }, \"Orchestration\": { \"TaskHistoryRetentionLimit\": 10 }, \"Raft\": { \"SnapshotInterval\": 10000, \"LogEntriesForSlowFollowers\": 500, \"HeartbeatTick\": 1, \"ElectionTick\": 3 }, \"Dispatcher\": { \"HeartbeatPeriod\": 5000000000 }, \"CAConfig\": { \"NodeCertExpiry\": 7776000000000000 } } } ] service 命令 [root@swarm-manager ~]#docker service --help Usage: docker service COMMAND Manage Docker services Options: --help Print usage Commands: create Create a new service inspect Inspect a service tasks List the tasks of a service ls List services rm Remove a service scale Scale one or multiple services update Update a service Run 'docker service COMMAND --help' for more information on a command. ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:8:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"docker create 命令 既 创建 一个 服务。 [root@swarm-manager ~]#docker service create --help Usage: docker service create [OPTIONS] IMAGE [COMMAND] [ARG...] Create a new service Options: --constraint value Placement constraints (default []) --endpoint-mode string Endpoint mode(Valid values: VIP, DNSRR) -e, --env value Set environment variables (default []) --help Print usage -l, --label value Service labels (default []) --limit-cpu value Limit CPUs (default 0.000) --limit-memory value Limit Memory (default 0 B) --mode string Service mode (replicated or global) (default \"replicated\") -m, --mount value Attach a mount to the service --name string Service name --network value Network attachments (default []) -p, --publish value Publish a port as a node port (default []) --replicas value Number of tasks (default none) --reserve-cpu value Reserve CPUs (default 0.000) --reserve-memory value Reserve Memory (default 0 B) --restart-condition string Restart when condition is met (none, on_failure, or any) --restart-delay value Delay between restart attempts (default none) --restart-max-attempts value Maximum number of restarts before giving up (default none) --restart-window value Window used to evalulate the restart policy (default none) --stop-grace-period value Time to wait before force killing a container (default none) --update-delay duration Delay between updates --update-parallelism uint Maximum number of tasks updated simultaneously -u, --user string Username or UID -w, --workdir string Working directory inside the container docker service create 里面有非常多的 参数。 这里有很详细的使用说明。 下面我们来 创建一个 service 试试看 首先pull 一个 nginx 镜像 下来 用于 测试 [root@swarm-manager ~]#docker pull nginx 创建 2 个 nginx : [root@swarm-manager ~]#docker service create --name nginx --replicas 2 -p 80:80/tcp nginx ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:9:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"ls 命令 查看 服务 启动情况。 [root@swarm-manager ~]#docker service ls ID NAME REPLICAS IMAGE COMMAND 1b9a58mlz330 nginx 1/2 nginx ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:10:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"overlay 网络 # 创建一个 覆盖 所有集群的 overlay 网络 docker network create --driver overlay --opt encrypted --subnet=10.0.9.0/24 my-net #使用 --opt encrypted 标识 # 创建 service 时添加 --endpoint-mode dnsrr 使用dns 做为服务发现，否则跨主机之间无法通讯 # 例: docker service create --replicas 3 --name my-nginx --network my-net --endpoint-mode dnsrr nginx:alpine # 使用 nslookup my-nginx 查询dns 情况 ----------------------------------------------------------------------------------------------------------- nslookup: can't resolve '(null)': Name does not resolve Name: my-nginx Address 1: 10.0.9.3 b177080c9e65 Address 2: 10.0.9.2 my-nginx.1.0p2gn3h3ujoghub8ilyyvbenq.my-net Address 3: 10.0.9.4 my-nginx.3.axmsfkrxd89gbp75j00cu5qqw.my-net ----------------------------------------------------------------------------------------------------------- ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:11:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"tasks 命令 查看 nginx 的情况。 [root@swarm-manager ~]#docker service tasks nginx ID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE 56er48j3hin9ysdi3sb1chbn1 nginx.1 nginx nginx Preparing 2 minutes Running swarm-node-1 e7vtvpkbstznoi8ogihaao1f5 nginx.2 nginx nginx Running 2 minutes Running swarm-manager Swarm模式下的引擎拥有自组织与自修复特性，意味着它们能够识别我们定义的应用，并在出现差错时持续检查并修复环境。 举例来说，如果大家关闭某台运行有Nginx实例的设备，则另一节点上会自动启动一套新的容器。 如果关闭Swarm内半数设备所使用的网络交换机，则另外一半设备会顶替而上，接管对应工作负载。 [root@swarm-manager ~]#docker service ls ID NAME REPLICAS IMAGE COMMAND 1b9a58mlz330 nginx 2/2 nginx [root@swarm-manager ~]#docker service tasks nginx ID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE 56er48j3hin9ysdi3sb1chbn1 nginx.1 nginx nginx Running 32 minutes Running swarm-node-1 e7vtvpkbstznoi8ogihaao1f5 nginx.2 nginx nginx Running 32 minutes Running swarm-manager ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:12:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"scale 命令 批量生成已有容器。 直接对 nginx=10 既可让 nginx 的容器生成10个。 [root@swarm-manager ~]#docker service scale nginx=10 nginx scaled to 10 使用 tasks 可以看到，已经在 2个 节点中生成了10个 nginx 容器 [root@swarm-manager ~]#docker service tasks nginx ID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE 56er48j3hin9ysdi3sb1chbn1 nginx.1 nginx nginx Running 43 minutes Running swarm-node-1 e7vtvpkbstznoi8ogihaao1f5 nginx.2 nginx nginx Running 43 minutes Running swarm-manager 9vqxcmskj1nawo8wl0fqr32j2 nginx.3 nginx nginx Preparing 20 seconds Running swarm-manager 0vbqoyestm7ob6r1zq9jwj6il nginx.4 nginx nginx Running 20 seconds Running swarm-node-1 13jf9mkl4k5e57pq4hoeb68ru nginx.5 nginx nginx Running 20 seconds Running swarm-node-1 a0tk6ni6a02diuo5u3t870qk7 nginx.6 nginx nginx Running 20 seconds Running swarm-manager cwplvo5wfqp3rn5ynvxv9wv90 nginx.7 nginx nginx Running 20 seconds Running swarm-manager 7feil5xqc5hdkseasthkq2nyx nginx.8 nginx nginx Running 20 seconds Running swarm-node-1 8jt5yovxoz7t89edinb9ydao1 nginx.9 nginx nginx Starting 20 seconds Running swarm-node-1 dst4ydun1upham0o7e8a9hj3w nginx.10 nginx nginx Running 20 seconds Running swarm-manager 当我们想 缩容 时间， 也可以使用 scale nginx=2 让容器变成2个。 [root@swarm-manager ~]#docker service scale nginx=2 nginx scaled to 2 在运行 nginx=2 时可以看到 容器已经缩小为 2个 。 当我们使用 docker ps 查看，会发现容器被 stop 而非 rm 。 当我们使用 docker service rm nginx 的时候，所有的容器都会被 删除，请注意。 [root@swarm-manager ~]#docker service tasks nginx ID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE 0vbqoyestm7ob6r1zq9jwj6il nginx.4 nginx nginx Running 12 minutes Running swarm-node-1 13jf9mkl4k5e57pq4hoeb68ru nginx.5 nginx nginx Running 12 minutes Running swarm-node-1 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:13:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"update 命令 对 服务的启动 参数 进行 更新/修改。 上面我们新建了一个服务，命令为： [root@swarm-manager ~]#docker service create --name nginx --replicas 2 -p 80:80/tcp nginx 如果我们先新加入了一个 node 想让 nginx 分布在 3个 node 上面， 我们可以使用 update 命令。 [root@swarm-manager ~]#docker service update --replicas 3 nginx nginx 更新完毕以后 我们可以查看到 REPLICAS 已经变成 3/3 [root@swarm-manager ~]#docker service ls ID NAME REPLICAS IMAGE COMMAND 1b9a58mlz330 nginx 3/3 nginx docker service update 命令，也可用于直接 升级 镜像等。 [root@swarm-manager ~]#docker service tasks nginx ID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE 0vbqoyestm7ob6r1zq9jwj6il nginx.4 nginx nginx Running 41 minutes Running swarm-node-1 340e1u31vadq3jtebzeddmatt nginx.5 nginx nginx Preparing 5 seconds Running swarm-manager 上面我们使用了 nginx 镜像启动了 任务。 使用 update –image 可直接对 image 进行更新。 [root@swarm-manager ~]#docker service update --image nginx:new nginx nginx 可以看到 IMAGE 已经变成 nginx:new [root@swarm-manager ~]#docker service tasks nginx ID NAME SERVICE IMAGE LAST STATE DESIRED STATE NODE 2ba3utpk6icf0w449kcwgxmnm nginx.4 nginx nginx:new Running 49 seconds Running swarm-manager 5wmmneiueeool09fs8d2g1ncq nginx.5 nginx nginx:new Running 49 seconds Running swarm-node-1 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:14:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"挂载目录, mount docker service create --mount type=bind,target=/container_data/,source=/host_data/ 例 - 本地目录： target = 容器里面的路径， source = 本地硬盘路径 docker service create --name nginx --mount type=bind,target=/usr/share/nginx/html/,source=/opt/web/ --replicas 2 --publish 80:80/tcp nginx docker service create --mount type=volume,source=\u003cVOLUME-NAME\u003e,target=\u003cCONTAINER-PATH\u003e,volume-driver=\u003cDRIVER\u003e, 例 - 挂载volume卷： source = volume 名称 , traget = 容器里面的路径 docker service create --name nginx --mount type=volume,source=myvolume,target=/usr/share/nginx/html,volume-driver=local --replicas 2 --publish 80:80/tcp nginx node 命令 [root@swarm-manager ~]#docker node --help Usage: docker node COMMAND Manage Docker Swarm nodes Options: --help Print usage Commands: demote Demote one or more nodes from manager in the swarm inspect Display detailed information on one or more nodes ls List nodes in the swarm promote Promote one or more nodes to manager in the swarm rm Remove one or more nodes from the swarm ps List tasks running on a node update Update a node Run 'docker node COMMAND --help' for more information on a command. ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:15:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"accept 命令 用于 同意 申请加入 swarm 集群。 在使用 docker swarm init 的时候，如果使用了 –auto-accept none 的话，需要使用 docker node accept 来通过申请。 在没有通过申请之前，节点 MEMBERSHIP 状态为 Pending 状态。 –auto-accept 可以设置三种角色 分别为 (worker, manager, or none) 。 使用 docker node accept + 节点 ID 既可通过申请。 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:16:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"promote 与 demote 命令 docker node promote 是 将 worker 普通节点，提升为 manager 节点。 docker node demote 是 将 manager 管理节点，降级为 worker 节点。 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:17:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"inspect 命令 可查看节点的具体信息 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:18:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"rm 命令 可删除一个节点 ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:19:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"tasks 命令 可查看节点中运行的 service 任务。 docker stack/deploy 目前 stack 还处于 测试阶段。 https://github.com/docker/docker/blob/master/experimental/docker-stacks-and-bundles.md ","date":"2016-10-25","objectID":"/docker-swarm-1.12/:20:0","tags":null,"title":"docker 1.12 swarm 集群","uri":"/docker-swarm-1.12/"},{"categories":["docker"],"content":"docker swarm name service 通信错误","date":"2016-10-25","objectID":"/docker-dns-error/","tags":null,"title":"docker swarm name service 通信错误","uri":"/docker-dns-error/"},{"categories":["docker"],"content":" docker swarm 之间使用 name service 通信，容器重启后IP变动导致的通信故障 ","date":"2016-10-25","objectID":"/docker-dns-error/:0:0","tags":null,"title":"docker swarm name service 通信错误","uri":"/docker-dns-error/"},{"categories":["docker"],"content":"故障说明 kafka 配置 zookeeper.connect=zookeeper-1:2181,zookeeper-2:2181,zookeeper-3:2181 当 zookeeper 容器重启后 overlay 分配给 zookeeper 的IP变动， 导致kakfa 出现故障。 重启 kafka 容器以后 恢复，但是 程序 连接 kafka 时间使用的 配置是 kafka name service 这时候 程序出现连接不上 kafka, 需要重启 程序。 ","date":"2016-10-25","objectID":"/docker-dns-error/:1:0","tags":null,"title":"docker swarm name service 通信错误","uri":"/docker-dns-error/"},{"categories":["docker"],"content":"解决方案 修改 JVM DNS 缓存, JVM默认 DNS 缓存时间是永远有效 两种方式设置dns缓存的方法： # 1.在JAVA_OPTS里设置 -Dsun.net.inetaddr.ttl=3 -Dsun.net.inetaddr.negative.ttl=1 # 2.修改property System.setProperty(\"sun.net.inetaddr.ttl\", \"3\"); System.setProperty(\"sun.net.inetaddr.negative.ttl\", \"1\"); sun.net.inetaddr.ttl=3 表示 DNS 缓存时间为 3 秒 sun.net.inetaddr.negative.ttl = 1 表示开启 DNS 缓存时间，默认为10秒。(0表示禁止缓存, -1表示永久缓存) docker 指定 ip docker network 创建网络时指定 subnets IP段 docker run 时 指定 ip (指定尽量最后面的IP地址，避免IP被占用) docker network create --driver overlay --subnet=10.1.0.0/16 my-net [root@swarm-master ~]# docker network inspect my-net [ { \"Name\": \"my-net\", \"Id\": \"83333c68bdddf95fd9f398735733258722950969c2a76a3929e70c16d845ffe4\", \"Scope\": \"global\", \"Driver\": \"overlay\", \"EnableIPv6\": false, \"IPAM\": { \"Driver\": \"default\", \"Options\": {}, \"Config\": [ { \"Subnet\": \"10.1.0.0/16\" } ] }, \"Internal\": false, \"Containers\": {}, \"Options\": {}, \"Labels\": {} } ] # 创建容器 docker run -d --name jicki --net=my-net --ip 10.1.10.10 alpine ping www.qq.com [root@swarm-master ~]# docker exec jicki ip addr 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 239: eth0@if240: \u003cBROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u003e mtu 1450 qdisc noqueue state UP link/ether 02:42:0a:01:0a:0a brd ff:ff:ff:ff:ff:ff inet 10.1.10.10/16 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::42:aff:fe01:a0a/64 scope link valid_lft forever preferred_lft forever 241: eth1@if242: \u003cBROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN\u003e mtu 1500 qdisc noqueue state UP link/ether 02:42:ac:12:00:0d brd ff:ff:ff:ff:ff:ff inet 172.18.0.13/16 scope global eth1 valid_lft forever preferred_lft forever inet6 fe80::42:acff:fe12:d/64 scope link valid_lft forever preferred_lft forever docker run -d --name jicki2 --net=my-net alpine ping www.qq.com # 测试通信 [root@swarm-master ~]# docker exec jicki ping jicki2 PING jicki2 (10.1.0.2): 56 data bytes 64 bytes from 10.1.0.2: seq=0 ttl=64 time=0.149 ms 64 bytes from 10.1.0.2: seq=1 ttl=64 time=0.096 ms 64 bytes from 10.1.0.2: seq=2 ttl=64 time=0.081 ms 64 bytes from 10.1.0.2: seq=3 ttl=64 time=0.080 ms 64 bytes from 10.1.0.2: seq=4 ttl=64 time=0.090 ms 64 bytes from 10.1.0.2: seq=5 ttl=64 time=0.089 ms ","date":"2016-10-25","objectID":"/docker-dns-error/:2:0","tags":null,"title":"docker swarm name service 通信错误","uri":"/docker-dns-error/"},{"categories":["jenkins","docker"],"content":"jenkins + docker + git 持续集成","date":"2016-10-20","objectID":"/jenkins-docker-git/","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":" git push 以后， jenkins 自动触发 代码打包，生成docker image , docker push 到 仓库，发布到环境里。 安装jenkins 这里不建议用 Docker 镜像，因为下面 Jenkins 自身会需要调用 Docker 来启动任务。 ","date":"2016-10-20","objectID":"/jenkins-docker-git/:0:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"导入 jenkins 源 wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins.io/redhat-stable/jenkins.repo rpm --import http://pkg.jenkins.io/redhat-stable/jenkins.io.key yum -y install jenkins ","date":"2016-10-20","objectID":"/jenkins-docker-git/:1:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"修改jenkins配置 vi /etc/sysconfig/jenkins # 修改jenkins 目录 JENKINS_HOME=\"/opt/jenkins\" # 修改jenkins 端口 JENKINS_PORT=\"9999\" ","date":"2016-10-20","objectID":"/jenkins-docker-git/:2:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"移动目录 # 将目录移动过来，否则程序报错 mv /var/lib/jenkins /opt/ ","date":"2016-10-20","objectID":"/jenkins-docker-git/:3:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"启动服务 systemctl start jenkins systemctl enable jenkins ","date":"2016-10-20","objectID":"/jenkins-docker-git/:4:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"访问WEB UI http://myip:9999/ 生成密钥 # 切换用户 su jenkins # 生成key ssh-keygen -t rsa -b 4096 -C \"jenkins@git\" # 查看key信息 cat /home/jenkins/.ssh/id_rsa.pub ","date":"2016-10-20","objectID":"/jenkins-docker-git/:5:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"jenkins 后台配置 进入 jenkins –\u003e Credentials –\u003e Add Credentials 选择 系统管理 – \u003e 管理插件 添加 Gradle Plugin 插件 添加 Git plugin 插件 常用插件 Build WIth Parameters # 执行 构建 前手工输入参数 pipeline Deploy Plugin # build war 包以后部署 Email Extension Plugin # 邮件发送 Multiple SCMs Plugin # 多项目构建工具 Git Parameter # 构建的时候选择 git 分支 description setter plugin # 配置 Build History 显示具体信息 user build vars plugin # 显示 构建用户名 而非 id 下载慢可直接下载 hpi 文件，通过高级 导入插件安装 选择 系统管理 – \u003e Global Tool Configuration 安装JDK 安装 Gradle 安装 Git 创建项目 选择 自由风格 的项目 源码管理选择 Git 构建 选择 Invoke Gradle script ","date":"2016-10-20","objectID":"/jenkins-docker-git/:6:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"构建触发器 # 勾选 Poll SCM # 每两分钟检查一次git代码是否有更新 H/2 * * * * ","date":"2016-10-20","objectID":"/jenkins-docker-git/:7:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["jenkins","docker"],"content":"配置 邮件 首先必须安装 Email Extension Plugin 插件 系统管理 –\u003e 系统设置 – \u003e Jenkins Location 配置系统管理员邮件地址 — \u003e xxx@163.com 配置 Extended E-mail Notification SMTP Server = 点击高级 勾选 Use SMTP Authentication 输入 发送 用户 与 密码 填写 SMTP port Default Content Type 选择 HTML (text/html) Default Subject = 构建通知:$PROJECT_NAME - Build # $BUILD_NUMBER - $BUILD_STATUS! Default Content = \u003cb style=\"font-size:12px\"\u003e(本邮件是程序自动下发的，请勿回复，\u003cspan style=\"color:red\"\u003e请相关人员fix it,重新提交到git 构建\u003c/span\u003e)\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e项目名称：$PROJECT_NAME\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建编号：$BUILD_NUMBER\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003eGIT版本号：${GIT_REVISION}\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建状态：$BUILD_STATUS\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e触发原因：${CAUSE}\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建日志地址：\u003ca href=\"${BUILD_URL}console\"\u003e${BUILD_URL}console\u003c/a\u003e\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e构建地址：\u003ca href=\"$BUILD_URL\"\u003e$BUILD_URL\u003c/a\u003e\u003cbr\u003e\u003c/b\u003e\u003chr\u003e \u003cb style=\"font-size: 12px;\"\u003e变更集:${JELLY_SCRIPT,template=\"html\"}\u003cbr\u003e\u003c/b\u003e\u003chr\u003e 项目 – \u003e 构建后操作 — \u003e 添加 Editable Email Notification 拉到最下面 — \u003e 点击 Advanced Settings… Recipient List 添加 收件邮箱 多个邮件以空格 隔开 ","date":"2016-10-20","objectID":"/jenkins-docker-git/:8:0","tags":null,"title":"jenkins + docker + git 持续集成","uri":"/jenkins-docker-git/"},{"categories":["docker"],"content":"rancher docker 集群管理于编排","date":"2016-10-20","objectID":"/docker-rancher/","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"rancher docker 集群管理于编排 官方 github https://github.com/rancher/rancher 环境说明 10.6.0.140 10.6.0.187 10.6.0.188 #修改主机名: 10.6.0.140 = hostnamectl --static set-hostname reancher-manager 10.6.0.187 = hostnamectl --static set-hostname reancher-node-1 10.6.0.188 = hostnamectl --static set-hostname reancher-node-2 安装 rancher ","date":"2016-10-20","objectID":"/docker-rancher/:0:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"初始化 rancher [root@reancher-manager ~]#mkdir -p /opt/rencher/mysql [root@reancher-manager ~]#docker run -d --name rencher --restart=always -v /opt/rencher/mysql:/var/lib/mysql -p 8080:8080 rancher/server [root@reancher-manager ~]#docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c8209da7add0 rancher/server \"/usr/bin/s6-svscan /\" 12 minutes ago Up 12 minutes 3306/tcp, 0.0.0.0:8080-\u003e8080/tcp rencher ","date":"2016-10-20","objectID":"/docker-rancher/:1:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"mysql 说明 使用自己的mysql数据库，可使用如下参数： docker run -d --restart=always -p 8080:8080 \\ -e CATTLE_DB_CATTLE_MYSQL_HOST=\u003chostname or IP of MySQL instance\u003e \\ -e CATTLE_DB_CATTLE_MYSQL_PORT=\u003cport\u003e \\ -e CATTLE_DB_CATTLE_MYSQL_NAME=\u003cName of database\u003e \\ -e CATTLE_DB_CATTLE_USERNAME=\u003cUsername\u003e \\ -e CATTLE_DB_CATTLE_PASSWORD=\u003cPassword\u003e \\ rancher/server:v1.1.3 ","date":"2016-10-20","objectID":"/docker-rancher/:2:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"rancher WEB UI http://10.6.0.140:8080 ","date":"2016-10-20","objectID":"/docker-rancher/:3:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"安装 rancher-agent # node 节点 docker run -d --privileged -v /var/run/docker.sock:/var/run/docker.sock -v /var/lib/rancher:/var/lib/rancher rancher/agent:v1.0.2 http://10.6.0.140:8080/v1/scripts/8944D0EC8BCFEB4F127C:1472544000000:BIX8IC8bWsRbx60NMhka4AmxmpQ [root@reancher-node-1 ~]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6c7f533527ec rancher/agent:v1.0.2 \"/run.sh run\" 4 minutes ago Up 4 minutes rancher-agent 7566aa61cdbe rancher/agent:v1.0.2 \"/run.sh state\" 4 minutes ago Exited (0) 4 minutes ago rancher-agent-state 032d85c88779 rancher/agent:v1.0.2 \"/run.sh http://10.6.\" 5 minutes ago Exited (0) 4 minutes ago fervent_morse ","date":"2016-10-20","objectID":"/docker-rancher/:4:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"添加 容器 ","date":"2016-10-20","objectID":"/docker-rancher/:5:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"部署 stack 与 service ","date":"2016-10-20","objectID":"/docker-rancher/:6:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"生成 Load Balance ","date":"2016-10-20","objectID":"/docker-rancher/:7:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"访问应用 rancher 网络 Rancher 中 网络+负载均衡 实现 与 说明 依赖镜像：rancher/agent-instance:v0.8.3 Rancher 网络是 采用SDN技术所建容器为虚拟ip地址，各host之间容器采用ipsec隧道实现跨主机通信，使用的是udp的500和4500端口。 启动任务时，在各个host部署容器之前会起一个Network Agent容器，负责组建网络环境。 破坏测试 破坏性测试 (以下为别人测试) server 是以容器方式运行，Mysql数据库保存了任务数据以及任务逻辑关系 ","date":"2016-10-20","objectID":"/docker-rancher/:8:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"破坏server端 操作：在server端和agent端正常运行状态下，stop掉server容器 结果：业务不受影响。start重启容器后恢复管理功能。 操作：将server端容器rm删除掉, Mysql数据未保存，重新再起一个server容器。 结果：1.当前业务不受影响 2.新server仍然能够识别和管理各个agent，因为agent端是连server的ip端口，ip不变就能连上 3.agent端原有的任务容器的命名和逻辑关系没有了。 操作：将server端容器rm删除掉（将mysql数据/var/lib/mysql 映射至宿主机），重新再起一个server容器。 结果：新起的容器能够识别任务状态，命名，逻辑关系。恢复到之前的状态。 ","date":"2016-10-20","objectID":"/docker-rancher/:9:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"破坏agent端 操作：host命令行下删除掉agent容器 结果：不影响当前业务状态，server端显示host失联，无法对该agent下发任务进行扩容和缩容。 重新启动agent后恢复正常。 操作：server控制端删除agent端的业务容器（例如删除nginx容器） 结果：删除后数秒内，在另一个host上重新启动一个新的业务容器。 操作：host命令行下删除agent端的业务容器（例如删除nginx容器） 结果：删除后数秒内，在当前host上重新启动一个新的业务容器。 操作：host命令行下删除掉agent容器后，再删除一个业务容器 结果：server端因为与agent失联，导致无法更新该host上的容器变化，没有新启动任何容器。 ","date":"2016-10-20","objectID":"/docker-rancher/:10:0","tags":null,"title":"rancher docker 集群管理于编排","uri":"/docker-rancher/"},{"categories":["docker"],"content":"GitLab + Runner","date":"2016-10-16","objectID":"/gitlab-runner-cn/","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"GitLab ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:0:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"编辑Yum源 cat \u003c\u003cEOF\u003e /etc/yum.repos.d/gitlab-ce.repo [gitlab-ce] name=gitlab-ce baseurl=https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/ repo_gpgcheck=0 gpgcheck=0 enabled=1 gpgkey=https://packages.gitlab.com/gpg.key EOF ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:1:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"安装软件 # 安装依赖 yum install curl openssh-server openssh-clients postfix cronie # 启动 postfix 邮件服务 systemctl start postfix systemctl enable postfix # 检查 postfix systemctl status postfix # 安装 GitLab 社区版 yum -y install gitlab-ce # 启动程序 gitlab-ctl start # 检测错误 gitlab-rake gitlab:check # 初始化 GitLab gitlab-ctl reconfigure ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:2:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"修改配置 vi /etc/gitlab/gitlab.rb # 修改域名 external_url 'http://my_url/' # 修改备份目录 备份周期7天 604800秒 gitlab_rails['backup_keep_time'] = 604800 gitlab_rails['backup_path'] = \"/opt/gitlab/backups\" # 更换安装源 /opt/gitlab/embedded/bin/gem sources --add https://ruby.taobao.org/ --remove https://rubygems.org/ # 让配置生效, 任何配置修改必须执行 reconfigure gitlab-ctl reconfigure ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:3:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"访问 gitlab http://my_url/ # 最开始会让你创建密码 # 登陆帐号为 root 持续集成(GitLab-CI) ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:4:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"安装源 curl -L https://packages.gitlab.com/install/repositories/runner/gitlab-ci-multi-runner/script.rpm.sh | bash ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:5:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"安装 gitlab-ci-runner yum install gitlab-ci-multi-runner -y ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:6:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"生成 runner token http://my_url/admin/runners ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:7:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"配置 runner [root@localhost ~]# gitlab-ci-multi-runner register Running in system-mode. Please enter the gitlab-ci coordinator URL (e.g. https://gitlab.com/): http://my_url/ Please enter the gitlab-ci token for this runner: 1RAM5gnaTjBec1ExYskF Please enter the gitlab-ci description for this runner: [localhost.localdomain]: my-runner Please enter the gitlab-ci tags for this runner (comma separated): docker WARNING: No TLS connection state Registering runner... succeeded runner=1RAM5gna Please enter the executor: parallels, shell, ssh, docker+machine, docker-ssh+machine, docker, docker-ssh, virtualbox, kubernetes: docker Please enter the default Docker image (eg. ruby:2.1): ruby:2.1 Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded! 这里边要说明的是 Please enter the executor: parallels, shell, ssh, docker+machine, docker-ssh+machine, docker, docker-ssh, virtualbox, kubernetes: 这一项。 这里的意思是~ 你这个 runner 是用来运行在什么情况下的。 如果物理机，一般是用 shell 来做 build . 如果是 docker 环境，就选择 docker , k8s集群 就选 kubernetes 。 ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:8:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"查看生成 # 登陆 http://my_url/admin/runners # Runners with last contact less than a minute ago: 1 # 可以看到有一条记录 ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:9:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"信息修改与检查 # runners 生成的信息存放于 /etc/gitlab-runner/config.toml ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:10:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"runners 使用 [root@localhost ~]# gitlab-runner -h NAME: gitlab-runner - a GitLab Runner USAGE: gitlab-runner [global options] command [command options] [arguments...] VERSION: 1.6.1 (c52ad4f) AUTHOR(S): Kamil Trzciński \u003cayufan@ayufan.eu\u003e COMMANDS: exec execute a build locally list List all configured runners run run multi runner service register register a new runner install install service uninstall uninstall service start start service stop stop service restart restart service status get status of a service run-single start single runner unregister unregister specific runner verify verify all registered runners artifacts-downloader download and extract build artifacts (internal) artifacts-uploader create and upload build artifacts (internal) cache-archiver create and upload cache artifacts (internal) cache-extractor download and extract cache artifacts (internal) help, h Shows a list of commands or help for one command GLOBAL OPTIONS: --debug debug mode [$DEBUG] --log-level, -l \"info\" Log level (options: debug, info, warn, error, fatal, panic) --cpuprofile write cpu profile to file [$CPU_PROFILE] --help, -h show help --version, -v print the version ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:11:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"runner yaml 文件 官方地址 https://docs.gitlab.com/ce/ci/yaml/README.html 运维维护 ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:12:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"定时备份 0 2 * * * gitlab-rake gitlab:backup:create ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:13:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"恢复数据 # 停止 unicorn gitlab-ctl stop unicorn # 停止 sidekiq gitlab-ctl stop sidekiq # 恢复数据 BACKUP= 指定恢复的时间戳 cd 备份目录 gitlab-rake gitlab:backup:restore BACKUP=1406691018 FAQ 问题 ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:14:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"gitlab-ctl reconfigure 问题 ruby_block[supervise_redis_sleep] action run 卡住 #执行 ls -al /opt/gitlab/sv/redis/supervise #提示 ls: cannot access /opt/gitlab/sv/redis/supervise: No such file or directory # 重启 supervise systemctl restart gitlab-runsvdir.service ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:15:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"execute[create gitlab database user] 问题 gitlab-ctl stop rm -rf /var/opt/gitlab/postgresql gitlab-ctl start gitlab-ctl reconfigure ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:16:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["docker"],"content":"psql: could not connect to server: No such file or directory 问题 # 查看 错误 gitlab-ctl tail postgresql # 发现 kernel with larger SHMMAX 增大 内核 shmmax 参数 或者 减少 gitlab 配置中 shared_buffers 参数 vi /etc/gitlab/gitlab.rb postgresql['shared_buffers'] = \"2560MB\" # 重启 gitlab-ctl restart ","date":"2016-10-16","objectID":"/gitlab-runner-cn/:17:0","tags":null,"title":"GitLab + Runner","uri":"/gitlab-runner-cn/"},{"categories":["kubernetes"],"content":"docker kubernetes 1.4 部署","date":"2016-10-13","objectID":"/kubernetes-1.4.0/","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"k8s 1.4 ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:0:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"环境说明 node-1: 10.6.0.140 node-2: 10.6.0.187 node-3: 10.6.0.188 kubernetes 集群，包含 master 节点，与 node 节点。 ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:1:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname 10.6.0.140 - k8s-master 10.6.0.187 - k8s-node-1 10.6.0.188 - k8s-node-2 配置 /etc/hosts 添加 10.6.0.140 k8s-master 10.6.0.187 k8s-node-1 10.6.0.188 k8s-node-2 安装kubernetes ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:2:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"安装依赖 yum install -y socat ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:3:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"增加yum 文件 cat \u003c\u003cEOF\u003e /etc/yum.repos.d/k8s.repo [kubelet] name=kubelet baseurl=http://files.rm-rf.ca/rpms/kubelet/ enabled=1 gpgcheck=0 EOF ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:4:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"yum 安装程序 yum makecache yum install -y kubelet kubeadm kubectl kubernetes-cni 由于 google 被墙, 所以使用 kubeadm init 创建 集群 的时候会出现卡住 国内已经有人将镜像上传至 docker hub 里面了 ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:5:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"下载镜像 docker pull chasontang/kube-proxy-amd64:v1.4.0 docker pull chasontang/kube-discovery-amd64:1.0 docker pull chasontang/kubedns-amd64:1.7 docker pull chasontang/kube-scheduler-amd64:v1.4.0 docker pull chasontang/kube-controller-manager-amd64:v1.4.0 docker pull chasontang/kube-apiserver-amd64:v1.4.0 docker pull chasontang/etcd-amd64:2.2.5 docker pull chasontang/kube-dnsmasq-amd64:1.3 docker pull chasontang/exechealthz-amd64:1.1 docker pull chasontang/pause-amd64:3.0 # 下载以后使用 docker tag 命令将其做别名改为 gcr.io/google_containers docker tag chasontang/kube-proxy-amd64:v1.4.0 gcr.io/google_containers/kube-proxy-amd64:v1.4.0 docker tag chasontang/kube-discovery-amd64:1.0 gcr.io/google_containers/kube-discovery-amd64:1.0 docker tag chasontang/kubedns-amd64:1.7 gcr.io/google_containers/kubedns-amd64:1.7 docker tag chasontang/kube-scheduler-amd64:v1.4.0 gcr.io/google_containers/kube-scheduler-amd64:v1.4.0 docker tag chasontang/kube-controller-manager-amd64:v1.4.0 gcr.io/google_containers/kube-controller-manager-amd64:v1.4.0 docker tag chasontang/kube-apiserver-amd64:v1.4.0 gcr.io/google_containers/kube-apiserver-amd64:v1.4.0 docker tag chasontang/etcd-amd64:2.2.5 gcr.io/google_containers/etcd-amd64:2.2.5 docker tag chasontang/kube-dnsmasq-amd64:1.3 gcr.io/google_containers/kube-dnsmasq-amd64:1.3 docker tag chasontang/exechealthz-amd64:1.1 gcr.io/google_containers/exechealthz-amd64:1.1 docker tag chasontang/pause-amd64:3.0 gcr.io/google_containers/pause-amd64:3.0 # 清除原来下载的镜像 docker rmi chasontang/kube-proxy-amd64:v1.4.0 docker rmi chasontang/kube-discovery-amd64:1.0 docker rmi chasontang/kubedns-amd64:1.7 docker rmi chasontang/kube-scheduler-amd64:v1.4.0 docker rmi chasontang/kube-controller-manager-amd64:v1.4.0 docker rmi chasontang/kube-apiserver-amd64:v1.4.0 docker rmi chasontang/etcd-amd64:2.2.5 docker rmi chasontang/kube-dnsmasq-amd64:1.3 docker rmi chasontang/exechealthz-amd64:1.1 docker rmi chasontang/pause-amd64:3.0 ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:6:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"启动 kubelet systemctl enable kubelet systemctl start kubelet ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:7:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"init 初始化集群 kubenetes 1.4 利用 kubeadm 创建 集群 [root@k8s-master ~]#kubeadm init --api-advertise-addresses=10.6.0.140 \u003cmaster/tokens\u003e generated token: \"eb4d40.67aac8417294a8cf\" \u003cmaster/pki\u003e created keys and certificates in \"/etc/kubernetes/pki\" \u003cutil/kubeconfig\u003e created \"/etc/kubernetes/kubelet.conf\" \u003cutil/kubeconfig\u003e created \"/etc/kubernetes/admin.conf\" \u003cmaster/apiclient\u003e created API client configuration \u003cmaster/apiclient\u003e created API client, waiting for the control plane to become ready \u003cmaster/apiclient\u003e all control plane components are healthy after 10.304645 seconds \u003cmaster/apiclient\u003e waiting for at least one node to register and become ready \u003cmaster/apiclient\u003e first node has registered, but is not ready yet \u003cmaster/apiclient\u003e first node has registered, but is not ready yet \u003cmaster/apiclient\u003e first node has registered, but is not ready yet \u003cmaster/apiclient\u003e first node has registered, but is not ready yet \u003cmaster/apiclient\u003e first node has registered, but is not ready yet \u003cmaster/apiclient\u003e first node is ready after 3.004762 seconds \u003cmaster/discovery\u003e created essential addon: kube-discovery, waiting for it to become ready \u003cmaster/discovery\u003e kube-discovery is ready after 4.002661 seconds \u003cmaster/addons\u003e created essential addon: kube-proxy \u003cmaster/addons\u003e created essential addon: kube-dns kubernetes master initialised successfully! You can now join any number of machines by running the following on each node: kubeadm join --token 8609e3.c2822cf312e597e1 10.6.0.140 查看 kubelet 状态 systemctl status kubelet ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:8:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"配置子节点 子节点 启动 kubelet 首先必须启动 docker systemctl enable kubelet systemctl start kubelet 加入集群 kubeadm join --token 8609e3.c2822cf312e597e1 10.6.0.140 查看 kubelet 状态 systemctl status kubelet 查看集群状态 [root@k8s-master ~]#kubectl get node NAME STATUS AGE k8s-master Ready 1d k8s-node-1 Ready 1d k8s-node-2 Ready 1d 此时可看到 三个节点 都已经 Ready , 但是其实 Pod 只会运行在 node 节点 如果需要所有节点，包括master 也运行 Pod 需要运行 kubectl taint nodes --all dedicated- ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:9:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"安装 POD 网络 这里使用官方推荐的 weave 网络 kubectl apply -f https://git.io/weave-kube 查看所有pod 状态 [root@k8s-master ~]#kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system etcd-k8s-master 1/1 Running 1 49m kube-system kube-apiserver-k8s-master 1/1 Running 1 48m kube-system kube-controller-manager-k8s-master 1/1 Running 1 48m kube-system kube-discovery-1971138125-0oq58 1/1 Running 1 49m kube-system kube-dns-2247936740-ojzhw 3/3 Running 3 49m kube-system kube-proxy-amd64-1hhdf 1/1 Running 1 49m kube-system kube-proxy-amd64-4c2qt 1/1 Running 0 47m kube-system kube-proxy-amd64-tc3kw 1/1 Running 1 47m kube-system kube-scheduler-k8s-master 1/1 Running 1 48m kube-system weave-net-9mrlt 2/2 Running 2 46m kube-system weave-net-oyguh 2/2 Running 4 46m kube-system weave-net-zc67d 2/2 Running 0 46m ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:10:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"GlusterFS 作为 volume 官方详细说明： https://github.com/kubernetes/kubernetes/tree/master/examples/volumes/glusterfs 配置 GlusterFS 集群，以及设置好 GlusterFS 的 volume , node 客户端安装 glusterfs-client k8s-master 创建一个 endpoints. 我这边 GlusterFS 有3个节点 vi glusterfs-endpoints.json # 每一个 GlusterFS 节点，必须写一列. 端口随意填写(1-65535) { \"kind\": \"Endpoints\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"glusterfs-cluster\" }, \"subsets\": [ { \"addresses\": [ { \"ip\": \"10.6.0.140\" } ], \"ports\": [ { \"port\": 1 } ] }, { \"addresses\": [ { \"ip\": \"10.6.0.187\" } ], \"ports\": [ { \"port\": 1 } ] }, { \"addresses\": [ { \"ip\": \"10.6.0.188\" } ], \"ports\": [ { \"port\": 1 } ] } ] } 创建 endpoints [root@k8s-master ~]#kubectl create -f glusterfs-endpoints.json endpoints \"glusterfs-cluster\" created 查看 endpoints [root@k8s-master ~]#kubectl get endpoints NAME ENDPOINTS AGE glusterfs-cluster 10.6.0.140:1,10.6.0.187:1,10.6.0.188:1 37s k8s-master 创建一个 service. vi glusterfs-service.json # 这里注意之前填写的 port { \"kind\": \"Service\", \"apiVersion\": \"v1\", \"metadata\": { \"name\": \"glusterfs-cluster\" }, \"spec\": { \"ports\": [ {\"port\": 1} ] } } 创建 service [root@k8s-master ~]#kubectl create -f glusterfs-service.json service \"glusterfs-cluster\" created 查看 service [root@k8s-master ~]#kubectl get service NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE glusterfs-cluster 100.71.255.174 \u003cnone\u003e 1/TCP 14s k8s-master 创建一个 Pod 来测试挂载 vi glusterfs-pod.json { \"apiVersion\": \"v1\", \"kind\": \"Pod\", \"metadata\": { \"name\": \"glusterfs\" }, \"spec\": { \"containers\": [ { \"name\": \"glusterfs\", \"image\": \"gcr.io/google_containers/pause-amd64:3.0\", \"volumeMounts\": [ { \"mountPath\": \"/mnt/glusterfs\", \"name\": \"glusterfsvol\" } ] } ], \"volumes\": [ { \"name\": \"glusterfsvol\", \"glusterfs\": { \"endpoints\": \"glusterfs-cluster\", \"path\": \"models\", \"readOnly\": false } } ] } } glusterfs 下 path 配置 glusterfs volume 的名称 readOnly: true (只读) and readOnly: false 查看 挂载的 volume [root@k8s-node-2 ~]# mount | grep models 10.6.0.140:models on /var/lib/kubelet/pods/947390da-8f6a-11e6-9ade-d4ae52d1f0c9/volumes/kubernetes.io~glusterfs/glusterfsvol type fuse.glusterfs (rw,relatime,user_id=0,group_id=0,default_permissions,allow_other,max_read=131072) ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:11:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"yaml 文件 编写一个 Deployment 的 yaml 文件 apiVersion: extensions/v1beta1 kind: Deployment metadata: name: nginx-deployment spec: replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx ports: - containerPort: 80 使用 kubectl create 进行创建 kubectl create -f nginx.yaml --record 查看 pod [root@k8s-master ~]#kubectl get pod NAME READY STATUS RESTARTS AGE nginx-deployment-646889141-459i5 1/1 Running 0 9m nginx-deployment-646889141-vxn29 1/1 Running 0 9m 查看 deployment [root@k8s-master ~]#kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE nginx-deployment 2 2 2 2 10m ","date":"2016-10-13","objectID":"/kubernetes-1.4.0/:12:0","tags":null,"title":"docker kubernetes 1.4 部署","uri":"/kubernetes-1.4.0/"},{"categories":["kubernetes"],"content":"docker k8s 1.3.8 + flannel","date":"2016-10-12","objectID":"/kubernetes-1.3.8/","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"docker k8s + flannel ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:0:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"说明 kubernetes 是谷歌开源的 docker 集群管理解决方案。 项目地址： http://kubernetes.io/ ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:1:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"环境说明 node-1: 10.6.0.140 node-2: 10.6.0.187 node-3: 10.6.0.188 kubernetes 集群，包含 master 节点，与 node 节点。 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:2:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"初始化环境 hostnamectl --static set-hostname hostname 10.6.0.140 - k8s-master 10.6.0.187 - k8s-node-1 10.6.0.188 - k8s-node-2 编辑 /etc/hosts 文件，配置hostname 通信 vi /etc/hosts 添加: 10.6.0.140 k8s-master 10.6.0.187 k8s-node-1 10.6.0.188 k8s-node-2 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:3:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"安装 etcd 集群 etcd 是k8s集群的基础组件。 yum -y install etcd 修改配置文件，/etc/etcd/etcd.conf 需要修改如下参数： ETCD_NAME=etcd1 ETCD_DATA_DIR=\"/var/lib/etcd/etcd1.etcd\" ETCD_LISTEN_PEER_URLS=\"http://10.6.0.140:2380\" ETCD_LISTEN_CLIENT_URLS=\"http://10.6.0.140:2379,http://127.0.0.1:2379\" ETCD_INITIAL_ADVERTISE_PEER_URLS=\"http://10.6.0.140:2380\" ETCD_INITIAL_CLUSTER=\"etcd1=http://10.6.0.140:2380,etcd2=http://10.6.0.187:2380,etcd3=http://10.6.0.188:2380\" ETCD_INITIAL_CLUSTER_STATE=\"new\" ETCD_INITIAL_CLUSTER_TOKEN=\"k8s-etcd-cluster\" ETCD_ADVERTISE_CLIENT_URLS=\"http://10.6.0.140:2379\" 其他etcd集群中： ETCD_NAME , 以及IP 需要变动 修改 etcd 启动文件 /usr/lib/systemd/system/etcd.service sed -i 's/\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\"/\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\" --listen-client-urls=\\\\\\\"${ETCD_LISTEN_CLIENT_URLS}\\\\\\\" --advertise-client-urls=\\\\\\\"${ETCD_ADVERTISE_CLIENT_URLS}\\\\\\\" --initial-cluster-token=\\\\\\\"${ETCD_INITIAL_CLUSTER_TOKEN}\\\\\\\" --initial-cluster=\\\\\\\"${ETCD_INITIAL_CLUSTER}\\\\\\\" --initial-cluster-state=\\\\\\\"${ETCD_INITIAL_CLUSTER_STATE}\\\\\\\"/g' /usr/lib/systemd/system/etcd.service 分别启动 所有节点的 etcd 服务 systemctl enable etcd systemctl start etcd systemctl status etcd 查看 etcd 集群状态： etcdctl cluster-health 出现 cluster is healthy 表示成功 查看 etcd 集群成员： etcdctl member list ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:4:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"flannel 网络 安装 flannel yum -y install flannel 清除网络中遗留的docker 网络 (docker0, flannel0 等) ifconfig 如果存在 请删除之，以免发生不必要的未知错误 ip link delete docker0 .... 设置 flannel 所用到的IP段 etcdctl --endpoint http://10.6.0.140:2379 set /flannel/network/config '{\"Network\":\"10.10.0.0/16\",\"SubnetLen\":25,\"Backend\":{\"Type\":\"vxlan\",\"VNI\":1}}' 接下来修改 flannel 配置文件 vim /etc/sysconfig/flanneld FLANNEL_ETCD=\"http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379\" # 修改为 集群地址 FLANNEL_ETCD_KEY=\"/flannel/network/config\" # 修改为 上面导入配置中的 /flannel/network FLANNEL_OPTIONS=\"--iface=em1\" # 修改为 本机物理网卡的名称 启动 flannel systemctl enable flanneld systemctl start flanneld systemctl status flanneld 下面还需要修改 docker 的启动文件 /usr/lib/systemd/system/docker.service 在 ExecStart 参数 dockerd 后面增加 ExecStart=/usr/bin/dockerd $DOCKER_NETWORK_OPTIONS 重新读取配置，启动 docker systemctl daemon-reload systemctl start docker 查看网络接管 ifconfig 可以看到 docker0 与 flannel.1 已经在我们设置的IP段内了，表示已经成功 安装 kubernetes 安装k8s 首先是 Master 端安装 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:5:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"下载 rpm 包 http://upyun.mritd.me/kubernetes/kubernetes-1.3.8-1.x86_64.rpm rpm -ivh kubernetes-1.3.8-1.x86_64.rpm 安装完 kubernetes 以后 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:6:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"配置 apiserver 编辑配置文件 vim /etc/kubernetes/apiserver ### # kubernetes system config # # The following values are used to configure the kube-apiserver # # The address on the local server to listen to. KUBE_API_ADDRESS=\"--insecure-bind-address=10.6.0.140\" # The port on the local server to listen on. KUBE_API_PORT=\"--port=8080\" # Port minions listen on KUBELET_PORT=\"--kubelet-port=10250\" # Comma separated list of nodes in the etcd cluster KUBE_ETCD_SERVERS=\"--etcd-servers=http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379\" # Address range to use for services KUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\" # default admission control policies KUBE_ADMISSION_CONTROL=\"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\" # Add your own! KUBE_API_ARGS=\"\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:7:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"启动 所有服务 systemctl start kube-apiserver systemctl start kube-controller-manager systemctl start kube-scheduler systemctl enable kube-apiserver systemctl enable kube-controller-manager systemctl enable kube-scheduler systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler node 端安装 wget http://upyun.mritd.me/kubernetes/kubernetes-1.3.8-1.x86_64.rpm rpm -ivh kubernetes-1.3.8-1.x86_64.rpm ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:8:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 配置 kubelet 编辑配置文件 vim /etc/kubernetes/kubelet ### # kubernetes kubelet (minion) config # The address for the info server to serve on (set to 0.0.0.0 or \"\" for all interfaces) KUBELET_ADDRESS=\"--address=10.6.0.187\" # The port for the info server to serve on KUBELET_PORT=\"--port=10250\" # You may leave this blank to use the actual hostname KUBELET_HOSTNAME=\"--hostname-override=k8s-node-1\" # location of the api-server KUBELET_API_SERVER=\"--api-servers=http://10.6.0.140:8080\" # Add your own! KUBELET_ARGS=\"--pod-infra-container-image=docker.io/kubernetes/pause:latest\" 注： KUBELET_HOSTNAME 这个配置中 配置的为 hostname 名称，主要用于区分 node 在集群中的显示 名称 必须能 ping 通，所以前面在 /etc/hosts 中要做配置 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:9:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 配置 k8s config 下面修改 kubernetes 的 config 文件 vim /etc/kubernetes/config ### # kubernetes system config # # The following values are used to configure various aspects of all # kubernetes services, including # # kube-apiserver.service # kube-controller-manager.service # kube-scheduler.service # kubelet.service # kube-proxy.service # logging to stderr means we get it in the systemd journal KUBE_LOGTOSTDERR=\"--logtostderr=true\" # journal message level, 0 is debug KUBE_LOG_LEVEL=\"--v=0\" # Should this cluster be allowed to run privileged docker containers KUBE_ALLOW_PRIV=\"--allow-privileged=false\" # How the controller-manager, scheduler, and proxy find the apiserver KUBE_MASTER=\"--master=http://10.6.0.140:8080\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:10:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 启动 所有服务 systemctl start kubelet systemctl start kube-proxy systemctl enable kubelet systemctl enable kube-proxy systemctl status kubelet systemctl status kube-proxy ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:11:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"测试 [root@k8s-master ~]#kubectl --server=\"http://10.6.0.140:8080\" get node NAME STATUS AGE k8s-master NotReady 50m k8s-node-1 Ready 1m k8s-node-2 Ready 57s 双向 TLS 认证配置 kubernetes 提供了多种安全认证机制 Token 或用户名密码的单向 tls 认证, 基于 CA 证书 双向 tls 认证。 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:12:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"创建 ca 证书 master 中 创建 openssl ca 证书 mkdir /etc/kubernetes/cert # 必须授权 chown kube:kube -R /etc/kubernetes/cert cd /etc/kubernetes/cert # 生成私钥 openssl genrsa -out k8sca-key.pem 2048 openssl req -x509 -new -nodes -key k8sca-key.pem -days 10000 -out k8sca.pem -subj \"/CN=kube-ca\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:13:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"配置 apiserver 证书 复制openssl 的配置文件到cert目录中 cp /etc/pki/tls/openssl.cnf . 编辑 配置文件，支持IP认证 vim openssl.cnf # 在 distinguished_name 上面添加 req_extensions = v3_req [ req ] ..... req_extensions = v3_req distinguished_name = req_distinguished_name ..... # 在 [ v3_req ] 下面添加 subjectAltName = @alt_names [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names ## 添加 如下所有内容 [ alt_names ] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster.local IP.1 = ${K8S_SERVICE_IP} # kubernetes server ip IP.2 = ${MASTER_HOST} # master ip(如果都在一台机器上写一个就行) ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:14:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"签署 apiserver 证书 然后开始签署 apiserver 相关的证书 openssl genrsa -out apiserver-key.pem 2048 openssl req -new -key apiserver-key.pem -out apiserver.csr -subj \"/CN=kube-apiserver\" -config openssl.cnf openssl x509 -req -in apiserver.csr -CA k8sca.pem -CAkey k8sca-key.pem -CAcreateserial -out apiserver.pem -days 365 -extensions v3_req -extfile openssl.cnf ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:15:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"生成 node 证书 下面 生成 每个 node 的证书, 在 master 中生成，然后复制到各 node 中 apiserver 证书签署完成后还需要签署每个节点 node 的证书 cp openssl.cnf worker-openssl.cnf 编辑配置文件: vim worker-openssl.cnf ## 主要修改如下[ alt_names ]内容 , 有多少个node 就写多少个IP配置： [ alt_names ] IP.1 = 10.6.0.187 IP.2 = 10.6.0.188 生成 k8s-node-1 私钥 openssl genrsa -out k8s-node-1-worker-key.pem 2048 openssl req -new -key k8s-node-1-worker-key.pem -out k8s-node-1-worker.csr -subj \"/CN=k8s-node-1\" -config worker-openssl.cnf openssl x509 -req -in k8s-node-1-worker.csr -CA k8sca.pem -CAkey k8sca-key.pem -CAcreateserial -out k8s-node-1-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf 生成 k8s-node-2 私钥 openssl genrsa -out k8s-node-2-worker-key.pem 2048 openssl req -new -key k8s-node-2-worker-key.pem -out k8s-node-2-worker.csr -subj \"/CN=k8s-node-2\" -config worker-openssl.cnf openssl x509 -req -in k8s-node-2-worker.csr -CA k8sca.pem -CAkey k8sca-key.pem -CAcreateserial -out k8s-node-2-worker.pem -days 365 -extensions v3_req -extfile worker-openssl.cnf ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:16:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"生成集群管理证书 所有 node 生成以后， 还需要生成集群管理证书 openssl genrsa -out admin-key.pem 2048 openssl req -new -key admin-key.pem -out admin.csr -subj \"/CN=kube-admin\" openssl x509 -req -in admin.csr -CA k8sca.pem -CAkey k8sca-key.pem -CAcreateserial -out admin.pem -days 365 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:17:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"apiserver 配置 编辑 master apiserver 配置文件 vim /etc/kubernetes/apiserver ### # kubernetes system config # # The following values are used to configure the kube-apiserver # # The address on the local server to listen to. KUBE_API_ADDRESS=\"--insecure-bind-address=10.6.0.140 --insecure-bind-address=127.0.0.1\" # The port on the local server to listen on. KUBE_API_PORT=\"--secure-port=6443 --insecure-port=8080\" # Port minions listen on KUBELET_PORT=\"--kubelet-port=10250\" # Comma separated list of nodes in the etcd cluster KUBE_ETCD_SERVERS=\"--etcd-servers=http://10.6.0.140:2379,http://10.6.0.187:2379,http://10.6.0.188:2379\" # Address range to use for services KUBE_SERVICE_ADDRESSES=\"--service-cluster-ip-range=10.254.0.0/16\" # default admission control policies KUBE_ADMISSION_CONTROL=\"--admission-control=NamespaceLifecycle,NamespaceExists,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota\" # Add your own! KUBE_API_ARGS=\"--tls-cert-file=/etc/kubernetes/cert/apiserver.pem --tls-private-key-file=/etc/kubernetes/cert/apiserver-key.pem --client-ca-file=/etc/kubernetes/cert/k8sca.pem --service-account-key-file=/etc/kubernetes/cert/apiserver-key.pem\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:18:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"controller manager 配置 接着编辑 controller manager 的配置 vim /etc/kubernetes/controller-manager ### # The following values are used to configure the kubernetes controller-manager # defaults from config and apiserver should be adequate # Add your own! KUBE_CONTROLLER_MANAGER_ARGS=\"--service-account-private-key-file=/etc/kubernetes/cert/apiserver-key.pem --root-ca-file=/etc/kubernetes/cert/k8sca.pem --master=http://127.0.0.1:8080\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:19:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"重启 所有服务 systemctl restart kube-apiserver systemctl restart kube-controller-manager systemctl restart kube-scheduler systemctl status kube-apiserver systemctl status kube-controller-manager systemctl status kube-scheduler ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:20:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 节点配置 先 copy 配置文件 到 node 节点 k8s-node-1 节点： mkdir /etc/kubernetes/cert/ # 必须授权 chown kube:kube -R /etc/kubernetes/cert # 拷贝如下三个文件到 /etc/kubernetes/cert/ 目录下 k8s-node-1-worker-key.pem k8s-node-1-worker.pem k8sca.pem k8s-node-2 节点： mkdir /etc/kubernetes/cert/ # 必须授权 chown kube:kube -R /etc/kubernetes/cert # 拷贝如下三个文件到 /etc/kubernetes/cert/ 目录下 k8s-node-2-worker-key.pem k8s-node-2-worker.pem k8sca.pem ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:21:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 节点 kubelet 配置 如下配置 所有 node 节点都需要配置 修改 kubelet 配置 vim /etc/kubernetes/kubelet ### # kubernetes kubelet (minion) config # The address for the info server to serve on (set to 0.0.0.0 or \"\" for all interfaces) KUBELET_ADDRESS=\"--address=10.6.0.188\" # The port for the info server to serve on KUBELET_PORT=\"--port=10250\" # You may leave this blank to use the actual hostname KUBELET_HOSTNAME=\"--hostname-override=k8s-node-2\" # location of the api-server KUBELET_API_SERVER=\"--api-servers=https://10.6.0.140:6443\" # Add your own! KUBELET_ARGS=\"--tls-cert-file=/etc/kubernetes/cert/k8s-node-1-worker.pem --tls-private-key-file=/etc/kubernetes/cert/k8s-node-1-worker-key.pem --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml --pod-infra-container-image=docker.io/kubernetes/pause:latest\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:22:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 节点 config 配置 修改 config 配置 vim /etc/kubernetes/config ### # kubernetes system config # # The following values are used to configure various aspects of all # kubernetes services, including # # kube-apiserver.service # kube-controller-manager.service # kube-scheduler.service # kubelet.service # kube-proxy.service # logging to stderr means we get it in the systemd journal KUBE_LOGTOSTDERR=\"--logtostderr=true\" # journal message level, 0 is debug KUBE_LOG_LEVEL=\"--v=0\" # Should this cluster be allowed to run privileged docker containers KUBE_ALLOW_PRIV=\"--allow-privileged=false\" # How the controller-manager, scheduler, and proxy find the apiserver KUBE_MASTER=\"--master=https://10.6.0.140:6443\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:23:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 创建 kube-proxy 创建 kube-proxy 配置文件 vim /etc/kubernetes/worker-kubeconfig.yaml apiVersion: v1 kind: Config clusters: - name: local cluster: certificate-authority: /etc/kubernetes/cert/k8sca.pem users: - name: kubelet user: client-certificate: /etc/kubernetes/cert/k8s-node-1-worker.pem client-key: /etc/kubernetes/cert/k8s-node-1-worker-key.pem contexts: - context: cluster: local user: kubelet name: kubelet-context current-context: kubelet-context ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:24:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node 节点 配置 kube-proxy 证书 配置 kube-proxy 使其使用证书 vim /etc/kubernetes/proxy ### # kubernetes proxy config # default config should be adequate # Add your own! KUBE_PROXY_ARGS=\"--master=https://10.6.0.140:6443 --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml\" ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:25:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":["kubernetes"],"content":"node节点重启服务 systemctl restart kubelet systemctl restart kube-proxy systemctl status kubelet systemctl status kube-proxy 至此，整个集群已经搭建完成，剩下的就是pod 的测试 ","date":"2016-10-12","objectID":"/kubernetes-1.3.8/:26:0","tags":null,"title":"docker k8s 1.3.8 + flannel","uri":"/kubernetes-1.3.8/"},{"categories":null,"content":"CentOS 7 安装 GlusterFS","date":"2016-10-11","objectID":"/glusterfs/","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"CentOS 7 GlusterFS ","date":"2016-10-11","objectID":"/glusterfs/:0:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"环境说明： 3台机器安装 GlusterFS 组成一个集群。 使用 docker volume plugin GlusterFS 服务器： 10.6.0.140 10.6.0.192 10.6.0.196 配置 hosts 10.6.0.140 swarm-manager 10.6.0.192 swarm-node-1 10.6.0.196 swarm-node-2 client: 10.6.0.94 node-94 ","date":"2016-10-11","objectID":"/glusterfs/:1:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"安装glusterfs CentOS 安装 glusterfs 非常的简单 在三个节点都安装glusterfs yum install centos-release-gluster yum install -y glusterfs glusterfs-server glusterfs-fuse glusterfs-rdma 启动 glusterFS systemctl start glusterd.service systemctl enable glusterd.service ","date":"2016-10-11","objectID":"/glusterfs/:2:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"配置 GlusterFS 在 swarm-manager 节点上配置，将 节点 加入到 集群中。 [root@swarm-manager ~]#gluster peer probe swarm-manager peer probe: success. Probe on localhost not needed [root@swarm-manager ~]#gluster peer probe swarm-node-1 peer probe: success. [root@swarm-manager ~]#gluster peer probe swarm-node-2 peer probe: success. 查看集群状态： [root@swarm-manager ~]#gluster peer status Number of Peers: 2 Hostname: swarm-node-1 Uuid: 41573e8b-eb00-4802-84f0-f923a2c7be79 State: Peer in Cluster (Connected) Hostname: swarm-node-2 Uuid: da068e0b-eada-4a50-94ff-623f630986d7 State: Peer in Cluster (Connected) 创建数据存储目录： [root@swarm-manager ~]#mkdir -p /opt/gluster/data [root@swarm-node-1 ~]# mkdir -p /opt/gluster/data [root@swarm-node-2 ~]# mkdir -p /opt/gluster/data 查看volume 状态： [root@swarm-manager ~]#gluster volume info No volumes present ","date":"2016-10-11","objectID":"/glusterfs/:3:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"volume 模式说明 一、 默认模式，既DHT, 也叫 分布卷: 将文件已hash算法随机分布到 一台服务器节点中存储。 gluster volume create test-volume server1:/exp1 server2:/exp2 二、 复制模式，既AFR, 创建volume 时带 replica x 数量: 将文件复制到 replica x 个节点中。 gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2 三、 条带模式，既Striped, 创建volume 时带 stripe x 数量： 将文件切割成数据块，分别存储到 stripe x 个节点中 ( 类似raid 0 )。 gluster volume create test-volume stripe 2 transport tcp server1:/exp1 server2:/exp2 四、 分布式条带模式（组合型），最少需要4台服务器才能创建。 创建volume 时 stripe 2 server = 4 个节点： 是DHT 与 Striped 的组合型。 gluster volume create test-volume stripe 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 五、 分布式复制模式（组合型）, 最少需要4台服务器才能创建。 创建volume 时 replica 2 server = 4 个节点：是DHT 与 AFR 的组合型。 gluster volume create test-volume replica 2 transport tcp server1:/exp1 server2:/exp2　server3:/exp3 server4:/exp4 六、 条带复制卷模式（组合型）, 最少需要4台服务器才能创建。 创建volume 时 stripe 2 replica 2 server = 4 个节点： 是 Striped 与 AFR 的组合型。 gluster volume create test-volume stripe 2 replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 七、 三种模式混合, 至少需要8台 服务器才能创建。 stripe 2 replica 2 , 每4个节点 组成一个 组。 gluster volume create test-volume stripe 2 replica 2 transport tcp server1:/exp1 server2:/exp2 server3:/exp3 server4:/exp4 server5:/exp5 server6:/exp6 server7:/exp7 server8:/exp8 ","date":"2016-10-11","objectID":"/glusterfs/:4:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"创建GlusterFS磁盘 [root@swarm-manager ~]#gluster volume create models replica 3 swarm-manager:/opt/gluster/data swarm-node-1:/opt/gluster/data swarm-node-2:/opt/gluster/data force volume create: models: success: please start the volume to access data 再查看 volume 状态： [root@swarm-manager ~]#gluster volume info Volume Name: models Type: Replicate Volume ID: e539ff3b-2278-4f3f-a594-1f101eabbf1e Status: Created Number of Bricks: 1 x 3 = 3 Transport-type: tcp Bricks: Brick1: swarm-manager:/opt/gluster/data Brick2: swarm-node-1:/opt/gluster/data Brick3: swarm-node-2:/opt/gluster/data Options Reconfigured: performance.readdir-ahead: on 启动 models [root@swarm-manager ~]#gluster volume start models volume start: models: success ","date":"2016-10-11","objectID":"/glusterfs/:5:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"gluster 性能调优 开启 指定 volume 的配额： (models 为 volume 名称) gluster volume quota models enable 限制 models 中 / (既总目录) 最大使用 80GB 空间 gluster volume quota models limit-usage / 80GB 设置 cache 4GB gluster volume set models performance.cache-size 4GB 开启 异步 ， 后台操作 gluster volume set models performance.flush-behind on 设置 io 线程 32 gluster volume set models performance.io-thread-count 32 设置 回写 (写数据时间，先写入缓存内，再写入硬盘) gluster volume set models performance.write-behind on ","date":"2016-10-11","objectID":"/glusterfs/:6:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"GlusterFS客户端 部署GlusterFS客户端并mount GlusterFS文件系统 [root@node-94 ~]#yum install -y glusterfs glusterfs-fuse [root@node-94 ~]#mkdir -p /opt/gfsmnt [root@node-94 ~]#mount -t glusterfs swarm-manager:models /opt/gfsmnt/ [root@node-94 ~]#df -h 文件系统 容量 已用 可用 已用% 挂载点 /dev/mapper/vg001-root 98G 1.2G 97G 2% / devtmpfs 32G 0 32G 0% /dev tmpfs 32G 0 32G 0% /dev/shm tmpfs 32G 130M 32G 1% /run tmpfs 32G 0 32G 0% /sys/fs/cgroup /dev/mapper/vg001-opt 441G 71G 370G 17% /opt /dev/sda2 497M 153M 344M 31% /boot tmpfs 6.3G 0 6.3G 0% /run/user/0 swarm-manager:models 441G 18G 424G 4% /opt/gfsmnt ","date":"2016-10-11","objectID":"/glusterfs/:7:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"测试： DHT 模式 客户端 创建一个 1G 的文件 [root@node-94 ~]#time dd if=/dev/zero of=hello bs=1000M count=1 记录了1+0 的读入 记录了1+0 的写出 1048576000字节(1.0 GB)已复制，9.1093 秒，115 MB/秒 real 0m9.120s user 0m0.000s sys 0m1.134s AFR 模式 客户端 创建一个 1G 的文件 [root@node-94 ~]#time dd if=/dev/zero of=hello.txt bs=1024M count=1 录了1+0 的读入 记录了1+0 的写出 1073741824字节(1.1 GB)已复制，27.4566 秒，39.1 MB/秒 real 0m27.469s user 0m0.000s sys 0m1.065s Striped 模式 客户端 创建一个 1G 的文件 [root@node-94 ~]#time dd if=/dev/zero of=hello bs=1000M count=1 记录了1+0 的读入 记录了1+0 的写出 1048576000字节(1.0 GB)已复制，9.10669 秒，115 MB/秒 real 0m9.119s user 0m0.001s sys 0m0.953s 条带复制卷模式 (Number of Bricks: 1 x 2 x 2 = 4) 客户端 创建一个 1G 的文件 [root@node-94 ~]#time dd if=/dev/zero of=hello bs=1000M count=1 记录了1+0 的读入 记录了1+0 的写出 1048576000字节(1.0 GB)已复制，17.965 秒，58.4 MB/秒 real 0m17.978s user 0m0.000s sys 0m0.970s 分布式复制模式 (Number of Bricks: 2 x 2 = 4) 客户端 创建一个 1G 的文件 [root@node-94 ~]#time dd if=/dev/zero of=haha bs=100M count=10 记录了10+0 的读入 记录了10+0 的写出 1048576000字节(1.0 GB)已复制，17.7697 秒，59.0 MB/秒 real 0m17.778s user 0m0.001s sys 0m0.886s 针对 分布式复制模式还做了如下测试： 4K随机写 测试: 安装 fio (yum -y install libaio-devel (否则运行fio 会报错engine libaio not loadable, 已安装需重新编译，否则一样报错)) [root@node-94 ~]#fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -size=10G -filename=1.txt -name=\"EBS 4KB randwrite test\" -iodepth=32 -runtime=60 write: io=352204KB, bw=5869.9KB/s, iops=1467, runt= 60002msec WRITE: io=352204KB, aggrb=5869KB/s, minb=5869KB/s, maxb=5869KB/s, mint=60002msec, maxt=60002msec 4K随机读 测试： fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randread -size=10G -filename=1.txt -name=\"EBS 4KB randread test\" -iodepth=8 -runtime=60 read: io=881524KB, bw=14692KB/s, iops=3672, runt= 60001msec READ: io=881524KB, aggrb=14691KB/s, minb=14691KB/s, maxb=14691KB/s, mint=60001msec, maxt=60001msec 512K 顺序写 测试： fio -ioengine=libaio -bs=512k -direct=1 -thread -rw=write -size=10G -filename=512.txt -name=\"EBS 512KB seqwrite test\" -iodepth=64 -runtime=60 write: io=3544.0MB, bw=60348KB/s, iops=117, runt= 60135msec WRITE: io=3544.0MB, aggrb=60348KB/s, minb=60348KB/s, maxb=60348KB/s, mint=60135msec, maxt=60135msec ","date":"2016-10-11","objectID":"/glusterfs/:8:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":null,"content":"维护命令 查看GlusterFS中所有的volume: [root@swarm-manager ~]#gluster volume list 删除GlusterFS磁盘： [root@swarm-manager ~]#gluster volume stop models #停止名字为 models 的磁盘 [root@swarm-manager ~]#gluster volume delete models #删除名字为 models 的磁盘 注： 删除 磁盘 以后，必须删除 磁盘( /opt/gluster/data ) 中的 （ .glusterfs/ .trashcan/ ）目录。 否则创建新 volume 相同的 磁盘 会出现文件 不分布，或者 类型 错乱 的问题。 卸载某个节点GlusterFS磁盘 [root@swarm-manager ~]#gluster peer detach swarm-node-2 设置访问限制,按照每个volume 来限制 [root@swarm-manager ~]#gluster volume set models auth.allow 10.6.0.*,10.7.0.* 添加GlusterFS节点： [root@swarm-manager ~]#gluster peer probe swarm-node-3 [root@swarm-manager ~]#gluster volume add-brick models swarm-node-3:/opt/gluster/data 注：如果是复制卷或者条带卷，则每次添加的Brick数必须是replica或者stripe的整数倍 配置卷 [root@swarm-manager ~]# gluster volume set 缩容volume: 先将数据迁移到其它可用的Brick，迁移结束后才将该Brick移除： [root@swarm-manager ~]#gluster volume remove-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data start 在执行了start之后，可以使用status命令查看移除进度： [root@swarm-manager ~]#gluster volume remove-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data status 不进行数据迁移，直接删除该Brick： [root@swarm-manager ~]#gluster volume remove-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data commit 注意，如果是复制卷或者条带卷，则每次移除的Brick数必须是replica或者stripe的整数倍。 扩容： gluster volume add-brick models swarm-node-2:/opt/gluster/data 修复命令: [root@swarm-manager ~]#gluster volume replace-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data commit -force 迁移volume: [root@swarm-manager ~]#gluster volume replace-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data start pause 为暂停迁移 [root@swarm-manager ~]#gluster volume replace-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data pause abort 为终止迁移 [root@swarm-manager ~]#gluster volume replace-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data abort status 查看迁移状态 [root@swarm-manager ~]#gluster volume replace-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data status 迁移结束后使用commit 来生效 [root@swarm-manager ~]#gluster volume replace-brick models swarm-node-2:/opt/gluster/data swarm-node-3:/opt/gluster/data commit 均衡volume: [root@swarm-manager ~]#gluster volume models lay-outstart [root@swarm-manager ~]#gluster volume models start [root@swarm-manager ~]#gluster volume models startforce [root@swarm-manager ~]#gluster volume models status [root@swarm-manager ~]#gluster volume models stop ","date":"2016-10-11","objectID":"/glusterfs/:9:0","tags":null,"title":"CentOS 7 安装 GlusterFS","uri":"/glusterfs/"},{"categories":["ceph"],"content":"Ceph RBD CephFS","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"Ceph RBD CephFS ##环境准备 (这里只做基础测试, ceph-manager , ceph-mon, ceph-osd 一共三台) 10.6.0.140 = ceph-manager 10.6.0.187 = ceph-mon-1 10.6.0.188 = ceph-osd-1 10.6.0.94 = node-94 ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:0:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"初始化环境 注： ceph 对时间要求很严格， 一定要同步所有的服务器时间 在 manager 上面修改 /etc/hosts : 10.6.0.187 ceph-mon-1 10.6.0.188 ceph-osd-1 10.6.0.94 node-94 修改各服务器上面的 hostname (说明：ceph-deploy工具都是通过主机名与其他节点通信) hostnamectl --static set-hostname ceph-manager hostnamectl --static set-hostname ceph-mon-1 hostnamectl --static set-hostname ceph-osd-1 hostnamectl --static set-hostname node-94 配置manager节点与其他节点ssh key 访问 [root@ceph-manager ~]# ssh-keygen # 将key 发送到各节点中 [root@ceph-manager ~]#ssh-copy-id ceph-mon-1 [root@ceph-manager ~]#ssh-copy-id ceph-osd-1 ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:1:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"安装 ceph 在manager节点安装 ceph-deploy [root@ceph-manager ~]#yum -y install centos-release-ceph [root@ceph-manager ~]#yum makecache [root@ceph-manager ~]#yum -y install ceph-deploy ntpdate 在其他各节点安装 ceph 的yum源 [root@ceph-mon-1 ~]# yum -y install centos-release-ceph [root@ceph-mon-1 ~]# yum makecache [root@ceph-osd-1 ~]# yum -y install centos-release-ceph [root@ceph-osd-1 ~]# yum makecache ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:2:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"创建ceph 目录 [root@ceph-manager ~]#mkdir -p /etc/ceph [root@ceph-manager ~]#cd /etc/ceph ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:3:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"创建监控节点： [root@ceph-manager /etc/ceph]#ceph-deploy new ceph-mon-1 执行完毕会生成 ceph.conf ceph.log ceph.mon.keyring 三个文件 编辑 ceph.conf 增加 osd 节点数量 在最后增加： osd pool default size = 1 使用ceph-deploy在所有机器安装ceph [root@ceph-manager /etc/ceph]# ceph-deploy install ceph-manager ceph-mon-1 ceph-osd-1 如果出现错误，也可以到各节点中直接 yum -y install ceph ceph-radosgw 进行安装 yum -y install ceph ceph-radosgw ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:4:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"初始化监控节点 [root@ceph-manager /etc/ceph]# ceph-deploy mon create-initial osd 节点创建存储空间 [root@ceph-osd-1 ~]# mkdir -p /opt/osd1 在管理节点上启动 并 激活 osd 进程 [root@ceph-manager ~]# ceph-deploy osd prepare ceph-osd-1:/opt/osd1 [root@ceph-manager ~]# ceph-deploy osd activate ceph-osd-1:/opt/osd1 把管理节点的配置文件与keyring同步至其它节点 [root@ceph-manager ~]# ceph-deploy admin ceph-mon-1 ceph-osd-1 查看集群健康状态 (HEALTH_OK 表示OK) [root@ceph-manager ~]# ceph health HEALTH_OK ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:5:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"客户端配置 客户端 挂载: ceph 有多种挂载方式, rbd 块设备映射， cephfs 挂载 等 注: 在生产环境中，客户端应该对应pool的权限，而不是admin 权限 [root@ceph-manager ~]# ssh-copy-id node-94 ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:6:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"安装ceph [root@ceph-manager ~]# ceph-deploy install node-94 或者 登陆 node-94 执行 yum -y install ceph ceph-radosgw 如果ssh 非22端口，会报错 可使用 scp 传 scp -P端口 ceph.conf node-94:/etc/ceph/ scp -P端口 ceph.client.admin.keyring node-94:/etc/ceph/ ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:7:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"创建pool [root@ceph-manager ~]# ceph osd pool create press 100 pool 'press' created 设置pool 的pgp_num [root@ceph-manager ~]# ceph osd pool set press pgp_num 100 查看创建的pool [root@ceph-manager ~]# ceph osd lspools 0 rbd,1 press, 设置副本数为2 (osd 必须要大于或者等于副本数，否则报错, 千万注意) [root@ceph-manager ~]# ceph osd pool set press size 2 创建一个100G 名为 image 镜像 [root@ceph-manager ~]# rbd create -p press --size 100000 image 查看一下镜像: [root@ceph-manager ~]# rbd -p press info image rbd image 'image': size 100000 MB in 25000 objects order 22 (4096 kB objects) block_name_prefix: rb.0.104b.74b0dc51 format: 1 ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:8:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"客户端块存储挂载： 在node-94 上面 map 镜像 [root@node-94 ~]# rbd -p press map image /dev/rbd0 格式化 image [root@node-94 ~]# mkfs.xfs /dev/rbd0 创建挂载目录 [root@node-94 ~]# mkdir /opt/rbd 挂载 rbd [root@node-94 ~]# mount /dev/rbd0 /opt/rbd [root@node-94 ~]# time dd if=/dev/zero of=haha bs=1M count=1000 取消 map 镜像 [root@node-94 ~]# umount /opt/rbd [root@node-94 ~]# rbd unmap /dev/rbd0 ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:9:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"cephFS 文件系统 客户端 cephFS 文件系统 (cephFS 必须要有2个osd 才能运行，请注意)： 使用 cephFS 集群中必须有 mds 服务 创建 mds 服务 (由于机器有限就在 mon 的服务器上面 创建 mds 服务) [root@ceph-manager ~]# ceph-deploy mds create ceph-mon-1 创建2个pool 做为文件系统的data 与 metadata [root@ceph-manager ~]# ceph osd pool create cephfs_data 99 pool 'cephfs_data' created [root@ceph-manager ~]# ceph osd pool create cephfs_metadata 99 pool 'cephfs_metadata' created 创建 文件系统： [root@ceph-manager ~]# ceph fs new jicki cephfs_metadata cephfs_data new fs with metadata pool 6 and data pool 5 查看所有文件系统： [root@ceph-manager ~]# ceph fs ls name: jicki, metadata pool: cephfs_metadata, data pools: [cephfs_data ] 删除一个文件系统 [root@ceph-manager ~]# ceph fs rm jicki --yes-i-really-mean-it ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:10:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"客户端挂载 cephFS 安装 ceph-fuse： [root@node-94 ~]# yum install ceph-fuse -y 创建挂载目录: [root@node-94 ~]# mkdir -p /opt/jicki [root@node-94 ~]# ceph-fuse /opt/jicki [root@node-94 ~]# df -h|grep ceph ceph-fuse 1.6T 25G 1.6T 2% /opt/jicki ceph 相关命令： ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:11:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"manager 篇 查看实时的运行状态信息： [root@ceph-manager ~]# ceph -w 查看状态信息： [root@ceph-manager ~]# ceph -s 查看存储空间： [root@ceph-manager ~]# ceph df 删除某个节点的所有的ceph数据包： [root@ceph-manager ~]# ceph-deploy purge ceph-mon-1 [root@ceph-manager ~]# ceph-deploy purgedata ceph-mon-1 为ceph创建一个admin用户并为admin用户创建一个密钥，把密钥保存到/etc/ceph目录下： [root@ceph-manager ~]# ceph auth get-or-create client.admin mds 'allow' osd 'allow *' mon 'allow *' -o /etc/ceph/ceph.client.admin.keyring 为osd.ceph-osd-1创建一个用户并创建一个key [root@ceph-manager ~]# ceph auth get-or-create osd.ceph-osd-1 mon 'allow rwx' osd 'allow *' -o /etc/ceph/keyring 为mds.ceph-mon-1创建一个用户并创建一个key [root@ceph-manager ~]# ceph auth get-or-create mds.ceph-mon-1 mon 'allow rwx' osd 'allow *' mds 'allow *' -o /etc/ceph/keyring 查看ceph集群中的认证用户及相关的key [root@ceph-manager ~]# ceph auth list 删除集群中的一个认证用户 [root@ceph-manager ~]# ceph auth del osd.0 查看集群健康状态详细信息 [root@ceph-manager ~]# ceph health detail 查看ceph log日志所在的目录 [root@ceph-manager ~]# ceph-conf --name mds.ceph-manager --show-config-value log_file ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:12:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"mon 篇 查看mon的状态信息 [root@ceph-manager ~]# ceph mon stat 查看mon的选举状态 [root@ceph-manager ~]# ceph quorum_status --format json-pretty 看mon的映射信息 [root@ceph-manager ~]# ceph mon dump 删除一个mon节点 [root@ceph-manager ~]# ceph mon remove ceph-mon-1 获得一个正在运行的mon map，并保存在mon-1-map.txt文件中 [root@ceph-manager ~]# ceph mon getmap -o mon-1-map.txt 查看mon-1-map.txt [root@ceph-manager ~]# monmaptool --print mon-1-map.txt 把上面的mon map注入新加入的节点 [root@ceph-manager ~]# ceph-mon -i ceph-mon-3 --inject-monmap mon-1-map.txt 查看mon的socket [root@ceph-manager ~]# ceph-conf --name mon.ceph-mon-1 --show-config-value admin_socket 查看mon的详细状态 [root@ceph-mon-1 ~]# ceph daemon mon.ceph-mon-1 mon_status 删除一个mon节点 [root@ceph-manager ~]# ceph mon remove ceph-mon-1 ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:13:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"msd 篇 查看msd状态 [root@ceph-manager ~]# ceph mds dump 删除一个mds节点 [root@ceph-manager ~]# ceph mds rm 0 mds.ceph-mds-1 ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:14:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"osd 篇 查看ceph osd运行状态 [root@ceph-manager ~]# ceph osd stat 查看osd映射信息 [root@ceph-manager ~]# ceph osd stat 查看osd的目录树 [root@ceph-manager ~]# ceph osd tree down掉一个osd硬盘 (ceph osd tree 可查看osd 的硬盘信息，下面为down osd.0 节点) [root@ceph-manager ~]# ceph osd down 0 在集群中删除一个osd硬盘 [root@ceph-manager ~]# ceph osd rm 0 在集群中删除一个osd 硬盘 并 crush map 清除map信息 [root@ceph-manager ~]# ceph osd crush rm osd.0 在集群中删除一个osd的host节点 [root@ceph-manager ~]# ceph osd crush rm ceph-osd-1 查看最大osd的个数 [root@ceph-manager ~]# ceph osd getmaxosd 设置最大的osd的个数（当扩大osd节点的时候必须扩大这个值） [root@ceph-manager ~]# ceph osd setmaxosd 10 设置osd crush的权重 ceph osd crush set ID WEIGHT NAME 使用 ceph osd tree 查看 [root@ceph-manager ~]# ceph osd crush set 1 3.0 host=ceph-osd-1 设置osd 的权重 ceph osd reweight [root@ceph-manager ~]# ceph osd reweight 1 0.5 把一个osd节点踢出集群 [root@ceph-manager ~]# ceph osd out osd.1 把踢出的osd重新加入集群 [root@ceph-manager ~]# ceph osd in osd.1 暂停osd （暂停后整个集群不再接收数据） [root@ceph-manager ~]# ceph osd pause 再次开启osd （开启后再次接收数据） [root@ceph-manager ~]# ceph osd unpause ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:15:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"PG 篇 查看pg组的映射信息 [root@ceph-manager ~]# ceph pg dump |more 查看一个PG的map [root@ceph-manager ~]# ceph pg map 0.3f 查看PG状态 [root@ceph-manager ~]# ceph pg stat 查询一个pg的详细信息 [root@ceph-manager ~]# ceph pg 0.39 query 查看pg中stuck的状态 (如有非正常pg会显示) [root@ceph-manager ~]# ceph pg dump_stuck unclean [root@ceph-manager ~]# ceph pg dump_stuck inactive [root@ceph-manager ~]# ceph pg dump_stuck stale 显示一个集群中的所有的pg统计 [root@ceph-manager ~]# ceph pg dump --format plain|more 恢复一个丢失的pg (og-id 为丢失的pg, 使用ceph pg dump_stuck inactive|unclean|stale 查找) [root@ceph-manager ~]# ceph pg {pg-id} mark_unfound_lost revert ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:16:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["ceph"],"content":"pool 篇 查看ceph集群中的pool数量 [root@ceph-manager ~]# ceph osd lspools 查看 PG组 号码： [root@ceph-manager ~]# ceph osd pool get rbd pg_num 在ceph集群中创建一个pool [root@ceph-manager ~]# ceph osd pool create test 100 (名称为 test, 100为PG组号码) 为一个ceph pool配置配额 [root@ceph-manager ~]# ceph osd pool set-quota test max_objects 10000 显示所有的pool [root@ceph-manager ~]# ceph osd pool ls 在集群中删除一个pool [root@ceph-manager ~]# ceph osd pool delete test test --yes-i-really-really-mean-it 显示集群中pool的详细信息 [root@ceph-manager ~]# rados df 给一个pool创建一个快照 [root@ceph-manager ~]# ceph osd pool mksnap test test-snap 删除pool的快照 [root@ceph-manager ~]# ceph osd pool rmsnap test test-snap 查看data池的pg数量 [root@ceph-manager ~]# ceph osd pool get test pg_num 设置data池的最大存储空间（默认是1T, 1T = 1000000000000, 如下为100T) [root@ceph-manager ~]# ceph osd pool set test target_max_bytes 100000000000000 设置data池的副本数 [root@ceph-manager ~]# ceph osd pool set test size 3 设置data池能接受写操作的最小副本为2 [root@ceph-manager ~]# ceph osd pool set test min_size 2 查看集群中所有pool的副本尺寸 [root@ceph-manager ~]# ceph osd dump | grep 'replicated size' 设置一个pool的pg数量 [root@ceph-manager ~]# ceph osd pool set test pg_num 100 设置一个pool的pgp数量 [root@ceph-manager ~]# ceph osd pool set test pgp_num 100 查看ceph pool中的ceph object (volumes 为pool名称)（这里的object是以块形式存储的） [root@ceph-manager ~]# rados ls -p volumes | more 创建一个对象object [root@ceph-manager ~]# rados create test-object -p test 查看object [root@ceph-manager ~]# rados -p test ls 删除一个对象 [root@ceph-manager ~]# rados rm test-object -p test 查看ceph中一个pool里的所有镜像 (volumes 为pool名称) [root@ceph-manager ~]# rbd ls volumes 在test池中创建一个命名为images的1000M的镜像 [root@ceph-manager ~]# rbd create -p test --size 1000 images 查看刚创建的镜像信息 [root@ceph-manager ~]# rbd -p test info images 删除一个镜像 [root@ceph-manager ~]# rbd rm -p test images 调整一个镜像的尺寸 [root@ceph-manager ~]# rbd resize -p test --size 2000 images 给镜像创建一个快照 (池/镜像名@快照名) [root@ceph-manager ~]# rbd snap create test/images@images1 删除一个镜像文件的一个快照 [root@ceph-manager ~]# rbd snap rm 快照池/快照镜像文件@具体快照 如果删除快照提示保护，需要先删除保护 [root@ceph-manager ~]# rbd snap unprotect 快照池/快照镜像文件@具体快照 删除一个镜像文件的所有快照 [root@ceph-manager ~]# rbd snap purge -p 快照池/快照镜像文件 把ceph pool中的一个镜像导出 [root@ceph-manager ~]# rbd export -p images --image \u003c具体镜像id\u003e /tmp/images.img ","date":"2016-10-11","objectID":"/ceph-rbd-cephfs/:17:0","tags":null,"title":"Ceph RBD CephFS","uri":"/ceph-rbd-cephfs/"},{"categories":["docker"],"content":"docker 容器日志集中 ELK + filebeat","date":"2016-10-11","objectID":"/docker-elk-filebeat/","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"docker 容器日志集中 ELK ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:0:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"说明 ELK 基于 ovr 网络下 ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:1:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"docker-compose yaml 文件 version: '2' networks: network-test: external: name: ovr0 services: elasticsearch: image: elasticsearch network-test: external: hostname: elasticsearch container_name: elasticsearch restart: always volumes: - /opt/elasticsearch/data:/usr/share/elasticsearch/data kibana: image: kibana network-test: external: hostname: kibana container_name: kibana restart: always environment: ELASTICSEARCH_URL: http://elasticsearch:9200/ ports: - 5601:5601 logstash: image: logstash network-test: external: hostname: logstash container_name: logstash restart: always volumes: - /opt/logstash/conf:/opt/logstash/conf command: logstash -f /opt/logstash/conf/ filebeat: image: prima/filebeat network-test: external: hostname: filebeat container_name: filebeat restart: always volumes: - /opt/filebeat/conf/filebeat.yml:/filebeat.yml - /opt/upload:/data/logs - /opt/filebeat/registry:/etc/registry ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:2:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"filebeat 说明 filebeat.yml 挂载为 filebeat 的配置文件 logs 为 容器挂载日志的目录 registry 读取日志的记录，防止filebeat 容器挂掉，需要重新读取所有日志 ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:3:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"logstash 说明 logstash 配置文件如下： 使用 patterns logstash.conf： (配置了二种输入模式， filebeats, syslog) input { beats { port =\u003e 5044 type =\u003e beats } tcp { port =\u003e 5000 type =\u003e syslog } } filter { if[type] == \"tomcat-log\" { multiline { patterns_dir =\u003e \"/opt/logstash/conf/patterns\" pattern =\u003e \"(^%{TOMCAT_DATESTAMP})|(^%{CATALINA_DATESTAMP})\" negate =\u003e true what =\u003e \"previous\" } if \"ERROR\" in [message] { #如果消息里有ERROR字符则将type改为自定义的标记 mutate { replace =\u003e { type =\u003e \"tomcat_catalina_error\" } } } else if \"WARN\" in [message] { mutate { replace =\u003e { type =\u003e \"tomcat_catalina_warn\" } } } else if \"DEBUG\" in [message] { mutate { replace =\u003e { type =\u003e \"tomcat_catalina_debug\" } } } else { mutate { replace =\u003e { type =\u003e \"tomcat_catalina_info\" } } } grok{ patterns_dir =\u003e \"/opt/logstash/conf/patterns\" match =\u003e [ \"message\", \"%{TOMCATLOG}\", \"message\", \"%{CATALINALOG}\" ] remove_field =\u003e [\"message\"] #这表示匹配成功后是否删除原始信息，这个看个人情况，如果为了节省空间可以考虑删除 } date { match =\u003e [ \"timestamp\", \"yyyy-MM-dd HH:mm:ss,SSS Z\", \"MMM dd, yyyy HH:mm:ss a\" ] } } if[type] == \"nginx-log\" { if '\"status\":\"404\"' in [message] { mutate { replace =\u003e { type =\u003e \"nginx_error_404\" } } } else if '\"status\":\"500\"' in [message] { mutate { replace =\u003e { type =\u003e \"nginx_error_500\" } } } else if '\"status\":\"502\"' in [message] { mutate { replace =\u003e { type =\u003e \"nginx_error_502\" } } } else if '\"status\":\"403\"' in [message] { mutate { replace =\u003e { type =\u003e \"nginx_error_403\" } } } else if '\"status\":\"504\"' in [message] { mutate { replace =\u003e { type =\u003e \"nginx_error_504\" } } } else if '\"status\":\"200\"' in [message] { mutate { replace =\u003e { type =\u003e \"nginx_200\" } } } grok { remove_field =\u003e [\"message\"] #这表示匹配成功后是否删除原始信息，这个看个人情况，如果为了节省空间可以考虑删除 } } } output { elasticsearch { hosts =\u003e [\"elasticsearch:9200\"] } #stdout { codec =\u003e rubydebug } #输出到屏幕上 } /opt/logstash/conf/patterns 下面存放 grok 文件 grok-patterns 文件 USERNAME [a-zA-Z0-9._-]+ USER %{USERNAME} INT (?:[+-]?(?:[0-9]+)) BASE10NUM (?\u003c![0-9.+-])(?\u003e[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))) NUMBER (?:%{BASE10NUM}) BASE16NUM (?\u003c![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+)) BASE16FLOAT \\b(?\u003c![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\\.[0-9A-Fa-f]*)?)|(?:\\.[0-9A-Fa-f]+)))\\b POSINT \\b(?:[1-9][0-9]*)\\b NONNEGINT \\b(?:[0-9]+)\\b WORD \\b\\w+\\b NOTSPACE \\S+ SPACE \\s* DATA .*? GREEDYDATA .* QUOTEDSTRING (?\u003e(?\u003c!\\\\)(?\u003e\"(?\u003e\\\\.|[^\\\\\"]+)+\"|\"\"|(?\u003e'(?\u003e\\\\.|[^\\\\']+)+')|''|(?\u003e`(?\u003e\\\\.|[^\\\\`]+)+`)|``)) UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12} # Networking MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC}) CISCOMAC (?:(?:[A-Fa-f0-9]{4}\\.){2}[A-Fa-f0-9]{4}) WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2}) COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2}) IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)(\\.(25[0-5]|2[0-4]\\d|1\\d\\d|[1-9]?\\d)){3}))|:)))(%.+)? IPV4 (?\u003c![0-9])(?:(?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2}))(?![0-9]) IP (?:%{IPV6}|%{IPV4}) HOSTNAME \\b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\\.(?:[0-9A-Za-z]","date":"2016-10-11","objectID":"/docker-elk-filebeat/:4:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"容器日志输出 docker里，标准的日志方式是用Stdout, docker 里面配置标准输出，只需要指定： syslog 就可以了。 对于 stdout 标准输出的 docker 日志，我们使用 logstash 来收集日志就可以。 我们在 docker-compose 中配置如下既可： logging: driver: syslog options: syslog-address: 'tcp://logstash:5000' 但是一般来说我们都是文件日志，那么我们就可以直接用filebeat 对于 filebeat 我们使用 官方的 dockerhub 的 prima/filebeat 镜像。 官方的镜像中，我们需要编译一个filebeat.yml 文件， 官方说明中有两种方案： 第一是 -v 挂载 -v /path/filebeat.yml:/filebeat.yml 第二是 dockerfile 的时候 FROM prima/filebeat COPY filebeat.yml /filebeat.yml 编译一个 filebeat.yml 文件。 filebeat.yml 支持单一路径的 prospector， 也支持多个 prospector或者每个prospector多个路径。 paths 可使用多层匹配， 如： /var/log/messages* , /var/log/* , /opt/nginx/*/*.log 例： filebeat: prospectors: - paths: - \"/data/logs/catalina.*.out\" input_type: filebeat-log document_type: tomcat-log - paths: - \"/data/logs/nginx*/logs/*.log\" input_type: filebeat-log document_type: nginx-log registry_file: /etc/registry/mark output: logstash: hosts: [\"logstash:5044\"] logging: files: rotateeverybytes: 10485760 # = 10MB filebeat 需要在每台需要采集的机器上面都启动一个容器，或者日志存于分布式存储中。 执行 docker-compose up -d 查看启动的 容器 ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:5:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"加载 filebeat 模板 进入 elasticsearch 容器中 ( docker exec -it elasticsearch bash ) curl -O https://gist.githubusercontent.com/thisismitch/3429023e8438cc25b86c/raw/d8c479e2a1adcea8b1fe86570e42abab0f10f364/filebeat-index-template.json curl -XPUT 'http://elasticsearch:9200/_template/filebeat?pretty' -d@filebeat-index-template.json filebeat-index-template.json { \"mappings\": { \"_default_\": { \"_all\": { \"enabled\": true, \"norms\": { \"enabled\": false } }, \"dynamic_templates\": [ { \"template1\": { \"mapping\": { \"doc_values\": true, \"ignore_above\": 1024, \"index\": \"not_analyzed\", \"type\": \"{dynamic_type}\" }, \"match\": \"*\" } } ], \"properties\": { \"@timestamp\": { \"type\": \"date\" }, \"message\": { \"type\": \"string\", \"index\": \"analyzed\" }, \"offset\": { \"type\": \"long\", \"doc_values\": \"true\" }, \"geoip\" : { \"type\" : \"object\", \"dynamic\": true, \"properties\" : { \"location\" : { \"type\" : \"geo_point\" } } } } } }, \"settings\": { \"index.refresh_interval\": \"5s\" }, \"template\": \"filebeat-*\" } 访问 http://kibana-IP:5601 可以看到已经出来 kibana 了，但是还没有数据 ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:6:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"测试 启动一个 nginx 容器 docker-compose nginx: image: alpine-nginx networks: network-test: hostname: nginx container_name: nginx restart: always ports: - 80:80 volumes: - /opt/upload/nginx/conf/vhost:/etc/nginx/vhost - /opt/upload/nginx/logs:/opt/nginx/logs 本地目录 /opt/upload/nginx 必须挂载到 filebeat 容器里面，让filebeat 可以采集到。 ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:7:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"转换 nginx 格式 将nginx日志格式转换为json 格式,方便后续的分析 log_format main '{\"@timestamp\":\"$time_iso8601\",' '\"host\":\"$server_addr\",' '\"clientip\":\"$remote_addr - $remote_user [$time_local]\",' '\"size\":$body_bytes_sent,' '\"responsetime\":$request_time,' '\"upstreamtime\":\"$upstream_response_time\",' '\"upstreamhost\":\"$upstream_addr\",' '\"http_host\":\"$host\",' '\"url\":\"$request\",' '\"domain\":\"$host\",' '\"xff\":\"$http_x_forwarded_for\",' '\"referer\":\"$http_referer\",' '\"user_agent\":\"$http_user_agent\",' '\"status\":\"$status\"}'; 最后我们查看效果: ","date":"2016-10-11","objectID":"/docker-elk-filebeat/:8:0","tags":null,"title":"docker 容器日志集中 ELK + filebeat","uri":"/docker-elk-filebeat/"},{"categories":["docker"],"content":"docker 镜像仓库 Harbor 部署","date":"2016-10-10","objectID":"/docker-harbor/","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"docker 镜像仓库 Harbor ","date":"2016-10-10","objectID":"/docker-harbor/:0:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"说明 Harbor 是 Vmware 公司开源的 企业级的 Docker Registry 管理项目 它主要 提供 Dcoker Registry 管理UI，可基于角色访问控制, AD/LDAP 集成，日志审核等功能，完全的支持中文。 Harbor 的所有组件都在 Dcoker 中部署，所以 Harbor 可使用 Docker Compose 快速部署。 注： 由于 Harbor 是基于 Docker Registry V2 版本，所以 docker 版本必须 \u003e = 1.10.0 docker-compose \u003e= 1.6.0 harbor 项目地址 ","date":"2016-10-10","objectID":"/docker-harbor/:1:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"下载 harbor git 下载 源码。 git clone https://github.com/vmware/harbor ","date":"2016-10-10","objectID":"/docker-harbor/:2:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"编辑配置文件 下载完以后 进入 harbor/Deploy 目录 初始化配置， 配置文件为harbor.cfg ## Configuration file of Harbor # hostname 设置访问地址，支持IP，域名，主机名，禁止设置127.0.0.1 hostname = reg.mydomain.com # 访问协议，可设置 http,https ui_url_protocol = http # 邮件通知, 配置邮件通知。 email_server = smtp.mydomain.com email_server_port = 25 email_username = sample_admin@mydomain.com email_password = abc email_from = admin \u003csample_admin@mydomain.com\u003e email_ssl = false # harbor WEB UI登陆使用的密码 harbor_admin_password = Harbor12345 # 认证方式，这里支持多种认证方式，默认是 db_auth ，既mysql数据库存储认证。 # 这里还支持 ldap 以及 本地文件存储方式。 auth_mode = db_auth # ldap 服务器访问地址。 ldap_url = ldaps://ldap.mydomain.com ldap_basedn = uid=%s,ou=people,dc=mydomain,dc=com # mysql root 账户的 密码 db_password = root123 self_registration = on use_compressed_js = on max_job_workers = 3 verify_remote_cert = on customize_crt = on # 一些显示的设置. crt_country = CN crt_state = State crt_location = CN crt_organization = organization crt_organizationalunit = organizational unit crt_commonname = example.com crt_email = example@example.com 修改为配置文件以后 运行./prepare脚本更新配置, 出现如下信息表示 更新完毕. ./prepare Generated configuration file: ./config/ui/env Generated configuration file: ./config/ui/app.conf Generated configuration file: ./config/registry/config.yml Generated configuration file: ./config/db/env Generated configuration file: ./config/jobservice/env Clearing the configuration file: ./config/ui/private_key.pem Clearing the configuration file: ./config/registry/root.crt Generated configuration file: ./config/ui/private_key.pem Generated configuration file: ./config/registry/root.crt The configuration files are ready, please use docker-compose to start the service. 执行完毕会生成一个 docker-compose.yml 文件 配置 docker-compose.yml 文件中的 挂载目录，启动方式等选项。 ","date":"2016-10-10","objectID":"/docker-harbor/:3:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"安装 docker-compose pip install docker-compose ","date":"2016-10-10","objectID":"/docker-harbor/:4:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"生成容器 docker-compose up -d 构建docker 容器 [root@localhost Deploy]# docker images REPOSITORY TAG IMAGE ID CREATED SIZE deploy_jobservice latest be822b50163d 43 minutes ago 804.6 MB deploy_mysql latest 5015ce56c9d5 49 minutes ago 328.8 MB deploy_ui latest 8596c12dbeba About an hour ago 808.1 MB deploy_log latest 6a74c6f52a2b About an hour ago 187.9 MB mysql 5.6 5e0f1b09e25e 2 days ago 328.8 MB ubuntu 14.04 0ccb13bf1954 12 days ago 187.9 MB golang 1.6.2 8ecba0e9bd48 5 weeks ago 753.5 MB nginx 1.9 c8c29d842c09 10 weeks ago 182.7 MB registry 2.4.0 8b162eee2794 3 months ago 171.1 MB [root@localhost Deploy]# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9704f42b05d5 deploy_jobservice \"/go/bin/harbor_jobse\" 4 minutes ago Up 4 minutes deploy_jobservice_1 0f8ff9b099d2 library/nginx:1.9 \"nginx -g 'daemon off\" 4 minutes ago Up 4 minutes 0.0.0.0:80-\u003e80/tcp, 0.0.0.0:443-\u003e443/tcp deploy_proxy_1 6b0159939874 deploy_ui \"/go/bin/harbor_ui\" 4 minutes ago Up 4 minutes 80/tcp deploy_ui_1 6f2298da0f67 deploy_mysql \"docker-entrypoint.sh\" 4 minutes ago Up 4 minutes 3306/tcp deploy_mysql_1 2dbca506e1ea library/registry:2.4.0 \"/bin/registry serve \" 4 minutes ago Up 4 minutes 5000/tcp, 0.0.0.0:5001-\u003e5001/tcp deploy_registry_1 fc5b1a201c72 deploy_log \"/bin/sh -c 'cron \u0026\u0026 \" 4 minutes ago Up 4 minutes 0.0.0.0:1514-\u003e514/tcp deploy_log_1 完成以后，使用 http://userIP/ 访问 Harbor 使用 帐号 admin, 密码为 配置文件中 harbor_admin_password = Harbor12345 的密码 登陆 至此， Harbor 已经搭建完成，具体在 WEB UI 下面操作也是非常的简单，只有几个选项。 ","date":"2016-10-10","objectID":"/docker-harbor/:5:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"上传镜像 docker 需要上传 push 镜像，需要在 docker 中配置 –insecure-registry userIP 或者在nginx 中配置 https 配置完毕以后，重启 docker 使用 docker login userIP 登陆 Harbor [root@swarm-manager ~]#docker login 10.6.0.192 Username (admin): admin Password: Login Succeeded 查看 本地 images [root@swarm-manager ~]#docker images REPOSITORY TAG IMAGE ID CREATED SIZE mongodb latest 8af05a33e512 3 weeks ago 958.4 MB sath89/oracle-12c latest 7effebcd18ee 11 weeks ago 5.692 GB centos latest 778a53015523 4 months ago 196.7 MB tag 修改 image 的名字. 格式为: userip/项目名/image名字:版本号 [root@swarm-manager ~]#docker tag mongodb 10.6.0.192/jicki/mongodb:1.0 [root@swarm-manager ~]#docker images REPOSITORY TAG IMAGE ID CREATED SIZE 10.6.0.192/jicki/mongodb 1.0 8af05a33e512 3 weeks ago 958.4 MB mongodb latest 8af05a33e512 3 weeks ago 958.4 MB sath89/oracle-12c latest 7effebcd18ee 11 weeks ago 5.692 GB centos latest 778a53015523 4 months ago 196.7 MB push 镜像到 Harbor [root@swarm-manager ~]#docker push 10.6.0.192/jicki/mongodb:1.0 The push refers to a repository [10.6.0.192/jicki/mongodb] c1e4cd91bcd4: Pushed d9a948970255: Pushed dd9b001e77ee: Pushed 625440e212f2: Pushed 75fa23acbccb: Pushed fd269370dcf4: Pushed 44e3199c59b3: Pushed db3474cfcfbc: Pushed 5f70bf18a086: Pushed 6a6c96337be1: Pushed 1.0: digest: sha256:c7d2e619d86089ffef373819a99df1390c4f2df4aeec9c1f7945c55d63edc670 size: 2824 登陆 WEB UI ， 选择项目， 项目名称 jicki ， 进入 既可查看刚才上传的 image 至此， Harbor 都已经部署完成。 二、 配置Docker 镜像复制。 配置 2个 Harbor IP 1 = 10.6.0.192 IP 2 = 10.6.0.196 在 10.6.0.192 上面我们已经push 了一个 镜像，所以我们将这台当作 主节点，10.6.0.196 为从复制节点。 进入 WEB UI 选择 项目， 选择项目为 jicki , 然后选择 复制 选项。 点击 新增策略 创建完毕以后，我们可以看 复制策略 已经有一栏。 复制任务里面 也已经有一个任务。 稍等一会，可以看到 复制任务里面 那个任务已经提示 完成。 登陆 10.6.0.196 的 WEB UI 我们可以看到， 镜像已经复制过来。而且连 日志操作 也会复制过来。 ","date":"2016-10-10","objectID":"/docker-harbor/:6:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"harbor 升级 cd harbor/Deploy/ docker-compose down 删除原有的容器 备份整个目录 mv harbor/ /tm/harbor 重新 下载新的源码 git clone https://github.com/vmware/harbor 如果harbor 是迁移到其他服务器，请先执行数据备份 cd harbor/migration/ 修改 migration.cfg 文件里面的 数据库 帐号密码 docker build -t migrate-tool . 运行一个临时数据库容器，注意：/data/database 为你设置的挂载数据库的目录 /path/to/backup 数据备份的目录 数据库备份： docker run -ti --rm -v /data/database:/var/lib/mysql -v /path/to/backup:/harbor-migration/backup migrate-tool backup 数据库还原： docker run -ti --rm -v /data/database:/var/lib/mysql migrate-tool up head 对比一下配置文件： cd harbor/Deploy/ diff harbor.cfg /tmp/harbor/Deploy/harbor.cfg diff docker-compose.yaml /tmp/harbor/Deploy/docker-compose.yaml 如果修改了端口 必须更新 nginx 里面的端口 harbor/Deploy/config/nginx/nginx.conf 执行 ./prepare 生成新的配置文件 cd /harbor/Deploy/ ./prepare 最后build 新的镜像，启动容器 cd /harbor/Deploy/ docker-compose up --build -d 登陆 WEB UI 检查是否OK ","date":"2016-10-10","objectID":"/docker-harbor/:7:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"FAQ 删除镜像，回收容量 # 首先确认 并打印是否有正在上传与下载的镜像 $ docker-compose stop $ docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect --dry-run /etc/registry/config.yml # 执行如下命令 GC 删除镜像 $ docker run -it --name gc --rm --volumes-from registry vmware/registry:2.6.2-photon garbage-collect /etc/registry/config.yml # 删除 重新启动 $ docker-compose start ","date":"2016-10-10","objectID":"/docker-harbor/:8:0","tags":null,"title":"docker 镜像仓库 Harbor 部署","uri":"/docker-harbor/"},{"categories":["docker"],"content":"基于 docker-compose 的docker编排","date":"2016-10-02","objectID":"/docker-compose/","tags":null,"title":"基于 docker-compose 的docker编排","uri":"/docker-compose/"},{"categories":["docker"],"content":"docker 编排 ","date":"2016-10-02","objectID":"/docker-compose/:0:0","tags":null,"title":"基于 docker-compose 的docker编排","uri":"/docker-compose/"},{"categories":["docker"],"content":"说明 docker-compose 是一款开源的docker 简化复杂容器环境的管理工具 。 docker-compose 在结合Swarm 与 docker 进程化容器部署可以很方便的部署一套环境。 具体的流程如下： ","date":"2016-10-02","objectID":"/docker-compose/:1:0","tags":null,"title":"基于 docker-compose 的docker编排","uri":"/docker-compose/"},{"categories":["docker"],"content":"docker-compose 安装 docker-compose 是用 python 写的，所以我们安装 使用 pip install docker-compose 升级 ssl_match_hostname 大于等于3.5.1 版本，才能使用 docker-compose pip install backports.ssl_match_hostname --upgrade ","date":"2016-10-02","objectID":"/docker-compose/:2:0","tags":null,"title":"基于 docker-compose 的docker编排","uri":"/docker-compose/"},{"categories":["docker"],"content":"编写 yaml 文件 以下是 V2 版本的 docker-compose.yaml docker-compose v2 版本 标签 官方文档： https://docs.docker.com/compose/compose-file/#version-2 version: '2' # 使用compose v2 版本 networks: # 定义一个网络 network-cn: # 定义网络名称 external: name: ovrcn # 使用自行创建的网络，如果不设置这个，会自动创建一个别的网络，与你原来网络不在一个网络中。 services: # 服务组名称 nginx-1: image: nginx # 镜像 networks: network-cn: # 自定义的网络名 aliases: - nginx # overlay 网络的网络别名 hostname: nginx # 容器里面的 hostname container_name: nginx-1 # 创建容器时的容器名称 ports: # 映射端口 - \"80:80\" - \"443:443\" environment: # --env 配置 - constraint:node==swarm-node-28 volumes: # 挂载目录 - /opt/data/nginx/logs:/opt/local/nginx/logs 编写完 docker-compose.yaml 使用 docker-compose up -d 使用 docker-compose ps 命令可以查看 容器的启动情况。 docker-compose ps Name Command State Ports ----------------------------------------------------------------------------------- nginx-1 /opt/local/nginx/sbin/ngin ... Up 172.16.1.28:443-\u003e443/tcp, 172.16.1.28:80-\u003e80/tcp ","date":"2016-10-02","objectID":"/docker-compose/:3:0","tags":null,"title":"基于 docker-compose 的docker编排","uri":"/docker-compose/"},{"categories":["docker"],"content":"docker-compose 约束标签 # 约束两个nginx相关的容器不在同一个节点中 affinity:container!=nginx-* # 约束两个服务，如master 与 slave 不会调度到同一个节点中 affinity:service!=slave # 约束 两个容器调度到不同的 可用区域中 availability:az==2 ","date":"2016-10-02","objectID":"/docker-compose/:4:0","tags":null,"title":"基于 docker-compose 的docker编排","uri":"/docker-compose/"},{"categories":["docker"],"content":"docker swarm 集群","date":"2016-10-01","objectID":"/docker-swarm/","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"Docker Swarm 集群 ","date":"2016-10-01","objectID":"/docker-swarm/:0:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"环境说明 IP 10.6.17.11 管理节点 IP 10.6.17.12 节点A IP 10.6.17.13 节点B IP 10.6.17.14 节点C ","date":"2016-10-01","objectID":"/docker-swarm/:1:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"安装 zookeeper 安装过程略 zk://10.6.17.11:2181 ","date":"2016-10-01","objectID":"/docker-swarm/:2:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"修改docker启动参数 修改 -H 添加 tcp 端口 sed -i 's/-H fd:\\/\\//-H tcp:\\/\\/10.6.17.11:2375 --cluster-store=zk:\\/\\/10.6.17.11:2181\\/store --cluster-advertise=10.6.17.11:2375/g' /lib/systemd/system/docker.service ","date":"2016-10-01","objectID":"/docker-swarm/:3:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"重启docker systemctl daemon-reload systemctl restart docker ","date":"2016-10-01","objectID":"/docker-swarm/:4:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"创建 master 节点 docker -H tcp://10.6.17.11:2375 run --name master --restart=always -d -p 8888:2375 swarm manage zk://10.6.17.11:2181/swarm ","date":"2016-10-01","objectID":"/docker-swarm/:5:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"添加node节点到集群 A: docker -H tcp://10.6.17.12:2375 run --name node_1 --restart=always -d swarm join --addr=10.6.17.12:2375 zk://10.6.17.11/swarm B: docker -H tcp://10.6.17.13:2375 run --name node_2 --restart=always -d swarm join --addr=10.6.17.13:2375 zk://10.6.17.11/swarm C: docker -H tcp://10.6.17.14:2375 run --name node_3 --restart=always -d swarm join --addr=10.6.17.14:2375 zk://10.6.17.11/swarm ","date":"2016-10-01","objectID":"/docker-swarm/:6:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"列出节点信息 docker -H tcp://10.6.17.11:8888 run --rm swarm list zk://10.6.17.11:2181/swarm 10.6.17.12:2375 10.6.17.13:2375 10.6.17.14:2375 ","date":"2016-10-01","objectID":"/docker-swarm/:7:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["docker"],"content":"管理集群 [root@localhost docker]# docker -H tcp://10.6.17.11:8888 ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 3629e76de67c swarm \"/swarm join --addr=1\" 4 minutes ago Up 4 minutes 2375/tcp localhost.localdomain/node_1 325b71855b86 swarm \"/swarm join --addr=1\" 6 minutes ago Up About a minute 2375/tcp localhost.localdomain/node_3 b888bbbfe594 swarm \"/swarm join --addr=1\" 6 minutes ago Up 6 minutes 2375/tcp localhost.localdomain/node_2 FAQ # swarm 集群中，出现加入 network 的错误，于某个 node 中。 Cannot start container: subnet sandbox join failed for \"10.0.0.0/24\": error creating vxlan interface: file exists network sandbox join failed: error creating vxlan interface: file exists # 参考解决方案 umount /var/run/docker/netns/* rm -rf /var/run/docker/netns/* # 重启 docker systemctl restart docker #如果还是不行 重启 这台 node 服务器 ","date":"2016-10-01","objectID":"/docker-swarm/:8:0","tags":null,"title":"docker swarm 集群","uri":"/docker-swarm/"},{"categories":["mongodb"],"content":"mongodb sharding cluster","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"mongodb sharding cluster ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:0:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"环境说明 分别在3台机器运行一个mongod实例（称为mongod shard11，mongod shard12，mongod shard13）组织replica set1，作为cluster的shard1 分别在3台机器运行一个mongod实例（称为mongod shard21，mongod shard22，mongod shard23）组织replica set2，作为cluster的shard2 每台机器运行一个mongod实例，作为3个config server 每台机器运行一个mongs进程，用于客户端连接 Server1 10.3.0.100 Mongod shard11:27017 Mongod shard21:17017 Mongod config1:20000 Mongos1:20001 Server1 10.3.0.101 Mongod shard12:27017 Mongod shard22:17017 Mongod config1:20000 Mongos1:20001 Server3 10.3.0.102 Mongod shard13:27017 Mongod shard23:17017 Mongod config1:20000 Mongos1:20001 ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:1:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"安装 mongodb 创建用户 mongodb (如果用root启用mongodb,可略此步) useradd -u 1001 mongodb 下载 mongodb-linux-x86_64-2.2.0 tar zxvf mongodb-linux-x86_64-2.2.0.tgz mv mongodb-linux-x86_64-2.2.0 /opt/local/mongodb 创建 数据目录 日志目录 配置目录 (server 1) mkdir -p /opt/local/mongodb/data/shard mkdir -p /opt/local/mongodb/data/logs mkdir -p /opt/local/mongodb/data/config 创建 数据目录 日志目录 配置目录 (server 2) mkdir -p /opt/local/mongodb/data/shard12 mkdir -p /opt/local/mongodb/data/shard22 mkdir -p /opt/local/mongodb/data/logs mkdir -p /opt/local/mongodb/data/config 创建 数据目录 日志目录 配置目录 (server 3) mkdir -p /opt/local/mongodb/data/shard13 mkdir -p /opt/local/mongodb/data/shard23 mkdir -p /opt/local/mongodb/data/logs mkdir -p /opt/local/mongodb/data/config 修改 目录 所有者 chown -R mongodb:mongodb /opt/local/mongodb 创建配置文件 vi mongodb.conf port=27017 #端口号 fork=true #以守护进程的方式运行，创建服务器进程 logpath=/opt/local/mongodb/data/logs/shard.log #日志输出文件路径 logappend=true #日志输出方式 dbpath=/opt/local/mongodb/data/shard/ #数据库路径 shardsvr=true #设置是否分片 maxConns=10000 #数据库的最大连接数 replSet=shard1 #设置副本集名称 oplogSize=5000 #设置oplog的大小(MB) ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:2:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"启动 mongodb 27017 server 1 /opt/local/mongodb/bin/mongod --shardsvr --maxConns 10000 --replSet shard1 --port 27017 --dbpath /opt/local/mongodb/data/shard11/ --oplogSize 500 --logpath=/opt/local/mongodb/data/logs/shard11.log --logappend --fork server 2 /opt/local/mongodb/bin/mongod --shardsvr --maxConns 10000 --replSet shard1 --port 27017 --dbpath /opt/local/mongodb/data/shard12/ --oplogSize 500 --logpath=/opt/local/mongodb/data/logs/shard12.log --logappend --fork server 3 /opt/local/mongodb/bin/mongod --shardsvr --maxConns 10000 --replSet shard1 --port 27017 --dbpath /opt/local/mongodb/data/shard13/ --oplogSize 500 --logpath=/opt/local/mongodb/data/logs/shard13.log --logappend --fork ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:3:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"初始化replica set shard1 用mongo连接其中一个mongod，执行: /opt/local/mongodb/bin/mongo --host 10.3.0.100:27017 \u003e config= {_id: 'shard1', members: [ {_id:0,host:'10.3.0.100:27017'}, ... {_id:1,host:'10.3.0.101:27017'}, ... {_id:2,host:'10.3.0.102:27017'},] ... } {...} \u003e rs.initiate(config); { \"info\" : \"Config now saved locally. Should come online in about a minute.\", \"ok\" : 1 } ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:4:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"启动 mongodb 17017 server ` /opt/local/mongodb/bin/mongod --shardsvr --maxConns 10000 --replSet shard2 --port 17017 --dbpath /opt/local/mongodb/data/shard21/ --oplogSize 100 --logpath=/opt/local/mongodb/data/logs/shard21.log --logappend --fork server 2 /opt/local/mongodb/bin/mongod --shardsvr --maxConns 10000 --replSet shard2 --port 17017 --dbpath /opt/local/mongodb/data/shard22/ --oplogSize 100 --logpath=/opt/local/mongodb/data/logs/shard22.log --logappend --fork server 3 /opt/local/mongodb/bin/mongod --shardsvr --maxConns 10000 --replSet shard2 --port 17017 --dbpath /opt/local/mongodb/data/shard23/ --oplogSize 100 --logpath=/opt/local/mongodb/data/logs/shard23.log --logappend --fork ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:5:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"初始化replica set shard2 用mongo连接其中一个mongod，执行: /opt/local/mongodb/bin/mongo --host 10.3.0.100:17017 \u003e config= {_id: 'shard2', members: [ {_id:0,host:'10.3.0.100:17017'}, ... {_id:1,host:'10.3.0.101:17017'}, ... {_id:2,host:'10.3.0.102:17017'},] ... } {...} \u003e rs.initiate(config); { \"errmsg\" : \"couldn't initiate : set name does not match the set name host 10.3.0.101:17017 expects\", \"ok\" : 0 } \u003e ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:6:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"启动 config server server 1 /opt/local/mongodb/bin/mongod --configsvr --dbpath /opt/local/mongodb/data/config/ --port 20000 --logpath /opt/local/mongodb/data/logs/config.log --logappend --fork server 2 /opt/local/mongodb/bin/mongod --configsvr --dbpath /opt/local/mongodb/data/config/ --port 20000 --logpath /opt/local/mongodb/data/logs/config.log --logappend --fork server 3 /opt/local/mongodb/bin/mongod --configsvr --dbpath /opt/local/mongodb/data/config/ --port 20000 --logpath /opt/local/mongodb/data/logs/config.log --logappend --fork ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:7:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"配置 mongs 在server1,server2,server3 上分别执行： /opt/local/mongodb/bin/mongos --configdb 10.3.0.100:20000,10.3.0.101:20000,10.3.0.102:20000 --port 20001 --chunkSize 5 --logpath /opt/local/mongodb/data/logs/mongos.log --logappend --fork Configuring the Shard Cluster 连接到其中一个mongos进程，并切换到admin数据库做以下配置 连接到mongs，并切换到admin /opt/local/mongodb/bin/mongo 10.3.0.100:20001/admin MongoDB shell version: 2.2.0 connecting to: 10.3.0.100:20001/admin mongos\u003e db admin ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:8:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"加入shards 如里shard是单台服务器: db.runCommand( { addshard : “\u003cserverhostname\u003e[:\u003cport\u003e]” } ) 如果shard是replica sets: db.runCommand( { addshard : “replicaSetName/\u003cserverhostname\u003e[:port]” } ) shard 1 db.runCommand( { addshard : \"shard1/10.3.0.100:27017,10.3.0.101:27017,10.3.0.102:27017\", maxsize:204800}); { \"shardAdded\" : \"shard1\", \"ok\" : 1 } shard 2 mongos\u003e db.runCommand( { addshard : \"shard2/10.3.0.100:17017,10.3.0.101:17017,10.3.0.102:17017\", maxsize:204800}); { \"shardAdded\" : \"shard2\", \"ok\" : 1 } ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:9:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"验证 shards mongos\u003e db.runCommand( { listshards : 1 } ) { \"shards\" : [ { \"_id\" : \"shard1\", \"host\" : \"shard1/10.3.0.100:27017,10.3.0.101:27017,10.3.0.102:27017\" }, { \"_id\" : \"shard2\", \"host\" : \"shard2/10.3.0.100:17017,10.3.0.101:17017,10.3.0.102:17017\" } ], \"ok\" : 1 } ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:10:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"删除 shards db.runCommand( { removeshard : \"shard1/10.3.0.100:27017,10.3.0.101:27017\"} ); ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:11:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"激活分片 命令： \u003e db.runCommand( { enablesharding : “\u003cdbname\u003e” } ); 通过执行以上命令，可以让数据库跨shard，如果不执行这步，数据库只会存放在一个shard，一旦激活数据库分片，数据库中不同的collection将被存放在不同的shard上，但一个collection仍旧存放在同一个shard上，要使单个collection也分片， 还需单独对collection作些操作 ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:12:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"增添节点: 以一个新建的mongodb服务为例, 连接 mongo /opt/local/mongodb/bin/mongo --host 10.3.0.100:27017 MongoDB shell version: 2.2.0 connecting to: 10.3.0.100:27017/test shard1:PRIMARY\u003ers.add(“IP:port”); ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:13:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"删除节点： 必须在主节点上操作： PRIMARY\u003ers.remove(“127.0.0.1:27020”) 查看同步状态 rs.status() 从有读的权限： 在主控上执行: PRIMARY\u003e db.getMongo().setSlaveOk(); SECONDARY\u003e rs.slaveOk(); ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:14:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"增加验证 auth 群集如果加了 auth 验证，群集之前互相取不到 主 就无法验证…必须要增加 keyFile 验证才行… 先创建auth 验证 \u003e use admin switched to db admin \u003e db.addUser('sa','sa') { \"_id\" : ObjectId(\"4e2914a585178da4e03a16c3\"), \"user\" : \"sa\", \"readOnly\" : false, \"pwd\" : \"75692b1d11c072c6c79332e248c4f699\" } \u003e 然后在每个server里创建 key 文件…. echo \"1234567890111111111\" /opt/local/mongodb/data/config/key chown mongodb:mongodb key chmod 600 key 然后分别启动 mongod server 1 /opt/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --dbpath /opt/local/mongodb/data/shard11/ --oplogSize 100 --logpath=/opt/local/mongodb/data/logs/shard11.log --logappend --fork --keyFile /opt/local/mongodb/data/config/key server 2 /opt/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --dbpath /opt/local/mongodb/data/shard12/ --oplogSize 100 --logpath=/opt/local/mongodb/data/logs/shard12.log --logappend --fork --keyFile /opt/local/mongodb/data/config/key server 3 /opt/local/mongodb/bin/mongod --shardsvr --replSet shard1 --port 27017 --dbpath /opt/local/mongodb/data/shard13/ --oplogSize 100 --logpath=/opt/local/mongodb/data/logs/shard13.log --logappend --fork --keyFile /opt/local/mongodb/data/config/key 再启动 config server 1 /opt/local/mongodb/bin/mongod --configsvr --dbpath /opt/local/mongodb/data/config/ --port 20000 --logpath /opt/local/mongodb/data/logs/config.log --logappend --fork --keyFile /opt/local/mongodb/data/config/key server 2 /opt/local/mongodb/bin/mongod --configsvr --dbpath /opt/local/mongodb/data/config/ --port 20000 --logpath /opt/local/mongodb/data/logs/config.log --logappend --fork --keyFile /opt/local/mongodb/data/config/key server 3 /opt/local/mongodb/bin/mongod --configsvr --dbpath /opt/local/mongodb/data/config/ --port 20000 --logpath /opt/local/mongodb/data/logs/config.log --logappend --fork --keyFile /opt/local/mongodb/data/config/key 最后分别启动mongos /opt/local/mongodb/bin/mongos --configdb 10.3.0.100:20000,10.3.0.101:20000,10.3.0.102:20000 --port 20001 --chunkSize 5 --logpath /opt/local/mongodb/data/logs/mongos.log --logappend --fork --keyFile /opt/local/mongodb/data/config/key 最后验证 auth ./mongo MongoDB shell version: 2.2.0 connecting to: test \u003e show dbs Wed Sep 5 01:51:44 uncaught exception: listDatabases failed:{ \"errmsg\" : \"need to login\", \"ok\" : 0 } \u003e use admin switched to db admin \u003e db.auth('sa','sa') 1 shard1:SECONDARY\u003e db.serverStatus() 注意: 添加auth 以后…导入数据 等一系列操作…都必须验证用户… 如： /opt/local/mongodb/bin/mongorestore -u sa -p sa --drop /opt/1 其他都需要进行操作！否则！报错！！！ ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:15:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"强制切换主 cfg = rs.conf() cfg.members[0].priority = 0.5 cfg.members[1].priority = 0.5 cfg.members[2].priority = 1 rs.reconfig(cfg) ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:16:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"限制 mongodb 占用内存 ulimit -s 4096 \u0026\u0026 ulimit -m 31457280 \u0026\u0026 sudo -u mongodb numactl --interleave=all /opt/local/mongodb/bin/mongod --shardsvr --maxConns 10000 --replSet shard1 --port 27017 --dbpath /opt/local/mongodb/data/shard16/ --oplogSize 1000 --logpath=/opt/local/mongodb/data/logs/shard16.log --logappend --fork ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:17:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"数据库命令 显示当前数据库服务器上的数据库 show dbs 切换到指定数据库pagedb的上下文，可以在此上下文中管理pagedb数据库以及其中的集合等 use pagedb 显示数据库中所有的集合（collection） show collections 查看数据库服务器的状态。 db.serverStatus() 查询指定数据库统计信息 use fragment db.stats() 查询指定数据库包含的集合名称列表 db.getCollectionNames() 复制数据库 db.copyDatabase(\"111\",\"222\") 删除数据库 db.dropDatabase() 创建集合 db.createCollection(name, { size : ..., capped : ..., max : ... } ) 删除集合 db.mycoll.drop() 查询一条记录 使用findOne()函数，参数为查询条件，可选，系统会随机查询获取到满足条件的一条记录（如果存在查询结果数量大于等于1）示例如下所示： db.storeCollection.findOne({'version':'3.5'}) { \"_id\" : ObjectId(\"4ef970f23c1fc4613425accc\"), \"version\" : \"3.5\", \"segment\" : \"e3ol6\" } 创建索引 可以使用集合的ensureIndex(keypattern[,options])方法， 示例如下所示： \u003e use pagedb switched to db pagedb \u003e db.page.ensureIndex({'title':1, 'url':-1}) \u003e db.system.indexes.find() { \"name\" : \"_id_\", \"ns\" : \"pagedb.page\", \"key\" : { \"_id\" : 1 }, \"v\" : 0 } { \"name\" : \"_id_\", \"ns\" : \"pagedb.system.users\", \"key\" : { \"_id\" : 1 }, \"v\" : 0} { \"_id\" : ObjectId(\"4ef977633c1fc4613425accd\"), \"ns\" : \"pagedb.page\", \"key\" : {\"title\" : 1, \"url\" : -1 }, \"name\" : \"title_1_url_-1\", \"v\" : 0 } 上述，ensureIndex方法参数中，数字1表示升序，-1表示降序。 查询全部索引 db.system.indexes.find() 删除索引 删除索引给出了两个方法： 第一个通过指定索引名称，第二个删除指定集合的全部索引。 db.mycoll.dropIndex(name) db.mycoll.dropIndexes() 索引重建 可以通过集合的reIndex()方法进行索引的重建 示例如下所示： \u003e db.page.reIndex() { \"nIndexesWas\" : 2, \"msg\" : \"indexes dropped for collection\", \"ok\" : 1, \"nIndexes\" : 2, \"indexes\" : [ { \"name\" : \"_id_\", \"ns\" : \"pagedb.page\", \"key\" : { \"_id\" : 1 }, \"v\" : 0 }, { \"_id\" : ObjectId(\"4ef977633c1fc4613425accd\"), \"ns\" : \"pagedb.page\", \"key\" : { \"title\" : 1, \"url\" : -1 }, \"name\" : \"title_1_url_-1\", \"v\" : 0 } ], \"ok\" : 1 } 统计集合记录数 use fragment db.baseSe.count() 查询指定数据库的集合当前可用的存储空间 use fragment \u003e db.baseSe.storageSize() 142564096 查询指定数据库的集合分配的存储空间 \u003e db.baseSe.totalSize() 144096000 ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:18:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mongodb"],"content":"运维命令 1、备份全部数据库 mkdir testbak cd testbak mongodump 说明：默认备份目录及数据文件格式为./dump/databasename/collectionname.bson 备份指定数据库 mongodump -d pagedb 说明：备份数据库pagedb中的数据。 备份一个数据库中的某个集合 mongodump -d pagedb -c page 说明：备份数据库pagedb的page集合。 恢复全部数据库 cd testbak mongorestore --drop /opt/dump 说明：将备份的所有数据库恢复到数据库，–drop指定恢复数据之前删除原来数据库数据，否则会造成回复后的数据中数据重复。 恢复某个数据库的数据 cd testbak mongorestore -d pagedb --drop /opt/dump/pagedb 说明：将备份的pagedb的数据恢复到数据库。 恢复某个数据库的某个集合的数据 cd testbak mongorestore -d pagedb -c page --drop 说明：将备份的pagedb的的page集合的数据恢复到数据库。 从MongoDB导出数据 mongoexport -d pagedb -c page -q {} -f _id,title,url,spiderName,pubDate --csv \u003e pages.csv 说明：将pagedb数据库中page集合的数据导出到pages.csv文件，其中各选项含义： -f 指定cvs列名为_id,title,url,spiderName,pubDate -q 指定查询条件 注意： 如果上面的选项-q指定一个查询条件，需要使用单引号括起来，如下所示： mongoexport -d page -c Article -q '{\"spiderName\": \"mafengwoSpider\"}' -f _id,title,content,images,publishDate,spiderName,url --jsonArray \u003e mafengwoArticle.txt 向MongoDB导入数据 mongoimport -d pagedb -c page --type csv --headerline --drop \u003c csvORtsvFile.csv 说明：将文件csvORtsvFile.csv的数据导入到pagedb数据库的page集合中，使用cvs或tsv文件的列名作为集合的列名。需要注意的是，使用–headerline选项时，只支持csv和tsv文件。 –type支持的类型有三个：csv、tsv、json ","date":"2016-09-30","objectID":"/mongodb-sharding-cluster/:19:0","tags":null,"title":"mongodb sharding cluster","uri":"/mongodb-sharding-cluster/"},{"categories":["mysql"],"content":"mysql 命令","date":"2016-09-30","objectID":"/mysql-commands/","tags":null,"title":"mysql 命令","uri":"/mysql-commands/"},{"categories":["mysql"],"content":"mysql 命令 ##增加/删除用户。 格式：grant select on 数据库.* to 用户名@登录主机 identified by “密码” 增加一个用户test1密码为abc，让他可以在任何主机上登录，并对所有数据库有查询、插入、修改、删除的权限 grant select,insert,update,delete on *.* to test1@\"%\" Identified by \"abc\"; 创建所有权限的帐号: GRANT ALL PRIVILEGES ON *.* TO 'admin'@'localhost' IDENTIFIED BY '12345678'; 指定用户拥有创建表的权限 - (index 为创建索引权限) grant select,insert,update,delete,create,drop,index on mydb.* to test@localhost identified by \"test\"; 删除指定用户 Delete FROM user Where User='test' and Host='localhost'; ","date":"2016-09-30","objectID":"/mysql-commands/:0:0","tags":null,"title":"mysql 命令","uri":"/mysql-commands/"},{"categories":["mysql"],"content":"显示命令 mysql\u003e select version(); 查看MySQL的版本号 mysql\u003e select current_date(); 查看MySQL的当前日期 mysql\u003e select version(),current_date(); 同时查看MySQL的版本号和当前日期 mysql\u003e show processlist; 显示语句执行时间 mysqladmin -uroot -p status 查看当前连接数(Threads就是连接数.) 显示当前数据库服务器中的数据库列表： mysql\u003e SHOW DATABASES; 显示数据库中的数据表： mysql\u003e USE 库名； mysql\u003e SHOW TABLES; 显示数据表的结构： mysql\u003e DESCRIBE 表名; 建立数据库： mysql\u003e CREATE DATABASE 库名; 建立数据表： mysql\u003e USE 库名; mysql\u003e CREATE TABLE 表名 (字段名 VARCHAR(20), 字段名 CHAR(1)); 删除数据库： mysql\u003e DROP DATABASE 库名; 删除数据表： mysql\u003e DROP TABLE 表名； 将表中记录清空： mysql\u003e DELETE FROM 表名; 显示表中的记录： mysql\u003e SELECT * FROM 表名; 插入记录： mysql\u003e INSERT INTO 表名 VALUES (\"hyq\",\"M\"); 更新表中数据： mysql-\u003e UPDATE 表名 SET 字段名1='a',字段名2='b' WHERE 字段名3='c'; ","date":"2016-09-30","objectID":"/mysql-commands/:1:0","tags":null,"title":"mysql 命令","uri":"/mysql-commands/"},{"categories":["mysql"],"content":"运维命令 导入.sql文件命令： mysql\u003e USE 数据库名; mysql\u003e SOURCE /opt/mysql.sql; 命令行修改root密码： mysql\u003e UPDATE mysql.user SET password=PASSWORD('新密码') WHERE User='root'; mysql\u003e FLUSH PRIVILEGES; 导出整个数据库 mysqldump -u 用户名 -p 数据库名 \u003e 导出的文件名 导出一个表 mysqldump -u 用户名 -p 数据库名 表名\u003e 导出的文件名 导出一个数据库结构 mysqldump -u user_name -p -d --add-drop-table database_name \u003e outfile_name.sql 带语言参数导出 mysqldump -uroot -p --default-character-set=latin1 --set-charset=gbk --skip-opt database_name \u003e outfile_name.sql ","date":"2016-09-30","objectID":"/mysql-commands/:2:0","tags":null,"title":"mysql 命令","uri":"/mysql-commands/"},{"categories":["docker"],"content":"Docker overlay 网络","date":"2016-09-20","objectID":"/docker-overlay-network/","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"Docker overlay 网络 ","date":"2016-09-20","objectID":"/docker-overlay-network/:0:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"说明 Overlay网络是指在不改变现有网络基础设施的前提下，通过某种约定通信协议，把二层报文封装在IP报文之上的新的数据格式。 这样不但能够充分利用成熟的IP路由协议进程数据分发，而且在Overlay技术中采用扩展的隔离标识位数，能够突破VLAN的4000数量限制， 支持高达16M的用户，并在必要时可将广播流量转化为组播流量，避免广播数据泛滥。 因此，Overlay网络实际上是目前最主流的容器跨节点数据传输和路由方案。 Overlay网络的实现方式可以有许多种，其中IETF（国际互联网工程任务组）制定了三种Overlay的实现标准 1. 虚拟可扩展LAN（VXLAN） 2. 采用通用路由封装的网络虚拟化（NVGRE） 3. 无状态传输协议（SST） Docker内置的Overlay网络是采用IETF标准的VXLAN方式，并且是VXLAN中普遍认为最适合大规模的云计算虚拟化环境的SDN Controller模式。 Docker的Overlay网络功能与其Swarm集群是紧密整合的，因此为了使用Docker的内置跨节点通信功能，最简单的方式就是采纳Swarm作为集群的解决方案。 ","date":"2016-09-20","objectID":"/docker-overlay-network/:1:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"overlay 条件 在 docker 1.9 中，要使用 Swarm + overlay 网络架构，还需要以下几个条件： 所有Swarm节点的Linux系统内核版本不低于3.16 (在 docker 1.10 后面版本中，已经支持内核3.10，升级内核实在是一个麻烦事情) 需要一个额外的配置存储服务，例如Consul、Etcd或ZooKeeper 所有的节点都能够正常连接到配置存储服务的IP和端口 所有节点运行的Docker后台进程需要使用『–cluster-store』和『–cluster-advertise』参数指定所使用的配置存储服务地址 ","date":"2016-09-20","objectID":"/docker-overlay-network/:2:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"环境说明 服务器3台 如下： 10.6.17.12 10.6.17.13 10.6.17.14 docker version Client: Version: 1.10.0-rc1 API version: 1.22 Go version: go1.5.3 Git commit: 677c593 Built: Fri Jan 15 20:50:15 2016 OS/Arch: linux/amd64 ","date":"2016-09-20","objectID":"/docker-overlay-network/:3:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"修改主机名 10.6.17.12 = hostnamectl --static set-hostname swarm-master 10.6.17.13 = hostnamectl --static set-hostname swarm-node-1 10.6.17.14 = hostnamectl --static set-hostname swarm-node-2 上面的4个条件中，第一个条件在docker 1.10 RC 版本中已经默认就满足了。 下面我们来创建第二个条件中的 配置存储服务，配置存储服务按照大家的使用习惯，自己选择一个配置存储。 由于我们java 项目一直在使用 ZooKeeper ，所以这边选择 ZooKeeper 作为存储服务，为了方便测试，这边只配置 单机的 ZooKeeper 服务 ","date":"2016-09-20","objectID":"/docker-overlay-network/:4:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"配置 swarm 集群 [10.6.17.12]# sed -i 's/-H fd:\\/\\//-H tcp:\\/\\/10.6.17.12:2375 --cluster-store=zk:\\/\\/10.6.17.12:2181/store --cluster-advertise=10.6.17.12:2376/g' /lib/systemd/system/docker.service [10.6.17.13]# sed -i 's/-H fd:\\/\\//-H tcp:\\/\\/10.6.17.13:2375 --cluster-store=zk:\\/\\/10.6.17.12:2181/store --cluster-advertise=10.6.17.13:2376/g' /lib/systemd/system/docker.service [10.6.17.14]# sed -i 's/-H fd:\\/\\//-H tcp:\\/\\/10.6.17.14:2375 --cluster-store=zk:\\/\\/10.6.17.12:2181/store --cluster-advertise=10.6.17.14:2376/g' /lib/systemd/system/docker.service systemctl daemon-reload systemctl restart docker.service 首先我们选择 10.6.17.12 这台机器做为 master 节点 创建 swarm： [10.6.17.12]# docker -H tcp://10.6.17.12:2375 run --name master --restart=always -d -p 8888:2375 swarm manage zk://10.6.17.12:2181/swarm 在其他两台Docker业务容器运行的节点上运行Swarm Agent服务： [10.6.17.13]# docker -H tcp://10.6.17.13:2375 run --name node_1 --restart=always -d swarm join --addr=10.6.17.13:2375 zk://10.6.17.12:2181/swarm [10.6.17.14]# docker -H tcp://10.6.17.14:2375 run --name node_2 --restart=always -d swarm join --addr=10.6.17.14:2375 zk://10.6.17.12:2181/swarm 查看所有节点上的信息： [10.6.17.12]# docker -H tcp://10.6.17.12:8888 ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 5fc7753caa2c swarm \"/swarm join --addr=1\" Less than a second ago Up Less than a second 2375/tcp swarm-node-1/node_1 330b964ba732 swarm \"/swarm join --addr=1\" Less than a second ago Up Less than a second 2375/tcp swarm-node-2/node_2 至此 swarm 集群已经搭建完成了。 ","date":"2016-09-20","objectID":"/docker-overlay-network/:5:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"创建 overlay 网络 Swarm提供与Docker服务完全兼容的API，因此可以直接使用docker命令进行操作。 注意上面命令中创建Master服务时指定的外部端口号8888，它就是用来连接Swarm服务的地址。 现在我们就可以创建一个Overlay类型的网络了： [10.6.17.12]# docker -H tcp://10.6.17.12:8888 network create --driver=overlay ovr0 这个命令被发送给了Swarm服务，Swarm会在所有Agent节点上添加一个属性完全相同的Overlay类型网络。 在每个节点上面 使用 docker network ls 可以查看 到已经有一个 ovr0 的 overlay 网络 docker network ls 在Swarm的网络里面，每个网络的名字都会加上节点名称作为前缀， 如： swarm-node-1/node_1 swarm-node-2/node_2 但Overlay类型的网络是没有这个前缀的，这也说明了这类网络是被所有节点共有的。 下面我们在Swarm中创建两个连接到Overlay网络的容器，并用Swarm的过滤器限制这两个容器分别运行在不同的节点上。 ","date":"2016-09-20","objectID":"/docker-overlay-network/:6:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"创建基于overlay的容器 nginx dockerfile FROM centos MAINTAINER jicki@qq.com RUN yum -y update; yum clean all RUN yum -y install epel-release; yum clean all RUN yum -y install wget; yum clean all ADD ./nginx.sh /root/ RUN /bin/bash /root/nginx.sh RUN rm -rf /root/nginx.sh RUN rm -rf /opt/local/nginx/conf/nginx.conf ADD ./nginx.conf /opt/local/nginx/conf/ RUN mkdir -p /opt/local/nginx/conf/vhost ADD ./docker.conf /opt/local/nginx/conf/vhost RUN chown -R upload:upload /opt/htdocs/web EXPOSE 80 443 CMD [\"/opt/local/nginx/sbin/nginx\", \"-g\", \"daemon off;\"] [10.6.17.12]# docker -H tcp://10.6.17.12:8888 run --name nginx_web_1 --net ovr0 --env=\"constraint:node==swarm-node-1\" -d -v /opt/data/nginx/logs:/opt/local/nginx/logs nginx [10.6.17.12]# docker -H tcp://10.6.17.12:8888 run --name nginx_web_2 --net ovr0 --env=\"constraint:node==swarm-node-2\" -d -v /opt/data/nginx/logs:/opt/local/nginx/logs nginx ","date":"2016-09-20","objectID":"/docker-overlay-network/:7:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"测试网络 创建完两个容器以后，下面来来测试一下 ovr0 这个网络的连通性 [10.6.17.12]# docker -H tcp://10.6.17.12:8888 exec -it nginx_web_1 ping nginx_web_2 PING nginx_web_2 (10.0.0.3) 56(84) bytes of data. 64 bytes from nginx_web_2.ovr0 (10.0.0.3): icmp_seq=1 ttl=64 time=0.360 ms 64 bytes from nginx_web_2.ovr0 (10.0.0.3): icmp_seq=2 ttl=64 time=0.247 ms 64 bytes from nginx_web_2.ovr0 (10.0.0.3): icmp_seq=3 ttl=64 time=0.234 ms 64 bytes from nginx_web_2.ovr0 (10.0.0.3): icmp_seq=4 ttl=64 time=0.241 ms 64 bytes from nginx_web_2.ovr0 (10.0.0.3): icmp_seq=5 ttl=64 time=0.212 ms 如上所示 我们已经在Docker的Overlay网络上成功的进行了跨节点的数据通信。 测试两个 ssh 的服务，创建两个 容器，查看容器所属 IP 。 [10.6.17.12]# docker -H tcp://10.6.17.12:8888 run --name ssh-1 --net ovr0 --env=\"constraint:node==swarm-node-1\" -d -p 8001:22 ssh [10.6.17.12]# docker -H tcp://10.6.17.12:8888 run --name ssh-2 --net ovr0 --env=\"constraint:node==swarm-node-2\" -d -p 8001:22 ssh 创建容器 IP 为 DHCP 分配， 按照从下向上分配， 重启不会改变overlay 的IP 。 首先创建 ssh-1 分配IP为 10.0.0.4 创建 ssh-2 分配IP为 10.0.0.5 销毁 ssh-1 再次创建 分配IP 为 10.0.0.4 销毁 ssh-1 ssh-2 先创建 ssh-2 分配 IP 为 10.0.0.4 ","date":"2016-09-20","objectID":"/docker-overlay-network/:8:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":["docker"],"content":"alias 在 docker 1.10 后面的版本中 –net-alias=[] 的使用！！ 在docker run 的时候 可指定相同的 alias ，可以实现 故障切换的效果。。 具体命令如： [10.6.17.12]# docker -H tcp://10.6.17.12:8888 run --name nginx_web_1 --net ovr0 --net-alias=\"nginx\" --env=\"constraint:node==swarm-node-1\" -d -v /opt/data/nginx/logs:/opt/local/nginx/logs nginx [10.6.17.12]# docker -H tcp://10.6.17.12:8888 run --name nginx_web_2 --net ovr0 --net-alias=\"nginx\" --env=\"constraint:node==swarm-node-2\" -d -v /opt/data/nginx/logs:/opt/local/nginx/logs nginx 当我们进入 机器里面的时候 使用 dig 查看 nginx A记录 看到的是一个，但是 一个容器 挂掉以后 A记录会自动绑定到另外一台机器中。 在 docker 1.11 后面的版本中 –net-alias=[] 已经支持 负载均衡。 当我们使用 dig 查看 A记录 时可以看到多个 A记录 network disconnect docker network disconnect 与 docker network connect 命令的使用！ 使用这两个命令可达到 A B 测试 以及 快速 回滚 的效果。 docker network connect ----\u003e 加入 指定网络 docker network disconnect ----\u003e 退出 指定网络 具体命令使用： docker network disconnect ovr0 nginx_web_2 nginx_web_2 这个容器退出 ovr0 这个网络。 docker network connect ovr0 nginx_web_2 nginx_web_2 这个容器重新加入 ovr0 这个网络。 ","date":"2016-09-20","objectID":"/docker-overlay-network/:9:0","tags":null,"title":"Docker overlay 网络","uri":"/docker-overlay-network/"},{"categories":null,"content":"MooseFS 部署","date":"2016-09-10","objectID":"/mfs/","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"MooseFS 分布式文件系统 ","date":"2016-09-10","objectID":"/mfs/:0:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"环境说明 172.16.32.85 MFS-Controller(master) 172.16.32.86 MFS-Controller(metalogger) 172.16.32.57 Chunk Server1 172.16.32.58 Chunk Server2 172.16.32.59 Chunk Server3 ","date":"2016-09-10","objectID":"/mfs/:1:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"MFS 说明 MFS优势 1. 数据高可用性（数据可以存储在多个机器上的多个副本） 2. 在线动态扩展存储 3. 垃圾回收站 4. 文件快照(本文不研究) MFS问题 1. 单点故障：即使V1.6增加了Metalogger，但不能达到故障自动切换 2. 数据校验：为了提高存储效率，MFS的数据块不创建校验值(checksum)，降低了一定的数据安全性 3. 规模试用：适用于小规模的分布式文件系统，除非进行相关的二次开发 MFS 安装与配置 ","date":"2016-09-10","objectID":"/mfs/:2:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"初始化环境 # 同步系统时间 # 创建用户 groupadd -g 1010 mfs \u0026\u0026 useradd -u 1010 -g 1010 mfs groupadd -g 1001 upload \u0026\u0026 useradd -u 1001 -g 1001 upload # 安装依赖 yum install -y zlib-devel gcc gcc-c++ #创建目录 mkdir /opt/{software,local} cd /opt/software # 下载软件 cd /opt/software wget http://ppa.moosefs.com/src/moosefs-2.0.89-1.tar.gz ","date":"2016-09-10","objectID":"/mfs/:3:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"Master安装配置 tar -xzvf moosefs-2.0.89-1.tar.gz cd moosefs-2.0.89 ./configure --prefix=/opt/local/mfs-2.0.89 --with-default-user=mfs --with-default-group=mfs --disable-mfsmount --disable-mfschunkserver make \u0026\u0026 make install cd /opt/local/ ln -s mfs-2.0.89 mfs cd mfs/etc/mfs/ cp mfsmaster.cfg.dist mfsmaster.cfg cp mfsexports.cfg.dist mfsexports.cfg cd /opt/local/mfs/var/mfs \u0026\u0026 cp metadata.mfs.empty metadata.mfs chown -R mfs:mfs /opt/local/mfs* ","date":"2016-09-10","objectID":"/mfs/:4:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"编译参数说明 --disable-mfsmaster # 不创建master --disable-mfschunkserver # 不创建chunkserver --disable-mfsmount # 不创建mfs客户端(mfsmount和mfstools) --disable-mfsmetalogger # 不创建mfsmetalogger(master和metalogger可能会故障切换，安装时master和metalogger都必须安装master和logger功能) ","date":"2016-09-10","objectID":"/mfs/:5:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"修改系统参数 修改/etc/security/limits.conf * - nofile 655350 ","date":"2016-09-10","objectID":"/mfs/:6:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"配置 mfsmaster.cfg WORKING_USER = mfs # mfs服务的运行用户，单进程 WORKING_GROUP = mfs # mfs服务的运行组 SYSLOG_IDENT = mfsmaste # master server在syslog中的标识，日志记录在/var/log/messages LOCK_MEMORY = 0 # 是否执行mlockall()以避免mfsmaster 进程溢出（默认为0） NICE_LEVEL = -19 # 运行的优先级(由于服务器上的应用进程只有mfs，此处无需额外设定) DATA_PATH = /opt/local/mfs/var/mfs # 元数据存放目录：changelog，sessions和stats等； EXPORTS_FILENAME = /opt/local/mfs/etc/mfs/mfsexports.cfg # 被挂接目录及其权限控制文件 BACK_LOGS = 10 # metadata 的改变log 文件数目(默认是50); METADATA_SAVE_FREQ = 1 # 单位Hour，flush内存汇总metadata的频率 BACK_META_KEEP_PREVIOUS = 1 # 本地历史metadata文件的保存数量 MATOML_LISTEN_HOST = master-ip # metalogger连接的IP MATOML_LISTEN_PORT = 9419 # metalogger连接的端口 MATOCS_LISTEN_ HOST = master-ip # chunkserver连接的IP MATOCS_LISTEN_PORT = 9420 # chunkserver连接的端口 MATOCU_LISTEN_HOST = master-ip # Client连接的IP MATOCU_LISTEN_PORT = 9421 # Client连接的端口地址 REJECT_OLD_CLIENTS = 1 # 弹出低于1.6.0的客户端挂接 ","date":"2016-09-10","objectID":"/mfs/:7:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"配置mfsexports.cfg 192.168.0.0/24 / rw,alldirs,maproot=0 # /... = path in mfs structure 192.168.0.0/24 . rw # .标识MFSMETA 文件系统 [IP] * | A.B.C.D | A.B.C.D/XX | A.B.C.D - A.B.C.G [path] / | /dir | . [privil] ro/rw/readonly/readwrite alldirs = any subdirectory can be mounted mapall=1000:1000 All users are mapped as users with uid:gid = 1000:1000 maproot=nobody Local roots are mapped as 'nobody' users password=TEXT ","date":"2016-09-10","objectID":"/mfs/:8:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"MFS CGI 服务 CGI服务操作： /opt/local/mfs/sbin/mfscgiserv start|stop #说明：最好在master的hosts中加入mfsmaster的解析，因为http访问CGI时，会自动通过主机名去定位master ","date":"2016-09-10","objectID":"/mfs/:9:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"metalogger安装配置 yum install fuse-devel fuse modprobe fuse tar -xzvf moosefs-2.0.89-1.tar.gz cd moosefs-2.0.89 ./configure --prefix=/opt/local/mfs-2.0.89 --with-default-user=mfs --with-default-group=mfs --disable-mfschunkserver --disable-mfsmount \u0026\u0026 make \u0026\u0026 make install cd /opt/local/ ln -s mfs-2.0.89 mfs chown -R mfs:mfs /opt/local/mfs* ","date":"2016-09-10","objectID":"/mfs/:10:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"配置mfsmetalogger.cfg WORKING_USER = mfs WORKING_GROUP = mfs SYSLOG_IDENT = mfslogger LOCK_MEMORY = 0 NICE_LEVEL = -19 DATA_PATH = /opt/local/mfs/var/mfs BACK_LOGS = 10 # 与master保持一致 META_DOWNLOAD_FREQ = 1 # 单位Hour，多久同步一次master的metadata.mfs.back; 默认24H BACK_META_KEEP_PREVIOUS = 3 # 保留3份历史metadata_ml.mfs.back MASTER_HOST = 10.6.0.233 MASTER_PORT = 9419 MASTER_TIMEOUT = 10 MASTER_RECONNECTION_DELAY = 5 # 拷贝master的mfsmaster.cfg和mfsexports.cfg，以备metalogger切换为master ","date":"2016-09-10","objectID":"/mfs/:11:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"ChunkServer 安装配置 tar -xzvf moosefs-2.0.89-1.tar.gz cd moosefs-2.0.89 ./configure --prefix=/opt/local/mfs-2.0.89 --with-default-user=mfs --with-default-group=mfs --disable-mfsmaster --disable-mfsmount \u0026\u0026 make \u0026\u0026 make install cd /opt/local/ ln -s mfs-2.0.89 mfs chown -R mfs:mfs /opt/local/mfs* ","date":"2016-09-10","objectID":"/mfs/:12:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"配置 mfschunkserver.cfg WORKING_USER = mfs WORKING_GROUP = mfs SYSLOG_IDENT = mfschunkserver LOCK_MEMORY = 0 NICE_LEVEL = -19 DATA_PATH = /opt/local/mfs-2.0.89/var/mfs HDD_CONF_FILENAME = /opt/local/mfs-2.0.89/etc/mfs/mfshdd.cfg BIND_HOST = chunkserver-ip MASTER_HOST = master-ip MASTER_PORT = master-port MASTER_TIMEOUT = 10 CSSERV_LISTEN_HOST = chunkserver-ip # IP address to listen for client (mount) connections CSSERV_LISTEN_PORT = 9422 # port to listen for client (mount) connections ","date":"2016-09-10","objectID":"/mfs/:13:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"配置 mfshdd.cfg /opt/mfsdata # 建议划分单独的空间给 MooseFS 使用，chunkserver进程需要有权限操作该存储目录 ","date":"2016-09-10","objectID":"/mfs/:14:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"Client安装配置 yum install fuse-devel fuse -y modprobe fuse tar -xzvf moosefs-2.0.89-1.tar.gz cd moosefs-2.0.89 ./configure --prefix=/opt/local/mfs-2.0.89 --with-default-user=mfs --with-default-group=mfs --enable-mfsmount --disable-mfsmaster --disable-mfschunkserver \u0026\u0026 make \u0026\u0026 make install cd /opt/local/ ln -s mfs-2.0.89 mfs chown -R mfs:mfs /opt/local/mfs* ","date":"2016-09-10","objectID":"/mfs/:15:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"配置mfsmount.cfg mfsmaster=10.6.0.233 mfspassword=secret ","date":"2016-09-10","objectID":"/mfs/:16:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"fuse.conf设置 mount_max = NNN # Set the maximum number of FUSE mounts allowed to non-root users. The default is 1000. user_allow_other # Allow non-root users to mount ","date":"2016-09-10","objectID":"/mfs/:17:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"挂载 mfsmount -m /mfsdata -H 10.6.0.233 # 挂载metadata mfsmount /mfsdata -H 10.6.0.233 # 挂载mfs文件系统 mfsmount /mfsdata -H 10.6.0.233 -S imagesdata # 挂载mfs文件系统的指定目录，挂载imagesdata后，/mfsdata对应10.6.0.233:/imagesdata 1. fuse无需modprobe fuse，执行mfsmount时会自动加载 2. 暂时没有找到办法进行自动挂载(rc.local没有测试) 3. Client不管chunks断没断掉，都是可以挂载的而且有目录信息，目录信息存在Master的内存中。 ","date":"2016-09-10","objectID":"/mfs/:18:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":null,"content":"master 的 mfsexports.cfg 用户权限控制 # Admin (only 10.6.0.192 can operate the / and metadata ) 10.6.0.192 . rw 10.6.0.192 / rw,alldirs,mapall=upload:upload # users (Users operate the share sites) 10.6.0.0/24 /imagesdata rw,alldirs,mapall=upload:upload 1. mapall=uid:gid # mfs client上的所有用户，均以uid:gid 挂载mfs 2. maproot=uid:gid # mfs client上的root用户以uid:gid挂载mfs, 其他用户以当前系统用户和用户组挂载 3. master更改map用户后，客户端必须remount才会生效，因为挂载用户身份的确认发生在mfsmount:\"#mfsmount --\u003e mfsmaster accepted connection with parameters: read-write,restricted_ip,map_all ; root mapped to mfs:mfs ; users mapped to mfs:mfs\" ","date":"2016-09-10","objectID":"/mfs/:19:0","tags":null,"title":"MooseFS 部署","uri":"/mfs/"},{"categories":["mysql"],"content":"centos 7 源码安装 mysql 5.7","date":"2016-08-02","objectID":"/mysql-5.7/","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"centos 7 源码安装 mysql 5.7 ","date":"2016-08-02","objectID":"/mysql-5.7/:0:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"创建 mysql 用户以及相关目录 /usr/sbin/groupadd mysql /usr/sbin/useradd -g mysql mysql mkdir -p /opt/local/mysql/data mkdir -p /opt/local/mysql/binlog mkdir -p /opt/local/mysql/logs mkdir -p /opt/local/mysql/relaylog mkdir -p /var/lib/mysql mkdir -p /opt/local/mysql/etc ","date":"2016-08-02","objectID":"/mysql-5.7/:1:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"下载软件 下载 Mysql 5.7 最新版本的 tar.gz 文件 mysql 下载地址 ","date":"2016-08-02","objectID":"/mysql-5.7/:2:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"安装相关依赖 yum -y install cmake ncurses ncurses-devel bison bison-devel boost boost-devel ","date":"2016-08-02","objectID":"/mysql-5.7/:3:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"编译mysql tar zxvf mysql-5.7.11.tar.gz cd mysql-5.7.11 cmake -DCMAKE_INSTALL_PREFIX=\"/opt/local/mysql\" -DDEFAULT_CHARSET=utf8 -DMYSQL_DATADIR=\"/opt/local/mysql/data/\" -DCMAKE_INSTALL_PREFIX=\"/opt/local/mysql\" -DINSTALL_PLUGINDIR=plugin -DWITH_INNOBASE_STORAGE_ENGINE=1 -DDEFAULT_COLLATION=utf8_general_ci -DENABLE_DEBUG_SYNC=0 -DENABLED_LOCAL_INFILE=1 -DENABLED_PROFILING=1 -DWITH_ZLIB=system -DWITH_EXTRA_CHARSETS=none -DMYSQL_MAINTAINER_MODE=OFF -DEXTRA_CHARSETS=all -DWITH_PERFSCHEMA_STORAGE_ENGINE=1 -DWITH_MYISAM_STORAGE_ENGINE=1 -DDOWNLOAD_BOOST=1 -DWITH_BOOST=/usr/local/boost ( -DDOWNLOAD_BOOST=1 会自动下载boost 到 DWITH_BOOST= 指定目录 或者自行下载，存放于指定目录 ) make -j `cat /proc/cpuinfo | grep processor| wc -l` make install ","date":"2016-08-02","objectID":"/mysql-5.7/:4:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"创建相关目录，授权 chmod +w /opt/local/mysql chown -R mysql:mysql /opt/local/mysql chmod +w /var/lib/mysql chown -R mysql:mysql /var/lib/mysql cp /opt/local/mysql/support-files/mysql.server /etc/init.d/mysqld chmod 755 /etc/init.d/mysqld echo 'basedir=/opt/local/mysql/' \u003e\u003e /etc/init.d/mysqld echo 'datadir=/opt/local/mysql/data' \u003e\u003e/etc/init.d/mysqld ","date":"2016-08-02","objectID":"/mysql-5.7/:5:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"创建相关链接 ln -s /opt/local/mysql/lib/mysql /usr/lib/mysql ln -s /opt/local/mysql/include/mysql /usr/include/mysql ln -s /opt/local/mysql/bin/mysql /usr/bin/mysql ln -s /opt/local/mysql/bin/mysqldump /usr/bin/mysqldump ln -s /opt/local/mysql/bin/myisamchk /usr/bin/myisamchk ln -s /opt/local/mysql/bin/mysqld_safe /usr/bin/mysqld_safe ln -s /tmp/mysql.sock /var/lib/mysql/mysql.sock ","date":"2016-08-02","objectID":"/mysql-5.7/:6:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"初始化数据库 rm -rf /etc/my.cnf cp /opt/local/mysql/support-files/my-default.cnf /opt/local/mysql/etc/my.cnf cd /opt/local/mysql/bin/ ./mysqld --initialize --user=mysql --basedir=/opt/local/mysql --datadir=/opt/local/mysql/data 初始化完毕会生成一个root 的 随机密码，请务必先记录一下。如果忘记了，请查看 (mysqld.log) ","date":"2016-08-02","objectID":"/mysql-5.7/:7:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"启动数据库 service mysqld start ","date":"2016-08-02","objectID":"/mysql-5.7/:8:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["mysql"],"content":"数据库安全设置 /opt/local/mysql/bin/mysql_secure_installation -uroot -p ","date":"2016-08-02","objectID":"/mysql-5.7/:9:0","tags":null,"title":"centos 7 源码安装 mysql 5.7","uri":"/mysql-5.7/"},{"categories":["tomcat"],"content":"tomcat 利用 cronolog 进行日志切割","date":"2016-07-30","objectID":"/tomcat-cronolog-log/","tags":null,"title":"tomcat 利用 cronolog 进行日志切割","uri":"/tomcat-cronolog-log/"},{"categories":["tomcat"],"content":"tomcat 利用 cronolog 切割日志 ","date":"2016-07-30","objectID":"/tomcat-cronolog-log/:0:0","tags":null,"title":"tomcat 利用 cronolog 进行日志切割","uri":"/tomcat-cronolog-log/"},{"categories":["tomcat"],"content":"安装 软件 cronolog官网 http://cronolog.org/ wget http://cronolog.org/download/cronolog-1.6.2.tar.gz cronolog-1.6.2.tar.gz tar zxvf cronolog-1.6.2.tar.gz cd cronolog-1.6.2 ./configure \u0026\u0026 make \u0026\u0026 make install ","date":"2016-07-30","objectID":"/tomcat-cronolog-log/:1:0","tags":null,"title":"tomcat 利用 cronolog 进行日志切割","uri":"/tomcat-cronolog-log/"},{"categories":["tomcat"],"content":"配置 tomcat 编辑 catalina.sh 文件 查找 if [ -z \"$CATALINA_OUT\" ] ; then CATALINA_OUT=/opt/htdocs/logs/catalina.out fi 修改为 if [ -z \"$CATALINA_OUT\" ] ; then CATALINA_OUT=/opt/htdocs/logs/catalina.%Y-%m-%d.out fi 查找 touch \"$CATALINA_OUT\" 修改为 #touch \"$CATALINA_OUT\" 查找 \"$CATALINA_OUT\" 2\u003e\u00261 \"\u0026\" 有两处 org.apache.catalina.startup.Bootstrap \"$@\" start \\ \u003e\u003e \"$CATALINA_OUT\" 2\u003e\u00261 \"\u0026\" 都修改 org.apache.catalina.startup.Bootstrap \"$@\" start \\ | /usr/local/sbin/cronolog \"$CATALINA_OUT\" \u003e\u003e /dev/null \u0026 重启 tomcat 服务，查看日志文件 ","date":"2016-07-30","objectID":"/tomcat-cronolog-log/:2:0","tags":null,"title":"tomcat 利用 cronolog 进行日志切割","uri":"/tomcat-cronolog-log/"},{"categories":["mysql"],"content":"mycat 安装配置","date":"2016-07-22","objectID":"/mycat/","tags":null,"title":"mycat 安装配置","uri":"/mycat/"},{"categories":["mysql"],"content":"mycat Mycat 项目地址 ","date":"2016-07-22","objectID":"/mycat/:0:0","tags":null,"title":"mycat 安装配置","uri":"/mycat/"},{"categories":["mysql"],"content":"安装程序 Mycat 需要jdk 环境，首先安装 jdk 下载 mycat server tar zxvf Mycat-server-1.3.0.3-release-20150321221622-linux.tar mv Mycat-server-1.3.0.3-release-20150321221622-linux /opt/local/mycat 创建mycat用户，改变目录权限为mycat useradd mycat chown –R mycat:mycat /opt/local/mycat 修改 schema.xml vi /opt/local/conf/schema.xml 配置参数说明 Schema 中 主要配置 mycat 数据库 ，mysql 表 ，分片规则，分片类型 \u003cschema name=\"TESTDB\"checkSQLschema=\"false\" sqlMaxLimit=\"100\"\u003e \u003c!-- auto sharding by id(long) --\u003e \u003ctablename=\"travelrecord\" dataNode=\"dn1,dn2,dn3\"rule=\"auto-sharding-long\" /\u003e mycat 数据库 TESTDB mysql 表 travelrecord mysql节点dn1,dn2,dn3 分片规则 auto-sharding-long rule分片规则 具体在 conf/rule.xml 中定义 \u003cdataNodename=\"dn1\" dataHost=\"localhost1\" database=\"db1\"/\u003e \u003cdataNodename=\"dn2\" dataHost=\"localhost1\" database=\"db2\"/\u003e \u003cdataNodename=\"dn3\" dataHost=\"localhost1\" database=\"db3\"/\u003e \u003cdataHostname=\"localhost1\" maxCon=\"1000\" minCon=\"10\"balance=\"0\" writeType=\"0\"dbType=\"mysql\" dbDriver=\"native\"\u003e 以上为mysql节点 信息 dn1 ,dn2 , dn3 为分片的mysql 节点, 既分片会存放到 3个mysql 或者群集中 db1 db2 db3 为 mysql 数据库中 三个表 Mysql节点 连接，用户名，密码: \u003cwriteHost host=\"hostM1\" url=\"127.0.0.1:3306\"user=\"root\" password=\"123456 \"\u003e 修改 /opt/local/conf/server.xml \u003cpropertyname=\"serverPort\"\u003e8066\u003c/property\u003e \u003cpropertyname=\"managerPort\"\u003e9066\u003c/property\u003e \u003cuser name=\"test\"\u003e \u003cpropertyname=\"password\"\u003etest\u003c/property\u003e \u003cpropertyname=\"schemas\"\u003eTESTDB\u003c/property\u003e \u003c/user\u003e serverPortMycat登录端口默认为 8066 managerPort管理端口 默认为 9066 username 为登录mycat 用户 password 为登录 密码 schemas 为上面schema name= 中设定的 mycat 数据库名 Mysql 创建 数据库 CREATE database db1; CREATEdatabase db2; CREATE database db3; 启动 mycat /opt/local/mycat/bin/mycat start ","date":"2016-07-22","objectID":"/mycat/:1:0","tags":null,"title":"mycat 安装配置","uri":"/mycat/"},{"categories":null,"content":"kafka 集群 多broker模式","date":"2016-07-20","objectID":"/kafka-cluster/","tags":null,"title":"kafka 集群 多broker模式","uri":"/kafka-cluster/"},{"categories":null,"content":"kafka 集群 ","date":"2016-07-20","objectID":"/kafka-cluster/:0:0","tags":null,"title":"kafka 集群 多broker模式","uri":"/kafka-cluster/"},{"categories":null,"content":"环境说明 172.16.1.35 zookeeper kafka 172.16.1.36 zookeeper kafka 172.16.1.37 zookeeper kafka 开放端口 2181 2888 3888 9092 ","date":"2016-07-20","objectID":"/kafka-cluster/:1:0","tags":null,"title":"kafka 集群 多broker模式","uri":"/kafka-cluster/"},{"categories":null,"content":"kafka 安装 略 ","date":"2016-07-20","objectID":"/kafka-cluster/:2:0","tags":null,"title":"kafka 集群 多broker模式","uri":"/kafka-cluster/"},{"categories":null,"content":"修改配置 编辑 server.properties 文件 (以下为 172.16.1.35 的配置) 在默认的配置上，我只修改了4个地方。 broker.id = 三个主机172.16.1.35,172.16.1.36,172.16.1.37 分别对应id为1，2，3 broker.id=1 advertised.host.name=172.16.1.35 #配置为连接IP 否则会获取本地网卡IP log.dirs=/opt/local/kafka/logs zookeeper.connect=172.16.1.35:2181,172.16.1.36:2181,172.16.1.37:2181 非物理网卡，zk绑定IP 需要配置为 127.0.0.1 , 否则获取到的IP为 其他绑定IP 如：zookeeper.connect=127.0.0.1:2181,172.16.1.36:2181,172.16.1.37:2181 编辑 consumer.properties 文件 配置 zookeeper.connect= 信息 如果 kafka 与 zookeeper 在同一台机器上，也可以不需要配置。 编辑 producer.properties 文件 编辑 metadata.broker.list= 信息 配置为 多 broker 。 如 kafka_1:9092,kafka_2:9092,kafka_3:9092 编辑 zookeeper.properties 文件 initLimit=5 syncLimit=2 server.1=0.0.0.0:2888:3888 server.2=172.16.1.36:2888:3888 server.3=172.16.1.37:2888:3888 dataDir=/tmp/zookeeper 非物理网卡，绑定IP 需要配置为 0.0.0.0 , 否则获取到的IP为 其他绑定IP 例 broker.id=1 server.1=0.0.0.0:2888:3888 server.2=172.16.1.36:2888:3888 server.3=172.16.1.37:2888:3888 broker.id=2 server.1=172.16.1.35:2888:3888 server.2=0.0.0.0:2888:3888 server.3=172.16.1.37:2888:3888 参数说明 initLimit：LF初始通信时限 集群中的follower服务器(F)与leader服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量）。 syncLimit：LF同步通信时限 集群中的follower服务器与leader服务器之间请求和应答之间能容忍的最多心跳数（tickTime的数量）。 server.N=YYY:A:B 服务器名称与地址：集群信息（服务器编号，服务器地址，LF通信端口，选举端口） 分别将1，2，3写入三个主机的myid文件 echo “1” » /tmp/zookeeper/myid ","date":"2016-07-20","objectID":"/kafka-cluster/:3:0","tags":null,"title":"kafka 集群 多broker模式","uri":"/kafka-cluster/"},{"categories":null,"content":"启动程序 分别启动三个服务器中的 zookeeper 和 kafka server /opt/local/kafka/bin/zookeeper-server-start.sh -daemon /opt/local/kafka/config/zookeeper.properties /opt/local/kafka/bin/kafka-server-start.sh -daemon /opt/local/kafka/config/server.properties ","date":"2016-07-20","objectID":"/kafka-cluster/:4:0","tags":null,"title":"kafka 集群 多broker模式","uri":"/kafka-cluster/"},{"categories":null,"content":"kafka 命令 1、创建主题（Topic） bin/kafka-topics.sh --zookeeper zk_host:port/chroot --create --topic my_topic_name --partitions 20 --replication-factor 3 --config x=y 2、查看所有主题 bin/kafka-topics.sh --list --zookeeper localhost:2181 3、查看指定主题： bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic 主题名 4、修改主题： bin/kafka-topics.sh --zookeeper zk_host:port/chroot --alter --topic 主题名 --deleteConfig x 5 查看主题分区 ./kafka-topics.sh --describe --zookeeper localhost ","date":"2016-07-20","objectID":"/kafka-cluster/:5:0","tags":null,"title":"kafka 集群 多broker模式","uri":"/kafka-cluster/"},{"categories":["codis"],"content":"Codis-2.0 版本 编译安装","date":"2016-07-01","objectID":"/codis2.0/","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"codis 2.0 版本编译安装 ","date":"2016-07-01","objectID":"/codis2.0/:0:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"安装相关依赖 yum install -y git 安装 go 语言, 请务必安装1.7.x 版本 wget https://storage.googleapis.com/golang/go1.7.5.linux-amd64.tar.gz tar zxvf go1.7.5.linux-amd64.tar.gz mv go /opt/local/ 配置环境变量 vi /etc/profile 增加 export GOROOT=/opt/local/go export PATH=$PATH:$GOROOT/bin export GOPATH=/opt/local/codis 让配置生效 source /etc/profile 检查 go version ","date":"2016-07-01","objectID":"/codis2.0/:1:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"安装codis go get -u -d github.com/CodisLabs/codis cd /opt/local/codis/src/github.com/CodisLabs/codis make make -j -C extern/redis-2.8.21/ Hint: It’s a good idea to run ‘make test’ ;) 表示安装完成 完成安装会在 codis/src/github.com/CodisLabs/codis/bin 生成 四个文件 assets codis-config codis-proxy codis-server 复制文件 方便管理 mkdir -p /opt/local/codis/{logs,config,data}/ cd /opt/local/codis/src/github.com/CodisLabs/codis/bin cp -rf * /opt/local/codis/bin cd /opt/local/codis/src/github.com/CodisLabs/codis/ cp config.ini /opt/local/codis/config/ cd /opt/local/codis/src/github.com/CodisLabs/codis/extern/redis-2.8.21/src cp redis-cli /opt/local/codis/bin/redis-cli-2.8.21 ln -s /opt/local/codis/bin/redis-cli-2.8.21 /opt/local/codis/redis-cli ","date":"2016-07-01","objectID":"/codis2.0/:2:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"配置 codis 修改配置文件 cd /opt/local/codis/config vi config.ini 修改IP，配置WEB管理 默认 18087 端口 多proxy 配置VIP地址，但只能启动一个dashboard ,程序掉线再启动其他 dashboard_addr ( zk 配置 zookeeper 地址，单机配置一个，群集配置多个~以，号隔开 ） proxy_id=codis_proxy_1 # 配置为唯一 product= # zookeeper 节点信息 redis-server 配置文件 cd /opt/local/codis/src/github.com/CodisLabs/codis/extern/redis-test/config cp 6379.conf 6380.conf /opt/local/codis/config/ 修改里面的配置端口与配置 需要修改的配置如下： daemonize yes #后台模式运行 pidfile var/run/redis_6379.pid #pid 文件 port 6379 #运行端口 timeout 50 #请求超时时间,默认0 logfile \"/opt/local/codis/logs/codis_6379.log\" #日志文件 maxmemory 20gb #最大内存设置 save 900 1 #打开保存快照的条件( 第一个*表示多长时间 , 第三个*表示执行多少次写操作 ) save 300 10 save 60 10000 dbfilename 6379.rdb #数据快照保存的名字 dir /opt/local/codis/data #数据快照的保存目录 appendfilename \"6379_appendonly.aof\" #Redis更加高效的数据库备份及灾难恢复方式。 appendfsync everysec # ( always: always表示每次有写操作都进行同步. everysec: 表示对写操作进行累积，每秒同步一次 ) ","date":"2016-07-01","objectID":"/codis2.0/:3:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"dashboard 设置 创建 dashboard 启动脚本 vi /opt/local/codis/start_dashboard.sh #!/bin/sh nohup /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini -L /opt/local/codis/logs/dashboard.log dashboard --addr=:18087 --http-log=/opt/local/codis/logs/requests.log \u0026\u003e/dev/null \u0026 启动 dashboard /opt/local/codis/start_dashboard.sh 初始化 slot , 该命令会在zookeeper上创建slot相关信息 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini slot init ","date":"2016-07-01","objectID":"/codis2.0/:4:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"codis-server 设置 启动codis-server服务 /opt/local/codis/bin/codis-server /opt/local/codis/config/6379.conf /opt/local/codis/bin/codis-server /opt/local/codis/config/6380.conf 添加 Redis Server Group ( 每一个 Server Group 作为一个 Redis 服务器组存在, 只允许有一个 master, 可以有多个 slave, group id 仅支持大于等于1的整数 ) 第一组 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 1 172.16.32.78:6379 master /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 1 172.16.32.79:6380 slave 第二组 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 2 172.16.32.79:6379 master /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 2 172.16.32.78:6380 slave 第三组 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 3 172.16.32.80:6379 master /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 3 172.16.32.81:6380 slave 第四组 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 4 172.16.32.81:6379 master /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini server add 4 172.16.32.80:6380 slave 设置 server group 服务的 slot 范围 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini slot range-set 0 255 1 online /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini slot range-set 256 511 2 online /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini slot range-set 512 767 3 online /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini slot range-set 768 1023 4 online slot 数据迁移 Codis 支持动态的根据实例内存, 自动对slot进行迁移, 以均衡数据分布. /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini slot rebalance 执行此命令需要满足下列条件： 所有的codis-server都必须设置了maxmemory参数 所有的 slots 都应该处于 online 状态, 即没有迁移任务正在执行 所有 server group 都必须有 Master 另外 codis 还支持比例迁移。 如: 将slot id 为 [0-511] 的slot的数据, 迁移到 server group 2 上, –delay 参数表示每迁移一个 key 后 sleep 的毫秒数, 默认是 0, 用于限速. /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini slot migrate 0 511 2 --delay=10 迁移的过程对于上层业务来说是安全且透明的, 数据不会丢失, 上层不会中止服务. 注意：迁移的过程中打断是可以的, 但是如果中断了一个正在迁移某个slot的任务, 下次需要先迁移掉正处于迁移状态的 slot, 否则无法继续 (即迁移程序会检查同一时刻只能有一个 slot 处于迁移状态). ","date":"2016-07-01","objectID":"/codis2.0/:5:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"codis-proxy 设置 创建 start_proxy.sh，启动codis-proxy服务 vi /opt/local/codis/start_proxy.sh #!/bin/sh echo \"shut down codis_proxy_1..\" /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini proxy offline codis_proxy_1 echo \"done\" echo \"start new proxy...\" nohup /opt/local/codis/bin/codis-proxy --log-level info -c /opt/local/codis/config/config.ini -L /opt/local/codis/logs/proxy.log --cpu=4 --addr=0.0.0.0:19000 --http-addr=0.0.0.0:11000 \u0026\u003e/dev/null \u0026 echo \"done\" 启动 proxy /opt/local/codis/start_proxy.sh 设置 proxy 为 online 状态 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini proxy online codis_proxy_1 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini proxy online codis_proxy_2 /opt/local/codis/bin/codis-config -c /opt/local/codis/config/config.ini proxy online codis_proxy_3 ","date":"2016-07-01","objectID":"/codis2.0/:6:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"访问dashboard http://localhost:18087/admin/ ","date":"2016-07-01","objectID":"/codis2.0/:7:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["codis"],"content":"codis-server 主从切换配置 codis-ha 是一个通过 是一个通过 codis开放的 api 实现自动切换主从 配置如下： go get github.com/ngaut/codis-ha cd /opt/local/codis/src/github.com/ngaut/codis-ha go build 启动： cd /opt/local/codis/bin ./codis-ha --codis-config=\"127.0.0.1:18087\" --productName=\"mycodis\" https://github.com/wlibo666/codis-ha 这个 codis-ha 添加了很多新功能，包括邮件报警，等功能。 ","date":"2016-07-01","objectID":"/codis2.0/:8:0","tags":null,"title":"Codis-2.0 版本 编译安装","uri":"/codis2.0/"},{"categories":["nginx"],"content":"Nginx 负载均衡支持的5种方式的分配","date":"2016-06-30","objectID":"/nginx-upstream/","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"目前nginx负载均衡支持的5种方式的分配 ","date":"2016-06-30","objectID":"/nginx-upstream/:0:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"轮询 每个请求按时间顺序逐一分配到不同的后端服务器,如果后端服务器down掉,能自动剔除. upstream backserver { server 192.168.5.205; server 192.168.5.206; } ","date":"2016-06-30","objectID":"/nginx-upstream/:1:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"权重(weight) 指定轮询几率,weight和访问比率成正比,用于后端服务器性能不均的情况. upstream backserver { server 192.168.5.205 weight=10; server 192.168.5.206 weight=10; } ","date":"2016-06-30","objectID":"/nginx-upstream/:2:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"IP_hash 每个请求按访问ip的hash结果分配,这样每个访客固定访问一个后端服务器,可以解决session的问题. upstream backserver { ip_hash; server 192.168.5.205:88; server 192.168.5.206:80; } ","date":"2016-06-30","objectID":"/nginx-upstream/:3:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"fair (第三方插件) 按后端服务器的响应时间来分配请求,响应时间短的优先分配. upstream backserver { fair; server 192.168.5.205; server 192.168.5.206; } ","date":"2016-06-30","objectID":"/nginx-upstream/:4:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"url_hash(第三方) 按访问url的hash结果来分配请求,使每个url定向到同一个后端服务器,后端服务器为缓存时比较有效. upstream backserver { server 192.168.5.205:3128; server 192.168.5.206:3128; hash $request_uri; hash_method crc32; } ","date":"2016-06-30","objectID":"/nginx-upstream/:5:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"混合策略 Nginx 也支持多种策略混合使用 upstream backserver{ ip_hash; server 127.0.0.1:9090 down; //(down 表示单前的server暂时不参与负载) server 127.0.0.1:8080 weight=2; //(weight 默认为1.weight越大,负载的权重就越大) server 127.0.0.1:6060; server 127.0.0.1:7070 backup; //(其它所有的非backup机器down或者忙的时候,请求backup机器) max_fails; //允许请求失败的次数默认为1.当超过最大次数时,返回proxy_next_upstream模块定义的错误 } ","date":"2016-06-30","objectID":"/nginx-upstream/:6:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["nginx"],"content":"负载均衡健康检测 健康检测模块 需要打补丁，并且重新编译 解压 并 打上补丁… nginx version \u003e= 1.2.1 patch -p1 \u003c check_1.2.1+.patch nginx version \u003c 1.2.1 patch -p1 \u003c check.patch 重新编译 ./configure --user=www --group=www --prefix=/opt/local/nginx --with-http_stub_status_module --with-http_ssl_module --add-module=/opt/software/yaoweibin-nginx_upstream_check_module-dfee401/ make \u0026\u0026 make install 修改配置文件： upstream webserver { server 10.3.0.100:8000 weight=1; server 10.3.0.101 weight=2; check interval=1000 rise=2 fall=2 timeout=1000; } interval检测间隔时间，rsie请求2次正常的话为up,fail请求2次失败的话为down,timeout检查超时时间(毫秒) check_http_send “GET /.test.html HTTP/1.0”; //所发送的检测请求 server { listen 80; server_name localhost; location / { proxy_pass http://peace; //引用 } location /status { //定义一个类似stub_status的方式输出检测信息 check_status; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } } ","date":"2016-06-30","objectID":"/nginx-upstream/:7:0","tags":null,"title":"Nginx 负载均衡模式","uri":"/nginx-upstream/"},{"categories":["redis"],"content":"redis sentinel 集群监控","date":"2016-06-30","objectID":"/redis-sentinel/","tags":null,"title":"redis sentinel 集群监控","uri":"/redis-sentinel/"},{"categories":["redis"],"content":"redis sentinel 集群监控 ","date":"2016-06-30","objectID":"/redis-sentinel/:0:0","tags":null,"title":"redis sentinel 集群监控","uri":"/redis-sentinel/"},{"categories":["redis"],"content":"环境说明 ip 172.16.1.31 26379 redis sentinel ip 172.16.1.30 6379 主 1 ip 172.16.1.31 6380 从 1 ip 172.16.1.31 6379 主 2 ip 172.16.1.30 6380 从 2 redis 主 服务器配置，按照默认的配置文件既可。 redis 从 服务器配置，需要在配置文件配置 slaveof 的配置,配置为主服务器IP 与 端口 配置完成以后，启动主服务，再启用从服务 查看主redis信息 redis-cli -h 172.16.1.30 info Replication ","date":"2016-06-30","objectID":"/redis-sentinel/:1:0","tags":null,"title":"redis sentinel 集群监控","uri":"/redis-sentinel/"},{"categories":["redis"],"content":"配置 redis sentinel redis 源码安装包 里面会包含 sentinel.conf 复制一份 编辑 sentinel.conf #redis-0 sentinel announce-ip 172.16.1.31 port 26379 #master1 sentinel monitor master1 172.16.1.30 6379 1 sentinel down-after-milliseconds master1 5000 sentinel parallel-syncs master1 2 sentinel failover-timeout master1 900000 #master2 sentinel monitor master2 172.16.1.31 6379 1 sentinel down-after-milliseconds master2 5000 sentinel parallel-syncs master2 2 sentinel failover-timeout master2 900000 sentinel announce-ip 设置消息中使用指定的ip地址，而不是自动发现的本地地址。 sentinel monitor 设置redis 群集名字，IP ，端口 ， 1 表示 多少台 sentinel 决定故障，如果设置为2 表示需要2台sentinel 监控到故障才会进行切换 ","date":"2016-06-30","objectID":"/redis-sentinel/:2:0","tags":null,"title":"redis sentinel 集群监控","uri":"/redis-sentinel/"},{"categories":["redis"],"content":"启动 sentinel /usr/local/bin/redis-sentinel /opt/local/redis/conf/sentinel.conf --sentinel ","date":"2016-06-30","objectID":"/redis-sentinel/:3:0","tags":null,"title":"redis sentinel 集群监控","uri":"/redis-sentinel/"},{"categories":["redis"],"content":"Redis twemproxy 集群","date":"2016-06-30","objectID":"/redis-twemproxy/","tags":null,"title":"Redis twemproxy 集群","uri":"/redis-twemproxy/"},{"categories":["redis"],"content":"#Redis twemproxy 集群 ","date":"2016-06-30","objectID":"/redis-twemproxy/:0:0","tags":null,"title":"Redis twemproxy 集群","uri":"/redis-twemproxy/"},{"categories":["redis"],"content":"环境说明 4台 redis 服务器 172.16.1.37:6379 - 1 172.16.1.36:6379 - 2 172.16.1.35:6379 - 3 172.16.1.34:6379 - 4 ","date":"2016-06-30","objectID":"/redis-twemproxy/:1:0","tags":null,"title":"Redis twemproxy 集群","uri":"/redis-twemproxy/"},{"categories":["redis"],"content":"安装依赖 安装 autoconf centos 7 yum 安装既可， autoconf 版本必须 2.64以上版本 yum -y install autoconf ","date":"2016-06-30","objectID":"/redis-twemproxy/:2:0","tags":null,"title":"Redis twemproxy 集群","uri":"/redis-twemproxy/"},{"categories":["redis"],"content":"安装 twemproxy git clone https://github.com/twitter/twemproxy.git autoreconf -fvi #生成configure文件 ./configure --prefix=/opt/local/twemproxy/ --enable-debug=log make \u0026\u0026 make install mkdir -p /opt/local/twemproxy/{run,conf,logs} ln -s /opt/local/twemproxy/sbin/nutcracker /usr/bin/ ","date":"2016-06-30","objectID":"/redis-twemproxy/:3:0","tags":null,"title":"Redis twemproxy 集群","uri":"/redis-twemproxy/"},{"categories":["redis"],"content":"配置 twemproxy cd /opt/local/twemproxy/conf/ vi nutcracker.yml #编辑配置文件 alpha: listen: 172.16.1.37:6380 #监听端口 hash: fnv1a_64 #key值hash算法，默认fnv1a_64 distribution: ketama #分布算法 #ketama一致性hash算法；modula非常简单，就是根据key值的hash值取模；random随机分布 auto_eject_hosts: true #摘除后端故障节点 redis: true #是否是redis缓存，默认是false timeout: 400 #代理与后端超时时间，毫秒 server_retry_timeout: 200000 #摘除故障节点后重新连接的时间，毫秒 server_failure_limit: 1 #故障多少次摘除 servers: - 172.16.1.34:6379:1 - 172.16.1.35:6379:1 - 172.16.1.36:6379:1 - 172.16.1.37:6379:1 检查配置文件是否正确 nutcracker -t -c /opt/local/twemproxy/conf/nutcracker.yml ","date":"2016-06-30","objectID":"/redis-twemproxy/:4:0","tags":null,"title":"Redis twemproxy 集群","uri":"/redis-twemproxy/"},{"categories":["redis"],"content":"启动twemproxy nutcracker -d -c /opt/local/twemproxy/conf/nutcracker.yml -p /opt/local/twemproxy/run/redisproxy.pid -o /opt/local/twemproxy/logs/redisproxy.log ","date":"2016-06-30","objectID":"/redis-twemproxy/:5:0","tags":null,"title":"Redis twemproxy 集群","uri":"/redis-twemproxy/"},{"categories":["tomcat"],"content":"tomcat 优化配置 以及说明","date":"2016-06-30","objectID":"/tomcat-optimised/","tags":null,"title":"tomcat 优化配置 以及说明","uri":"/tomcat-optimised/"},{"categories":["tomcat"],"content":"tomcat 优化配置 以及说明 ","date":"2016-06-30","objectID":"/tomcat-optimised/:0:0","tags":null,"title":"tomcat 优化配置 以及说明","uri":"/tomcat-optimised/"},{"categories":["tomcat"],"content":"并发优化 JVM调优 以下为1G物理内存tomcat配置: 编辑 catalina.sh 文件 JAVA_OPTS=\"-server -Xms512M -Xmx512M -Xss256K\" 参数说明 -server: 一定要作为第一个参数，在多个CPU时性能佳 -Xms： 初始Heap大小，使用的最小内存,cpu性能高时此值应设的大一些 -Xmx： java heap最大值，使用的最大内存 -Xms 与 -Xmx 两个值是分配JVM的最小和最大内存，取决于硬件物理内存的大小，建议均设为物理内存的一半。 -Xss： 每个线程的Stack大小 以下为32G物理内存tomcat配置: JAVA_OPTS=\"-server -Xms20480m -Xmx20480m -Xss1024K\" ","date":"2016-06-30","objectID":"/tomcat-optimised/:1:0","tags":null,"title":"tomcat 优化配置 以及说明","uri":"/tomcat-optimised/"},{"categories":["tomcat"],"content":"开启 apr 模式 安装apr 以及 tomcat-native yum -y install apr apr-devel 进入tomcat/bin目录，比如 cd /opt/local/tomcat/bin/ tar xzfv tomcat-native.tar.gz cd tomcat-native-1.1.32-src/jni/native/ ./configure --with-apr=/usr/bin/apr-1-config make \u0026\u0026 make install 安装成功后还需要对tomcat设置环境变量，方法是在catalina.sh文件中增加1行： CATALINA_OPTS=\"-Djava.library.path=/usr/local/apr/lib\" 修改8080端对应的conf/server.xml 查找 protocol=\"org.apache.coyote.http11.Http11AprProtocol” \u003cConnector executor=\"tomcatThreadPool\" port=\"8080\" protocol=\"org.apache.coyote.http11.Http11AprProtocol\" connectionTimeout=\"20000\" enableLookups=\"false\" redirectPort=\"8443\" URIEncoding=\"UTF-8\" /\u003e PS:启动以后查看日志 显示如下表示开启 apr 模式 INFO: APR capabilities: IPv6 [true], sendfile [true], accept filters [false], random [true]. ","date":"2016-06-30","objectID":"/tomcat-optimised/:2:0","tags":null,"title":"tomcat 优化配置 以及说明","uri":"/tomcat-optimised/"},{"categories":["tomcat"],"content":"FAQ 日志乱码的解决办法 编辑 catalina.sh 文件 JAVA_OPTS=\"$JAVA_OPTS -Djavax.servlet.request.encoding=UTF-8 -Dfile.encoding=UTF-8 -Duser.timezone=GMT+8\" ","date":"2016-06-30","objectID":"/tomcat-optimised/:3:0","tags":null,"title":"tomcat 优化配置 以及说明","uri":"/tomcat-optimised/"},{"categories":null,"content":"Centos kafka 消息队列","date":"2016-06-02","objectID":"/kafka/","tags":null,"title":"Centos kafka 消息队列","uri":"/kafka/"},{"categories":null,"content":"kafka 消息队列 ","date":"2016-06-02","objectID":"/kafka/:0:0","tags":null,"title":"Centos kafka 消息队列","uri":"/kafka/"},{"categories":null,"content":"安装配置 查询下载最新版本 kafka http://kafka.apache.org/downloads.html wget http://mirror.bit.edu.cn/apache/kafka/0.8.2.0/kafka-0.8.2.0-src.tgz tar zxvf kafka-0.8.2.0-src.tgz mv kafka-0.8.2.0-src /opt/local/kafka cd /opt/local/kafka ./gradlew jar 提示: 错误: 找不到或无法加载主类 org.gradle.wrapper.GradleWrapperMain 需要先安装 gradle http://www.scala-lang.org/files/archive/scala-2.10.4.tgz tar zxvf scala-2.10.4.tgz mv scala-2.10.4 /usr/lib64/scala 设置环境变量 vi /etc/profile export SACLA_HOME=/usr/lib64/scala/ export PATH=$SACLA_HOME/bin:$PATH source /etc/profile 然后再执行 gradlew jar ./gradlew jarAll 如果 jarAll 会报错，java 版本不能为1.8 不然会报不兼容的错误，请使用1.7版本 ./gradlew jar –stacktrace –info –debug 创建日志目录 mkdir -p /opt/local/kafka/logs 编辑配置文件 vim config/server.properties 将 log.dirs=/tmp/kafka-logs 改为 log.dirs=/opt/local/kafka/logs 将 zookeeper.connect=localhost:2181 改为 zookeeper.connect=172.24.0.100:2181 启动程序 nohup /opt/local/kafka/bin/zookeeper-server-start.sh /opt/local/kafka/config/zookeeper.properties \u0026 nohup /opt/local/kafka/bin/kafka-server-start.sh /opt/local/kafka/config/server.properties \u0026 创建主题 /opt/local/kafka/bin/kafka-topics.sh --create --zookeeper 192.168.20.200:2181 --replication-factor 1 --partitions 1 --topic LJ200 查看现有主题 /opt/local/kafka/bin/kafka-topics.sh --list --zookeeper 192.168.20.200:2181 ","date":"2016-06-02","objectID":"/kafka/:1:0","tags":null,"title":"Centos kafka 消息队列","uri":"/kafka/"},{"categories":["rsync"],"content":"Centos rsync 文件同步","date":"2016-05-30","objectID":"/rsync/","tags":null,"title":"Centos rsync 文件同步","uri":"/rsync/"},{"categories":["rsync"],"content":"Centos rsync 文件同步 服务器端配置 ","date":"2016-05-30","objectID":"/rsync/:0:0","tags":null,"title":"Centos rsync 文件同步","uri":"/rsync/"},{"categories":["rsync"],"content":"安装依赖 yum -y install xinetd CentOS默认已经安装了rsync 服务.. 输入 rsync 命令可查看是否安装. ","date":"2016-05-30","objectID":"/rsync/:1:0","tags":null,"title":"Centos rsync 文件同步","uri":"/rsync/"},{"categories":["rsync"],"content":"编辑配置文件 vi /etc/xinetd.d/rsync 修改如下代码 disable = yes 改成 disable = no 并 启动 xinetd service rsync { disable = yes socket_type = stream wait = no user = root server = /usr/bin/rsync server_args = –daemon log_on_failure += USERID } /etc/init.d/xinetd start 或 service xinetd start 防火墙打开…端口 默认端口是873 创建 rsync 配置文件 mkdir /opt/local/rsync vi /opt/local/rsync/rsyncd.conf （这个文件不存在自己创建） #Global Settings uid = root #以什么身份运行rsync gid = root use chroot = no #不使用chroot max connections = 20 #最大连接数 secrets file = /opt/local/rsync/rsyncd.secrets #密码文件位置，认证文件设置，设置用户名和密码 log file = /opt/local/rsync/log/rsyncd.log #指定rsync的日志文件，而不将日志发送给syslog pid file = /opt/local/rsync/rsyncd.pid #指定rsync的pid文件 lock file = /opt/local/rsync/rsync.lock #指定支持max connections参数的锁文件，默认值是/var/run/rsyncd.lock comment = hello world #motd file = /opt/local/rsync/rsyncd.motd #欢迎信息文件名称和存放位置（此文件没有，可以自行添加） [backup] # 这里是认证的模块名，在client端需要指定 path = /data/resource # 需要做镜像的目录 auth users = rsync # 授权帐号。认证的用户名，如果没有这行，则表明是匿名，多个用户用,分隔 read only = no # yes只读 值为NO意思为可读可写模式，数据恢复用NO hosts allow = * #允许访问的服务器IP hosts deny = * #黑名单 list = true # 允许列文件 #ignore errors # 可以忽略一些无关的IO错误 #exclude = cache/111/ cache/222/ #忽略的目录 创建密码认证文件. vi /opt/local/rsync/rsyncd.secrets rsync:111111 #用户名:密碼 给文件正确的权限 # chown root:root /opt/local/rsync/rsyncd.secrets # chmod 600 /opt/local/rsync/rsyncd.secrets #（必须是600） 启动rsync rsync --daemon --config=/opt/local/rsync/rsyncd.conf 客户端配置 CentOS默认已经安装了rsync 服务,如果没有请执行 yum -y install rsync 创建密码认证文件 vi /data/rsync/rsyncd.pas 加入密码 rsyncpass 注意，客户端的密码文件只需要密码，而不需要用户名！ 更改密码文件的权限 chmod 0600 /data/rsync/rsyncd.pas 执行异步同步操作 rsync -vzrtopgu --progress --delete --password-file=/data/rsync/rsyncd.pas rsync@10.6.0.2::backup /data/resourcebak/ ","date":"2016-05-30","objectID":"/rsync/:2:0","tags":null,"title":"Centos rsync 文件同步","uri":"/rsync/"},{"categories":["rsync"],"content":"rsync 参数说明 命令行中-vzrtopg v 是verbose， z 是压缩传输， r 是recursive， topg 都是保持文件原有属性如属主、时间的参数。 u 是只同步已经更新的文件，避免没有更新的文件被重复更新一次，不过要注意两者机器的时钟的同步。 –progress 是指显示出详细的进度情况， –delete 是指如果服务器端删除了这一文件，那么客户端也相应把文件删除，保持真正的一致。 后面的rsync@10.6.0.2::backup中，之后的backup是模块名， 也就是在/opt/local/rsync/rsyncd.conf中自定义的名称，rsync是指定模块中指定的可以同步的用户名。 最后的/data/resourcebak/是备份到本地的目录名。 在这里面，还可以用-e ssh的参数建立起加密的连接。 可以用–password-file=/password/path/file来指定密码文件，这样就可以在脚本中使用而无需交互式地输入验证密码了，这里需要注意的是这份密码文件权限属性要设得只有属主可读。 ","date":"2016-05-30","objectID":"/rsync/:3:0","tags":null,"title":"Centos rsync 文件同步","uri":"/rsync/"},{"categories":["linux"],"content":"linux FAQ 以及 命令","date":"2016-05-30","objectID":"/linux-faq-commonds/","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"Linux 篇： ","date":"2016-05-30","objectID":"/linux-faq-commonds/:0:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"CentOs 7 修改主机名 hostnamectl --static set-hostname \u003chost-name\u003e ","date":"2016-05-30","objectID":"/linux-faq-commonds/:1:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"删除0字节文件 find -type f -size 0c | xargs rm -f ","date":"2016-05-30","objectID":"/linux-faq-commonds/:2:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"授权某用户，用户组可执行 # jenkins 为更新权限, nginx 为 web 运行权限 usermod -a -G nginx jenkins # 授权 用户组执行权限 (jenkins 可以在nginx 权限中执行 操作权限) chmod -R ug+w directories # 创建公私钥 ssh-keygen -t rsa -b 4096 -C \"jicki@qq.com\" -N \"\" -f /home/jicki/id_rsa ","date":"2016-05-30","objectID":"/linux-faq-commonds/:3:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"查看系统启动时间 date -d \"$(awk -F. '{print $1}' /proc/uptime) second ago\" +\"%Y-%m-%d %H:%M:%S\" ","date":"2016-05-30","objectID":"/linux-faq-commonds/:4:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"查看系统运行时间 cat /proc/uptime| awk -F. '{run_days=$1 / 86400;run_hour=($1 % 86400)/3600;run_minute=($1 % 3600)/60;run_second=$1 % 60; printf(\"系统已运行：%d天%d时%d分%d秒\",run_days,run_hour,run_minute,run_second)}' ","date":"2016-05-30","objectID":"/linux-faq-commonds/:5:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"截取 N - M 的日志 sed '/13:30:00/,/13:50:00/!d' catalina.out \u003e\u003e 22222.txt ","date":"2016-05-30","objectID":"/linux-faq-commonds/:6:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"Sed 替换变量带特殊字符 #/bin/bash IMAGE_NAME=\"jicki/hd-cloud-admin:0fjdskfjdsklfds0\" echo $IMAGE_NAME # 原替换语句为 # sed -i \"s/jicki\\/hd-cloud-admin.*$/${IMAGE_NAME}/g\" 1.txt # 因为变量包含特殊字符 / 号，所以以上语句会报错 # 修改为以下语句: sed -i 's%jicki/hd-cloud-admin.*$%'\"${IMAGE_NAME}\"'%g' 1.txt # 1.txt 内容如下: jicki hd-cloud-admin image: jicki/hd-cloud-admin:12342423 ","date":"2016-05-30","objectID":"/linux-faq-commonds/:6:1","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"添加主机路由 方法1： cat /etc/sysconfig/network-scripts/route-em1 # route-em1有严格的要求，em1必须与实际网卡名称对应，否则会失败 ADDRESS0=10.6.0.0 # 可以添加多条路由，必须从编号0开始 NETMASK0=255.255.0.0 GATEWAY0=172.16.1.1 方法2： cat /etc/sysconfig/network-scripts/route-em1 10.6.0.0/16 via 172.16.1.1 dev em1 ","date":"2016-05-30","objectID":"/linux-faq-commonds/:7:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"centos 7 内核顺序变更 查看内核顺序: awk -F\\' '$1==\"menuentry \" {print $2}' /etc/grub2.cfg # 选择内核0为默认 grub2-set-default 0 ","date":"2016-05-30","objectID":"/linux-faq-commonds/:8:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"查询缺少的.so 文件 如: ldd nginx libcrypto.so.6 =\u003e not found yum provides libcrypto.so.6 openssl098e-0.9.8e-29.el7.centos.i686 : A compatibility version of a general cryptography and TLS library Repo : base Matched from: Provides : libcrypto.so.6 yum -y install openssl098e ","date":"2016-05-30","objectID":"/linux-faq-commonds/:9:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"crontab FAQ 关于 Crontab 不能使用的问题..没安装等.. yum install vixie-cron yum install crontabs service crond start ","date":"2016-05-30","objectID":"/linux-faq-commonds/:10:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"修改时区 vi /etc/sysconfig/clock ZONE=\"Aisa/Shanghai\" UTC=true ARC=false # 更新时间不生效，还是原来的时区... cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime ","date":"2016-05-30","objectID":"/linux-faq-commonds/:11:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"创建大于2T的分区 Fdisk所使用的分区表为MBR，Master Boot Record，即主引导记录。 硬盘的0面、0道、1扇区就是主引导扇区，Fdisk将会写512个字节的记录在此，即MBR记录。 MBR分区表：（MBR含义：Master Boot Record，主引导记录） 所支持的最大卷：2T （T; terabytes,1TB=1024GB） 对分区的设限：最多4个主分区或3个主分区加一个扩展分区（扩展分区中支持无限制的逻辑驱动器） GPT分区表：（GPT含义：GUID分区表） 支持最大卷：18EB，（E：exabytes,1EB=2(10) PB=1024PB，1PB=2(10) TB=1024TB） 每个磁盘最多支持128个分区 # parted /dev/sdb (parted) mkpart primary 0% 10% (parted) mkpart primary 10% 100% (parted) p Model: DELL MD3000 (scsi) Disk /dev/sdb: 13.0TB Sector size (logical/physical): 512B/512B Partition Table: gpt Number Start End Size File system Name Flags 1 17.4kB 1300GB 1300GB primary 2 1300GB 13.0TB 11.7TB primary (parted)quit ","date":"2016-05-30","objectID":"/linux-faq-commonds/:12:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"shell 下 sudu FAQ Shell命令下使用sudo 提示 sudo: sorry, you must have a tty to run sudo 的错误 编辑 /etc/sudoers 文件 查找 “ Defaults requiretty 修改为 Defaults:username !requiretty Shell 命令下使用 sudo echo \u003e 这样的命令 依然提示 权限不够 这是因为重定向符号 “\u003e” 也是 bash 的命令。sudo 只是让 echo 命令具有了 root 权限， 但是没有让 “\u003e” 命令也具有root 权限，所以 bash 会认为这个命令没有写入信息的权限。 可以利用 “sh -c” 命令，它可以让 bash 将一个字串作为完整的命令来执行，这样就可以将 sudo 的影响范围扩展到整条命令。 sudo sh -c \"echo 654321 \u003e 1.txt\" ","date":"2016-05-30","objectID":"/linux-faq-commonds/:13:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"SVN FAQ 钩子日志输出 编辑 post-commit 文件 写入 svnlook changed /svn/yx \u003e /svn/yx/changed.log \u0026\u0026 /shell/commit.sh svnlook changed 命令 将 svn/yx 库操作记录到 changed.log 文件内… 然后用shell读取 changed.log 内的操作~执行脚本… svn url 变更 svn switch --relocate svn://123.123.123.123/rl/api svn://192.168.0.74/rl/api # svn switch --relocate 原url地址 新URL地址 ","date":"2016-05-30","objectID":"/linux-faq-commonds/:14:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"Git FAQ git pull / push 不输入密码 1. 首先执行一次 git pull 或者 git push 然后会在 ~/ 目录下生成一个 .git-credentials 文件 2. 在终端下执行 git config --global credential.helper store 3. 可以看到~/.gitconfig文件，会多了一项： [credential] helper = store 4. 最后测试一下 不用输入密码了 ","date":"2016-05-30","objectID":"/linux-faq-commonds/:15:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"Mysql FAQ 删除mysql 的binlog 一：查看备份的日志。 mysql\u003e show binary logs; +------------------+------------+ | Log_name | File_size | +------------------+------------+ | mysql-bin.000001 | 392914665 | | mysql-bin.000002 | 2765 | | mysql-bin.000003 | 1073742259 | | mysql-bin.000004 | 1073741949 | +------------------+------------+ 11 rows in set (0.11 sec) 删除指定binglog , 如下语句，指删除3 之前的所有binlog,而非 一个binlog mysql\u003e purge binary logs to 'mysql-bin.00003'; mysql\u003e show binary logs; +------------------+------------+ | Log_name | File_size | +------------------+------------+ | mysql-bin.000004 | 1073741949 | +------------------+------------+ 11 rows in set (0.11 sec) ","date":"2016-05-30","objectID":"/linux-faq-commonds/:16:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"Mongodb FAQ 查看当前性能 mongodb/bin/mongostat -h xx.xx.xx.xx:27017 查看读写 mongodb/bin/mongotop -h xx.xx.xx.xx:27017 查看当前执行语句 db.currentOp() 杀掉进程(先执行 db.currentOp()获取进程号,类似ps -ef) db.killOP(2920488) 查看最近错误 db.getLastError()db.getLastError() ","date":"2016-05-30","objectID":"/linux-faq-commonds/:17:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"Python FAQ # 模块版本不兼容 packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.21.1) or chardet (2.2.1) doesn‘t match a supported version! # 依次执行如下命令 (一条一条执行) pip uninstall urllib3 pip uninstall requests pip uninstall chardet pip install requests ","date":"2016-05-30","objectID":"/linux-faq-commonds/:18:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"Docker FAQ # 红帽系统安装 docker-ce # 安装pigz yum -y install http://mirror.centos.org/centos/7/extras/x86_64/Packages/pigz-2.3.3-1.el7.centos.x86_64.rpm # 安装 container-selinux yum -y install http://mirror.centos.org/centos/7/extras/x86_64/Packages/container-selinux-2.55-1.el7.noarch.rpm ","date":"2016-05-30","objectID":"/linux-faq-commonds/:19:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["linux"],"content":"AliYun FAQ # 删除阿里云盾 curl -sSL http://update.aegis.aliyun.com/download/quartz_uninstall.sh | sudo bash rm -rf /usr/local/aegis rm -rf /usr/sbin/aliyun-service ","date":"2016-05-30","objectID":"/linux-faq-commonds/:20:0","tags":["linux"],"title":"linux FAQ 以及 命令","uri":"/linux-faq-commonds/"},{"categories":["mysql"],"content":"Mariadb galera 群集","date":"2016-05-30","objectID":"/mariadb-galera-cluster/","tags":null,"title":"Mariadb galera 群集","uri":"/mariadb-galera-cluster/"},{"categories":["mysql"],"content":"Mariadb galera 群集 ","date":"2016-05-30","objectID":"/mariadb-galera-cluster/:0:0","tags":null,"title":"Mariadb galera 群集","uri":"/mariadb-galera-cluster/"},{"categories":["mysql"],"content":"环境说明: CentOS 7 x64 * 3 IP : 192.168.0.100 IP : 192.168.0.101 IP : 192.168.0.102 ","date":"2016-05-30","objectID":"/mariadb-galera-cluster/:1:0","tags":null,"title":"Mariadb galera 群集","uri":"/mariadb-galera-cluster/"},{"categories":["mysql"],"content":"安装相关软件 配置mariadb YUM 源 这里 mariadb-galera 使用 源码安装，其他使用yum 安装 http://yum.mariadb.org/10.0.15/rhel7-amd64/rpms/ 下载软件 MariaDB-Galera-10.0.15 wget http://mirrors.neusoft.edu.cn/mariadb/mariadb-galera-10.0.15/source/mariadb-galera-10.0.15.tar.gz 创建yum 文件 vi /etc/yum.repos.d/mariadb.repo [mariadb] name = MariaDB baseurl = http://yum.mariadb.org/10.0.15/centos7-amd64/ enabled = 1 gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDB gpgcheck=1 安装依赖软件 yum makecache yum -y install galera MariaDB-client socat cmake lsof perl-DBD-MySQL perl-Digest-MD5 perl-Time-HiRes rsync libev zlib-devel percona-xtrabackup 同步软件，请使用2.2.x 版本 rpm -ivh percona-xtrabackup-2.2.8-5059.el7.x86_64.rpm 安装 mariadb /usr/sbin/groupadd mysql /usr/sbin/useradd -g mysql mysql mkdir -p /opt/local/mysql/data mkdir -p /opt/local/mysql/binlog mkdir -p /opt/local/mysql/logs mkdir -p /opt/local/mysql/relaylog mkdir -p /var/lib/mysql mkdir -p /opt/local/mysql/wsrep tar zxvf mariadb-galera-10.0.15.tar.gz cd mariadb-10.0.15 cmake -DCMAKE_INSTALL_PREFIX=\"/opt/local/mysql\" -DDEFAULT_CHARSET=utf8 -DMYSQL_DATADIR=\"/opt/local/mysql/data/\" -DCMAKE_INSTALL_PREFIX=\"/opt/local/mysql\" -DINSTALL_PLUGINDIR=plugin -DWITH_INNOBASE_STORAGE_ENGINE=1 -DDEFAULT_COLLATION=utf8_general_ci -DENABLE_DEBUG_SYNC=0 -DENABLED_LOCAL_INFILE=1 -DENABLED_PROFILING=1 -DWITH_ZLIB=system -DWITH_EXTRA_CHARSETS=none -DMYSQL_MAINTAINER_MODE=OFF -DEXTRA_CHARSETS=all -DWITH_FAST_MUTEXES=ON -DWITH_PERFSCHEMA_STORAGE_ENGINE=1 -DWITH_MYISAM_STORAGE_ENGINE=1 make -j `cat /proc/cpuinfo | grep processor| wc -l` make install 授权，并初始化数据库 chmod +w /opt/local/mysql chown -R mysql:mysql /opt/local/mysql chmod +w /var/lib/mysql chown -R mysql:mysql /var/lib/mysql ./scripts/mysql_install_db --defaults-file=/opt/local/mysql/my.cnf --basedir=/opt/local/mysql --datadir=/opt/local/mysql/data --user=mysql cp ./support-files/mysql.server /etc/init.d/mysqld chmod 755 /etc/init.d/mysqld echo 'basedir=/opt/local/mysql/' \u003e\u003e /etc/init.d/mysqld echo 'datadir=/opt/local/mysql/data' \u003e\u003e/etc/init.d/mysqld 启动服务 service mysqld start chkconfig mysqld on 创建链接 ln -s /opt/local/mysql/lib/mysql /usr/lib/mysql ln -s /opt/local/mysql/include/mysql /usr/include/mysql ln -s /opt/local/mysql/bin/mysql /usr/bin/mysql ln -s /opt/local/mysql/bin/mysqldump /usr/bin/mysqldump ln -s /opt/local/mysql/bin/myisamchk /usr/bin/myisamchk ln -s /opt/local/mysql/bin/mysqld_safe /usr/bin/mysqld_safe ln -s /tmp/mysql.sock /var/lib/mysql/mysql.sock 设置密码，并加固数据库安全 /opt/local/mysql/bin/mysqladmin -u root password 'rldb123' /opt/local/mysql/bin/mysql_secure_installation 授权用于集群同步的用户和密码 /opt/local/mysql/bin/mysql -uroot -p GRANT ALL PRIVILEGES ON *.* TO 'sst'@'localhost' IDENTIFIED BY 'rldb123'; GRANT ALL PRIVILEGES ON *.* TO 'sst'@'同步ip' IDENTIFIED BY 'rldb123'; FLUSH PRIVILEGES; 配置wsrep.cnf文件 cp /opt/local/mysql/support-files/wsrep.cnf /opt/local/mysql/wsrep vi /opt/local/mysql/wsrep/wsrep.cnf 修改如下 wsrep_provider=/usr/lib64/galera/libgalera_smm.so wsrep_cluster_address=\"gcomm://\" wsrep_sst_auth=sst:123456 wsrep_sst_method=xtrabackup wsrep_node_address=192.168.0.100 注意: \"gcomm://\" 是特殊的地址,仅仅是Galera cluster初始化启动时候使用。 如果集群启动以后，我们关闭了第一个节点，那么再次启动的时候必须先修改，\"gcomm://\"为其他节点的集群地址 例如 wsrep_cluster_address=\"gcomm://192.168.0.101:4567\" 修改 mairadb 配置文件 vi /opt/local/mysql/my.cnf 在[client] 下面 增加 !includedir /opt/local/mysql/wsrep 配置完成以后，重启mariadb service mysqld restart netstat -tulpn |grep -e 4567 -e 3306 登录mysql 查看 global 的 status /opt/local/mysql/bin/mysql -uroot -p show global status like '%state%'; 接下来就是 其他节点的添加！ 步骤大致相同 构造新节点的操作步骤如下: wsrep_cluster_address的配置不一样: wsrep_cluster_address=\"gcomm://Node-A-IP,Node-B-IP\" # 这里指向是指上一层的集群地址 重起MariaDB service mysqld restart 所有节点配置完成以后,更改节点1的 wsrep_cluster_address=\"gcomm://” 为其他IP 加入 仲裁人 (可选) “仲裁人\"节点上没有数据,它在集群中的作用就是在集群发生分裂时进行仲裁，集群中可以有多个\"仲裁人\"节点。 “仲裁人\"节点加入集群的方法如下: garbd -a gcomm://192.168.0.100:4567 -g my_wsrep_cluster -d 注释：参数说明: -d：以daemon模式运行 -a：集群地址 -g： 集群名称 检查集群状况： SHOW VARIABLES LIKE 'wsrep_cluster_address'; show status like 'wsrep%'; ","date":"2016-05-30","objectID":"/mariadb-galera-cluster/:2:0","tags":null,"title":"Mariadb galera 群集","uri":"/mariadb-galera-cluster/"},{"categories":["mysql"],"content":"FAQ 配置相同，出现无法启动同步的时候，通常都是因为权限问题。 所有节点都执行授权. chmod +w /opt/local/mysql chown -R mysql:mysql /opt/local/mysql 然后再启动同步.. 启动完成以后，再次执行授权命令.. 通常是因为 data 目录下面的 ib_logfile0 ib_logfile1 ib_logfile2 为root 权限 所以不能同步，导致的。 ","date":"2016-05-30","objectID":"/mariadb-galera-cluster/:3:0","tags":null,"title":"Mariadb galera 群集","uri":"/mariadb-galera-cluster/"},{"categories":["mysql"],"content":"MariaDB GTID 复制同步","date":"2016-05-30","objectID":"/mariadb-gtid/","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"MariaDB GTID 复制同步 ","date":"2016-05-30","objectID":"/mariadb-gtid/:0:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"说明 GTID：Global Transaction ID,全局事务ID，在整个主从复制架构中任何两个事物ID是不能相同的。全局事务ID是Mster服务器生成一个128位的UUID+事物的ID号组成的，UUID标示主服务器的身份，此UUID在整个主从复制架构中是绝对唯一，而且即使更换主服务器后UUID也不会改变而是继承当前主服务器的UUID身份。 ","date":"2016-05-30","objectID":"/mariadb-gtid/:1:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"环境说明 master IP ： 10.6.0.96 slave IP : 10.6.0.138 配置本地hosts vim /etc/hosts 10.6.0.96 master.mysql 10.6.0.138 slave.mysql ","date":"2016-05-30","objectID":"/mariadb-gtid/:2:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"确认配置文件 binlog_format 二进制日志的格式，有row、statement和mixed几种类型； log-slave-updates、 report-port report-host： 用于启动GTID及满足附属的其它需求； master-info-repository relay-log-info-repository 启用此两项，可用于实现在崩溃时保证二进制及从服务器安全的功能； sync-master-info 启用之可确保无信息丢失； slave-parallel-workers 设定从服务器的SQL线程数；0表示关闭多线程复制功能； binlog-checksum master-verify-checksum slave-sql-verify-checksum 启用复制有关的所有校验功能； binlog-rows-query-log-events 启用之可用于在二进制日志记录事件相关的信息，可降低故障排除的复杂度； log-bin 启用二进制日志，这是保证复制功能的基本前提； server-id 同一个复制拓扑中的所有服务器的id号必须惟一. ","date":"2016-05-30","objectID":"/mariadb-gtid/:3:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"master 配置 //修改如下内容 log-bin=/opt/local/mysql/binlog/mysql-bin #二进制日志文件目录 server-id = 1 #从服务器不能跟此id重复 binlog_format=ROW #二进制日志文件格式 innodb_file_per_table=1 #innodb表空间独立 log-slave-updates=true #从master取得并执行的二进制日志写入自己的二进制日志文件中 //添加以下内容 binlog-do-db=mysql #指定只同步mysql库 master-info-repository=TABLE #用于实现在崩溃时保证二进制及从服务器安全的功能； relay-log-info-repository=TABLE #用于实现在崩溃时保证二进制及从服务器安全的功能； sync-master-info=1 #启用之可确保无信息丢失 slave-parallel-threads=2 #设定从服务器的SQL线程数；0表示关闭多线程复制功能 binlog-checksum=CRC32 #启用复制有关的所有校验功能 master-verify-checksum=1 #启用复制有关的所有校验功能 slave-sql-verify-checksum=1 #启用复制有关的所有校验功能 binlog-rows-query-log_events=1 #启用之可用于在二进制日志记录事件相关的信息，可降低故障排除的复杂度； report-host=master.mysql #master 主机名，必须能ping通 report-port=3306 #端口 重启 mysql service mysqld restart 创建同步帐号 mysql -uroot -p grant replication slave,replication client on *.* to \"rep\"@'10.6.0.138' identified by 'rep12345'; flush privileges; ","date":"2016-05-30","objectID":"/mariadb-gtid/:4:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"slave 配置 //修改如下内容 log-bin=/opt/local/mysql/binlog/mysql-bin #二进制日志文件目录 server-id = 10 #从服务器不能跟此id重复 binlog_format=ROW #二进制日志文件格式 innodb_file_per_table=1 #innodb表空间独立 log-slave-updates=true #从master取得并执行的二进制日志写入自己的二进制日志文件中 relay-log=/opt/local/mysql/relaylog/s74-relay-bin //添加以下内容 replicate-do-db=mysql #指定只同步mysql库 master-info-repository=TABLE #用于实现在崩溃时保证二进制及从服务器安全的功能； relay-log-info-repository=TABLE #用于实现在崩溃时保证二进制及从服务器安全的功能； sync-master-info=1 #启用之可确保无信息丢失 slave-parallel-threads=2 #设定从服务器的SQL线程数；0表示关闭多线程复制功能 binlog-checksum=CRC32 #启用复制有关的所有校验功能 master-verify-checksum=1 #启用复制有关的所有校验功能 slave-sql-verify-checksum=1 #启用复制有关的所有校验功能 binlog-rows-query-log_events=1 #启用之可用于在二进制日志记录事件相关的信息，可降低故障排除的复杂度； report-host=slave.mysql #slave 主机名，必须能ping通 report-port=3306 #端口 在slave服务器使用主mysql上创建的账号密码登陆 mysql -uroot -p change master to master_host='10.6.0.96',master_user='rep',master_password='rep12345',master_use_gtid=current_pos; start slave; 查看是否启用 gtid: show processlist; ","date":"2016-05-30","objectID":"/mariadb-gtid/:5:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"检查同步状态： show slave status\\G; ","date":"2016-05-30","objectID":"/mariadb-gtid/:6:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["mysql"],"content":"Position 复制 show master status; 记录 Position +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000008 | 1062 | | | +------------------+----------+--------------+------------------+ 在 slave 上面 执行 CHANGE MASTER TO MASTER_HOST='10.6.0.96',MASTER_USER='rep',MASTER_PASSWORD='rep12345',MASTER_LOG_FILE='mysql-bin.000008',MASTER_LOG_POS=1062; start slave; 查看状态 show slave status \\G; ","date":"2016-05-30","objectID":"/mariadb-gtid/:7:0","tags":null,"title":"MariaDB GTID 复制同步","uri":"/mariadb-gtid/"},{"categories":["nginx"],"content":"Nginx 做为前端负载均衡时，后端服务器获取的IP为 Nginx 的本机IP，让Nginx 把用户IP传递到后端里面去。","date":"2016-05-30","objectID":"/nginx-ip/","tags":null,"title":"Nginx 后端服务器获取前端用户真实IP","uri":"/nginx-ip/"},{"categories":["nginx"],"content":"Nginx 做为前端负载均衡时，后端服务器获取的IP为 Nginx 的本机IP，让Nginx 把用户IP传递到后端里面去。 ","date":"2016-05-30","objectID":"/nginx-ip/:0:0","tags":null,"title":"Nginx 后端服务器获取前端用户真实IP","uri":"/nginx-ip/"},{"categories":["nginx"],"content":"问题 Nginx 做为前端负载均衡时，后端服务器获取的IP为 Nginx 的本机IP. ","date":"2016-05-30","objectID":"/nginx-ip/:1:0","tags":null,"title":"Nginx 后端服务器获取前端用户真实IP","uri":"/nginx-ip/"},{"categories":["nginx"],"content":"解决办法 1. Nginx 开启 http_realip_module 模块 ./configure --with-http_realip_module make \u0026\u0026 make install 2. Nginx 前端 增加 proxy_set_header proxy_set_header X-Real-IP $remote_addr; 3. Nginx 后端 增加 real_ip_header real_ip_header X-Real-IP; 后端配置 必须重启 Nginx 才能生效， reload 不生效。 4. Tomcat 后端 需要修改 Service.xml 找到 pattern=\"%h %l %u %t \u0026quot;%r\u0026quot; %s %b\" /\u003e 修改为 pattern=\"%{X-FORWARDED-FOR}i %l %u %t %r %s %b %D %q %{User-Agent}i %T\" resolveHosts=\"false\"/\u003e ","date":"2016-05-30","objectID":"/nginx-ip/:2:0","tags":null,"title":"Nginx 后端服务器获取前端用户真实IP","uri":"/nginx-ip/"},{"categories":["sersync"],"content":"sersync 实时同步文件","date":"2016-05-30","objectID":"/sersync/","tags":null,"title":"sersync 实时同步文件","uri":"/sersync/"},{"categories":["sersync"],"content":"sersync 实时同步文件 sersync 主要用于服务器同步，web镜像等功能。sersync是使用c++编写，在结合rsync同步的时候，节省了运行时耗和网络资源。因此更快。sersync配置起来很简单。另外本项目相比较其他脚本开源项目，使用多线程进行同步，尤其在同步较大文件时，能够保证多个服务器实时保持同步状态，同步及时快速。 ","date":"2016-05-30","objectID":"/sersync/:0:0","tags":null,"title":"sersync 实时同步文件","uri":"/sersync/"},{"categories":["sersync"],"content":"测试环境 10.8.8.9 10.8.8.10 centos 5.8 x64 10.8.8.9 为 sersync服务器 ","date":"2016-05-30","objectID":"/sersync/:1:0","tags":null,"title":"sersync 实时同步文件","uri":"/sersync/"},{"categories":["sersync"],"content":"服务端配置 安装 sersync 服务 项目地址 下载最新64位版本 wget http://sersync.googlecode.com/files/sersync2.5.4_64bit_binary_stable_final.tar.gz mkdir -p /opt/sersync tar zxvf sersync2.5.4_64bit_binary_stable_final.tar.gz -C /opt/sersync 编辑配置文件 cd /opt/sersync/GNU-Linux-x86/ vi confxml.xml 修改以下部分 \u003csersync\u003e \u003clocalpath watch=\"/opt/htdocs/\"\u003e \u003cremote ip=\"10.8.8.10\" name=\"system\"/\u003e \u003c!--\u003cremote ip=\"192.168.8.39\" name=\"tongbu\"/\u003e--\u003e \u003c!--\u003cremote ip=\"192.168.8.40\" name=\"tongbu\"/\u003e--\u003e \u003c/localpath\u003e 修改好以后保存… 安装 rsync , sersync 是直接调用 rsync 服务 yum -y install rsync 启动 sersync /opt/sersync/GNU-Linux-x86/sersync2 -d -o /opt/sersync/GNU-Linux-x86/confxml.xml ","date":"2016-05-30","objectID":"/sersync/:2:0","tags":null,"title":"sersync 实时同步文件","uri":"/sersync/"},{"categories":["sersync"],"content":"客户端配置 安装服务 yum -y install rsync 编辑配置文件 mkdir /opt/local/rsync cd /opt/local/rsync/ vi /opt/local/rsync/rsyncd.conf #Global Settings uid = root #以什么身份运行rsync gid = root use chroot = no #不使用chroot max connections = 100 #最大连接数 log file = /opt/local/rsync/log/rsyncd.log #指定rsync的日志文件，而不将日志发送给syslog pid file = /opt/local/rsync/rsyncd.pid #指定rsync的pid文件 lock file = /opt/local/rsync/rsync.lock #指定支持max connections参数的锁文件，默认值是/var/run/rsyncd.lock comment = hello world ","date":"2016-05-30","objectID":"/sersync/:3:0","tags":null,"title":"sersync 实时同步文件","uri":"/sersync/"},{"categories":["sersync"],"content":"rsync 配置文件说明 [system] # 这里是认证的模块名，在client端需要指定 path = /opt/htdocs/ # 需要做镜像的目录 read only = no # yes只读 值为NO意思为可读可写模式，数据恢复用NO hosts allow = 10.8.8.9 10.8.8.10 #允许访问的服务器IP hosts deny = * #黑名单 list = true # 允许列文件 ignore errors = yes # 可以忽略一些无关的IO错误 启动rsync 服务 rsync --daemon --config=/opt/local/rsync/rsyncd.conf ","date":"2016-05-30","objectID":"/sersync/:4:0","tags":null,"title":"sersync 实时同步文件","uri":"/sersync/"},{"categories":["tomcat"],"content":"tomcat jdk 安装","date":"2016-05-30","objectID":"/tomcat-install/","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":" 简单记录一下…虽然安装很简单… Java JDK 首先下载配置安装 jdk http://www.oracle.com/technetwork/java/javase/downloads/jdk-6u25-download-346242.html # 下载bin 或者 rpm 的安装 （需要注册个帐号） # 32位 http://download.oracle.com/otn/java/jdk/8u25-b06/jdk-8u25-linux-i586-rpm.bin # 64位 http://download.oracle.com/otn/java/jdk/8u25-b06/jdk-8u25-linux-x64-rpm.bin chmod 777 jdk-8u25-linux-x64-rpm.bin ./jdk-8u25-linux-x64-rpm.bin ","date":"2016-05-30","objectID":"/tomcat-install/:0:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":"配置 jdk ENV #安装完成以后...安装目录为 /usr/java/jdk1.8.0_25/ #接下来配置一下JDK的环境.. vi /etc/profile #在最下面增加下面三行： export JAVA_HOME=/usr/java/jdk1.8.0_25/ export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH export CLASSPATH=.:/usr/java/jdk1.8.0_25/lib:/usr/java/jdk1.8.0_25/jre/lib:$CLASSPATH # 使设置生效: source /etc/profile ","date":"2016-05-30","objectID":"/tomcat-install/:1:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":"验证 java java -version ----------------------------------------------------------------------- java version \"1.8.0_25\" Java(TM) SE Runtime Environment (build 1.8.0_25-b06) Java HotSpot(TM) Client VM (build 20.12-b01, mixed mode, sharing) ----------------------------------------------------------------------- tomcat #下载最新稳定版本: http://tomcat.apache.org/index.html http://apache.dataguru.cn/tomcat/tomcat-7/v7.0.34/bin/apache-tomcat-7.0.34.tar.gz ","date":"2016-05-30","objectID":"/tomcat-install/:2:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":"安装 tar zxvf apache-tomcat-7.0.34.tar.gz mv apache-tomcat-7.0.34 /opt/local/tomcat ","date":"2016-05-30","objectID":"/tomcat-install/:3:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":"配置tomcat环境 vi /etc/profile #下面添加 export TOMCAT_HOME=/opt/local/tomcat/ #使设置生效 source /etc/profile cd /opt/local/tomcat/bin/ chmod 777 *.sh ","date":"2016-05-30","objectID":"/tomcat-install/:4:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":"启动 Tomcat /bin/bash catalina.sh start ","date":"2016-05-30","objectID":"/tomcat-install/:5:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":"测试 #访问 http://ip:8080 ","date":"2016-05-30","objectID":"/tomcat-install/:6:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["tomcat"],"content":"配置JMX vi catalina.sh # 在 CATALINA_OPTS 里面添加如下配置 CATALINA_OPTS=\"-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=false -Dcom.sun.management.jmxremote.authenticat e=false -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.rmi.port=9999 -Djava.rmi.server.hostname=172.16.1.47 -Dcom.sun.management.jmxremote.ssl=false\" # 特别注意 -Djava.rmi.server.hostname=172.16.1.47 # -Djava.rmi.server.hostname= 在 docker 下必须填写 宿主机IP 否则连接不上 配置完毕 重启 tomcat Windows 下安装 jdk 在 java_home/bin/ 下面 找到 jconsole.exe 选择远程连接， 输入 IP 与 端口 既可连接 ","date":"2016-05-30","objectID":"/tomcat-install/:7:0","tags":null,"title":"tomcat jdk 安装","uri":"/tomcat-install/"},{"categories":["kernel","centos"],"content":"centos update kernel","date":"2016-01-01","objectID":"/update-kernel/","tags":null,"title":"centos update kernel","uri":"/update-kernel/"},{"categories":["kernel","centos"],"content":"Update Kernel # 导入 Key rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 安装 Yum 源 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm # 更新 kernel yum --enablerepo=elrepo-kernel install -y kernel-lt kernel-lt-devel # 配置 内核优先 grub2-set-default 0 ","date":"2016-01-01","objectID":"/update-kernel/:0:0","tags":null,"title":"centos update kernel","uri":"/update-kernel/"},{"categories":["docker"],"content":"docker 基础知识","date":"2016-01-01","objectID":"/docker-popularization/","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"Docker 本文用于科普 docker 的一些基础知识以及概念 LXC = Linux Container ","date":"2016-01-01","objectID":"/docker-popularization/:0:0","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"一、什么是docker Docker 是一个开放源代码软件项目, 让应用程序部署在软件货柜下的工作可以自动化进行, 借此在 Linux 操作系统上, 提供一个额外的软件抽象层, 以及操作系统层虚拟化的自动管理机制。Docker 利用 Linux 核心中的资源分离机制，例如 cgroups,以及 Linux 核心名字空间(LXC Namespace)，来创建独立的容器。 ","date":"2016-01-01","objectID":"/docker-popularization/:1:0","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"二、资源限制与隔离 资源隔离 (Namespace): Linux 实现了 6 项资源隔离 Namespace 系统调用参数 隔离内容 支持内核版本 UTS CLONE_NEWUTS 主机名和域名 2.6.19 IPC CLONE_NEWIPC 信号量、消息队列和共享内存 2.6.19 PID CLONE_NEWPID 进程编号 2.6.24 NetWork CLONE_NEWNET 网络设备、网络栈、端口等 2.6.29 Mount CLONE_NEWNS 挂载点(文件系统) 2.4.19 User CLONE_NEWUSER 用户和用户组 3.8 资源限制 (cgroups): Linux 通过 cgroups 实现 cpu、内存、IO的限制以及网络的分配, (Linux 系统目录 /sys/fs/cgroup/ ) 限制内存以及虚拟内存并关闭oom-kill docker run --memory 500M --memory-swap 200M --oom-kill-disable 绑定CPU核心 docker run --cpu-cpus 1 , docker run --cpuset-cpus=\"0-2\" 限制CPU docker run -c 2048 --cpu-period=50000 --cpu-quota=50000 --cpu-quota cpu调度范围 --cpu-period cpu调度周期内使用时间 -c cpu 核心ms值 限制磁盘IO --device-read-bps 限制此设备上的读速度(bytes per second), 单位可以是kb、mb或者gb --device-read-iops 通过每秒读IO次数来限制指定设备的读速度 --device-write-bps 限制此设备上的写速度（bytes per second），单位可以是kb、mb或者gb。 --device-write-iops 通过每秒写IO次数来限制指定设备的写速度。 --blkio-weight 容器默认磁盘IO的加权值，有效值范围为10-100。 --blkio-weight-device 针对特定设备的IO加权控制。其格式为DEVICE_NAME:WEIGHT ","date":"2016-01-01","objectID":"/docker-popularization/:2:0","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"三、容器 VS 虚拟机 相比传统的虚拟化技术(vm), 使用Docker在CPU, Memory, Disk IO, Network IO 上的性能损耗都有同样水平甚至更优的表现。Container的快速创建、启动、销毁受到很多赞誉。 容器是直接使用宿主机的内核的, 而虚拟机(VM)是虚拟化一个完整操作系统。所以单数量上一台物理机能跑的容器是远大于虚拟机的数量。 特性 容器 虚拟机 启动 秒级 分钟级 使用容量 一般为MB 一般为GB 性能 接近宿主机性能 弱于宿主机性能 系统支持量 单机可支持上千容器 单机可支持几十个 ","date":"2016-01-01","objectID":"/docker-popularization/:3:0","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"四、docker 基础概念 ","date":"2016-01-01","objectID":"/docker-popularization/:4:0","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"镜像(image) Docker镜像是一个特殊的文件系统，提供容器运行时所需的程序、库、资源、配置等文件，另外还包含了一些为运行时准备的一些配置参数（如匿名卷、环境变量、用户等）。 镜像是一个静态的概念，不包含任何动态数据，其内容在构建之后也不会被改变。 镜像是分层结构 docker利用存储驱动 overlay2 的存储技术实现了镜像的分层结构, 分层结构中内容相同的层只占用一个空间, 意思就是只有不同的结构层才会占用存储空间, 相同的层都是直接引用。 构建一个镜像 docker build [option] [-t \u003cimage\u003e:\u003ctag\u003e] \u003cpath\u003e 命令构建镜像 dockerfile FROM: 指定基础镜像 ENV: key = value 格式 - 设置一个环境变量, 可以被dockerfile里后续的指令使用, 也在容器运行过程中保持。 ARG: key = value 格式 docker build --build-arg \u003cvarname\u003e=\u003cvalue\u003e 将该变量传递给构建器替换默认变量。 RUN: 命令操作层,每一个RUN 会新增一层,所以目的相同的RUN尽量合并在同一个RUN里减少大小。 COPY: 从本地复制 文件/文件夹 到镜像中。 ADD: 从本地复制 文件/文件夹 到镜像中, 支持压缩文件自解压和网络中下载文件。 WORKDIR: 声明工作目录,可以支持多个目录, 会自动创建目录。 USER: 指定构建时 USER 后续操作的 用户。 EXPOSE: 声明需要使用的端口, 仅用于声明与显示并不会实际映射端口到外部。docker ps PORTS 栏 会显示声明的端口。 VOLUME: 容器运行时应该尽量保持容器存储层不发生写操作, 对于数据库类需要保存动态数据的应用, 其数据库文件应该保存于卷(volume)中, 为了防止运行时用户忘记将动态文件所保存目录挂载为卷，在 Dockerfile 中，我们可以事先指定某些目录挂载为匿名卷，这样在运行时如果用户不指定挂载，其应用也可以正常运行，不会向容器存储层写入大量数据。 CMD: CMD 指令允许用户指定容器的默认执行的命令。此命令会在容器启动且 docker run 没有指定其他命令时运行。 CMD 可以用作 ENTRYPOINT 默认参数, 或者用作 Docker 的默认命令。 ENTRYPOINT: ENTRYPOINT 的 Exec 格式用于设置容器启动时要执行的命令及其参数, 同时可通过CMD命令或者命令行参数提供额外的参数。ENTRYPOINT 中的参数始终会被使用, 这是与CMD命令不同的一点。 ","date":"2016-01-01","objectID":"/docker-popularization/:4:1","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"容器(Container) Docker的容器就是 docker 通过镜像启动一个实例就叫 docker 容器。 docker run \u003cimages name\u003e 命令启动容器。 ","date":"2016-01-01","objectID":"/docker-popularization/:4:2","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"仓库(Repository) ","date":"2016-01-01","objectID":"/docker-popularization/:4:3","tags":null,"title":"docker 基础知识","uri":"/docker-popularization/"},{"categories":["docker"],"content":"docker 基础设置","date":"2016-01-01","objectID":"/docker-options/","tags":null,"title":"docker 基础设置","uri":"/docker-options/"},{"categories":["docker"],"content":"docker 基础 ","date":"2016-01-01","objectID":"/docker-options/:0:0","tags":null,"title":"docker 基础设置","uri":"/docker-options/"},{"categories":["docker"],"content":"升级内核 # 导入 Key rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org # 安装 Yum 源 rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm # 更新 kernel yum --enablerepo=elrepo-kernel install -y kernel-lt kernel-lt-devel # 配置 内核优先 grub2-set-default 0 ","date":"2016-01-01","objectID":"/docker-options/:1:0","tags":null,"title":"docker 基础设置","uri":"/docker-options/"},{"categories":["docker"],"content":"开启内核namespace支持 # 执行如下 grubby --args=\"user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\" # 必须重启系统 reboot ","date":"2016-01-01","objectID":"/docker-options/:2:0","tags":null,"title":"docker 基础设置","uri":"/docker-options/"},{"categories":["docker"],"content":"修改内核参数 cat\u003c\u003cEOF \u003e /etc/sysctl.d/docker.conf # 要求iptables不对bridge的数据进行处理 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-arptables = 1 EOF # 生效配置 sysctl --system # 检查系统 curl -s https://raw.githubusercontent.com/docker/docker/master/contrib/check-config.sh | bash ","date":"2016-01-01","objectID":"/docker-options/:3:0","tags":null,"title":"docker 基础设置","uri":"/docker-options/"},{"categories":["docker"],"content":"docker install # 指定安装,并指定安装源 export VERSION=19.03 curl -fsSL \"https://get.docker.com/\" | bash -s -- --mirror Aliyun ","date":"2016-01-01","objectID":"/docker-options/:4:0","tags":null,"title":"docker 基础设置","uri":"/docker-options/"},{"categories":["docker"],"content":"docker 设置 # 第一种方式, 增加daemon.json mkdir -p /etc/docker/ cat\u003e/etc/docker/daemon.json\u003c\u003cEOF { \"bip\": \"172.17.0.1/16\", \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\",\"https://gcr.azk8s.cn\",\"https://quay.azk8s.cn\"], \"data-root\": \"/opt/docker\", \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\", \"max-file\": \"5\" } } EOF # kubernetes docker mkdir -p /etc/docker/ cat\u003e/etc/docker/daemon.json\u003c\u003cEOF { \"bip\": \"172.17.0.1/16\", \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"registry-mirrors\": [\"https://dockerhub.azk8s.cn\",\"https://gcr.azk8s.cn\",\"https://quay.azk8s.cn\"], \"data-root\": \"/opt/docker\", \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\", \"max-file\": \"5\" }, \"dns-search\": [\"default.svc.cluster.local\", \"svc.cluster.local\", \"localdomain\"], \"dns-opts\": [\"ndots:2\", \"timeout:2\", \"attempts:2\"] } EOF # 第二种方式，增加 opts mkdir -p /etc/systemd/system/docker.service.d/ # 增加 docker.service 文件 cat \u003e\u003e /etc/systemd/system/docker.service \u003c\u003c EOF [Unit] Description=Docker Application Container Engine Documentation=http://docs.docker.com After=network.target docker-storage-setup.service Wants=docker-storage-setup.service [Service] Type=notify Environment=GOTRACEBACK=crash ExecReload=/bin/kill -s HUP $MAINPID Delegate=yes KillMode=process ExecStart=/usr/bin/dockerd \\ $DOCKER_OPTS \\ $DOCKER_STORAGE_OPTIONS \\ $DOCKER_NETWORK_OPTIONS \\ $DOCKER_DNS_OPTIONS \\ $INSECURE_REGISTRY LimitNOFILE=1048576 LimitNPROC=1048576 LimitCORE=infinity TimeoutStartSec=1min # restart the docker process if it exits prematurely Restart=on-failure StartLimitBurst=3 StartLimitInterval=60s [Install] WantedBy=multi-user.target EOF # 增加配置文件 cat \u003e\u003e /etc/systemd/system/docker.service.d/docker-options.conf \u003c\u003c EOF [Service] Environment=\"DOCKER_OPTS=--insecure-registry=10.254.0.0/16 \\ --registry-mirror=https://registry.docker-cn.com \\ --exec-opt native.cgroupdriver=systemd \\ --data-root=/opt/docker --log-opt max-size=50m --log-opt max-file=5\" EOF ","date":"2016-01-01","objectID":"/docker-options/:5:0","tags":null,"title":"docker 基础设置","uri":"/docker-options/"},{"categories":["python"],"content":"Python","date":"2016-01-01","objectID":"/my-python/","tags":null,"title":"Python","uri":"/my-python/"},{"categories":["python"],"content":"随机数 # 一位大神写的 红包 的程序 import random dic={} lis = ['KeLan','Monkey','Dexter','Superman','Iron Man','Robin'] def redpacket(cash,person,index): if cash\u003e0 and person !=1: n = round(random.uniform(0.01,cash-(0.01*person)),2) dic[lis[index]] = n print(str(n).ljust(4,\"0\")) person-=1 cash-=n index+=1 redpacket(cash,person,index) else: dic[lis[index]]=round(cash,2) print(str(cash).ljust(4,\"0\")) redpacket(200,len(lis),0) print(dic) print(\"手气最佳:\",max(dic.items(),key=lambda x:x[1])) ","date":"2016-01-01","objectID":"/my-python/:1:0","tags":null,"title":"Python","uri":"/my-python/"},{"categories":["Linux"],"content":"Shell Scrpit","date":"2016-01-01","objectID":"/shell-scrpit/","tags":null,"title":"Shell Scrpit","uri":"/shell-scrpit/"},{"categories":["Linux"],"content":" 一些自己折腾过的Shell脚本 很弱的脚本 ","date":"2016-01-01","objectID":"/shell-scrpit/:0:0","tags":null,"title":"Shell Scrpit","uri":"/shell-scrpit/"},{"categories":["Linux"],"content":"截取文件内容 利用sed 截取文件中，匹配前后的中间内容 #!/bin/bash curl=http://127.0.0.1/wars if [ ! -n \"$1\" ] ;then echo \"请输入目录名\" else echo \"$curl/$1\" wget $curl/$1 cat $1 |grep war |sed -nr '/href/{s/.*=\"(.+)\"\u003e.*/\\1/;p}'\u003e war.file rm -rf $1 for i in $(cat war.file) do wget $curl/$1/$i done fi # Cat 文件 \u003chtml\u003e \u003chead\u003e\u003ctitle\u003eIndex of /wars/201702241/\u003c/title\u003e\u003c/head\u003e \u003cbody bgcolor=\"white\"\u003e \u003ch1\u003eIndex of /wars/201702241/\u003c/h1\u003e\u003chr\u003e\u003cpre\u003e\u003ca href=\"../\"\u003e../\u003c/a\u003e \u003ca href=\"mo_sb1.war\"\u003emo_sb1.war\u003c/a\u003e 24-Feb-2017 07:35 97110689 \u003ca href=\"mo_sc2.war\"\u003emo_sc2.war\u003c/a\u003e 24-Feb-2017 07:35 85160737 \u003ca href=\"mo_sd3.war\"\u003emo_sd3.war\u003c/a\u003e 24-Feb-2017 09:07 111437764 \u003ca href=\"mo_sf4.war\"\u003emo_sf4.war\u003c/a\u003e 24-Feb-2017 07:45 87313275 \u003ca href=\"mo_sg5.war\"\u003emo_sg5.war\u003c/a\u003e 24-Feb-2017 07:45 93228435 \u003ca href=\"mo_st6.war\"\u003emo_st6.war\u003c/a\u003e 24-Feb-2017 07:35 90557505 \u003c/pre\u003e\u003chr\u003e\u003c/body\u003e \u003c/html\u003e ","date":"2016-01-01","objectID":"/shell-scrpit/:1:0","tags":null,"title":"Shell Scrpit","uri":"/shell-scrpit/"},{"categories":["golang"],"content":"Context","date":"2000-01-01","objectID":"/golang-web-note-11/","tags":["golang"],"title":"Context","uri":"/golang-web-note-11/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-11/:0:0","tags":["golang"],"title":"Context","uri":"/golang-web-note-11/"},{"categories":["golang"],"content":"Context Go 语言的 Context 既, 上下文。 是Golang 语言标准库 golang.org/x/net/context。 golang 的 Context包, 是专门用来简化对于处理单个请求的多个goroutine之间与请求域的数据、取消信号、截止时间等相关操作, 这些操作可能涉及多个API调用。 比如有一个网络请求Request, 每个Request都需要开启一个goroutine做一些事情, 这些goroutine又会开启其他的goroutine。这样的话, 我们就可以通过Context, 来跟踪这些goroutine, 并且可以通过Context来控制他们, 这就是Go语言为我们提供的Context。 在Go服务器程序中, 每个请求都会有一个goroutine去处理。然而, 处理程序往往还需要创建额外的goroutine去访问后端资源, 比如数据库、RPC服务等。由于这些goroutine都是在处理同一个请求, 所以它们往往需要访问一些共享的资源, 比如用户身份信息、认证token、请求截止时间等。而且如果请求超时或者被取消后, 所有的goroutine都应该马上退出并且释放相关的资源。这种情况也需要用Context来为我们取消掉所有goroutine。 context.Context 实例是一个接口, 该接口定义了四个需要实现的方法。 type Context interface { // 返回一个设置结束的时间，和OK Deadline() (deadline time.Time, ok bool) // Done 是一个只读的 channel Done() \u003c-chan struct{} // 错误 Err() error // 接收一个 key,返回一个 value Value(key interface{}) interface{} } 方法解释: Deadline 方法 - 需要返回当前Context被取消的时间, 也就是完成工作的截止时间（deadline）。 Done方法 - 需要返回一个Channel, 这个Channel会在当前工作完成或者上下文被取消之后关闭, 多次调用Done方法会返回同一个Channel。 Err方法 - 返回当前Context结束的原因, 它只会在Done返回的Channel被关闭时才会返回非空的值。 如果当前Context被取消就会返回Canceled错误。 如果当前Context超时就会返回DeadlineExceeded错误。 Value方法 - 会从Context中返回键对应的值, 对于同一个上下文来说, 多次调用 Value 并传入相同的Key会返回相同的结果，该方法仅用于传递跨API和进程间跟请求域的数据。 ","date":"2000-01-01","objectID":"/golang-web-note-11/:1:0","tags":["golang"],"title":"Context","uri":"/golang-web-note-11/"},{"categories":["golang"],"content":"Background()和TODO() Go内置两个函数: Background()和TODO(), 这两个函数分别返回一个实现了Context接口的background()和todo()。我们代码中最开始都是以这两个内置的上下文对象作为最顶层的partent context, 衍生出更多的子上下文对象。 Background() - 主要用于main函数、初始化以及测试代码中, 作为Context这个树结构的最顶层的Context, 也就是根Context。 TODO() - 它目前还不知道具体的使用场景, 如果我们不知道该使用什么Context的时候, 可以使用这个。 background和todo - 本质上都是emptyCtx结构体类型, 是一个不可取消, 没有设置截止时间, 没有携带任何值的Context。 ","date":"2000-01-01","objectID":"/golang-web-note-11/:1:1","tags":["golang"],"title":"Context","uri":"/golang-web-note-11/"},{"categories":["golang"],"content":"With 系列函数 context包中定义了四个With系列函数。 WithCancel // WithCancel 需在Main中使用,并且需要传根Context。( Background()或TODO() ) func WithCancel(parent Context) (ctx Context, cancel CancelFunc) // 返回一个 ctx Context 和 一个 cancel函数 WithCancel - 返回带有新Done通道的父节点的副本。当调用返回的cancel函数或当关闭父上下文的Done通道时, 将关闭返回上下文的Done通道, 无论先发生什么情况。 例子: package main import ( \"context\" \"fmt\" ) // 创建一个 gen 函数, 返回一个 只读 int类型的 channel func gen(ctx context.Context) \u003c-chan int { // 创建并初始化一个 int 类型的通道 dst := make(chan int) n := 1 // 启动 goroutine // 闭包 go func() { // 无限循环 for { select { // 当 ctx.Done() 可以取到值的时候 case \u003c-ctx.Done(): // return 结束该 goroutine 防止内存泄露 return case dst \u003c- n: // n = 1; n = n + 1 n++ } } }() // 返回 dst 通道 return dst } func main() { // 创建一个 ctx 子 context ctx, cancel := context.WithCancel(context.Background()) // main函数运行完以后执行 cancel。 退出 goroutine 。 // main函数通过 ctx ( context ) 传递信号。 defer cancel() // 循环 gen 函数 return 回来的 dst 通道 for n := range gen(ctx) { fmt.Println(n) if n == 5 { break } } } WithDeadline func WithDeadline(parent Context, deadline time.Time) (Context, CancelFunc) 函数 WithDeadline - 返回父上下文的副本, 并将deadline调整为不迟于 设置的时间 d。如果父上下文的deadline已经早于 设置的时间 d, 则WithDeadline(parent, d)在语义上等同于父上下文。当截止日过期时, 当调用返回的cancel函数时, 或者当父上下文的Done通道关闭时, 返回上下文的Done通道将被关闭, 以最先发生的情况为准。 package main import ( \"context\" \"fmt\" \"time\" ) func main() { // 获取当前时间 加 50 毫秒 d := time.Now().Add(50 * time.Millisecond) // context 调用 withDeadline 传入 d ctx, cancel := context.WithDeadline(context.Background(), d) // main 函数执行完以后,发送关闭指令 // ctx 会自动过期,但是官方建议还是尽量调用 cancel() 。 defer cancel() select { // 1秒 后 会走 这个 case 分支 case \u003c-time.After(1 * time.Second): fmt.Println(\"overslept\") // d = 50毫秒 结束, 会先走这个 case 分支 case \u003c-ctx.Done(): // context 会报: context deadline exceeded 错误 fmt.Println(ctx.Err()) } } 输出: context deadline exceeded WithTimeout func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) WithTimeout - 返回 WithDeadline(parent, time.Now().Add(timeout)) , 一个时间间隔。 package main import ( \"context\" \"fmt\" \"sync\" \"time\" ) // context.WithTimeout // 定义全局的 等待组 var wg sync.WaitGroup func worker(ctx context.Context) { LOOP: for { fmt.Println(\"db connecting ...\") // 假设正常连接数据库耗时10毫秒 time.Sleep(time.Millisecond * 10) select { // 第一个case 分支 50毫秒后自动调用 case \u003c-ctx.Done(): // 跳出这 for 循环到 LOOP 标签 break LOOP default: } } // WithTimeout 超时,或者手动调用了 cancel() 会走到这里。 fmt.Println(\"worker done!\") // 完成 等待组 wg.Done() } func main() { // 设置 timeout = 50毫秒 超时。 ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*50) // 增加一个 等待组 wg.Add(1) // 运行 goroutine 执行 worker 函数 go worker(ctx) // 等待 5 秒 for i := 1; i \u003c 6; i++ { fmt.Printf(\"waiting %d ...\\n\", i) time.Sleep(time.Second * 1) } // 通知 子 goroutine 结束 cancel() // 等待组 等待 wg.Done() wg.Wait() fmt.Println(\"over\") } waiting 1 ... db connecting ... db connecting ... db connecting ... db connecting ... db connecting ... worker done! waiting 2 ... waiting 3 ... waiting 4 ... waiting 5 ... over WithValue func WithValue(parent Context, key, val interface{}) Context WithValue - 返回父节点的副本, 其中与key关联的值为val。 WithValue - 仅对API和进程间传递请求域的数据使用上下文值, 而不是使用它来传递可选参数给函数。所提供的键必须是可比较的, 并且不应该是string类型或任何其他内置类型, 以避免使用上下文在包之间发生冲突。WithValue的用户应该为键定义自己的类型。为了避免在分配给interface{}时进行分配, 上下文键通常具有具体类型struct{}。或者, 导出的上下文关键变量的静态类型应该是指针或接口。 package main import ( \"context\" \"fmt\" \"sync\" \"time\" ) // context.WithValue type TraceCode string var wg sync.WaitGroup func worker(ctx context.Context) { key1 := TraceCode(\"TRACE_CODE1\") key2 := TraceCode(\"TRACE_CODE2\") // 在子goroutine中获取trace code 1 traceCode1, ok := ctx.Value(key1).(string) traceCode2, ok := ctx.Value(key2).(int64) if !ok { fmt.Println(\"invalid trace code\") } LOOP: for { fmt.Printf(\"worker, trace code 1:%s\\n\", traceCode1) fmt.Printf(\"worker, trace code 2:%d\\n\", traceCode2) // 假设正常连接数据库耗时10毫秒 time.Sleep(time.Millisecond * 10) select { // 50毫秒后自动调用 case \u003c-ctx.Done(): break LOOP default: } } fmt.Println(\"worker done!\") wg.Done() } func main() { // 设置一个50毫秒的超时 ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*50) ","date":"2000-01-01","objectID":"/golang-web-note-11/:1:2","tags":["golang"],"title":"Context","uri":"/golang-web-note-11/"},{"categories":["golang"],"content":"Cookie 与 Session","date":"2000-01-01","objectID":"/golang-web-note-9/","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Go Web 编程 HTTP 协议是无状态的，对于一个浏览器发出的多次请求，WEB 服务器无法区分, 是不是来源于同一个浏览器, 所以诞生了Cookie 与 Session 使某个域名下的所有网页能够共享某些数据. ","date":"2000-01-01","objectID":"/golang-web-note-9/:0:0","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Cookie Cookie 实际上是一小段的文本信息（key-value) 形式, Cookie 是纯文本格式，不包含任何可执行的代码. 客户端向服务器发起请求，如果服务器需要记录该用户状态，就使用Response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。 ","date":"2000-01-01","objectID":"/golang-web-note-9/:1:0","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Cookie 机制 客户端User-Agent发送一个请求到服务器。 服务器发送一个HttpResponse响应到客户端，其中包含Set-Cookie的头部。 客户端保存Cookie, 之后向服务器发送请求时, HttpRequest请求中会包含一个Cookie的头部。 服务器返回响应数据。 ","date":"2000-01-01","objectID":"/golang-web-note-9/:1:1","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Cookie 特点 客户端发送请求的时候, 会携带服务端HttpResponse 之前Set-Cookie的Cookie信息。 服务端可以设置Cookie数据key/value信息。 Cookie是针对单个域名的，不同域名之间的Cookie是独立的。 Cookie数据可以配置过期时间，过期的Cookie数据会被系统清除。 ","date":"2000-01-01","objectID":"/golang-web-note-9/:1:2","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Golang 使用 Cookie Go语言中 net/http 标准库定义了 Cookie 。 type Cookie struct { Name string Value string Path string Domain string Expires time.Time RawExpires string // MaxAge=0 表示未设置Max-Age属性 // MaxAge\u003c0 表示立刻删除该Cookie，等价于\"Max-Age: 0\" // MaxAge\u003e0 表示存在Max-Age属性，单位是秒 MaxAge int Secure bool HttpOnly bool Raw string Unparsed []string // 未解析的\"key/value\"对的原始文本 } 使用 net/http 标准库中的 SetCookie 函数, 设置 Cookie. func SetCookie(w ResponseWiter, cookie *Cookie) 获取 Cookie, Request 对象拥有两个获取Cookie的方法和一个添加Cookie的方法 获取 Cookie 方法 // 1. 解析并返回该请求的Cookie头设置的所有Cookie func (r *Request) Cookies() []*Cookie // 2. 返回请求中名为name的Cookie，如果未找到该Cookie会返回nil, ErrNoCookie。 func (r *Request) Cookie(name string) (*Cookie, error) 添加Cookie的方法 // AddCookie向请求中添加一个Cookie。 func (r *Request) AddCookie(c *Cookie) ","date":"2000-01-01","objectID":"/golang-web-note-9/:1:3","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Gin 框架 Cookie 利用 c *gin.Context c.SetCookie 设置Cookie c.Cookie 获取 Cookie 例子: package main import ( \"net/http\" \"github.com/gin-gonic/gin\" ) // 创建一个 User 的结构体 type UserInfo struct { UserName string `form:\"username\"` Password string `form:\"password\"` } func autoCookie(c *gin.Context) gin.HandlerFunc { } func loginHandler(c *gin.Context) { if c.Request.Method == \"POST\" { var u UserInfo if err := c.ShouldBind(\u0026u); err != nil { c.HTML(http.StatusOK, \"login.html\", gin.H{ \"ShouldBindErr\": \"用户名密码禁止为空\", }) return } if u.UserName == \"jicki\" \u0026\u0026 u.Password == \"123456\" { // 设置 Session session := sessions.Default(c) // 清除旧的 session.Clear() session.Set(\"username\", u.UserName) _ = session.Save() // 登录成功跳转到 home 页 c.Redirect(http.StatusFound, \"/index\") } else { c.HTML(http.StatusOK, \"login.html\", gin.H{ \"ShouldBindErr\": \"用户名密码错误\", }) return } } else { c.HTML(http.StatusOK, \"login.html\", nil) } } func indexHandler(c *gin.Context) { c.HTML(http.StatusOK, \"index.html\", nil) } func homeHandler(c *gin.Context) { // 判断是否有 Cookie cookie, err := c.Cookie(\"username\") if err != nil { c.Redirect(http.StatusFound, \"/login\") return } c.HTML(http.StatusOK, \"home.html\", gin.H{ \"username\": cookie, }) } func main() { r := gin.Default() r.LoadHTMLGlob(\"templates/*\") r.GET(\"/index\", indexHandler) r.GET(\"/login\", loginHandler) r.POST(\"/login\", loginHandler) r.GET(\"/home\", homeHandler) _ = r.Run(\":8888\") } Html 文件 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e首页\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e首页\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e登录页面\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"\" method=\"POST\" enctype=\"application/x-www-form-urlencoded\" \u003e \u003cdiv\u003e \u003clabel\u003e 用户名: \u003cinput type=\"text\" name=\"username\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003clabel\u003e 密码: \u003cinput type=\"password\" name=\"password\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput type=\"submit\"\u003e \u003c/div\u003e \u003cp style=\"color:red\"\u003e{{ .ShouldBindErr }}\u003c/p\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eHome\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e{{ .username }}家目录\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2000-01-01","objectID":"/golang-web-note-9/:1:4","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Session Session 与 Cookie 都是会话保持的机制, Session 是记录客户状态的机制, 不同的是Cookie 保存在客户端浏览器中，而 Session 保存在服务器上。 ","date":"2000-01-01","objectID":"/golang-web-note-9/:2:0","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Session 原理 浏览器向服务器发送登录请求(post), 携带账号和密码。 登录成功, 服务器记录登录的状态, req.session.user = user; 服务器记录这些信息。 服务器返回的响应头中携带服务器生成的 Session ID 并将 Session ID 记录到Cookie中，作为身份标识。 浏览器再次访问服务器的时候会通过Cookie携带Session ID。 服务器获取浏览器发送的Session ID后, 在服务器查找Session ID, 如果找不到, 返回未登录状态。 如果找到 Session ID , 根据 Session ID 查找对应的对象, 返回登录成功。 ","date":"2000-01-01","objectID":"/golang-web-note-9/:2:1","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Gin 框架 Session Gin middleware for Session management https://github.com/gin-contrib/sessions Gin 框架可以使用基于 Gin 中间件的 第三方模块 处理 Session。 gin-sessions 支持多种后端存储 Session cookie-based Redis Memcached MongoDB Memstore Download and install: go get -u github.com/gin-contrib/sessions import: import \"github.com/gin-contrib/sessions\" 例子: package main import ( \"fmt\" \"net/http\" \"github.com/gin-contrib/sessions\" \"github.com/gin-contrib/sessions/cookie\" \"github.com/gin-gonic/gin\" ) // 创建一个 User 的结构体 type UserInfo struct { UserName string `form:\"username\"` Password string `form:\"password\"` } func homeHandler(c *gin.Context) { session := sessions.Default(c) username := session.Get(\"username\") c.HTML(http.StatusOK, \"home.html\", gin.H{ \"username\": username, }) } func loginHandler(c *gin.Context) { if c.Request.Method == \"POST\" { var u UserInfo if err := c.ShouldBind(\u0026u); err != nil { c.HTML(http.StatusOK, \"login.html\", gin.H{ \"ShouldBindErr\": \"用户名密码禁止为空\", }) return } if u.UserName == \"jicki\" \u0026\u0026 u.Password == \"123456\" { // 登录成功设置一个 Session SetSession(c, u) // 登录成功跳转到 home 页 c.Redirect(http.StatusFound, \"/index\") } else { c.HTML(http.StatusOK, \"login.html\", gin.H{ \"ShouldBindErr\": \"用户名密码错误\", }) return } } else { c.HTML(http.StatusOK, \"login.html\", nil) } } func indexHandler(c *gin.Context) { c.HTML(http.StatusOK, \"index.html\", nil) } // 包装一个 Session 中间件, 并初始化session func Session(secret string) gin.HandlerFunc { // 1. 创建一个 Cookie 实例 用于存储 Session store := cookie.NewStore([]byte(secret)) // 2 创建一个 Redis 实例 用于存储 Session //store, _ := redis.NewStore(10, \"tcp\", \"localhost:6379\", \"\", []byte(\"secret\")) // 3 创建一个 MemCached 实例 用于存储 Session //store := memcached.NewStore(memcache.New(\"localhost:11211\"), \"\", []byte(\"secret\")) // 4 创建一个 MongoDB 实例 用于存储 Session //session, err := mgo.Dial(\"localhost:27017/test\") //if err != nil { // // handle err //} //c := session.DB(\"\").C(\"sessions\") //store := mongo.NewStore(c, 3600, true, []byte(\"secret\")) // 5 创建一个 memstore 实例 用于存储 Session //store := memstore.NewStore([]byte(\"secret\")) //Also set Secure: true if using SSL, you should though store.Options(sessions.Options{HttpOnly: true, MaxAge: 7 * 86400, Path: \"/\"}) return sessions.Sessions(\"gin-session\", store) } // 包装 一个 设置 Session 的函数 func SetSession(c *gin.Context, user UserInfo) { session := sessions.Default(c) session.Clear() session.Set(\"username\", user.UserName) err := session.Save() if err != nil { fmt.Printf(\"Save Session Failed: %s\\n\", err) return } } // 判断是否登录中间件 func AuthSessionMiddle() gin.HandlerFunc { return func(c *gin.Context) { session := sessions.Default(c) username := session.Get(\"username\") if username == nil { c.Redirect(http.StatusFound, \"/login\") return } // 设置一个键值对 c.Set(\"username\", username) // 执行下一个 程序 c.Next() return } } func main() { r := gin.Default() r.LoadHTMLGlob(\"templates/*\") // 定义一个Session 加密串 secret := \"SetSessionPassword\" r.GET(\"/index\", indexHandler) // 开始使用 gin中间件 Sessions r.Use(Session(secret)) r.GET(\"/login\", loginHandler) r.POST(\"/login\", loginHandler) r.GET(\"/home\", AuthSessionMiddle(), homeHandler) _ = r.Run(\":8888\") } html 文件 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e首页\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e首页\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eHome\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e{{ .username }}家目录\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e登录页面\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"\" method=\"POST\" enctype=\"application/x-www-form-urlencoded\" \u003e \u003cdiv\u003e \u003clabel\u003e 用户名: \u003cinput type=\"text\" name=\"username\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003clabel\u003e 密码: \u003cinput type=\"password\" name=\"password\"\u003e \u003c/label\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput type=\"submit\"\u003e \u003c/div\u003e \u003cp style=\"color:red\"\u003e{{ .ShouldBindErr }}\u003c/p\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2000-01-01","objectID":"/golang-web-note-9/:2:2","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"gob 序列化 标准库gob是golang提供的 “私有” 的编解码方式, 它的效率会比json，xml等更高, 特别适合在Go语言程序间传递数据。 例子: package main import ( \"bytes\" \"encoding/gob\" \"fmt\" ) type s struct { data map[string]interface{} } func gobDemo() { var s1 = s{ data: make(map[string]interface{}, 8), } s1.data[\"count\"] = 1 // encode 编码 // 创建一个 指针空间 buf := new(bytes.Buffer) // 创建一个 编码器对象 enc := gob.NewEncoder(buf) // 对 s1.data 进行编码 err := enc.Encode(s1.data) if err != nil { fmt.Println(\"gob encode failed, err:\", err) return } // 获取 编码后的 字节(Bytes)数据 b := buf.Bytes() fmt.Println(b) var s2 = s{ data: make(map[string]interface{}, 8), } // decode 解码 // 创建一个 解码器对象 dec := gob.NewDecoder(bytes.NewBuffer(b)) // 对 s2.data 指针 进行解码 err = dec.Decode(\u0026s2.data) if err != nil { fmt.Println(\"gob decode failed, err\", err) return } fmt.Println(s2.data) for _, v := range s2.data { fmt.Printf(\"value:%v, type:%T\\n\", v, v) } } func main() { gobDemo() } 输出: [14 255 129 4 1 2 255 130 0 1 12 1 16 0 0 18 255 130 0 1 5 99 111 117 110 116 3 105 110 116 4 2 0 2] map[count:1] value:1, type:int ","date":"2000-01-01","objectID":"/golang-web-note-9/:2:3","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"gin-session 的 gob 问题 使用gin-session的时候报错: securecookie: error - caused by: securecookie: error - caused by: gob: type not registered for interface: 自定义类型或高级对象 需要解决以上错误,需要对gob.Register(自定义类型或高级对象) 类型进行注册。 错误例子: package main import ( \"bytes\" \"encoding/gob\" \"encoding/json\" \"fmt\" \"log\" ) func CloneObject(a, b interface{}) []byte { // 创建一个 指针空间 buff := new(bytes.Buffer) // 创建一个 编码器对象 enc := gob.NewEncoder(buff) // 创建一个 解码器对象 dec := gob.NewDecoder(buff) // 对 a 进行编码 err := enc.Encode(a) if err != nil { log.Panic(\"e1: \", err) } // 获取 a 编码后的 字节(Bytes)数据 b1 := buff.Bytes() // 对 b 进行解码 err = dec.Decode(b) if err != nil { log.Panic(\"e2: \", err) } // 返回编码后的的 bytes 数据 b1 return b1 } func main() { // 定义 a 为空结构体类型 var a interface{} // 初始化并赋值 a a = map[string]interface{}{\"X\": 1} // json 序列化 \u0026a b2, err := json.Marshal(\u0026a) // 打印序列化后的数据 b2 fmt.Println(string(b2), err) // 定义 b 为空结构体类型 var b interface{} // 使用 gob 对 \u0026a \u0026b 进行序列化与反序列化 b1 := CloneObject(\u0026a, \u0026b) fmt.Println(string(b1)) } 输出: {\"X\":1} \u003cnil\u003e 2019/12/23 15:13:49 e1: gob: type not registered for interface: map[string]interface {} 修改后的例子: package main import ( \"bytes\" \"encoding/gob\" \"encoding/json\" \"fmt\" \"log\" ) func CloneObject(a, b interface{}) []byte { // 创建一个 指针空间 buff := new(bytes.Buffer) // 创建一个 编码器对象 enc := gob.NewEncoder(buff) // 创建一个 解码器对象 dec := gob.NewDecoder(buff) // 对 a 进行编码 err := enc.Encode(a) if err != nil { log.Panic(\"e1: \", err) } // 获取 a 编码后的 字节(Bytes)数据 b1 := buff.Bytes() // 对 b 进行解码 err = dec.Decode(b) if err != nil { log.Panic(\"e2: \", err) } // 返回编码后的的 bytes 数据 b1 return b1 } func main() { // 定义 a 为空结构体类型 var a interface{} // 初始化并赋值 a a = map[string]interface{}{\"X\": 1} // json 序列化 \u0026a b2, err := json.Marshal(\u0026a) // 打印序列化后的数据 b2 fmt.Println(string(b2), err) // 定义 b 为空结构体类型 var b interface{} // 注册一下 gob.Register(map[string]interface{}{}) // 使用 gob 对 \u0026a \u0026b 进行序列化与反序列化 b1 := CloneObject(\u0026a, \u0026b) fmt.Println(string(b1)) } ","date":"2000-01-01","objectID":"/golang-web-note-9/:2:4","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang"],"content":"Cookie 与 Session 优劣 Cookie 数据存放在客户端(浏览器等..), Session 数据放在服务器端(内存、关系型数据库、Redis、Memcache等)。 Cookie 不是很安全, 别人可以分析存放在本地的Cookie 并进行 Cookie 欺骗 考虑到安全应当使用Session。 Session 会在一定时间内保存在服务器上。当访问增多, 会比较占用你服务器的性能 考虑到减轻服务器性能方面, 应当使用 Cookie 。 单个Cookie保存的数据不能超过4KB, 很多浏览器都限制一个站点最多保存20个Cookie。 将登陆信息等重要信息存放为 Session、其他信息如果需要保留, 可以放在Cookie中。 ","date":"2000-01-01","objectID":"/golang-web-note-9/:3:0","tags":["golang"],"title":"Cookie 与 Session","uri":"/golang-web-note-9/"},{"categories":["golang","Go","gin"],"content":"Gin templates","date":"2000-01-01","objectID":"/golang-web-note-13/","tags":["golang","gin"],"title":"Gin templates","uri":"/golang-web-note-13/"},{"categories":["golang","Go","gin"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-13/:0:0","tags":["golang","gin"],"title":"Gin templates","uri":"/golang-web-note-13/"},{"categories":["golang","Go","gin"],"content":"gin 框架 templates templates 的基础语法以及概念在前面的文章中有记录 HTML 基础概念 ","date":"2000-01-01","objectID":"/golang-web-note-13/:1:0","tags":["golang","gin"],"title":"Gin templates","uri":"/golang-web-note-13/"},{"categories":["golang","Go","gin"],"content":"gin 下的例子 go 代码 package main import ( \"fmt\" \"html/template\" \"net/http\" \"github.com/gin-gonic/gin\" ) type User struct { Name string Age int Hobby []string } // template 语法 func main() { s1 := User{ Name: \"张大仙\", Age: 20, Hobby: []string{ \"王者\", \"英雄联盟\", \"说骚话\", }, } s2 := User{ Name: \"魔教教主\", Age: 30, Hobby: []string{ \"英雄联盟\", \"刺激战场\", \"说骚话\", }, } r := gin.Default() // 导入 自定义函数到 模板中 // 导入 自定义函数必须在 加载模板 之前,否则会报错 r.SetFuncMap(template.FuncMap{ \"custom\": func(name string) template.HTML { return template.HTML(fmt.Sprintf(\"hello %s\", name)) }, }) // 加载模板 r.LoadHTMLGlob(\"templates/*\") // html 基础 r.GET(\"/basic\", func(c *gin.Context) { c.HTML(http.StatusOK, \"basic.tmpl\", gin.H{ \"s1\": s1, \"s2\": s2, }) }) // html 自定义函数 r.GET(\"/custom\", func(c *gin.Context) { c.HTML(http.StatusOK, \"custom.tmpl\", gin.H{ \"s1\": s1, }) }) // html 嵌套 r.GET(\"/nest\", func(c *gin.Context) { c.HTML(http.StatusOK, \"nest.tmpl\", nil) }) _ = r.Run(\":8888\") } {% raw %} html 代码 basic.tmpl \u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c!-- 标题 --\u003e \u003ctitle\u003eHTML基础\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003c!-- .变量 的使用--\u003e \u003cp\u003e欢迎光临 {{ .s1.Name }}\u003c/p\u003e \u003cp\u003e姓名: {{ .s1.Name }}\u003c/p\u003e \u003cp\u003e年龄: {{ .s1.Age }}\u003c/p\u003e \u003c!-- 变量定义 与 判断语句 --\u003e {{ $age := .s1.Age}} {{ if lt $age 22}} \u003cp\u003e {{ .s1.Name }} 好好学习 \u003c/p\u003e {{else}} \u003cp\u003e {{ .s1.Name }} 好好工作 \u003c/p\u003e {{end}} \u003c!-- range 循环 --\u003e \u003cp\u003e爱好: \u003cbr\u003e {{ range $index, $hobby := .s1.Hobby }} {{ $index }} - {{ $hobby }} \u003c/p\u003e {{end}} \u003chr\u003e \u003c!-- with 的使用 --\u003e {{ with .s2 }} \u003cp\u003e欢迎光临 {{ .Name }}\u003c/p\u003e \u003cp\u003e姓名: {{ .Name }}\u003c/p\u003e \u003cp\u003e年龄: {{ .Age }}\u003c/p\u003e \u003c!-- 变量定义 与 判断语句 --\u003e {{ $age := .Age }} {{ if lt $age 22 }} \u003cp\u003e {{ .Name }} 好好学习 \u003c/p\u003e {{else}} \u003cp\u003e {{ .Name }} 好好工作 \u003c/p\u003e {{end}} \u003c!-- range 循环 --\u003e \u003cp\u003e爱好: \u003cbr\u003e {{ range $index, $hobby := .Hobby }} {{ $index }} - {{ $hobby }} \u003c/p\u003e {{end}} {{end}} \u003chr\u003e \u003c!-- index 索引取值 --\u003e \u003cp\u003eindex 取 s2 Hobby 索引为[2]的值 = {{index .s2.Hobby 2}} \u003c/p\u003e \u003c/body\u003e \u003c/html\u003e custom.tmpl \u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c!-- 标题 --\u003e \u003ctitle\u003eHTML 进阶\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e {{ with .s1 }} \u003c!-- 引用 自定义函数 --\u003e \u003cp\u003e {{ custom .Name }}\u003c/p\u003e {{end}} \u003c/body\u003e \u003c/html\u003e nest.tmpl \u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c!-- 标题 --\u003e \u003ctitle\u003eHTML 嵌套\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003c!-- 嵌套另一个单独的模板文件 --\u003e {{template \"nesting\" }} \u003chr\u003e {{template \"temp\"}} \u003c/body\u003e \u003c/html\u003e \u003c!-- define 自定义模板 --\u003e {{ define \"temp\"}} \u003col\u003e \u003cli\u003e青铜\u003c/li\u003e \u003cli\u003e白银\u003c/li\u003e \u003cli\u003e黄金\u003c/li\u003e \u003c/ol\u003e {{end}} nesting.tmpl {{ define \"nesting\" }} \u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c!-- 标题 --\u003e \u003ctitle\u003eHTML 嵌套的模板\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eHTML 嵌套模板\u003c/p\u003e \u003cul\u003e \u003cli\u003e钻石\u003c/li\u003e \u003cli\u003e星耀\u003c/li\u003e \u003cli\u003e王者\u003c/li\u003e \u003c/ul\u003e \u003c/body\u003e \u003c/html\u003e {{end}} ","date":"2000-01-01","objectID":"/golang-web-note-13/:1:1","tags":["golang","gin"],"title":"Gin templates","uri":"/golang-web-note-13/"},{"categories":["golang","Go","gin"],"content":"自定义模板渲染符 自定义模板渲染符可使用 r.Delims(\"左边符号\", \"右边符号\") 来重新定义。 go 代码 package main import ( \"fmt\" \"html/template\" \"net/http\" \"github.com/gin-gonic/gin\" ) type User struct { Name string Age int Hobby []string } // template 语法 func main() { s1 := User{ Name: \"张大仙\", Age: 20, Hobby: []string{ \"王者\", \"英雄联盟\", \"说骚话\", }, } s2 := User{ Name: \"魔教教主\", Age: 30, Hobby: []string{ \"英雄联盟\", \"刺激战场\", \"说骚话\", }, } r := gin.Default() // 自定义渲染分隔符 // 自定义渲染分隔符必须在 导入模板之前定义 r.Delims(\"{[{\", \"}]}\") // 加载模板 r.LoadHTMLGlob(\"templates/*\") // html 自定义渲染符号 r.GET(\"/delimit\", func(c *gin.Context) { c.HTML(http.StatusOK, \"delimit.tmpl\", gin.H{ \"name\": s1.Name, \"age\": s1.Age, }) }) _ = r.Run(\":8888\") } html \u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c!-- 标题 --\u003e \u003ctitle\u003eHTML 自定义分隔符\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003c!-- 自定义分隔符 --\u003e \u003cp\u003e姓名: {[{ .name }]}\u003c/p\u003e \u003cp\u003e年龄: {[{ .age }]}\u003c/p\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2000-01-01","objectID":"/golang-web-note-13/:1:2","tags":["golang","gin"],"title":"Gin templates","uri":"/golang-web-note-13/"},{"categories":["golang","Go","gin"],"content":"多模板继承 Gin 框架下的模板都是单模板,多模板继承需要使用 第三方的包。 go get -u github.com/gin-contrib/multitemplate go 代码 package main import ( \"fmt\" \"html/template\" \"net/http\" \"github.com/gin-contrib/multitemplate\" \"github.com/gin-gonic/gin\" ) type User struct { Name string Age int Hobby []string } // 创建一个方法用来处理多模板继承 func createMyRender() multitemplate.Renderer { r := multitemplate.NewRenderer() // 添加两个多模板继承, 初始模板必须写在前面。 r.AddFromFiles(\"index\", \"templates/inherit.tmpl\", \"templates/i1.tmpl\") r.AddFromFiles(\"home\", \"templates/inherit.tmpl\", \"templates/i2.tmpl\") return r } // html 继承 func main() { s1 := User{ Name: \"张大仙\", Age: 20, Hobby: []string{ \"王者\", \"英雄联盟\", \"说骚话\", }, } s2 := User{ Name: \"魔教教主\", Age: 30, Hobby: []string{ \"英雄联盟\", \"刺激战场\", \"说骚话\", }, } r := gin.Default() // 加载模板 r.LoadHTMLGlob(\"templates/*\") // 接收多模板函数定义的返回值 r.HTMLRender = createMyRender() // 定义一个组 inGroup := r.Group(\"/inherit\") { inGroup.GET(\"/index\", func(c *gin.Context) { // 这里 c.HTML 写入的\"index\" 模板为 createMyRender 函数定义的名称 c.HTML(http.StatusOK, \"index\", gin.H{ \"name\": s1.Name, }) }) inGroup.GET(\"/home\", func(c *gin.Context) { // 这里 c.HTML 写入的\"home\" 模板为 createMyRender 函数定义的名称 c.HTML(http.StatusOK, \"home\", gin.H{ \"name\": s2.Name, }) }) } _ = r.Run(\":8888\") } html 模板 inherit.tmpl \u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"\u003e \u003c!-- 标题 --\u003e \u003ctitle\u003eHTML 继承\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv id=\"container\" style=\"width:100%\"\u003e \u003cdiv id=\"header\" style=\"background-color:lightseagreen;\"\u003e \u003ch1 style=\"margin-bottom:0;text-align:center;\"\u003eHTML 继承\u003c/h1\u003e\u003c/div\u003e \u003cdiv id=\"menu\" style=\"background-color:lightcyan;height:200px;width:100px;float:left;\"\u003e \u003cb\u003e\u003c/b\u003e \u003cbr\u003e Golang \u003cbr\u003e Gin \u003cbr\u003e HTML \u003c/div\u003e \u003cdiv id=\"content\" style=\"text-align:center;\"\u003e {{ block \"content\" . }} {{ end }} \u003c/div\u003e \u003cdiv id=\"footer\" style=\"background-color:cornflowerblue;clear:both;text-align:center;\"\u003e 联系方式: https://jicki.cn\u003c/div\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e i1.tmpl {{/*继承 inherit 模板*/}} {{template \"inherit\"}} {{/* 重新定义 inherit 模板中的内容 */}} {{define \"content\"}} \u003ch1\u003e这是index页面\u003c/h1\u003e \u003cp\u003ehello {{ .name }}\u003c/p\u003e {{end}} i2.tmpl {{/*继承 inherit 模板*/}} {{template \"inherit\"}} {{/* 重新定义 inherit 模板中的内容 */}} {{define \"content\"}} \u003ch1\u003e这是home页面\u003c/h1\u003e \u003cp\u003ehello {{ .name }}\u003c/p\u003e {{end}} {% endraw %} ","date":"2000-01-01","objectID":"/golang-web-note-13/:1:3","tags":["golang","gin"],"title":"Gin templates","uri":"/golang-web-note-13/"},{"categories":["golang"],"content":"Gin 框架","date":"2000-01-01","objectID":"/golang-web-note-2/","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-2/:0:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 框架 官方 中文文档 https://gin-gonic.com/zh-cn/docs/ ","date":"2000-01-01","objectID":"/golang-web-note-2/:1:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 简介 Gin 是一个用 Go (Golang) 编写的 HTTP web 框架。 使用 httprouter, 因此是一个拥有很好性能的API框架。 ","date":"2000-01-01","objectID":"/golang-web-note-2/:2:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 特性 快速 基于 Radix 树的路由，内存占用小。没有使用反射。API 性能可以直观的测试出来。 支持中间件 传入的 HTTP 请求可以由一系列中间件和最终操作来处理。 例如：Logger，Authorization，GZIP，最终操作 DB。 Crash 处理 Gin 可以 捕获 发生在 HTTP 请求中的 panic 并 recover 它。这样，你的服务器将始终可用。例如，你可以向 Sentry 报告这个 panic . JSON 验证 Gin 可以解析并验证请求的 JSON，例如检查所需值的存在。 路由组 更好地组织路由。是否需要授权，不同的 API 版本, 此外，这些组可以无限制地嵌套而不会降低性能。 错误管理 Gin 提供了一种方便的方法来收集 HTTP 请求期间发生的所有错误。使用中间件可以将错误写入日志文件，数据库里。 内置渲染 Gin 为 JSON，XML 和 HTML 渲染提供了易于使用的 API。 可扩展性 可以很简单创建中间件。 ","date":"2000-01-01","objectID":"/golang-web-note-2/:3:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"安装使用 通过 go get 并使用 import 导入包, 既可。 go get -u github.com/gin-gonic/gin import \"github.com/gin-gonic/gin\" ","date":"2000-01-01","objectID":"/golang-web-note-2/:4:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"JSON 格式 c.JSON(状态码, JSON序列化的数据gin.H{\"\":\"”,}) package main import ( \"github.com/gin-gonic/gin\" \"log\" ) func main() { // 创建一个 gin实例,返回一个 *engine 路由引擎 r := gin.Default() // 创建一个GET 方法 的 /hello 的路由 // func 使用 1. 匿名函数方式,拼接json r.GET(\"/hello\",func(c *gin.Context){ // 使用 JSON格式,方式, 状态码为 200 // gin.H 是返回一个map c.JSON(200,gin.H{ \"message\":\"hello world\", }) }) // 2. 结构体的形式返回 r.GET(\"/json\", func(c *gin.Context) { // 定义一个结构体 stu := struct { // 首字母必须大写,否则序列化不出值 Name string Age int }{Name: \"jicki\", Age: 20} // 传入状态码, 结构体 c.JSON(http.StatusOK, stu) }) // 启动 gin 服务 if err := r.Run(\":8888\");err!=nil{ log.Fatal(err.Error()) } } # 访问 http://127.0.0.1:8888/hello # 显示一个 json 格式的数据 {\"message\":\"hello world\"} # 访问 http://127.0.0.1:8888/json {\"Name\":\"jicki\",\"Age\":20} ","date":"2000-01-01","objectID":"/golang-web-note-2/:5:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"XML 格式输出 c.XML(状态码, 具体的结构体类型) package main import ( \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) func main() { r := gin.Default() r.GET(\"/xml\", func(c *gin.Context) { // 定义一个结构体 type Student struct { // 首字母必须大写,否则序列化不出值 Name string Age int } stu1 := Student{ Name: \"学生\", Age: 20, } // XML 需要传入具体的结构体类型 c.XML(http.StatusOK, stu1) }) // 启动 if err := r.Run(\":8888\"); err != nil { log.Fatalf(\"Server Run Failed err:%v\\n\", err) return } } 输出 \u003cStudent\u003e \u003cName\u003e学生\u003c/Name\u003e \u003cAge\u003e20\u003c/Age\u003e \u003c/Student\u003e ","date":"2000-01-01","objectID":"/golang-web-note-2/:6:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"YAML 格式 package main import ( \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) func main() { r := gin.Default() r.GET(\"/yaml\", func(c *gin.Context) { // 定义一个结构体 stu := struct { // 首字母必须大写,否则序列化不出值 Name string Age int }{Name: \"jicki\", Age: 20} // 传入 结构体 c.YAML(http.StatusOK, stu) }) // 启动 if err := r.Run(\":8888\"); err != nil { log.Fatalf(\"Server Run Failed err:%v\\n\", err) return } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:7:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"RESTful API 基于Gin 的 RESTful API 写法 (GET,POST,PUT,DELETE) func main() { r := gin.Default() // HTTP 四个请求方式 // 基于 GET 请求 r.GET(\"/hello\",func(c *gin.Context){ // gin.H 是 map[string]interface{} 的一种快捷方式 c.JSON(http.StatusOK, gin.H{ \"Message\":\"GET\", }) }) // 基于 POST 请求 r.POST(\"/hello\",func(c *gin.Context){ c.JSON(http.StatusOK,gin.H{ \"Message\":\"POST\", }) }) // 基于 PUT 请求 r.PUT(\"/hello\",func(c *gin.Context){ c.JSON(http.StatusOK,gin.H{ \"Message\":\"PUT\", }) }) // 基于 DELETE 请求 r.DELETE(\"/hello\",func(c *gin.Context){ c.JSON(http.StatusOK,gin.H{ \"Message\":\"DELETE\", }) }) // 启动服务 if err := r.Run(\":8888\");err!=nil{ fmt.Printf(\"Server Run Failed err: %v\\n\",err) return } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:8:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Any 请求方式 package main import ( \"net/http\" \"github.com/gin-gonic/gin\" ) // Any 请求,判断请求方式 func indexHandler(c *gin.Context) { UserName := c.PostForm(\"username\") PassWord := c.PostForm(\"password\") // 利用 Request.Method 判断 请求方式 if c.Request.Method == \"POST\" { c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"username\": UserName, \"password\": PassWord, }) // 其他请求方式直接返回到 login.html } else { c.HTML(http.StatusOK, \"login.html\", nil) } } func main() { r := gin.Default() r.LoadHTMLGlob(\"templates/*\") r.Any(\"/index\", indexHandler) _ = r.Run(\":8888\") } HTML页面 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e登录页面\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"/index\" method=\"post\"\u003e 用户名: \u003cinput type=\"text\" name=\"username\" \u003e 密码: \u003cinput type=\"password\" name=\"password\"\u003e \u003cinput type=\"submit\" value=\"Submit\"\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2000-01-01","objectID":"/golang-web-note-2/:8:1","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 框架的渲染 ","date":"2000-01-01","objectID":"/golang-web-note-2/:9:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"HTML渲染 c.HTML(状态码, 模板文件, gin.H{模板内容}) Gin 可以渲染 单个 或者 多个 html 文件, 也可以渲染 整个 html 目录. LoadHTMLFiles 和 LoadHTMLGlob。 r.LoadHTMLFiles(\"/模板文件路径/1.html”, “/模板文件路径/2.html”) r.LoadHTMLGlob(\"/模板文件路径/*\") r.LoadHTMLGlob(\"/模板文件路径/**/*\"), /**/* 需要在 html 文件中引入 {{define 路径/x.html}} 声明. func main() { r := gin.Default() // 使用 LoadHTMLGlob() 或者 LoadHTMLFiles() // LoadHTMLFiles 指定单个或多个 文件 // r.LoadHTMLFiles(\"./templates/index.html\", \"./templates/login.html\") // LoadHTMLGlob 指定目录 ,使用通配符 r.LoadHTMLGlob(\"./templates/*\") r.GET(\"/index\", func(c *gin.Context) { c.HTML(http.StatusOK, \"index.html\", gin.H{ // 将内容映射到 index.html 对应的标签中. \"title\": \"Gin WebSite\", \"status\": http.StatusOK, }) }) // 启动 if err := r.Run(\":8888\"); err != nil { log.Fatalf(\"Server Run Failed err:%v\\n\", err) return } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:9:1","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Static 静态文件 当我们渲染的HTML文件中引用了静态文件时，我们只需要按照以下方式在渲染页面前调用gin.Static方法加载。 静态资源文件包含 (css, js, image 等) r.Static(\"/代码中配置的路径”,“实际文件的路径”) HTML 文件index.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e{{ .title }}\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e{{ .title }}\u003c/h1\u003e \u003cp\u003eStatus: {{ .status }}\u003c/p\u003e \u003cimg src={{ .img }} width=\"100\" height=\"150\" alt=\"golang\"/\u003e \u003c/body\u003e \u003c/html\u003e main func main() { r := gin.Default() r.LoadHTMLGlob(\"./templates/*\") //Static 相当于挂载目录, images 是挂载后的路径(代码), static 是实际的路径 r.Static(\"/images\", \"./static\") r.GET(\"/index\", func(c *gin.Context) { c.HTML(http.StatusOK, \"index.html\", gin.H{ \"title\": \"Gin WebSite\", \"status\": http.StatusOK, \"img\": \"/images/111.png\", }) }) // 启动 if err := r.Run(\":8888\"); err != nil { log.Fatalf(\"Server Run Failed err:%v\\n\", err) return } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:9:2","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"参数解析三种方法 ","date":"2000-01-01","objectID":"/golang-web-note-2/:10:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"QueryString 获取参数 QueryString 查询字符串, 对 http 请求所带的数据进行解析 http://localhost/search?name=xx\u0026city=xx (?号后面为QueryString 多组 key-value \u0026号分割) package main import ( \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // query_string func queryString(c *gin.Context) { // DefaultQuery 当 name 不存在值时,会返回设置的值 nameStr := c.DefaultQuery(\"name\", \"小炒肉\") // Query 当 city 不存在时, 会返回 空字符串 cityStr := c.Query(\"city\") // 配置JSON格式化 c.JSON(http.StatusOK, gin.H{ \"name\": nameStr, \"city\": cityStr, }) } func main() { r := gin.Default() //query string - 查询: http://127.0.0.1/search?name=小炒肉\u0026city=深圳 // ? 号后面为 query 值 key=value \u0026 分割 key=value r.GET(\"/search\", queryString) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\", err) } } 输出 { \"city\": \"深圳\", \"name\": \"小炒肉\" } ","date":"2000-01-01","objectID":"/golang-web-note-2/:10:1","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"form 表单 请求数据通过表单进行 提交 c.DefaultPostForm(\"key\", \"默认值\") package main import ( \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // form 表单 func formHandler(c *gin.Context) { // 当 name 不存在值时,会返回设置的值 nameStr := c.DefaultPostForm(\"name\", \"小炒肉\") // 当 city 不存在时, 会返回 空字符串 cityStr := c.PostForm(\"city\") // 配置JSON格式化输出 c.JSON(http.StatusOK, gin.H{ \"name\": nameStr, \"city\": cityStr, }) } func main() { r := gin.Default() // form表单: POST: http://127.0.0.1:8888/form (form-data, name=小炒肉 city=深圳) // POST 请求 form 表单获取数据. r.POST(\"/form\", formHandler) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\", err) } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:10:2","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"path 提交数据 c.Param(\"key\") 通过路径参数 提交数据 http://127.0.0.1:8888/path/add/1 , http://127.0.0.1:8888/path/delete/2 package main import ( \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // path 路径 func pathHandler(c *gin.Context) { actionStr := c.Param(\"action\") idStr := c.Param(\"id\") c.JSON(http.StatusOK, gin.H{ \"action\": actionStr, \"id\": idStr, }) } func main() { r := gin.Default() // path 参数: http://127.0.0.1:8888/path/add/1 // 通过路径参数 提交数据 r.GET(\"/path/:action/:id\", pathHandler) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\", err) } } 输出: # http://127.0.0.1:8888/path/add/1 { \"action\": \"add\", \"id\": \"1\" } # http://127.0.0.1:8888/path/delete/2 { \"action\": \"delete\", \"id\": \"2\" } ","date":"2000-01-01","objectID":"/golang-web-note-2/:10:3","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"参数绑定(ShouldBind) 为了能够更方便的获取参数, 基于请求的Content-Type识别请求数据类型并利用反射机制自动提取请求中QueryString、form表单、JSON、XML等参数到struct结构体中。Gin 框架 利用 ShouldBind()函数 强大的功能，它能够基于请求自动提取JSON、form表单和QueryString类型的数据，并把值绑定到指定的struct结构体对象。 ShouldBind会按照下面的顺序解析请求中的数据完成绑定: 如果是 GET 请求，只使用Form绑定引擎（query）。 如果是POST请求，首先检查content-type 是否为 JSON 或 XML，然后再使用 Form（form-data）。 ","date":"2000-01-01","objectID":"/golang-web-note-2/:11:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"ShouldBind JSON package main import ( \"fmt\" \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // 定义一个 用户结构体 type UserDB struct { // ShouldBind 通过对 binding 这个 tag 的参数进行 限制 User string `form:\"user\" json:\"user\" binding:\"required\"` Password string `form:\"password\" json:\"password\" binding:\"required\"` } // 绑定 json 格式 func ShouldBindJson(c *gin.Context) { // 创建一个 UserDB结构体类型的变量 var userdb UserDB // 绑定 userdb , 通过 结构体 binding 的 tag 限制 if err := c.ShouldBind(\u0026userdb); err == nil { fmt.Printf(\"UserDB Info: %#v\\n\", userdb) c.JSON(http.StatusOK, gin.H{ \"user\": userdb.User, \"password\": userdb.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) } } func main() { r := gin.Default() // shouldBind json r.POST(\"/bindJson\", ShouldBindJson) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\", err) } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:11:1","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"ShouldBind Form表单 Form 表单与 Json 写法相同, ShouldBind 会根据 Content-Type 自行选择绑定器 package main import ( \"fmt\" \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // 定义一个 用户结构体 type UserDB struct { User string `form:\"user\" json:\"user\" binding:\"required\"` Password string `form:\"password\" json:\"password\" binding:\"required\"` } // 绑定 form 表单 func ShouldBindForm(c *gin.Context) { var userdb UserDB // Form 表单跟 JSON 一样写法 // ShouldBind 会根据请求的Content-Type自行选择绑定器 if err := c.ShouldBind(\u0026userdb); err == nil { fmt.Printf(\"UserDB Info: %#v\\n\", userdb) c.JSON(http.StatusOK, gin.H{ \"user\": userdb.User, \"password\": userdb.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) } } func main() { r := gin.Default() // shouldBind form r.POST(\"bindForm\", ShouldBindForm) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\", err) } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:11:2","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"ShouldBind QueryString package main import ( \"fmt\" \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // 定义一个 用户结构体 type UserDB struct { User string `form:\"user\" json:\"user\" binding:\"required\"` Password string `form:\"password\" json:\"password\" binding:\"required,min=4,max=20\"` } // 绑定 QueryString func ShouldBindQuery(c *gin.Context) { var userdb UserDB // ShouldBind 会根据请求的Content-Type自行选择绑定器 if err := c.ShouldBind(\u0026userdb); err == nil { fmt.Printf(\"UserDB Info: %#v\\n\", userdb) c.JSON(http.StatusOK, gin.H{ \"user\": userdb.User, \"password\": userdb.Password, }) } else { c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) } } func main() { r := gin.Default() // shouldBind QueryString // http://127.0.0.1:8888/bindQuery?user=小炒肉\u0026password=123456 r.GET(\"/bindQuery\", ShouldBindQuery) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\", err) } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:11:3","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"shouldBind 的例子 index.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e{{ .title }}\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e{{ .title }}\u003c/h1\u003e \u003cp\u003eStatus: {{ .status }}\u003c/p\u003e \u003cimg src={{ .img }} alt=\"golang\"/\u003e \u003c/body\u003e \u003c/html\u003e login.html \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e登录页面\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"/user/login\" method=\"post\"\u003e 用户名: \u003cinput type=\"text\" name=\"username\" \u003e \u003cbr\u003e 密码: \u003cinput type=\"password\" name=\"password\"\u003e \u003cinput type=\"submit\" value=\"Submit\"\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e main.go package main import ( \"net/http\" \"github.com/gin-gonic/gin\" ) type UserDB struct { UserName string `form:\"username\" binding:\"required\"` PassWord string `form:\"password\" binding:\"required\"` } func loginHandler(c *gin.Context) { // 判断请求方式为 POST if c.Request.Method == \"POST\" { // 定义一个 UserDB 结构体类型的变量 var user UserDB // ShouldBind : Gin 会尝试根据 Content-Type 推断如何绑定。 // 使用 ShouldBind 会自动解析 json, form, xml 等格式。 if err := c.ShouldBind(\u0026user); err != nil { c.JSON(http.StatusBadRequest, gin.H{\"error\": err.Error()}) return } // 模拟验证 if user.UserName != \"jicki\" || user.PassWord != \"123456\" { c.JSON(http.StatusUnauthorized, gin.H{ \"Code\": http.StatusUnauthorized, \"Msg\": \"unauthorized\", }) return } c.HTML(http.StatusOK, \"index.html\", gin.H{ \"title\": \"首页\", \"status\": \"you are logged in\", \"img\": \"/user/static/1.png\", }) } else { c.HTML(http.StatusOK, \"login.html\", nil) } } func main() { r := gin.Default() r.LoadHTMLGlob(\"templates/*\") r.Static(\"/user/static\", \"./static\") userGroup := r.Group(\"/user\") { userGroup.GET(\"/login\", loginHandler) userGroup.POST(\"/login\", loginHandler) } _ = r.Run(\":8888\") } ","date":"2000-01-01","objectID":"/golang-web-note-2/:11:4","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"重定向 ","date":"2000-01-01","objectID":"/golang-web-note-2/:12:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"HTML 重定向 HTTP 重定向 支持内部、外部重定向。 c.Redirect(\"状态码\", \"地址\") package main import ( \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // 重定向 func main() { r := gin.Default() // Redirect 301跳转 // 外部跳转 r.GET(\"/redirect1\", func(c *gin.Context) { // StatusMovedPermanently error code 301 c.Redirect(http.StatusMovedPermanently, \"https://www.jicki.cn/\") }) // 内部跳转 r.GET(\"/redirect2\", func(c *gin.Context) { // StatusMovedPermanently error code 301 c.Redirect(http.StatusMovedPermanently, \"/redirect1\") }) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\\n\", err) } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:12:1","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"路由重定向 路由重定向, 使用HandleContext package main import ( \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) // 重定向 func main() { r := gin.Default() // HandleContext r.GET(\"/redirect3\", func(c *gin.Context) { // 指定重定向的URL c.Request.URL.Path = \"/redirect4\" r.HandleContext(c) }) r.GET(\"/redirect4\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Redirect\": \"redirect4\", \"code\": http.StatusOK, }) }) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Error: %v\\n\", err) } } ","date":"2000-01-01","objectID":"/golang-web-note-2/:12:2","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 路由 与 路由组 ","date":"2000-01-01","objectID":"/golang-web-note-2/:13:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"普通的路由请求 import ( \"net/http\" \"github.com/gin-gonic/gin\" ) func sayHello(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"Msg\": \"hello 1\", }) } // 路由 func main() { r := gin.Default() // 普通的路由, 指定请求方式 GET (路径, 函数) r.GET(\"/hello1\", sayHello) // 普通的路由, 指定请求方式 POST (路径, 匿名函数) r.POST(\"/hello2\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"Msg\": \"hello 2\", }) }) // 普通的路由, 匹配所有的请求方式 r.Any(\"/hello3\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"Msg\": \"hello 3\", }) }) // 普通路由, 不匹配路径的路由默认返回 r.NoRoute(func(c *gin.Context) { c.JSON(404, gin.H{ \"Code\": 404, \"Msg\": \"404 Not Found\", }) }) _ = r.Run(\":8888\") } ","date":"2000-01-01","objectID":"/golang-web-note-2/:13:1","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"路由组(Group) 我们可以将拥有共同URL前缀的路由划分为一个路由组。习惯性一对{}包裹同组的路由，这只是为了看着清晰，你用不用{}包裹功能上没什么区别。 通常我们将路由分组用在划分业务逻辑或划分API版本. package main import ( \"net/http\" \"github.com/gin-gonic/gin\" ) // 路由 func main() { r := gin.Default() // 路由组, 以组的形式定义功能组 // Url http://127.0.0.1:888/v1/student v1Group := r.Group(\"/v1\") { v1Group.POST(\"/student\", func(c *gin.Context) { c.JSON(200, gin.H{ \"Code\": 200, \"Msg\": \"v1 Version Create\", }) }) v1Group.DELETE(\"/student\", func(c *gin.Context) { c.JSON(200, gin.H{ \"Code\": 200, \"Msg\": \"v1 Version Delete\", }) }) v1Group.GET(\"/student\", func(c *gin.Context) { c.JSON(200, gin.H{ \"Code\": 200, \"Msg\": \"v1 Version Search\", }) }) v1Group.PUT(\"/student\", func(c *gin.Context) { c.JSON(200, gin.H{ \"Code\": 200, \"Msg\": \"v1 Version Update\", }) }) } _ = r.Run(\":8888\") } ","date":"2000-01-01","objectID":"/golang-web-note-2/:13:2","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 上传文件 ","date":"2000-01-01","objectID":"/golang-web-note-2/:14:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"单个文件上传 package main import ( \"fmt\" \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) func main() { r := gin.Default() r.LoadHTMLGlob(\"./upload/*\") r.GET(\"/upload\", func(c *gin.Context) { c.HTML(http.StatusOK, \"upload.html\", nil) }) // 上传文件后的操作 r.POST(\"/upload1\", func(c *gin.Context) { // 获取上传后文件的 对象. // c.FormFile(\"Form 表单中 name=\"filename\" 的名字\") file, err := c.FormFile(\"filename\") if err != nil { c.JSON(500, gin.H{ \"Code\": 500, \"Error\": err.Error(), }) } // 将文件保存到指定路径 filePath := fmt.Sprintf(\"./%s\", file.Filename) err = c.SaveUploadedFile(file, filePath) if err == nil { c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"Data\": filePath, }) } }) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Err: %v\\n\", err) } } HTML 文件 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eGin 上传文件\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"/upload1\" method=\"post\" enctype=\"multipart/form-data\"\u003e 单个文件: \u003cinput type=\"file\" name=\"filename\"\u003e \u003cinput type=\"submit\"\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2000-01-01","objectID":"/golang-web-note-2/:14:1","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"多个文件上传 c.MultipartForm() 方法. package main import ( \"fmt\" \"log\" \"net/http\" \"github.com/gin-gonic/gin\" ) func main() { r := gin.Default() r.LoadHTMLGlob(\"./upload/*\") r.GET(\"/upload\", func(c *gin.Context) { c.HTML(http.StatusOK, \"upload.html\", nil) }) // 多个文件上传文件后的操作 r.POST(\"/upload2\", func(c *gin.Context) { // 获取多个上传后的 对象. // c.MultipartForm() data, err := c.MultipartForm() if err != nil { c.JSON(500, gin.H{ \"Code\": 500, \"Error\": err.Error(), }) } // 将文件保存到指定路径 // data.File[\"Form 表单中 name=\"filename\" 的名字\"] files := data.File[\"filename\"] // 循环遍历所有文件 for _, file := range files { filesPath := fmt.Sprintf(\"./%s\", file.Filename) err = c.SaveUploadedFile(file, filesPath) if err == nil { c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"Data\": filesPath, }) } } }) err := r.Run(\":8888\") if err != nil { log.Fatalf(\"Server Run Err: %v\\n\", err) } } html 文件 \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003eGin 上传文件\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cform action=\"/upload2\" method=\"post\" enctype=\"multipart/form-data\"\u003e 多个文件: \u003cinput type=\"file\" name=\"filename\" multiple=\"multiple\" /\u003e \u003cinput type=\"submit\" /\u003e \u003c/form\u003e \u003c/body\u003e \u003c/html\u003e ","date":"2000-01-01","objectID":"/golang-web-note-2/:14:2","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 中间件 Gin框架允许开发者在处理请求的过程中，加入用户自己的钩子（Hook）函数。这个钩子函数就叫中间件，中间件适合处理一些公共的业务逻辑，比如登录校验、日志打印、耗时统计等。 Gin中的中间件必须是一个gin.HandlerFunc类型。使用 r.Use(HandlerFunc) 调用中间件，可在全局, 路由组, 或者 单路由的某一个函数前。 package main import ( \"net/http\" \"github.com/gin-gonic/gin\" ) func ShowIndexHandler(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Code\": 200, \"Msg\": \"首页\", }) } func ShowPingHandler(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Code\": 200, \"Msg\": \"购物页\", }) } func authLogin(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Code\": 200, \"Msg\": \"Login OK\", }) } func userInfoHandler(c *gin.Context) { userInfo := struct { Name string Age int }{Name: \"小炒肉\", Age: 20} c.JSON(http.StatusOK, gin.H{ \"Code\": 200, \"Data\": userInfo, }) } // gin 中间件 func main() { r := gin.Default() // 创建一个路由组 ShopGroup := r.Group(\"/show\") { ShopGroup.GET(\"/index\", ShowIndexHandler) ShopGroup.GET(\"/show\", ShowPingHandler) } // 创建另一个路由组 // 可以在`r.Group(\"/user\",authLogin)` 也可以写到第一行 UserGroup := r.Group(\"/user\", authLogin) { // 路由组内 执行中间件 // UserGroup.Use(authLogin) UserGroup.GET(\"/info\", userInfoHandler) } _ = r.Run(\":8888\") } package main import ( \"fmt\" \"net/http\" \"time\" \"github.com/gin-gonic/gin\" ) func indexHandle(c *gin.Context) { fmt.Println(\"start indexHandle\") c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"Msg\": \"index OK\", }) } // 定义一个中间件, 中间件必须包含 gin.Context func middle(c *gin.Context) { // 设置一个值,传递到其他的 context 中 c.Set(\"name\", \"小炒肉\") fmt.Println(\"Middle in\") // 统计函数开始的时间 start := time.Now() // c.Next 执行下一个函数, 既 middle 后续的一个函数 c.Next() // 计算 函数消耗的时间 cost := time.Since(start) fmt.Printf(\"Cost = %v \\n\", cost) fmt.Println(\"Middle out\") } // 定义一个中间件, 中间件必须包含 gin.Context func m2(c *gin.Context) { fmt.Println(\"M2 in\") // 接收 Context 中 set 的值 name, ok := c.Get(\"name\") if !ok { // 阻止 执行下一个函数 c.Abort() } else { // c.Next 执行下一个函数 fmt.Println(name) c.Next() } fmt.Println(\"M2 out\") } // 判断是否登录的中间件 (一般都使用闭包的方式来写中间件) func authMiddleWere(auth bool) gin.HandlerFunc { // 操作认证查询的工作 // 比如查询数据库等 return func(c *gin.Context) { // 判断是否登录 //1. if 是登录用户 //2. 为真 执行 c.Next() //3. 为假 执行 c.Abort() if auth { c.Next() } else { c.Abort() } } } func main() { r := gin.Default() // 全局中使用中间件, 注意顺序 r.Use(middle, m2, authMiddleWere(true)) r.GET(\"/index\", indexHandle) _ = r.Run(\":8888\") } ","date":"2000-01-01","objectID":"/golang-web-note-2/:15:0","tags":["golang"],"title":"Gin 框架","uri":"/golang-web-note-2/"},{"categories":["golang"],"content":"Gin 框架集成 Zap 日志库","date":"2000-01-01","objectID":"/golang-web-gin-zap-logs/","tags":["golang"],"title":"Gin 集成 Zap 日志库","uri":"/golang-web-gin-zap-logs/"},{"categories":["golang"],"content":"Gin Zap 集成 Gin 框架中配置使用 Zap 日志库。 Gin 框架输出的日志, 包括项目中自己输出的日志, 以及 Gin 框架本身的日志输出。 Gin 框架的日志 Logger 是在我们调用 gin.Default() 的时候, 会加载两个中间件 engine.Use(Logger(), Recovery()) Logger() - 中间件是将 Gin 框架中日志输出到标准输出. Recovery() - 中间件是 程序出现 panic 的时候尝试恢复程序,然后将错误代码写入到日志中. Gin 框架中要集成 Zap 日志库 就需要重新实现 Logger() 和 Recovery() 两个中间件函数 如下转载 七米 老师的代码 // GinLogger 接收gin框架默认的日志 func GinLogger(logger *zap.Logger) gin.HandlerFunc { return func(c *gin.Context) { start := time.Now() path := c.Request.URL.Path query := c.Request.URL.RawQuery c.Next() cost := time.Since(start) logger.Info(path, zap.Int(\"status\", c.Writer.Status()), zap.String(\"method\", c.Request.Method), zap.String(\"path\", path), zap.String(\"query\", query), zap.String(\"ip\", c.ClientIP()), zap.String(\"user-agent\", c.Request.UserAgent()), zap.String(\"errors\", c.Errors.ByType(gin.ErrorTypePrivate).String()), zap.Duration(\"cost\", cost), ) } } // GinRecovery recover掉项目可能出现的panic func GinRecovery(logger *zap.Logger, stack bool) gin.HandlerFunc { return func(c *gin.Context) { defer func() { if err := recover(); err != nil { // Check for a broken connection, as it is not really a // condition that warrants a panic stack trace. var brokenPipe bool if ne, ok := err.(*net.OpError); ok { if se, ok := ne.Err.(*os.SyscallError); ok { if strings.Contains(strings.ToLower(se.Error()), \"broken pipe\") || strings.Contains(strings.ToLower(se.Error()), \"connection reset by peer\") { brokenPipe = true } } } httpRequest, _ := httputil.DumpRequest(c.Request, false) if brokenPipe { logger.Error(c.Request.URL.Path, zap.Any(\"error\", err), zap.String(\"request\", string(httpRequest)), ) // If the connection is dead, we can't write a status to it. c.Error(err.(error)) // nolint: errcheck c.Abort() return } if stack { logger.Error(\"[Recovery from panic]\", zap.Any(\"error\", err), zap.String(\"request\", string(httpRequest)), zap.String(\"stack\", string(debug.Stack())), ) } else { logger.Error(\"[Recovery from panic]\", zap.Any(\"error\", err), zap.String(\"request\", string(httpRequest)), ) } c.AbortWithStatus(http.StatusInternalServerError) } }() c.Next() } } ","date":"2000-01-01","objectID":"/golang-web-gin-zap-logs/:0:0","tags":["golang"],"title":"Gin 集成 Zap 日志库","uri":"/golang-web-gin-zap-logs/"},{"categories":["golang"],"content":"Gin New 将两个中间件函数添加到 Gin 框架实例中 func main() { // r := gin.Default() r := gin.New() r.Use(GinLogger(), GinRecovery()) r.GET(\"/\", func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \"Code\": http.StatusOK, \"Msg\": \"OK\", }) }) _ = r.Run() } ","date":"2000-01-01","objectID":"/golang-web-gin-zap-logs/:1:0","tags":["golang"],"title":"Gin 集成 Zap 日志库","uri":"/golang-web-gin-zap-logs/"},{"categories":["golang"],"content":"Go 使用 Jwt 实现 Token 认证","date":"2000-01-01","objectID":"/golang-web-jwt-token/","tags":["golang"],"title":"Go Jwt With Token","uri":"/golang-web-jwt-token/"},{"categories":["golang"],"content":"JWT JWT全称JSON Web Token是一种跨域认证解决方案, 属于一个开放的标准, 它规定了一种 Token 实现方式, 目前多用于前后端分离项目和OAuth2.0业务场景下。 JWT 本身没有定义任何技术实现, 它只是定义了一种基于Token的会话管理的规则, 涵盖Token需要包含的标准内容和Token的生成过程, 特别适用于分布式的单点登录(OSS) 场景。 JWT 优势 JWT 拥有基于Token的会话管理方式所拥有的一切优势, 不依赖 Cookie, 防止CSRF攻击, 也可以在禁止 Cookie 的浏览器环境中使用。 服务端不需要存储 Session, 服务端认证鉴权业务扩展方便, 避免存储Session 所需要配置的如: Redis等组件, 降低系统架构复杂度。 JWT 劣势 由于Token鉴权有效期存储于 JWT 中, 所以 JWT Token 一段签发, 就会在有效期内一直有效, 无法在服务端进行中止, 只能通过客户端进行废除。 ","date":"2000-01-01","objectID":"/golang-web-jwt-token/:0:0","tags":["golang"],"title":"Go Jwt With Token","uri":"/golang-web-jwt-token/"},{"categories":["golang"],"content":"JWT 组成 一个 JWT 有三个部分组成, 以. 进行分割. 这三部分都是单独经过 Base64 编码。 Header 头部 header 中存储了所使用的加密算法和Token类型. JSON 格式: {\"alg\": \"HS256\", \"typ\": \"JWT\"} Payload 负载 payload 中包含官方提供的7个字段. (也可以自定义字段) JSON 格式: iss(issuer): 签名人 exp(expiration time): 过期时间 sub(subject): 主题 aud(audience): 受众 nbf(Not Before): 生效时间 iat(Issued At): 签发时间 jti(JWT ID): 编号。 Signature 签名 signature 是对前面两部分(Header、Payload) 进行签名, 防止数据篡改。 签名的过程首先需要有一个定义的密钥(Secret)。然后使用Header 里定义的算法进行签名。(默认为 HMAC SHA256) ","date":"2000-01-01","objectID":"/golang-web-jwt-token/:1:0","tags":["golang"],"title":"Go Jwt With Token","uri":"/golang-web-jwt-token/"},{"categories":["golang"],"content":"Go 使用 JWT Go 语言使用 jwt-go 第三方库实现 JWT 的生成 和 解析。 package jwt import ( \"errors\" \"time\" \"github.com/dgrijalva/jwt-go\" ) // 定义 Token 的过期时间 2 小时 const TokenExpireDuration = time.Hour * 2 // 定义一个 密钥 Secret var mySecret = []byte(\"小炒肉的秘密\") // 定义一个结构体用于自定义的 payload 信息 type MyClaims struct { UserID int64 `json:\"user_id\"` UserName string `json:\"username\"` // jwt.standardClaims 包含 payload 官方提供的7个字段 jwt.StandardClaims } // GenToken: 生成 Token 的函数 func GenToken(userID int64, username string) (string, error) { // 创建一个 MyClaims 的实例 payload c := MyClaims{ UserID: userID, UserName: username, StandardClaims: jwt.StandardClaims{ // 过期时间 ExpiresAt: time.Now().Add(TokenExpireDuration).Unix(), // 签发人 Issuer: \"DouHu\", }, } // 创建 Token ( jwt.SigningMethodES256 是加密算法 ) token := jwt.NewWithClaims(jwt.SigningMethodHS256, c) // 使用自己定义 密钥 Secret 进行加密返回完整的 token return token.SignedString(mySecret) } // ParseToken: 解析 JWT 的函数 func ParseToken(tokenString string) (*MyClaims, error) { // 定义一个 var mc = new(MyClaims) // 解析 Token 将传入的 tokenString 解析到 \u0026MyClaims 这个结构体中去 token, err := jwt.ParseWithClaims(tokenString, mc, func(token *jwt.Token) (interface{}, error) { return mySecret, nil }) if err != nil { return nil, err } if token.Valid { return mc, nil } return nil, errors.New(\"ParseToken invalid token\") } ","date":"2000-01-01","objectID":"/golang-web-jwt-token/:2:0","tags":["golang"],"title":"Go Jwt With Token","uri":"/golang-web-jwt-token/"},{"categories":["golang"],"content":"Gin JWT 认证 在 Gin 框架中, 实现一个 认证 JWT Token 的中间件。后续在需要认证的路由中直接嵌入这个中间件就可以。 // 认证 JWT Token 中间件 func JWTAuthMiddleware() func(c *gin.Context) { return func(c *gin.Context) { // 客户端携带 Token 的三种方式 1.请求头. 2. 请求体. 3. URL中. // 请求头中: 既 Header: Authorization 下 Bearer 后 // Authorization: Bearer token(header).token(payload).token(signature) authHeader := c.Request.Header.Get(\"Authorization\") // 如果为空 if authHeader == \"\" { c.JSON(http.StatusOK, gin.H{ \"Code\": 2003, \"Msg\": \"Header Authorization 为空\", \"Data\": \"nil\", }) c.Abort() return } // 如果 authHeader 不为空, Split 以空格 切割为2部分 parts := strings.SplitN(authHeader, \" \", 2) // 如果切割后 不止2部分, 或者 第1 部分未 包含 Bearer if !(len(parts) == 2 \u0026\u0026 parts[0] == \"Bearer\") { c.JSON(http.StatusOK, gin.H{ \"Code\": 2004, \"Msg\": \"Header Bearer 格式有误\", \"Data\": \"nil\", }) c.Abort() return } // 解析 parts 第2部分的 JWT Token // jwt.ParseToken() 是上面我们实现的一个解析Jwt Token的函数 mc, err := jwt.ParseToken(parts[1]) if err != nil { c.JSON(http.StatusOK, gin.H{ \"Code\": 2005, \"Msg\": \"无效的 Token\", \"Data\": \"nil\", }) c.Abort() return } // 将解析后获取的信息, 保存到 上下文 (c *gin.Context) 中 c.Set(\"userID\", mc.UserID) // 执行下一个函数 c.Next() } } ","date":"2000-01-01","objectID":"/golang-web-jwt-token/:3:0","tags":["golang"],"title":"Go Jwt With Token","uri":"/golang-web-jwt-token/"},{"categories":["golang"],"content":"Go configuration with fangs","date":"2000-01-01","objectID":"/golang-web-viper-config/","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":" Viper Viper Viper 是 Go 语言项目中相对完善的一个配置文件解决方案。 Viper 可以处理所有类型的配置需求和格式。 注: 目前 Viper 是大小写不敏感的。 Viper 特性: 设置默认值 支持 JSON、TOML、YAML、INT、HCL、envfile、Java properties 格式的配置文件中读取配置信息。 实时监控与重新读取加载配置文件, 既热加载。( 可选 ) 环境变量读取配置。 支持从远程配置系统 (etcd、zk、Consul) 读取并监控配置信息变化。 命令行标准输入读取配置。 buffer 里读取配置。 支持直接通过代码 Set 配置值。 Viper 读取配置的优先级 显示调用 Set 设置值 ( 代码中通过 Viper.Set 设置值 ) 程序命令行参数 ( flag - 程序运行时指定的参数 ) 环境变量 配置文件 远程 key/value 存储系统 默认值 ","date":"2000-01-01","objectID":"/golang-web-viper-config/:0:0","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":"安装 Viper go get and go module go get -u github.com/spf13/viper ","date":"2000-01-01","objectID":"/golang-web-viper-config/:1:0","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":"Viper 实践 设置默认值, 在项目中设置默认值非常的有必要。 viper.SetDefault(\"ConfDir\", \"./conf/\") 读取配置文件 Viper 支持搜索多个路径, 但是目前 Viper 只支持读取一个配置文件。 Viper 没有默认的配置文件搜索路径, 需要程序自行定义。 func main() { // 读取配置文件 // SetConfigFile 设置完整配置文件 //viper.SetConfigFile(\"config.yaml\") // SetConfigName 设置配置文件名称 - 不需要定义文件扩展名 viper.SetConfigName(\"config\") // SetConfigType 设置配置文件类型 - 专用于 远程配置中心定义类型 //viper.SetConfigType(\"yaml\") // AddConfigPath 添加配置文件读取目录, 支持添加多个 // 这里注意 搜索路径是从上到下搜索, 如果下面有配置写入会生成 viper.AddConfigPath(\"./conf/\") viper.AddConfigPath(\"./\") // ReadInConfig 查找并读取配置文件 if err := viper.ReadInConfig(); err != nil { panic(fmt.Errorf(\"Read Config Error %s \\n\", err)) } } 写入配置文件 写入配置文件一般用于程序运行时需要更新配置文件时使用。 func main() { // 写入配置文件 // WriteConfig 将当前配置 写入/覆盖 AddConfigPath 和 SetConfigName 设置的预定路径。 if err := viper.WriteConfig(); err != nil { fmt.Printf(\"WriteConfig Failed Error %s \\n\", err) return } // SafeWriteConfig 将当前配置 写入设置的预定路径, 如果存在不会覆盖 而是报错。 if err := viper.SafeWriteConfig(); err != nil { fmt.Printf(\"Safe WriteConfig Failed Error %s \\n\", err) return } // WriteConfigAs 将当前配置 写入/覆盖 到自定义的路径中 if err := viper.WriteConfigAs(\"./config\"); err != nil { fmt.Printf(\"WriteConfig As Failed Error %s \\n\", err) return } // SafeWriteConfigAs 将当前配置 写入 到自定义的路径中, 如果文件存在不会覆盖 而是报错。 if err := viper.SafeWriteConfigAs(\"./config\"); err != nil { fmt.Printf(\"Safe WriteConfig As Failed Error %s \\n\", err) return } } 监控并重新读取配置文件 Viper 支持在运行时实时读取配置文件, 并且动态加载配置。 func main(){ // WatchConfig 监控配置文件变化 viper.WatchConfig() // OnConfigChange 是监控配置文件有变化后调用的一个回调函数 viper.OnConfigChange(func(e fsnotify.Event) { fmt.Println(\"Config file changed:\", e.Name) }) } 应用使用例子 func InitViper() { // 设置默认值 viper.SetDefault(\"ConfDir\", \"./conf/\") // 读取配置文件 // SetConfigFile 设置完整配置文件 //viper.SetConfigFile(\"config.yaml\") // SetConfigName 设置配置文件名称 - 不需要定义文件扩展名 viper.SetConfigName(\"config\") // SetConfigType 设置配置文件类型 - 专用于 远程配置中心定义类型 //viper.SetConfigType(\"yaml\") // AddConfigPath 添加配置文件读取目录, 支持添加多个 viper.AddConfigPath(\"./conf/\") // ReadInConfig 查找并读取配置文件 if err := viper.ReadInConfig(); err != nil { panic(fmt.Errorf(\"Read Config Error %s \\n\", err)) } // WatchConfig 监控配置文件变化 viper.WatchConfig() // OnConfigChange 是监控配置文件有变化后调用的一个回调函数 viper.OnConfigChange(func(e fsnotify.Event) { fmt.Println(\"配置文件发生变化:\", e.Name) }) // 写入配置文件 // SafeWriteConfig 将当前配置 写入设置的预定路径, 如果存在不会覆盖 而是报错。 if err := viper.SafeWriteConfig(); err != nil { fmt.Printf(\"Safe WriteConfig Failed Error %s \\n\", err) return } } func main() { InitViper() r := gin.Default() r.GET(\"/version\", func(c *gin.Context) { c.String(http.StatusOK, viper.GetString(\"version\")) }) addr := fmt.Sprintf(\"%s:%s\", viper.GetString(\"host\"), viper.GetString(\"port\")) if err := r.Run(addr); err != nil { fmt.Printf(\"Gin Run Failed Error %v \\n\", err) return } } 别名 Alias 将配置项中某个键设置别名。 // 将lodu 与 Verbose 注册别名, 既关联在一起 viper.RegisterAlias(\"loud\", \"Verbose\") // 设置其中一个配置项的 值 viper.Set(\"loud\", true) // 如下 Get 两个配置项的值都会等于 true viper.GetBool(\"loud\") viper.GetBool(\"Verbose\") 环境变量 SetEnvPrefix() - 定义以什么为前缀的 环境变量 这里设置的 环境变量前缀, 强制转换为 大写 。 BindEnv() - 绑定一个环境变量 AllowEmptyEnv(bool) - 是否允许为空 注: 环境变量的使用建议全部使用大写, viper 也会强制转换为大写 func viperEnv() { viper.SetEnvPrefix(\"VIP\") if err := viper.BindEnv(\"NAME\"); err != nil { fmt.Printf(\"BindEnv Failed Error %v \\n\", err) return } if err := os.Setenv(\"VIP_NAME\", \"小炒肉\"); err != nil { fmt.Printf(\"Os Set Env Failed Error %v \\n\", err) return } fmt.Printf(\"ViperEnv Get VIP_NAME = [%v] \\n\", viper.Get(\"NAME\")) } func main() { viperEnv() } # 输出结果 ViperEnv Get VIP_NAME = [小炒肉] 使用 Flags Viper 支持 Cobra 库的 Pflag func viperPFlag() { // pflag 设置 pflag.Int(\"flagname\", 123, \"message flag name\") // 生效配置 pflag.Parse() // 绑定 pflag if err := viper.BindPFlags(pflag.CommandLine); err != nil { fmt.Printf(\"Bind PFlags Failed Error %v \\n\", err) return } fmt.Printf(\"flagname = [%v] \\n\", viper.GetInt(\"flagname\")) } func main() { viperPFlag() } # 运行命令 go run main.go --flagname 99998 输出结果 flagname = [99998] ","date":"2000-01-01","objectID":"/golang-web-viper-config/:2:0","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":"定义配置源 func yamlExample() { viper.SetConfigType(\"yaml\") // 定义一个 yaml 的配置源,这里注意 yaml 对格式支持很严格,要对齐首行 var yamlConfig = []byte(` host: \"127.0.0.1\" port: 9999 version: \"v1.0\" `) // 通过 io.Reader 读取 buffer 内的配置 config := bytes.NewBuffer(yamlConfig) if err := viper.ReadConfig(config); err != nil { fmt.Printf(\"Read Buffer Config Failed Error %s \\n\", err) return } // Get 配置信息 fmt.Printf(\"Version: %s \\n\", viper.Get(\"version\")) } func main() { yamlExample() } # 输出结果 Version: v1.0 ","date":"2000-01-01","objectID":"/golang-web-viper-config/:2:1","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":"远程 Key/Value 存储 读取 Key/Value 存储中的配置信息。 需要 匿名导入 viper/remote 包如: import _ \"github.com/spf13/viper/remote\" etcd etcd 未加密 http viper.AddRemoteProvider(\"etcd\", \"http://127.0.0.1:4001\", \"/config/\") // 设置 etcd 中存储类型。 viper.SetConfigType(\"json\") err := viper.ReadRemoteConfig() Consul viper.AddRemoteProvider(\"consul\", \"http://127.0.0.1:8500\", \"MY_CONSUL_KEY\") viper.SetConfigType(\"json\") err := viper.ReadRemoteConfig() ","date":"2000-01-01","objectID":"/golang-web-viper-config/:2:2","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":"获取 Viper 值 获取 Viper 的值有如下方法: ( Get 方法查询不到值时 会返回空值 ) func (v *Viper) Get(key string) interface{} func (v *Viper) GetBool(key string) bool func (v *Viper) GetFloat64(key string) float64 func (v *Viper) GetInt(key string) int func (v *Viper) GetIntSlice(key string) []int func (v *Viper) GetString(key string) string func (v *Viper) GetStringMap(key string) map[string]interface{} func (v *Viper) GetStringMapString(key string) map[string]string func (v *Viper) GetStringSlice(key string) []string func (v *Viper) GetTime(key string) time.Time func (v *Viper) GetDuration(key string) time.Duration func (v *Viper) IsSet(key string) bool func (v *Viper) AllSettings() map[string]interface{} 访问嵌套的值 支持深度嵌套键值的格式化路径。如: JSON { \"host\": { \"address\": \"127.0.0.1\", \"port\": 9999 }, \"datastore\": { \"mysql\": { \"host\": \"127.0.0.1\", \"port\": 3306 }, \"redis\": { \"host\": \"127.0.0.1\", \"port\": 6379 } } } 访问嵌套例子 func jsonExample() { viper.SetConfigType(\"json\") var jsonConfig = []byte(` { \"host\": { \"address\": \"127.0.0.1\", \"port\": 9999 }, \"datastore\": { \"mysql\": { \"host\": \"127.0.0.1\", \"port\": 3306 }, \"redis\": { \"host\": \"127.0.0.1\", \"port\": 6379 } } } `) // 通过 io.Reader 读取 buffer 内的配置 config := bytes.NewBuffer(jsonConfig) if err := viper.ReadConfig(config); err != nil { fmt.Printf(\"Read Buffer Config Failed Error %s \\n\", err) return } // 访问嵌套 通过 [ . ] 来直接获取 fmt.Printf(\"Mysql: [%s:%v] \\nRedis:[%s:%v] \\n\", viper.Get(\"datastore.mysql.host\"), viper.Get(\"datastore.mysql.port\"), viper.Get(\"datastore.redis.host\"), viper.Get(\"datastore.redis.port\"), ) } # 输出结果 Mysql: [127.0.0.1:3306] Redis:[127.0.0.1:6379] ","date":"2000-01-01","objectID":"/golang-web-viper-config/:2:3","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":"反序列化 需要注意 Structt 中的 tag 需要统一配置为 mapstructure 使用 viper.Unmarshal(\u0026c) 反序列化到 结构体实例中。 配置文件: host:\"127.0.0.1\"port:9999version:\"v1.1\"mysql:host:\"127.0.0.1\"port:3306dbname:\"viper_demo\" 代码如下: type Config struct { // 结构体 viper 配置文件 tag 统一使用 mapstructure 这个tag Host string `mapstructure:\"host\"` Port int `mapstructure:\"port\"` Version string `mapstructure:\"version\"` MysqlConfig `mapstructure:\"mysql\"` } type MysqlConfig struct { Host string `mapstructure:\"host\"` Port int `mapstructure:\"port\"` DBName string `mapstructure:\"dbname\"` } // 定义一个 Config 类型的实例 c var c Config func InitViper() { // 设置默认值 viper.SetDefault(\"ConfDir\", \"./conf/\") // SetConfigName 设置配置文件名称 - 不需要定义文件扩展名 viper.SetConfigName(\"config\") // SetConfigType 设置配置文件类型 - 专用于 远程配置中心定义类型 //viper.SetConfigType(\"yaml\") // AddConfigPath 添加配置文件读取目录, 支持添加多个 viper.AddConfigPath(\"./conf/\") // ReadInConfig 查找并读取配置文件 if err := viper.ReadInConfig(); err != nil { panic(fmt.Errorf(\"Read Config Error %s \\n\", err)) } // WatchConfig 监控配置文件变化 viper.WatchConfig() // OnConfigChange 是监控配置文件有变化后调用的一个回调函数 viper.OnConfigChange(func(e fsnotify.Event) { fmt.Println(\"配置文件发生变化:\", e.Name) }) // 将配置 反序列化到 实例 c 中 if err := viper.Unmarshal(\u0026c); err != nil { fmt.Printf(\"Viper Unmarshal Failed Error %v \\n\", err) return } } func main() { InitViper() fmt.Printf(\"c = [%#v]\\n\", c) } # 输出结果 c = [main.Config{Host:\"127.0.0.1\", Port:9999, Version:\"v1.1\", \\ MysqlConfig:main.MysqlConfig{Host:\"127.0.0.1\", Port:3306, DBName:\"viper_demo\"}}] ","date":"2000-01-01","objectID":"/golang-web-viper-config/:2:4","tags":["golang"],"title":"Go Viper 配置管理库","uri":"/golang-web-viper-config/"},{"categories":["golang"],"content":"Go Web validator 参数校验","date":"2000-01-01","objectID":"/golang-web-validator/","tags":["golang"],"title":"Go Web validator 参数校验","uri":"/golang-web-validator/"},{"categories":["golang"],"content":"Validator Go Struct and Field validation, including Cross Field, Cross Struct, Map, Slice and Array diving。 github - https://github.com/go-playground/validator ","date":"2000-01-01","objectID":"/golang-web-validator/:0:0","tags":["golang"],"title":"Go Web validator 参数校验","uri":"/golang-web-validator/"},{"categories":["golang"],"content":"validator Fields Tag Description eqcsfield Field Equals Another Field (relative) eqfield Field Equals Another Field fieldcontains NOT DOCUMENTED IN doc.go fieldexcludes NOT DOCUMENTED IN doc.go gtcsfield Field Greater Than Another Relative Field gtecsfield Field Greater Than or Equal To Another Relative Field gtefield Field Greater Than or Equal To Another Field gtfield Field Greater Than Another Field ltcsfield Less Than Another Relative Field ltecsfield Less Than or Equal To Another Relative Field ltefield Less Than or Equal To Another Field ltfield Less Than Another Field necsfield Field Does Not Equal Another Field (relative) nefield Field Does Not Equal Another Field 注意: 一般来说 Struct 的结构体 tag 使用 validate 进行约束. 在 Gin 框架中, 使用 binding 就可以约束。 ","date":"2000-01-01","objectID":"/golang-web-validator/:1:0","tags":["golang"],"title":"Go Web validator 参数校验","uri":"/golang-web-validator/"},{"categories":["golang"],"content":"Gin 框架使用例子 ","date":"2000-01-01","objectID":"/golang-web-validator/:2:0","tags":["golang"],"title":"Go Web validator 参数校验","uri":"/golang-web-validator/"},{"categories":["golang"],"content":"翻译器 package validator import ( \"fmt\" \"github.com/gin-gonic/gin/binding\" \"github.com/go-playground/locales/en\" \"github.com/go-playground/locales/zh\" ut \"github.com/go-playground/universal-translator\" \"github.com/go-playground/validator/v10\" enTranslations \"github.com/go-playground/validator/v10/translations/en\" zhTranslations \"github.com/go-playground/validator/v10/translations/zh\" ) // validator 翻译器 // 定义一个全局的翻译器 trans var trans ut.Translator // 初始化翻译器 func InitTrans(locale string) (err error) { // 更改Gin框架中validator引擎的属性,实现翻译 if v, ok := binding.Validator.Engine().(*validator.Validate); ok { // 注册一个获取 json tag 的自定义方法 v.RegisterTagNameFunc(func(field reflect.StructField) string { name := strings.SplitN(field.Tag.Get(\"json\"), \",\", 2)[0] if name == \"-\" { return \"\" } return name }) // 中文 zhT := zh.New() // 英文 enT := en.New() uni := ut.New(enT, zhT, enT) // locale 取决于 http 请求中的 'Accept-Language' 决定 var ok bool trans, ok = uni.GetTranslator(locale) if !ok { return fmt.Errorf(\"uni.GetTranslator(%s) Failed\", locale) } // 注册翻译器 switch locale { case \"en\": err = enTranslations.RegisterDefaultTranslations(v, trans) case \"zh\": err = zhTranslations.RegisterDefaultTranslations(v, trans) default: err = enTranslations.RegisterDefaultTranslations(v, trans) } return } return } // 定义请求参数的结构体 type ParamSignUp struct { UserName string `json:\"username\" binding:\"required\"` Password string `json:\"password\" binding:\"required\"` RePassword string `json:\"re_password\" binding:\"required\"` } func main() { // 初始化 validator 翻译器 if err := validator.InitTrans(\"zh\"); err!=nil { fmt.Println(\"InitTrans Failed Error\", err) return } r := gin.Default() r.POST(\"/signup\", func(c *gin.Context) { var p ParamSignUp if err := c.ShouldBind(\u0026p); err != nil { // 判断这个错误是否属于 vilidator 类型校检错误。 verr, ok := err.(vilidator.ValidationErrors) if !ok { // 如果不是 vilidator 类型错误就返回正常的错误类型 c.JSON(http.StatusOK, gin.H{ \"Msg\": err.Error(), }) return } // 如果是 vilidator 类型错误 c.JSON(http.StatusOK, gin.H{ \"Msg\": verr.Translate(trans), }) return } }) r.Run() } ","date":"2000-01-01","objectID":"/golang-web-validator/:2:1","tags":["golang"],"title":"Go Web validator 参数校验","uri":"/golang-web-validator/"},{"categories":["golang"],"content":"Go 项目中使用第三方 Zap 日志库","date":"2000-01-01","objectID":"/golang-web-zap-logs/","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"Go 项目中 使用 Zap 日志库 Zap 日志库是 Uber 开源的一个 性能非常好, 结构化的日志库。 ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:0:0","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"日志 日志系统需要具备的功能 能够将项目事件记录到文件中, 而不单单是应用程序控制台输出。 日志切割 - 能够根据文件大小、时间间隔等条件来切割日志文件。 支持不同的日志级别。如: DEBUG、INFO、ERROR等。 支持打印项目运行基础信息, 调用文件路径、函数名和行号, 日志时间等。 ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:1:0","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"安装 go get -u go.uber.org/zap ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:2:0","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"初始化 Zap Logger Zap 提供了两种类型的日志记录器 分别是 Sugared Logger 与 Logger。 Sugared Logger - 在不是很关键的上下文中使用. 它支持 结构化和printf风格的日志记录。 Logger - 在每微秒和每次内存分配都很重要的上下文中使用. 它速度比 Sugared Logger 速度更快, 内存分配次数更少, 但是它只支持强类型的结构化日志记录。 ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:3:0","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"Logger 通过调用 zap.NewProduction() 、 zap.NewDevelopment() 、 zap.Example() 创建一个 Logger 实例。 NewProduction() NewDevelopment() Example() 区别在于记录的信息不一样, 调用函数信息、日期、时间、日志级别等。 默认配置下, 以上三种都会将日志 打印到应用程序的 Console 界面中。 Logger 使用例子 // 定义一个全局的 zap.Logger 实例 Logger var Logger *zap.Logger // 初始化 zap Logger 函数 func InitLogger() { Logger, _ = zap.NewProduction() } func simpleHttpGet(url string) { resp, err := http.Get(url) if err != nil { Logger.Error(\"Get Url Error\", zap.String(\"url\", url), zap.Error(err)) } else { Logger.Info(\"Get Url Success\", zap.String(\"Status\", resp.Status), zap.String(\"url\", url)) _ = resp.Body.Close() } } func main() { InitLogger() // 模拟访问 simpleHttpGet(\"http://www.google.com\") simpleHttpGet(\"http://www.baidu.com\") // Sync 是将内存中的数据保存到本地磁盘 defer Logger.Sync() } # 输出结果 {\"level\":\"error\",\"ts\":1595230339.586619,\"caller\":\"zap/main.go:20\",\"msg\":\"Get Url Error\",\"url\":\"http://www.google.com\",\"error\":\"Get \\\"http://www.google.com\\\": dial tcp 69.63.184.30:80: i/o timeout\",\"stacktrace\":\"main.simpleHttpGet\\n\\t/Users/jicki/jicki/web/zap/main.go:20\\nmain.main\\n\\t/Users/jicki/jicki/web/zap/main.go:35\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} {\"level\":\"info\",\"ts\":1595230294.988758,\"caller\":\"zap/main.go:24\",\"msg\":\"Get Url Success\",\"Status\":\"200 OK\",\"url\":\"http://www.baidu.com\"} Logger.Info 方法 func (log *Logger) Info(msg string, fields ...Field) 其中Info可以是其他的日志级别, fields 接收任意数量的zapcore.Field 的参数。 zapcore.Field 每一组都是 key/value 形式的结构化参数。 ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:3:1","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"Sugared Logger Sugared Logger 例子 // 定义一个全局的 zap.Logger 实例 Logger var Logger *zap.Logger // 定义一个全局的 zap.SugaredLogger 实例 sugaredLogger var sugaredLogger *zap.SugaredLogger // 初始化 zap SugaredLogger 函数 func InitSugaredLogger() { // Logger 这里是赋值 不是 定义, 所以使用 = 赋值 Logger, _ = zap.NewProduction() // 这里是 赋值 所以使用 = sugaredLogger = Logger.Sugar() } func sugaredHttpGet(url string) { // sugared Logger 支持 printf 的输出 sugaredLogger.Debugf(\"Get request for %s \\n\", url) resp, err := http.Get(url) if err != nil { sugaredLogger.Errorf(\"Get for %s Error %s \\n\", url, err) } else { sugaredLogger.Infof(\"Get for %s Success \\n\", url) _ = resp.Body.Close() } } func main() { InitSugaredLogger() // 调用日志 sugaredHttpGet(\"http://www.google.com\") sugaredHttpGet(\"http://www.baidu.com\") // Sync 是将内存中的数据保存到本地磁盘 defer sugaredLogger.Sync() } # 输出结果 {\"level\":\"error\",\"ts\":1595236309.9537542,\"caller\":\"zap/main.go:48\",\"msg\":\"Get for http://www.google.com Error Get \\\"http://www.google.com\\\": dial tcp 75.126.135.131:80: i/o timeout \\n\",\"stacktrace\":\"main.sugaredHttpGet\\n\\t/Users/jicki/jicki/web/zap/main.go:48\\nmain.main\\n\\t/Users/jicki/jicki/web/zap/main.go:59\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} {\"level\":\"info\",\"ts\":1595236309.971819,\"caller\":\"zap/main.go:50\",\"msg\":\"Get for http://www.baidu.com Success \\n\"} ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:3:2","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"自定义 New() 方法 func New(core zapcore.Core, options ...Option) *Logger zapcore.Core 主要的一些参数其中的三个配置项 Encoder - 编码器, 1. 更改输出日志格式. 如: zapcore.NewJSONEncoder(zap.NewProductionEncoderConfig()) 或 zapcore.NewConsoleEncoder(zap.NewProductionEncoderConfig()) 。 2. 更改日志时间的格式. 如: encoderConfig := zap.NewProductionEncoderConfig() 、 encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder。 WriteSyncer - 配置日志输出方式. 如需要输出到文件需使用 zapcore.AddSync() 函数,并将打开的文件句柄传入函数中. file, _ := os.OpenFile(\"./HttpGet.log\", os.O_CREATE|os.O_APPEND|os.O_RDWR, 0644) LogLevel - 日志级别. ( Debug、Info、Warn、Error、DPanic、Panic、Fatal ) options 配置, 可以增加一些额外的输出 zap.AddCaller() - 函数调用信息记录。日志中输出如: \"caller\":\"zap/main.go:47\" 此类信息。 自定义 New 例子 // 定义一个全局的 zap.Logger 实例 Logger var Logger *zap.Logger // 定义一个全局的 zap.SugaredLogger 实例 SugarLogger var SugarLogger *zap.SugaredLogger // 初始化 zap Logger 函数 func InitLogger() { encoder := NewEncoder() writeSync := NewLogWriter() core := zapcore.NewCore(encoder, writeSync, zapcore.DebugLevel) Logger = zap.New(core, zap.AddCaller()) SugarLogger = Logger.Sugar() } // InitLogger: zap.New() 函数的 code 部分 Encoder 配置项 func NewEncoder() zapcore.Encoder { //return zapcore.NewConsoleEncoder(zap.NewProductionEncoderConfig()) encoderConfig := zap.NewProductionEncoderConfig() encoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder return zapcore.NewJSONEncoder(encoderConfig) } // InitLogger: zap.New() 函数的 code 部分 LogWriter 配置项 func NewLogWriter() zapcore.WriteSyncer { file, _ := os.OpenFile(\"./HttpGet.log\", os.O_CREATE|os.O_APPEND|os.O_RDWR, 0644) return zapcore.AddSync(file) } func simpleHttpGet(url string) { resp, err := http.Get(url) if err != nil { SugarLogger.Errorf(\"Get [%s] Error: %s \\n\", url, err) } else { SugarLogger.Infof(\"Get [%s] Success StatusCode: %d \\n\", url, resp.StatusCode) _ = resp.Body.Close() } } func main() { InitLogger() simpleHttpGet(\"http://www.baidu.com\") // Sync 是将内存中的数据保存到本地磁盘 defer SugarLogger.Sync() } # 输出文件的日志 {\"level\":\"info\",\"ts\":\"2020-07-21T14:38:01.090+0800\",\"msg\":\"Get [http://www.baidu.com] Success StatusCode: 200 \\n\"} {\"level\":\"info\",\"ts\":\"2020-07-21T14:48:29.474+0800\",\"caller\":\"zap/main.go:47\",\"msg\":\"Get [http://www.baidu.com] Success StatusCode: 200 \\n\"} ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:3:3","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"Lumberjack 切割日志 lumberjack is a log rolling package for Go Zap 本身并不支持日志的切割, 需要配合第三方库实现。 ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:4:0","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"安装 go get -u github.com/natefinch/lumberjack ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:4:1","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"zap 结合lumberjack zap 使用 lumberjack 需要在 zapcore.NewCore() 函数中的 WriteSyncer 方法中集成。 NewLogWriter 函数 // InitLogger: zap.New() 函数的 code 部分 LogWriter 配置项 func NewLogWriter() zapcore.WriteSyncer { lumberjackLogger := \u0026lumberjack.Logger{ // 日志文件路径 Filename: \"./HttpGet.log\", // 最大切割文件大小 单位是 MB MaxSize: 500, // 最多备份数量 MaxBackups: 5, // 最多保存天数 MaxAge: 30, // 是否进行压缩 Compress: false, } return zapcore.AddSync(lumberjackLogger) } 切割日志的格式为: HttpGet-2020-07-21T07-35-29.789.log 。 ","date":"2000-01-01","objectID":"/golang-web-zap-logs/:4:2","tags":["golang"],"title":"Go Zap 日志库","uri":"/golang-web-zap-logs/"},{"categories":["golang"],"content":"Go 操作 Redis","date":"2000-01-01","objectID":"/golang-web-note-6/","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-6/:0:0","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"Golang go-Redis 官方 github https://github.com/go-redis/redis ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:0","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"docs 文档 https://godoc.org/github.com/go-redis/redis ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:1","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"Install go get -u github.com/go-redis/redis ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:2","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"import import \"github.com/go-redis/redis\" ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:3","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"初始化连接 单 Redis 节点 // 创建一个全局的 redis 客户端实例 Rdb var Rdb *redis.Client func initRdb() (err error) { // 这里是赋值, 而不是 创建一个变量, 所以只需要使用 = 号 Rdb = redis.NewClient(\u0026redis.Options{ Addr: \"127.0.0.1:6379\", // 留空为没密码 Password: \"\", // 默认的 DB 为 0 DB: 0, // 连接池大小 PoolSize: 100, }) // 测试 redis 是否可以连接 _, err = Rdb.Ping().Result() return err } Redis 哨兵模式 // 创建一个全局的 redis 客户端实例 Rdb var Rdb *redis.Client func initRdb() (err error) { // 这里是赋值, 而不是 创建一个变量, 所以只需要使用 = 号 Rdb = redis.NewFailoverClient(\u0026redis.FailoverOptions{ MasterName: \"master\", SentinelAddrs: []string{\"127.0.0.1:6379\", \"127.0.0.1:6380\", \"127.0.0.1:6381\"}, }) // 测试 redis 是否可以连接 _, err = Rdb.Ping().Result() return err } Redis Cluster 集群模式 集群模式的 客户端实例与单节点是不一样的 *redis.ClusterClient // 创建一个全局的 redis 客户端实例 Rdb var Rdb *redis.ClusterClient func initRdb() (err error) { // 这里是赋值, 而不是 创建一个变量, 所以只需要使用 = 号 Rdb = redis.NewClusterClient(\u0026redis.ClusterOptions{ Addrs: []string{\"127.0.0.1:7000\", \"127.0.0.1:7001\", \"127.0.0.1:7002\", \"127.0.0.1:7003\"}, }) // 测试 redis 是否可以连接 _, err = Rdb.Ping().Result() return err } ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:4","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"连接redis func main() { if err := initRdb(); err != nil { fmt.Printf(\"initRdb Failed Error %v\\n\", err) return } fmt.Println(\"Connect Redis Client Success\") defer Rdb.Close() } ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:5","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"操作 Redis Set/Get 操作 Set 操作的函数 func redisSet(key string, value interface{}, expr time.Duration) { err := Rdb.Set(key, value, expr).Err() if err != nil { fmt.Printf(\"redisSet Failed Error %v\\n\", err) return } fmt.Printf(\"Redis Set key: [%s] value: [%v] Success \\n\", key, value) } func main() { if err := initRdb(); err != nil { fmt.Printf(\"initRdb Failed Error %v\\n\", err) return } fmt.Println(\"Connect Redis Client Success\") defer Rdb.Close() redisSet(\"RedisKey\", \"Value1\", 0) } 输出: Connect Redis Client Success Redis Set key: [RedisKey] value: [Value1] Success Get 操作的函数 func redisGet(key string) { value, err := Rdb.Get(key).Result() // 应该首先使用 redis.Nil 判断 key 是否存在 if err == redis.Nil { fmt.Printf(\"Key: [%s] is Null \\n\", key) // 这里再判断 err != nil } else if err != nil { fmt.Printf(\"Redis Get Key Failed Error %v \\n\", err) } else { fmt.Printf(\"Key: [%s] value: [%v] \\n\", key, value) } } func main() { if err := initRdb(); err != nil { fmt.Printf(\"initRdb Failed Error %v\\n\", err) return } fmt.Println(\"Connect Redis Client Success\") defer Rdb.Close() // Get 一个不存在的 key redisGet(\"redisKey\") } # 输出结果 Connect Redis Client Success Key: [redisKey] is Null ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:6","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"Redis Pipeline Pipeline 管道技术, 指的是客户端允许将多个请求依次发给服务器, 过程中而不需要等待请求的回复, 在最后再一并读取结果即可。 Pipeline 例子 func redisPipeline() { pipe := Rdb.Pipeline() pipe.Set(\"key1\", \"value1\", 0) pipe.Set(\"key2\", \"value2\", 0) pipe.Set(\"key3\", \"value3\", 0) pipe.Get(\"key1\") cmder, err := pipe.Exec() if err != nil { fmt.Printf(\"Pipeline Exec Failed Error %v\\n\", err) return } for _, cmd := range cmder { fmt.Printf(\"Pipe Exec: [%v] \\n\", cmd) } } func main() { if err := initRdb(); err != nil { fmt.Printf(\"initRdb Failed Error %v\\n\", err) return } fmt.Println(\"Connect Redis Client Success\") defer Rdb.Close() redisPipeline() } # 输出结果 Connect Redis Client Success Pipe Exec: [set key1 value1: OK] Pipe Exec: [set key2 value2: OK] Pipe Exec: [set key3 value3: OK] Pipe Exec: [get key1: value1] ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:7","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"Redis 事务操作 TxPipeline Redis 是单线程, 因此单个命令始终是原子的, 但是来自不同客户端的两个命令可以依次执行, 但是 Multi/exec 能够确保在multi/exec两个语句之间的命令没有其他客户端正在执行命令。基于这样的场景下我们需要使用TxPipeline来解决。 TxPipeline 类似于 Pipeline 的方式, 不过 TxPipeline 内部会使用 Multi/exec 封装排列的命令。 func redisTxPipeline() { pipe := Rdb.TxPipeline() pipe.Set(\"key1\", \"value1\", 0) pipe.Set(\"key2\", \"value2\", 0) pipe.Set(\"key3\", \"value3\", 0) pipe.Get(\"key1\") cmder, err := pipe.Exec() if err != nil { fmt.Printf(\"Pipeline Exec Failed Error %v\\n\", err) return } for _, cmd := range cmder { fmt.Printf(\"Pipe Exec: [%v] \\n\", cmd) } } func main() { if err := initRdb(); err != nil { fmt.Printf(\"initRdb Failed Error %v\\n\", err) return } fmt.Println(\"Connect Redis Client Success\") defer Rdb.Close() redisTxPipeline() } # 输出结果 Connect Redis Client Success Pipe Exec: [set key1 value1: OK] Pipe Exec: [set key2 value2: OK] Pipe Exec: [set key3 value3: OK] Pipe Exec: [get key1: value1] WATCH 某些场景下, 我们除了要使用 Multi/Exec 命令外, 还需要配合 WATCH 命令使用。 如: 在使用 WATCH 命令监视某个键之后, 直到执行 Exec 命令的这段时间内, 如果有其他用户抢先对被监视的键进行了替换、更新、删除等操作, 那么当用户尝试执行Exec 的时候, 事务将返回一个错误, 用户可以根据这个错误选择重试事务或者放弃事务。 func txWatch() { key := \"watch_count\" err := Rdb.Watch(func(tx *redis.Tx) error { n, err := tx.Get(key).Int() if err != nil \u0026\u0026 err != redis.Nil { return err } _, err = tx.Pipelined(func(pipe redis.Pipeliner) error { pipe.Set(key, n+1, 0) return nil }) return err }, key) if err != nil { fmt.Printf(\"WATCH Failed Error %v\\n\", err) return } fmt.Printf(\"WATCH Get watch_count: %v \\n\", Rdb.Get(key).Val()) } ","date":"2000-01-01","objectID":"/golang-web-note-6/:1:8","tags":["golang"],"title":"Go 操作 Redis","uri":"/golang-web-note-6/"},{"categories":["golang"],"content":"Golang flag 库","date":"2000-01-01","objectID":"/golang-study-note-10/","tags":["golang"],"title":"Golang flag 库","uri":"/golang-study-note-10/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-10/:0:0","tags":["golang"],"title":"Golang flag 库","uri":"/golang-study-note-10/"},{"categories":["golang"],"content":"Golang flag 库 Go语言内置 - flag包实现了命令行参数的解析, flag包使得开发命令行工具更为简单。 flag参数类型 flag包 - 支持的命令行参数类型有bool、int、int64、uint、uint64、float、float64、string、duration。 flag参数 有效值 字符串flag 合法字符串 整数flag 1234、0664、0x1234等类型, 也可以是负数。 浮点数flag 合法浮点数 bool类型flag 1, 0, t, f, T, F, true, false, TRUE, FALSE, True, False 时间段flag 任何合法的时间段字符串。如:“300ms”、\"-1.5h”、“2h45m”。合法的单位。如: “s”、“ms”、“h”。 ","date":"2000-01-01","objectID":"/golang-study-note-10/:1:0","tags":["golang"],"title":"Golang flag 库","uri":"/golang-study-note-10/"},{"categories":["golang"],"content":"定义命令行flag参数 flag.Type() flag.Type(flag名, 默认值, 帮助信息)*Type 注意: 如下: name、age、married、delay 均为对应类型的指针。 package main import ( \"flag\" \"fmt\" ) func main() { // 分别是 名称, 具体内容, 帮助信息 name := flag.String(\"name\", \"张大仙\", \"姓名\") age := flag.Int(\"age\", 20, \"年龄\") married := flag.Bool(\"married\", false, \"婚否\") delay := flag.Duration(\"delay\", 0, \"时间间隔\") // 解析命令行参数 flag.Parse() // 输出 fmt.Println(*name, *age, *married, *delay) } # go run main.go -name \"张小仙\" -age 22 -married true 张小仙 22 true 0s flag.TypeVar() flag.TypeVar(Type指针, flag名, 默认值, 帮助信息) package main import ( \"flag\" \"fmt\" \"time\" ) func cmdFlag() { var ( name string age int married bool delay time.Duration ) flag.StringVar(\u0026name, \"name\", \"张大仙\", \"姓名\") flag.IntVar(\u0026age, \"age\", 20, \"年龄\") flag.BoolVar(\u0026married, \"married\", false, \"婚否\") flag.DurationVar(\u0026delay, \"delay\", 0, \"时间间隔\") flag.Parse() fmt.Println(name, age, married, delay) } func main() { cmdFlag() } # go run main.go -age=50 -married=true -delay=20s 张大仙 50 true 20s flag.Parse() flag.Parse() 是用来对定义好的 flag 命令参数进行解析。 支持的命令行参数格式有以下几种: -flag xxx - (使用空格, 一个-符号) --flag xxx - (使用空格, 两个-符号) -flag=xxx - (使用等号, 一个-符号) --flag=xxx - (使用等号，两个-符号) 其中, 布尔类型的参数必须使用等号的方式指定。 Flag解析在第一个非flag参数（单个”-“不是flag参数）之前停止, 或者在终止符”–“之后停止。 ","date":"2000-01-01","objectID":"/golang-study-note-10/:1:1","tags":["golang"],"title":"Golang flag 库","uri":"/golang-study-note-10/"},{"categories":["golang"],"content":"其他 flag 函数 flag.Args() - 返回命令行参数后的其他参数, 以[]string类型。 flag.NArg() - 返回命令行参数后的其他参数个数。 flag.NFlag() - 返回使用的命令行参数个数。 package main import ( \"flag\" \"fmt\" \"time\" ) func cmdFlag() { var ( name string age int married bool delay time.Duration ) flag.StringVar(\u0026name, \"name\", \"张大仙\", \"姓名\") flag.IntVar(\u0026age, \"age\", 20, \"年龄\") flag.BoolVar(\u0026married, \"married\", false, \"婚否\") flag.DurationVar(\u0026delay, \"delay\", 0, \"时间间隔\") flag.Parse() fmt.Printf(\"额外使用了%d个命令行外参数\\n\", flag.NArg()) fmt.Printf(\"使用了%d个命令行参数\\n\", flag.NFlag()) fmt.Println(name, age, married, delay, flag.Args()) } func main() { cmdFlag() } # go run main.go -age 20 \"我来了\" \"我走了\" 额外使用了2个命令行外参数 使用了1个命令行参数 张大仙 20 false 0s [我来了 我走了] ","date":"2000-01-01","objectID":"/golang-study-note-10/:1:2","tags":["golang"],"title":"Golang flag 库","uri":"/golang-study-note-10/"},{"categories":["golang"],"content":"flag 应用 package main import ( \"flag\" \"fmt\" \"time\" ) func cmdFlag() { var ( password string ) flag.StringVar(\u0026password, \"pw\", \"123456\", \"密码\") flag.Parse() //判断输入的值 if password == \"654321\" { fmt.Println(\"密码正确\") fmt.Printf(\"当前时间为 %v \\n\", time.Now().Format(\"2006-01-02 15:04:05\")) } else { fmt.Println(\"密码不正确,请确认后再试。\") } } func main() { cmdFlag() } ","date":"2000-01-01","objectID":"/golang-study-note-10/:1:3","tags":["golang"],"title":"Golang flag 库","uri":"/golang-study-note-10/"},{"categories":["golang"],"content":"Golang to Kafka","date":"2000-01-01","objectID":"/golang-web-note-12/","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-12/:0:0","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"Kafka Apache Kafka 是由著名职业社交公司Linkedln开发的, 最初是被设计用于解决Linkedln 公司内部海量日志传输等问题。Kafka 使用 Scala 语言编写。 2011年Linkedln 将Kafka开源 并进入 Apache 孵化器, 2012年10月正式毕业,成为Apache 顶级项目。 ","date":"2000-01-01","objectID":"/golang-web-note-12/:1:0","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"消息队列通信模型 点对点模式 (Queue) 生产者 生产消息发送到 queue 中, 消费者从 queue 中取出消息并且消费消息。 一条消息被消费以后, queue 中就没有了, 不会有重复消费。 发布/订阅 (topic) 生产者 (发布消息) 将消息发布到 topic 中, 同时有多个 消费者 (订阅topic) 消费这条消息。相对于 点对点(queue)方式, 发布到 topic 中的消息会被 所有 订阅了该 topic 的消费者进行消费。 ","date":"2000-01-01","objectID":"/golang-web-note-12/:1:1","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"Kafka 介绍 Kafka 是一个分布式数据流服务, 可以运行在单台服务器上, 也可以在多个服务器中部署成集群模式。它提供了发布和订阅的功能, 使用者可以发送数据到 Kafka 中, 也可以从 Kafka 中读取数据。 Kafka 特点: 高吞吐量、低延迟 - 每秒可以生产约 25 万消息 (50 MB) , 每秒处理 55 万消息 (110 MB)。 持久化数据存储 - 可进行持久化操作。将消息持久化到磁盘, 因此可用于批量消费, 例如 ETL, 以及实时应用程序。通过将数据持久化到硬盘以及 replication 防止数据丢失。 高容错 - 分布式系统易于扩展, 所有的 producer、broker 和 consumer 都可以配置多个, 均为分布式的。无需停机即可扩展机器。 消息被处理的状态是在 consumer 端维护, 而不是由 server 端维护。当失败时能自动平衡。 ","date":"2000-01-01","objectID":"/golang-web-note-12/:1:2","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"Kafka 架构 Producer - 生产者 Kafka Cluster - Kafka 集群 Broker - 每一个Kafka Server 可以成为一个Broker, 多个Broker就是 Kafka Cluster。(单机服务器也可以部署多个Broker, 多个 Broker 连接到同一个 Zookeeper 中,就形成了 Kafka Cluster )。 Topic - 消息类别名, 一个Topic存放一类的消息。每个Topic都有一个或者多个订阅者( consumer ), Producer 负责将消息推送到 Topic, 然后由 订阅了该 Topic 的 consumer 从该 Topic 中读取消息。一个 Broker 可以创建一个或多个Topic , 同一个Topic 可以分布在同一个 Kafka Cluster 下的多个Broker 中。 Partition - Kafka 为每个 Topic 维护多个 Partition 分区 ( 既数据分片 ), 每个分区都会映射到同一个 逻辑日志文件中。当一条 消息 被发布到 Topic 上, 这条消息会分布在其中一个 Partition 中, 并且 Broker 会将这条 消息 追到逻辑 日志文件的最后一个 segment 中。 每个 Partition 都是一个有序的、不可变的结构化的提交日志记录的序列。在每个Partition中每一条日志记录都会被分配一个序号——通常称为offset, offset在Partition内是唯一的。逻辑日志文件会被化分为多个文件segment（每个segment的大小一样的）。 Broker 集群将会保留所有已发布的 消息 records, 不管这些消息是否已被消费。保留时间依赖于一个可配的保留周期。 Partition 是分布式的存在于一个Kafka Cluster的多个Broker上。每个Partition会被复制多份存在于不同的Broker上。这样做是为了容灾。具体会复制几份, 会复制到哪些Broker上, 都是可以配置的。经过相关的复制策略后, 每个Topic在每个Broker上会驻留一到多个Partition。 对于同一个Partition, 它所在任何一个Broker, 都有能扮演两种角色: leader、follower。 Partition 在服务器中表现形式为一个一个的文件夹,每个Partition 包含多个 segment 文件。每组的 segment 文件中 包含三种文件, .index 文件， .log 文件, .timeindex 文件, .log 文件是存储 具体 消息 的, .index 与 .timeindex 文件是 索引文件,用于检索与查找具体的消息。 Consumer - 消费者, consumer 可以是一个,也可以形成一个 consumer Group ,每个组包含多个 consumer, 共同消费订阅的Topic消息, 提高效率。 ","date":"2000-01-01","objectID":"/golang-web-note-12/:1:3","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"Kafka 生产消费流程 Producer 首先连接 Kafka Cluster 并获取 Partition 的信息, 查找具体的 Leader 。 Producer 将 消息 发送给 具体的 Partition 的 Leader。 Partition 的 Leader 将消息写入 本地磁盘中。 Partition 的 follower 此时会拉取 Leader 的消息。 Partition 的 follower 将消息写入 本地磁盘中, 并发送 ACK 给 Leader。 Partition 的 Leader 收到 所有 follower 的 ACK 后 给 Producer 发送 ACK。 注意: ACK = RequiredAcks , 一共包含三种确认方式, 分别是 0 不需要 ACK 确认。 1 只需要 Leader 确认既可。 ALL或-1 表示 既需要 Leader 确认 也需要 follower 确认。 ","date":"2000-01-01","objectID":"/golang-web-note-12/:1:4","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"Golang 操作 Kafka Go语言操作 Kafka 使用 sarama 这个库。 install go get -u github.com/shopify/sarama 发送消息到 kafka package main import ( \"flag\" \"fmt\" \"github.com/Shopify/sarama\" ) func main() { // 初始化 Kafka Client 配置 实例 kafkaConf := sarama.NewConfig() // 配置 Producer Ack 确认(分别为 NoResponse, WaitForLocal, WaitForAll) // WaitForAll 表示 leader 与 follower 都需要确认. kafkaConf.Producer.RequiredAcks = sarama.WaitForAll // 配置 Producer 写入哪一个 分区。 // NewRandomPartitioner 表示 随机写入一个分区。 kafkaConf.Producer.Partitioner = sarama.NewRandomPartitioner // 配置 Producer 成功以后返回 确认。 kafkaConf.Producer.Return.Successes = true // 配置连接 Kafka Server client, err := sarama.NewSyncProducer([]string{\"127.0.0.1:9092\"}, kafkaConf) if err != nil { fmt.Printf(\"Producer 连接失败: %v\\n\", err) return } // 关闭 连接 defer client.Close() // 创建一条 需要写入到 kafka 的信息。 // The request attempted to perform an operation on an invalid topic. // 如上报错: Topic 必须符合 [a-zA-Z0-9\\\\._\\\\-] 规则。 var value string flag.StringVar(\u0026value, \"value\", \"\", \"消息内容\") flag.Parse() if len(value) == 0 { fmt.Println(\"消息不能为空,使用 -value='' 输入消息内容 \") return } msg := \u0026sarama.ProducerMessage{ Topic: \"TestMessage\", Key: nil, Value: sarama.StringEncoder(value), } // 发送信息到 kafka Part, offset, err := client.SendMessage(msg) if err != nil { fmt.Printf(\"SendMessage Failed: %v\\n\", err) return } fmt.Printf(\"Send Message partition: %v, offset: %v \\n\", Part, offset) } # go run main.go -value=\"这是一条测试信息\" Send Message partition: 0, offset: 5 consumer 消费消息 package main import ( \"flag\" \"fmt\" \"sync\" \"github.com/Shopify/sarama\" ) var ( topic string wg sync.WaitGroup ) func main() { // 创建一个 consumer 实例, 连接到 Kafka Server consumer, err := sarama.NewConsumer([]string{\"127.0.0.1:9092\"}, nil) if err != nil { fmt.Printf(\"consumer 连接失败, err:%v\\n\", err) return } defer consumer.Close() flag.StringVar(\u0026topic, \"topic\", \"\", \"topic名称\") flag.Parse() if len(topic) == 0 { fmt.Println(\"topic 不能为空, 使用 -topic='' 指定topic\") return } // 获取指定 Topic 的分区列表 id partitionList, err := consumer.Partitions(topic) if err != nil { fmt.Printf(\"获取 Topic Partitions分区失败: err%v\\n\", err) return } // 打印 partition 列表的 id ( []int32 ) fmt.Println(partitionList) // 遍历 partition 列表 for partition := range partitionList { // 针对每个分区创建一个对应的分区消费者 // OffsetNewest: 从当前的偏移量开始消费, OffsetOldest: 从最老的偏移量开始消费 pc, err := consumer.ConsumePartition(topic, int32(partition), sarama.OffsetNewest) if err != nil { fmt.Printf(\"failed to start consumer for partition %d,err:%v \\n\", partition, err) return } // Possible resource leak, 'defer' is called in a for loop. defer pc.AsyncClose() // 异步从每个分区消费信息 wg.Add(1) go func(sarama.PartitionConsumer) { for msg := range pc.Messages() { fmt.Printf(\"Partition:%d Offset:%d Key:%v Value:%v \\n\", msg.Partition, msg.Offset, string(msg.Key), string(msg.Value)) } wg.Done() }(pc) } wg.Wait() } 启动后会一直等待接收消息 # go run main.go -topic=\"TestMessage\" Partition:0 Offset:13 Key: Value:这是条测试消息 Partition:0 Offset:14 Key: Value:这是条测试消息 ","date":"2000-01-01","objectID":"/golang-web-note-12/:1:5","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"tail 库的使用 使用 hpcloud/tail 第三方库, 实现了 类似于 Linux 命令中 tail -f 的效果。 install go get -u github.com/hpcloud/tail 一个例子: package main import ( \"fmt\" \"time\" \"github.com/hpcloud/tail\" ) func main() { file := \"./access.log\" // 初始化 tail 实例 config := tail.Config{ Location: \u0026tail.SeekInfo{ Offset: 0, Whence: 2, }, ReOpen: true, MustExist: false, Poll: true, Pipe: false, RateLimiter: nil, Follow: true, MaxLineSize: 0, Logger: nil, } tails, err := tail.TailFile(file, config) if err != nil { fmt.Printf(\"tail file err: %v\\n\", err) return } var ( msg *tail.Line ok bool ) for { // 一直往这里写数据 msg, ok = \u003c-tails.Lines if !ok { fmt.Printf(\"tail file close reopen, filename: %s \\n\", tails.Filename) time.Sleep(time.Second) continue } fmt.Println(\"msg:\", msg.Text) } } ","date":"2000-01-01","objectID":"/golang-web-note-12/:1:6","tags":["golang"],"title":"Golang to Kafka","uri":"/golang-web-note-12/"},{"categories":["golang"],"content":"Golang 性能调优","date":"2000-01-01","objectID":"/golang-study-note-9/","tags":["golang"],"title":"Golang 性能调优","uri":"/golang-study-note-9/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-9/:0:0","tags":["golang"],"title":"Golang 性能调优","uri":"/golang-study-note-9/"},{"categories":["golang"],"content":"Golang 性能调优 在计算机性能调试领域里, profiling 是指对应用程序的画像, 画像就是应用程序使用 CPU 和内存的情况。 Go语言是一个对性能特别看重的语言, 因此语言中自带了 profiling 的库。 Go语言项目中的性能优化主要有以下几个方面: CPU profile: - 报告程序的 CPU 使用情况, 按照一定频率去采集应用程序在 CPU 和寄存器上面的数据。 Memory Profile (Heap Profile): - 报告程序的内存使用情况。 Block Profiling: - 报告多个 goroutine 不在运行状态的情况, 可以用来分析和查找死锁等性能瓶颈。 Goroutine Profiling：- 报告多个 goroutine 的使用情况, 有哪些 goroutine, 它们的调用关系是怎样的。 ","date":"2000-01-01","objectID":"/golang-study-note-9/:1:0","tags":["golang"],"title":"Golang 性能调优","uri":"/golang-study-note-9/"},{"categories":["golang"],"content":"性能调优 - 数据采集 Go语言内置了获取程序的运行数据的工具, 包括以下两个标准库 runtime/pprof: - 采集工具型应用运行数据进行分析。 net/http/pprof: - 采集服务型应用运行时数据进行分析。 pprof - 开启后每隔一段时间（10ms）就会收集下当前的堆栈信息, 获取每个函数占用的CPU以及内存资源, 最后通过对这些采样数据进行分析，形成一个性能分析报告。 注意: - 我们只应该在性能测试的时候才在代码中引入pprof。 ","date":"2000-01-01","objectID":"/golang-study-note-9/:1:1","tags":["golang"],"title":"Golang 性能调优","uri":"/golang-study-note-9/"},{"categories":["golang"],"content":"runtime/pprof 模块 CPU性能分析 开启CPU性能分析 - pprof.StartCPUProfile(w io.Writer) 停止CPU性能分析 - pprof.StopCPUProfile() 应用执行结束后, 就会生成一个文件, 保存了我们的 CPU profiling 数据。得到采样数据之后, 使用go tool pprof工具进行CPU性能分析。 内存性能分析 开启内存分析 - pprof.WriteHeapProfile(w io.Writer) ","date":"2000-01-01","objectID":"/golang-study-note-9/:1:2","tags":["golang"],"title":"Golang 性能调优","uri":"/golang-study-note-9/"},{"categories":["golang"],"content":"net/http/pprof 模块 http.DefaultServeMux - 默认的 http.ListenAndServe() 绑定服务, 只需要在代码中导入 import _ \"net/http/pprof\" 包既可对程序进行分析。 使用自定义Mux, 需要手动注册 pprof 模块的路由规则。 r.HandleFunc(\"/debug/pprof/\", pprof.Index) r.HandleFunc(\"/debug/pprof/cmdline\", pprof.Cmdline) r.HandleFunc(\"/debug/pprof/profile\", pprof.Profile) r.HandleFunc(\"/debug/pprof/symbol\", pprof.Symbol) r.HandleFunc(\"/debug/pprof/trace\", pprof.Trace) gin 框架中需要进行分析, 可使用 github.com/DeanThompson/ginpprof 。 使用 pprof 模块 以后自动生成一个 路由组 /debug/pprof 包含如下几个路由。 /debug/pprof/profile: - 访问这个链接会自动进行 CPU profiling, 持续 30s, 并生成一个文件供下载。 /debug/pprof/heap: - Memory Profiling 的路径, 访问这个链接会得到一个内存 Profiling 结果的文件。 /debug/pprof/block: - block Profiling 的路径。 /debug/pprof/goroutines: - 运行的 goroutines 列表, 以及调用关系。 访问地址如: http://localhost:8080/debug/pprof 。 ","date":"2000-01-01","objectID":"/golang-study-note-9/:1:3","tags":["golang"],"title":"Golang 性能调优","uri":"/golang-study-note-9/"},{"categories":["golang"],"content":"go tool pprof 不管是runtime/pprof 还是 net/http/pprof, 我们使用相应的pprof库获取数据之后, 下一步的都要对这些数据进行分析, 我们可以使用go tool pprof命令行工具。 go tool pprof [binary] [source] binary - 是应用的二进制文件, 用来解析各种符号。 source - 表示 profile 数据的来源, 可以是本地的文件, 也可以是 http 地址。 注意: - 获取的 Profiling 数据是动态的, 要想获得有效的数据, 请保证应用处于较大的负载 (比如正在生成中运行的服务, 或者通过其他工具模拟访问压力时) 。 例子: package main import ( \"flag\" \"fmt\" \"os\" \"runtime/pprof\" \"time\" ) // 一段有问题的代码 func logicCode() { var c chan int for { select { // 因为c 未初始化 一直在此分支中等待 值 case v := \u003c-c: fmt.Printf(\"recv from chan, value:%v\\n\", v) default: // 未做任何处理 } } } func main() { var isCPUPprof bool var isMemPprof bool flag.BoolVar(\u0026isCPUPprof, \"cpu\", false, \"turn cpu pprof on\") flag.BoolVar(\u0026isMemPprof, \"mem\", false, \"turn mem pprof on\") flag.Parse() if isCPUPprof { file, err := os.Create(\"./cpu.pprof\") if err != nil { fmt.Printf(\"create cpu pprof failed, err:%v\\n\", err) return } _ = pprof.StartCPUProfile(file) defer pprof.StopCPUProfile() } for i := 0; i \u003c 8; i++ { go logicCode() } time.Sleep(20 * time.Second) if isMemPprof { file, err := os.Create(\"./mem.pprof\") if err != nil { fmt.Printf(\"create mem pprof failed, err:%v\\n\", err) return } _ = pprof.WriteHeapProfile(file) _ = file.Close() } } # go run main.go -cpu 分析生成的报告 # go tool pprof ./cpu.pprof 进入交互模式 Type: cpu Time: Dec 25, 2019 at 4:00pm (CST) Duration: 20.14s, Total samples = 1.85mins (550.58%) Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top 命令 查看占用资源排行 (pprof) top 4 Showing nodes accounting for 110.86s, 100% of 110.89s total Dropped 9 nodes (cum \u003c= 0.55s) flat flat% sum% cum cum% 45.01s 40.59% 40.59% 86.34s 77.86% runtime.selectnbrecv 30.19s 27.23% 67.81% 33.47s 30.18% runtime.chanrecv 24.52s 22.11% 89.93% 110.86s 100% main.logicCode 11.14s 10.05% 100% 11.14s 10.05% runtime.newstack 选项解释: flat: 当前函数占用CPU的耗时 flati%: 当前函数占用CPU的耗时百分比 sun%: 函数占用CPU的耗时累计百分比 cum: 当前函数加上调用当前函数的函数占用CPU的总耗时 cum%: 当前函数加上调用当前函数的函数占用CPU的总耗时百分比 最后一列: 函数名称 list 命令 分析具体程序问题 (pprof) list main.logicCode Total: 1.85mins ROUTINE ======================== main.logicCode in main.go 24.52s 1.85mins (flat, cum) 100% of Total . . 12:func logicCode() { . . 13: var c chan int . . 14: for { . . 15: select { . . 16: // 因为c 未初始化 一直在此分支中等待 值 24.52s 1.85mins 17: case v := \u003c-c: . . 18: fmt.Printf(\"recv from chan, value:%v\\n\", v) . . 19: default: . . 20: // 未做任何处理 . . 21: } . . 22: } 24.52s 1.85mins 17: case v := \u003c-c: 这一段标注了具体的耗时,说明有问题 ","date":"2000-01-01","objectID":"/golang-study-note-9/:1:4","tags":["golang"],"title":"Golang 性能调优","uri":"/golang-study-note-9/"},{"categories":["golang"],"content":"Golang 操作 Mysql","date":"2000-01-01","objectID":"/golang-web-note-4/","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-4/:0:0","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"SQL 科普 ","date":"2000-01-01","objectID":"/golang-web-note-4/:1:0","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"bindvars 绑定变量 在 MySQL 语句中经常会出现占位符 ? , 在 SQL 内部称为 bindvars 查询占位符。 我们在日常的 SQL 语句中 任何时候都不应该自己拼接SQL语句, 应该始终使用 bindvars 占位符 来对数据发送值, 它可以防止SQL注入攻击。 在Go语言的 database/sql 中, 不会尝试对查询文本进行任何校验, 它与编码参数一起按照原样发送到服务器。 不同的数据库有不同的占位符 MySQL 中使用 ? PostgreSQL 中使用 $1、$2 等枚举的bindvars语法 SQLite 中 ? 与 $1、$2 等枚举 的语法都支持 Oracle 中使用 :name 的语法 注意: bindvars 仅用于参数化, 不允许更改 SQL语句的结构。 bindvars 占位符不能用于插入 表名。 如: select * from ? 这是不允许的 bindvars 占位符不能用于插入 列名。如: select ?,? from user 这也是不允许的 ","date":"2000-01-01","objectID":"/golang-web-note-4/:1:1","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"sql 标准库 Go 语言中的 database/sql 包提供了保证SQL或类SQL数据库的范接口, 但是并不提供具体的数据库驱动。 使用database/sql包时 必须注入至少一个数据库驱动。 ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:0","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"下载 Mysql 驱动 go get -u github.com/go-sql-driver/mysql ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:1","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"驱动说明 // sql.Open 函数 func Open(driverName, dataSourceName string) (*DB, error) driverName - 指具体的数据库类型, 如: mysql dataSourceName - 指具体的数据信息, 如: 账号、密码、协议、IP、端口、数据库名 等。 ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:2","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"连接数据库例子 package main import ( \"database/sql\" // 引入 mysql 驱动, 只需要用到 init() 方法 _ \"github.com/go-sql-driver/mysql\" ) func main() { dsn := \"user:password@tcp(127.0.0.1:3306)/dbname\" db, err := sql.Open(\"mysql\", dsn) if err != nil { panic(err) } // 关闭数据库连接 defer db.Close() // Ping 函数 尝试校验 dsn err = DB.Ping() if err != nil { return err } } ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:3","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"初始化连接的例子 sql.Open 返回的 DB 对象可以安全的被多个goroutine并发使用, 并维护自己的空闲连接池, 因此很少需要关闭 DB 这个对象。 SetMaxOpenConns 设置与数据库建立连接的最大连接数。默认为0 (无限制) SetMaxIdleConns 设置连接池中最大闲置连接数。 package main import ( \"database/sql\" \"fmt\" // 引入 mysql 驱动, 只需要用到 init() 方法 _ \"github.com/go-sql-driver/mysql\" ) // 定义全局的 数据库对象 DB var DB *sql.DB func initDB() (err error) { dsn := \"user:password@tcp(127.0.0.1:3306)/dbname?charset=utf8mb4\u0026parseTime=True\" // 这里 DB 赋值是给 上面定义的全局变量赋值, 不要使用 := DB, err = sql.Open(\"mysql\", dsn) if err != nil { return err } // 调用 Ping() 方法会尝试连接数据库,校检 dsn 是否正确 err = DB.Ping() if err != nil { return err } // 最大连接数 DB.SetMaxOpenConns(100) // 连接池中空闲连接数 DB.SetMaxIdleConns(50) return nil } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() } ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:4","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"增删改查操作 查询操作 单行查询 - 单行查询使用DB.QueryRow() 方法执行一次查询, 并返回最多一行结果(Row) QueryRow方法 总是返回非nil值, 直到返回值的Scan方法被调用时才会返回被延迟的错误. 如: 未查询到结果 func (db *DB) QueryRow(query string, args ...interface{}) *Row 方法 type user struct { id int name string age int } func queryRowToID(id int) { var u user sqlStr := \"select id, name, age from user where id=?\" // .Scan 是链式操作, Scan是将查询出来的结果赋值到结构体对应的字段中. // 使用 QueryRow 以后必须要调用 Scan 方法,否则数据库连接不会被释放. err := DB.QueryRow(sqlStr, id).Scan(\u0026u.id, \u0026u.name, \u0026u.age) if err != nil { fmt.Printf(\"QueryRow Scan Failed, Error %v\\n\", err) return } fmt.Printf(\"ID: %d Name: %s Age: %d \\n\", u.id, u.name, u.age) } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() queryRowToID(1) } # 输出结果 ID: 1 Name: jicki Age: 20 多行查询 - 多行查询使用 DB.Query() 方法执行一次查询, 返回多行结果(Row)。 func (db *DB) Query(query string, args ...interface{}) (*Rows, error) 方法 type user struct { id int name string age int } func queryMultiRowToID(id int) { var u user sqlStr := \"select id, name, age from user where id \u003e ?\" rows, err := DB.Query(sqlStr, id) if err != nil { fmt.Printf(\"QueryMulti Failed, Error %v\\n\", err) return } // 关闭数据库连接 defer rows.Close() // 遍历读取的多条数据 for rows.Next() { err := rows.Scan(\u0026u.id, \u0026u.name, \u0026u.age) if err != nil { fmt.Printf(\"QueryMulti Scan Failed, Error %v\\n\", err) return } fmt.Printf(\"ID: %d Name: %s Age: %d \\n\", u.id, u.name, u.age) } } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() queryMultiRowToID(0) } # 输出结果 ID: 1 Name: jicki Age: 20 ID: 2 Name: Jack Age: 30 ID: 3 Name: Vicki Age: 30 插入数据操作 插入、更新、删除操作都可以使用Exec方法。 Exec 执行一次命令(包含查询、删除、更新、插入等), 返回的Result是对已执行的SQL命令的总结。 func (db *DB) Exec(query string, args ...interface{}) (Result, error) 方法 func insertRow(name string, age int) { sqlStr := \"insert into user(name,age) values (?,?)\" ret, err := DB.Exec(sqlStr, name, age) if err != nil { fmt.Printf(\"Insert Row Error %v\\n\", err) return } var inID int64 // 获取Last最后一次插入数据的ID inID, err = ret.LastInsertId() if err != nil { fmt.Printf(\"Get LastInsertId Error %v\\n\", err) return } fmt.Printf(\"Insert Success ID %d \\n\", inID) } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() insertRow(\"小炒肉\", 10) queryMultiRowToID(0) } # 输出结果 Insert Success ID 4 ID: 1 Name: jicki Age: 20 ID: 2 Name: Jack Age: 30 ID: 3 Name: Vicki Age: 30 ID: 4 Name: 小炒肉 Age: 10 更新数据操作 func updateRowToAge(age, id int) { sqlStr := \"update user set age = ? where id = ?\" ret, err := DB.Exec(sqlStr, age, id) if err != nil { fmt.Printf(\"Update Failed Error %v\\n\", err) return } var n int64 // RowsAffected 返回更新影响的行数 n, err = ret.RowsAffected() if err != nil { fmt.Printf(\"Get RowAffected Failed Error %v\\n\", err) return } fmt.Printf(\"Update Success Affected Row: %d \\n\", n) } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() queryRowToID(1) updateRowToAge(11, 1) queryRowToID(1) } # 输出结果 ID: 1 Name: jicki Age: 20 Update Success Affected Row: 1 ID: 1 Name: jicki Age: 11 删除数据操作 func deleteRowToID(id int) { sqlStr := \"delete from user where id = ?\" ret, err := DB.Exec(sqlStr, id) if err != nil { fmt.Printf(\"Delete Failed Error %v\\n\", err) return } var n int64 // RowsAffected 返回更新影响的行数 n, err = ret.RowsAffected() if err != nil { fmt.Printf(\"Get RowAffected Failed Error %v\\n\", err) return } fmt.Printf(\"Delete Success Affected Row: %d \\n\", n) } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() queryMultiRowToID(0) deleteRowToID(3) queryMultiRowToID(0) } # 输出结果 ID: 1 Name: jicki Age: 11 ID: 2 Name: Jack Age: 30 ID: 3 Name: Vicki Age: 30 ID: 4 Name: 小炒肉 Age: 10 Delete Success Affected Row: 1 ID: 1 Name: jicki Age: 11 ID: 2 Name: Jack Age: 30 ID: 4 Name: 小炒肉 Age: 10 ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:5","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"Mysql 预处理 预处理 普通SQL 语句执行过程: 客户端对 SQL 语句进行占位符替换得到完整的SQL 语句。 客户端发送完整SQL 语句到 MySQL 服务端。 MySQL 服务端执行完整的 SQL 语句并将结果返回给客户端。 预处理执行过程: 把SQL 语句分成两部分, 命令部分与数据部分。 把命令部分发送给 MySQL 服务端, MySQL 服务端进行SQL 预处理。 数据部分发送给 MySQL 服务端, MySQL 服务端对SQL 语句进行占位符替换。 MySQL 服务端执行完整的 SQL 语句并将结果返回给客户端。 预处理的优点: 优化 MySQL 服务重复执行SQL 语句, 可提高 MySQL 服务性能, 一次编译多次执行, 节省编译成本。 避免 SQL 注入的安全问题。 Go 实现MySQL 预处理 database/sql 包中使用 Prepare 方法实现预处理操作。 func (db *DB) Prepare(query string) (*Stmt, error) Prepare 方法会先将 SQL 语句发送给 MySQL 服务端, 服务端返回一个状态用于后续的查询和操作。返回值可以同时执行多个查询和操作。 预处理 查询例子 type user struct { id int name string age int } func perPareQuery(id int) { sqlStr := \"select id, name, age from user where id \u003e ?\" stmt, err := DB.Prepare(sqlStr) if err != nil { fmt.Printf(\"Prepare Failed Error %v\\n\", err) return } defer stmt.Close() rows, err := stmt.Query(id) if err != nil { fmt.Printf(\"Prepare Query Error %v\\n\", err) return } defer rows.Close() for rows.Next() { var u user err := rows.Scan(\u0026u.id, \u0026u.name, \u0026u.age) if err != nil { fmt.Printf(\"Perpare Scan Failed Error %v\\n\", err) return } fmt.Printf(\"ID: %d Name: %s Age: %d \\n\", u.id, u.name, u.age) } } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() perPareQuery(0) } # 输出结果 ID: 1 Name: jicki Age: 11 ID: 2 Name: Jack Age: 30 ID: 4 Name: 小炒肉 Age: 10 预处理 插入处理 func perPareInsert(name string, age int) { sqlStr := \"insert into user(name, age) values (?, ?)\" stmt, err := DB.Prepare(sqlStr) if err != nil { fmt.Printf(\"Prepare Failed Error %v\\n\", err) return } defer stmt.Close() result, err := stmt.Exec(name, age) if err != nil { fmt.Printf(\"Prepare Exec Error %v\\n\", err) return } InID, err := result.LastInsertId() if err != nil { fmt.Printf(\"Get LastInsertID Error %v \\n\", err) return } fmt.Printf(\"Insert ID: %d Success \\n\", InID) } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() perPareQuery(0) perPareInsert(\"大炒肉\", 20) perPareQuery(0) } # 输出结果 ID: 1 Name: jicki Age: 11 ID: 2 Name: Jack Age: 30 ID: 4 Name: 小炒肉 Age: 10 ID: 5 Name: 小小肉 Age: 20 Insert ID: 6 Success ID: 1 Name: jicki Age: 11 ID: 2 Name: Jack Age: 30 ID: 4 Name: 小炒肉 Age: 10 ID: 5 Name: 小小肉 Age: 20 ID: 6 Name: 大炒肉 Age: 20 SQL 注入问题 我们任何时候都不应该自己拼接SQL语句。 func sqlInject(name string) { sqlStr := fmt.Sprintf(\"select id,name,age from user where name='%s'\", name) fmt.Println(\"SQL: \", sqlStr) var u user err := DB.QueryRow(sqlStr).Scan(\u0026u.id, \u0026u.name, \u0026u.age) if err != nil { fmt.Printf(\"QueryRow Falied Error %v\\n\", err) return } fmt.Printf(\"ID: %d Name: %s Age: %d \\n\", u.id, u.name, u.age) } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() sqlInject(\"xxxx' or 1=1#\") } # 输出结果 SQL: select id,name,age from user where name='xxxx' or 1=1#' ID: 1 Name: jicki Age: 11 ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:6","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"Go 实现 MySQL 事务 MySQL 只有使用 Innodb 数据引擎的数据库或表才支持事务。事务处理可以用来维护数据库的完整性, 保证多条联合 SQL语句要么全部执行, 要么全部不执行(回滚)。 什么是事务 事务: 一个最小的不可再分的工作单元, 通常一个事务对应一个完整的业务, 这个完整的业务需要执行多次(增Insert, 删Delete, 改Update)语句共同联合完成。 事务的ACID 通常事务必须满足4个条件既满足(ACID) Atomictiy 原子性、不可分割性 一个事务中的所有操作, 要么全部完成, 要么全部不执行, 不会结束在中间环节。 一个事务在执行过程中出现错误, 整个事务都会回滚(Rollback)到事务开始前的状态。 Consistency 一致性 事务开始之前和事务结束以后, 数据的完整性没有被破坏。 数据写入必须完全符合所规定的规则, 包含数据的精准度、串联性以及后续自发性地完成预定工作。 Isolation 隔离性、独立性 数据库允许多个并发事务同时对数据进行读写和修改的能力。 隔离性可以防止多个事务并发执行时由于交叉执行导致的数据不一致问题。 事务隔离分为不同级别, 包括只读不提交(Read uncommitted)、读提交(Read committed)、可重复读(Repeatable Read)、串行化(Serializable)。 Durability 持久性 事务处理结束后, 对数据的修改是永久性的, 故障不丢失。 Go 操作事务的方法 开始事务 func (db *DB) Begin() (*Tx, error) 提交事务 func (db *DB) Commit() error 回滚事务 func (tx *Tx) Rollback() error 事务操作例子 # MySQL 表结果 ID: 1 Name: jicki Age: 11 ID: 2 Name: Jack Age: 18 ID: 4 Name: 小炒肉 Age: 10 ID: 5 Name: 小小肉 Age: 20 ID: 6 Name: 大炒肉 Age: 20 func transaction() { tx, err := DB.Begin() if err != nil { if tx != nil { err = tx.Rollback() if err != nil { fmt.Printf(\"Rollback Failed Error %v\\n\", err) return } fmt.Printf(\"Transaction Begin Error %v\\n\", err) return } } sqlStr := \"update user set age = 18 where id = ?\" rs1, err := tx.Exec(sqlStr, 2) if err != nil { err = tx.Rollback() fmt.Printf(\"rs1 Exec Failed Error %v\\n\", err) if err != nil { fmt.Printf(\"Rollback Failed Error %v\\n\", err) return } } rowID1, _ := rs1.RowsAffected() sqlStr = \"update user set age = 40 where id = ?\" rs2, err := tx.Exec(sqlStr, 3) if err != nil { err = tx.Rollback() fmt.Printf(\"rs2 Exec Failed Error %v\\n\", err) if err != nil { fmt.Printf(\"Rollback Failed Error %v\\n\", err) return } } rowID2, _ := rs2.RowsAffected() if rowID1 == 1 \u0026\u0026 rowID2 == 1 { fmt.Println(\"事务提交....\") err = tx.Commit() if err != nil { err = tx.Rollback() fmt.Printf(\"Exec Failed Error %v\\n\", err) if err != nil { fmt.Printf(\"Rollback Failed Error %v\\n\", err) return } } } else { fmt.Println(\"事务回滚....\") err = tx.Rollback() if err != nil { fmt.Printf(\"Rollback Failed Error %v\\n\", err) return } } } func main() { err := initDB() if err != nil { fmt.Printf(\"initDB failed, err:%v \\n\", err) return } defer DB.Close() transaction() } # 输出结果 事务回滚.... ","date":"2000-01-01","objectID":"/golang-web-note-4/:2:7","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"sqlx 模块 官方地址 https://github.com/jmoiron/sqlx sqlx 是 go 标准库 database/sql 的一个扩展库, 在保持 database/sql 标准接口不变的情况下增加了一些扩展。 sqlx 的扩展 可将行记录映射给 struct（内嵌struct也支持）， map与slices 支持在preprared statement 中使用命名参数 将Get和Select的查询结果保存到struct 和 slice 中 ","date":"2000-01-01","objectID":"/golang-web-note-4/:3:0","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"sqlx 模块使用 go get -u github.com/jmoiron/sqlx ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:0","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"初始化数据库 package main import ( \"fmt\" \"time\" // 引入 mysql 驱动, 只需要用到 init() 方法 _ \"github.com/go-sql-driver/mysql\" \"github.com/jmoiron/sqlx\" ) // 定义全局的 数据库对象 DB var DB *sqlx.DB func initDB() (err error) { dsn := \"root:jicki@tcp(127.0.0.1:13306)/jicki?charset=utf8mb4\u0026parseTime=True\" // 这里 DB 赋值是给 上面定义的全局变量赋值, 不要使用 := DB, err = sqlx.Connect(\"mysql\", dsn) if err != nil { return } // 连接存活时间,超时会被自动关闭 DB.SetConnMaxLifetime(time.Second * 10) // 最大连接数 DB.SetMaxOpenConns(100) // 连接池中空闲连接数 DB.SetMaxIdleConns(50) return } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } } ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:1","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"插入数据库 /* 数据库 sqlx 表 CREATE TABLE `sqlx` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(64) NOT NULL, `age` int(6) NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; */ // 创建一个结构体,并为 db 打上标签 type user struct { Id int `db:\"id\"` Name string `db:\"name\"` Age int `db:\"age\"` } func InsertSqlX(u user) { // (:后面是数据库字段), 最后传入整个 结构体 sqlStr := \"insert into sqlx values (:id, :name, :age)\" result, err := DB.NamedExec(sqlStr, u) if err != nil { fmt.Printf(\"Insert NamedExec Failed Error %v\\n\", err) return } var TheID int64 TheID, err = result.LastInsertId() if err != nil { fmt.Printf(\"InsertSqlx Get RowsAffected Falied Error %v\\n\", err) return } fmt.Printf(\"InsertSqlx Exec Success %d \\n\", TheID) } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } s1 := user{ Id: 1, Name: \"小炒肉\", Age: 20, } s2 := user{ Id: 2, Name: \"大炒肉\", Age: 30, } InsertSqlX(s1) InsertSqlX(s2) } # 输出结果 InsertSqlx Exec Success 1 InsertSqlx Exec Success 2 ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:2","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"查询数据 查询单条数据 // 创建一个结构体,并为 db 打上标签 type User struct { Id int `db:\"id\"` Name string `db:\"name\"` Age int `db:\"age\"` } func queryRowToID(id int64) { sqlStr := \"select id, name, age from sqlx where id = ?\" var u User err := DB.Get(\u0026u, sqlStr, id) if err != nil { fmt.Printf(\"QueryRowToID Get Failed Error %v\\n\", err) return } fmt.Printf(\"ID: %d Name: %s Age: %d \\n\", u.Id, u.Name, u.Age) } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } queryRowToID(1) } # 输出结果 ID: 1 Name: 小炒肉 Age: 20 查询多条数据 // 创建一个结构体,并为 db 打上标签 type User struct { Id int `db:\"id\"` Name string `db:\"name\"` Age int `db:\"age\"` } func queryMultiRows(id int64) { sqlStr := \"select id, name, age from sqlx where id \u003e ?\" // 查询的结果存储到 User结构体类型的切片 u 中 var u []User // Select 方法用于查询多条数据 err := DB.Select(\u0026u, sqlStr, id) if err != nil { fmt.Printf(\"QueryMultiRows Select Failed Error %v\\n\", err) return } for _, v := range u { fmt.Printf(\"ID: %d Name: %s Age: %d \\n\", v.Id, v.Name, v.Age) } } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } queryMultiRows(0) } # 输出结果 ID: 1 Name: 小炒肉 Age: 20 ID: 2 Name: 大炒肉 Age: 30 ID: 3 Name: 小小肉 Age: 18 查询指定的多条数据 // 批量查询 func queryRowByIDs(ids []int) (users []User, err error) { query, args, err := sqlx.In(\"select id,name,age FROM sqlx WHERE id IN (?)\", ids) if err != nil { fmt.Printf(\"QueryRowByIDs In Failed Error %v\\n\", err) return } // 使用 Rebind() 重新绑定 组合后的 SQL 语句 query = DB.Rebind(query) err = DB.Select(\u0026users, query, args...) for _, user := range users { fmt.Printf(\"ID: %d Name: %s Age: %d \\n\", user.Id, user.Name, user.Age) } return } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } // 这里返回一个 []user 的切片 和 一个 error _, err := queryRowByIDs([]int{1, 3, 5, 6}) if err != nil { fmt.Printf(\"QueryRowByIDs Failed Error %v\\n\", err) return } } # 输出结果 ID: 1 Name: 小炒肉 Age: 22 ID: 5 Name: 大肉肉 Age: 48 ID: 6 Name: 中肉肉 Age: 38 ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:3","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"更新数据 更新数据 func updateRow(id, age int64) { sqlStr := \"update sqlx set age = ? where id = ?\" result, err := DB.Exec(sqlStr, age, id) if err != nil { fmt.Printf(\"UpdateRow Exec Failed Error %v\\n\", err) return } var n int64 n, err = result.RowsAffected() if err != nil { fmt.Printf(\"updateRow Get RowsAffected Failed Error %v\\n\", err) return } fmt.Printf(\"update id: %d To age = %d Success Rows: %d \\n\", id, age, n) } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } queryMultiRows(0) updateRow(1, 22) queryMultiRows(0) } # 输出结果 ID: 1 Name: 小炒肉 Age: 20 ID: 2 Name: 大炒肉 Age: 30 ID: 3 Name: 小小肉 Age: 18 update id: 1 To age = 22 Success Rows: 1 ID: 1 Name: 小炒肉 Age: 22 ID: 2 Name: 大炒肉 Age: 30 ID: 3 Name: 小小肉 Age: 18 ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:4","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"删除数据 删除数据 func deleteRowToId(id int64) { sqlStr := \"delete from sqlx where id = ?\" result, err := DB.Exec(sqlStr, id) if err != nil { fmt.Printf(\"UpdateRow Exec Failed Error %v\\n\", err) return } var n int64 n, err = result.RowsAffected() if err != nil { fmt.Printf(\"DeleteRow Get RowsAffected Failed Error %v\\n\", err) return } fmt.Printf(\"delete id: %d Success Rows: %d \\n\", id, n) } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } queryMultiRows(0) deleteRowToId(3) queryMultiRows(0) } # 输出结果 ID: 1 Name: 小炒肉 Age: 22 ID: 2 Name: 大炒肉 Age: 30 ID: 3 Name: 小小肉 Age: 18 delete id: 3 Success Rows: 1 ID: 1 Name: 小炒肉 Age: 22 ID: 2 Name: 大炒肉 Age: 30 ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:5","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"事务的操作 事务操作的例子 func transaction() (err error) { tx, err := DB.Begin() if err != nil { fmt.Printf(\"Transaction Begin Failed Error %v \\n\", err) return } // 这里使用 defer 调用一个 匿名函数, 用于捕获 panic 执行 Rollback 操作 defer func() { if p := recover(); p != nil { tx.Rollback() panic(p) } else if err != nil { fmt.Println(\"RollBack...\") tx.Rollback() } else { err = tx.Commit() fmt.Println(\"Commit...\") } }() sqlStr1 := \"update sqlx set age = 22 where id = ?\" result, err := tx.Exec(sqlStr1, 1) if err != nil { return err } n, err := result.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\"exec sqlStr1 Failed\") } sqlStr2 := \"update sqlx set age = 22 where id = ?\" result, err = tx.Exec(sqlStr2, 2) if err != nil { return err } n, err = result.RowsAffected() if err != nil { return err } if n != 1 { return errors.New(\"exec sqlStr2 Failed\") } return err } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } if err := transaction(); err != nil { fmt.Println(\"Transaction Error \", err) } } # 输出结果 RollBack... Transaction Error exec sqlStr1 Failed ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:6","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"批量插入数据 sqlx.In 函数 sqlx.In 是 sqlx 提供的一个函数。 批量插入数据 # user 表结构如下: +-------+-------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+----------------+ | id | bigint | NO | PRI | NULL | auto_increment | | name | varchar(20) | YES | | | | | age | int | YES | | 0 | | +-------+-------------+------+-----+---------+----------------+ # User 结构体如下: type User struct { Id int `db:\"id\"` Name string `db:\"name\"` Age int `db:\"age\"` } 使用 sqlx.In 需要对数据映射的结构体实现一个 driver.Valuer 的接口返回值的方法. func (u User) Value() (driver.Value, error) { return []interface{}{u.Name, u.Age}, nil } // SqlX.In func BatchInsertSqlIn(users []interface{}) error { query, args, _ := sqlx.In(\"INSERT INTO user (name, age) VALUES (?),(?),(?)\", users...) fmt.Println(query) fmt.Println(args) _, err := DB.Exec(query, args...) return err } func main() { if err := initDB(); err != nil { fmt.Printf(\"InitDB Failed Error %v\\n\", err) return } u1 := User{ Name: \"小肉肉\", Age: 28, } u2 := User{ Name: \"大肉肉\", Age: 48, } u3 := User{ Name: \"中肉肉\", Age: 38, } // 需要先创建 interface 的切片 users := []interface{}{u1, u2, u3} if err := BatchInsertSqlIn(users); err != nil { fmt.Printf(\"Batch Insert Failed Error %v \\n\", err) return } queryMultiRows(0) } INSERT INTO user (name, age) VALUES (?, ?),(?, ?),(?, ?) [小肉肉 28 大肉肉 48 中肉肉 38] ID: 1 Name: 小炒肉 Age: 22 ID: 2 Name: 大炒肉 Age: 30 ID: 4 Name: 小肉肉 Age: 28 ID: 5 Name: 大肉肉 Age: 48 ID: 6 Name: 中肉肉 Age: 38 NamedExec 函数 注意: sqlx 版本必须 \u003e= v1.2.1 批量插入数据 # User 结构体如下: type User struct { Id int `db:\"id\"` Name string `db:\"name\"` Age int `db:\"age\"` } // NamedExec func BatchInsertNamed(users []*User) error { _, err := DB.NamedExec(\"INSERT INTO user (name, age) VALUES (:name, :age)\", users) return err } ","date":"2000-01-01","objectID":"/golang-web-note-4/:4:7","tags":["golang"],"title":"Golang 操作 Mysql","uri":"/golang-web-note-4/"},{"categories":["golang"],"content":"Golang 正则表达式","date":"2000-01-01","objectID":"/golang-study-note-11/","tags":["golang"],"title":"Golang 正则表达式","uri":"/golang-study-note-11/"},{"categories":["golang"],"content":"正则表达式 在 Golang 中 正则表达式 使用 regexp 包 ","date":"2000-01-01","objectID":"/golang-study-note-11/:0:0","tags":["golang"],"title":"Golang 正则表达式","uri":"/golang-study-note-11/"},{"categories":["golang"],"content":"常用正则语法 语法 说明 表达式实例 \\d 表示数字[0-9] a\\dc \\D 表示非数字的其他字符 a\\Dc \\w 单词字符: 大小写字母、数字、下划线 a\\wc \\W 非单词字符 a\\Wc \\s 空白字符: \\t、\\n、\\r、\\f 其中之一 a\\sc \\S 非空白字符 a\\Sc . 除了换行符之外的任意字符 a.c \\. 表示符号 . abc\\.com + 匹配前一个字符1次或无限次 abc+ * 匹配前一个字符0或无限次 abc* ? 匹配前一个字符0次或1次 abc? {m} 匹配前一个字符m次 ab{2}c {m,n} 匹配前一个字符m至n次。 ab{1,2}c {,n} 匹配前面一个字符0至n次。 ab{,2}c {m,} 匹配前面一个字符m至无限次 ab{3,} [...] 匹配[]内的字符其中之一 a[bcd]c [\\s\\S] 匹配任意字符 12[\\s\\S]d [a-z] 匹配 a到z中的任意一个小写字符 1[a-z]2 [^abc] 匹配 除了abc之外的任意字符 aa[^abc]bb | 代表左右表达式任意匹配一个 abc|def ^ 匹配字符串开头 ^abc $ 匹配字符串末尾 abc$ ","date":"2000-01-01","objectID":"/golang-study-note-11/:1:0","tags":["golang"],"title":"Golang 正则表达式","uri":"/golang-study-note-11/"},{"categories":["golang"],"content":"regexp 模块使用 package main import ( \"fmt\" \"regexp\" ) func main() { buf := \"abc azc a7c aac 888 a9c tac\" //1) 解释规则, 解析正则表达式，如果成功返回解释器 reg1 := regexp.MustCompile(`a[^a]c`) if reg1 == nil { fmt.Println(\"regexp err\") return } //2) 根据规则提取关键信息 result1 := reg1.FindAllStringSubmatch(buf, -1) fmt.Println(\"result1 = \", result1) } 输出结果 result1 = [[abc] [azc] [a7c] [a9c]] ","date":"2000-01-01","objectID":"/golang-study-note-11/:2:0","tags":["golang"],"title":"Golang 正则表达式","uri":"/golang-study-note-11/"},{"categories":["golang"],"content":"模拟过滤页面 package main import ( \"fmt\" \"regexp\" ) func main() { //`` 截取了一段页面原生字符串 buf := ` \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"\u003e \u003cmeta name=\"google-site-verification\" content=\"xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI\" /\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1, viewport-fit=cover\"\u003e \u003cmeta name=\"description\" content=\"Your future depends on your dreams\"\u003e \u003cmeta name=\"keywords\" content=\"小炒肉, 运维工程师, Jicki, DevOps, Docker, Kubernetes\"\u003e \u003cmeta name=\"theme-color\" content=\"#000000\"\u003e \u003c!-- Open Graph --\u003e \u003cmeta property=\"og:title\" content=\"About - 小炒肉 Blog | Jicki Blog\"\u003e \u003cmeta property=\"og:type\" content=\"website\"\u003e \u003cmeta property=\"og:description\" content=\"你是我的梦想\"\u003e \u003cmeta property=\"og:image\" content=\"https://jicki.cn/img/avatar-jicki.png\"\u003e \u003cmeta property=\"og:url\" content=\"https://jicki.cn/about/\"\u003e \u003cmeta property=\"og:site_name\" content=\"小炒肉 Blog | Jicki Blog\"\u003e \u003ctitle\u003eAbout - 小炒肉 Blog | Jicki Blog\u003c/title\u003e \u003cheader class=\"intro-header\" style=\"background-image: url('/img/about-bg.jpg')\"\u003e \u003cdiv class=\"container\"\u003e \u003cdiv class=\"row\"\u003e \u003cdiv class=\"col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1\"\u003e \u003cdiv class=\"site-heading\"\u003e \u003ch1\u003eAbout\u003c/h1\u003e \u003cspan class=\"subheading\"\u003e你是我的梦想\u003c/span\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/header\u003e \u003c!-- Main Content --\u003e \u003cdiv class=\"container\"\u003e \u003cdiv class=\"row\"\u003e \u003c!-- USE SIDEBAR --\u003e \u003c!-- PostList Container --\u003e \u003cdiv class=\" col-lg-8 col-lg-offset-1 col-md-8 col-md-offset-1 col-sm-12 col-xs-12 postlist-container \"\u003e \u003c!-- Language Selector --\u003e \u003cselect class=\"sel-lang\" onchange= \"onLanChange(this.options[this.options.selectedIndex].value)\"\u003e \u003coption value=\"0\" selected\u003e 中文 Chinese \u003c/option\u003e \u003coption value=\"1\"\u003e 英文 English \u003c/option\u003e \u003c/select\u003e \u003c!-- Chinese Version --\u003e \u003cdiv class=\"zh post-container\"\u003e \u003cblockquote\u003e \u003cp\u003e搞搞 docker, 弄弄 kubernetes,\u003c/p\u003e \u003cp\u003e学学 golang, 写写 shell。\u003c/p\u003e \u003c/blockquote\u003e \u003ch3 id=\"一-序\"\u003e一、 序\u003c/h3\u003e \u003cblockquote\u003e \u003cp\u003e\u003cstrong\u003eYour future depends on your dreams\u003c/strong\u003e\u003c/p\u003e \u003c/blockquote\u003e \u003cblockquote\u003e \u003cp\u003e\u003cstrong\u003e不要活在别人的眼里，不要活在别人的嘴里\u003c/strong\u003e\u003c/p\u003e \u003c/blockquote\u003e \u003cblockquote\u003e \u003cp\u003e\u003cstrong\u003e要活在自己的心里，生活过的洒脱一点，不要为别人去活\u003c/strong\u003e\u003c/p\u003e \u003c/blockquote\u003e \u003c/body\u003e \u003c/html\u003e ` //解释正则表达式, 匹配标签内的字符 reg1 := regexp.MustCompile(`\u003ctitle\u003e(?s:(.*?))\u003c/title\u003e`) reg2 := regexp.MustCompile(`\u003cp\u003e\u003cstrong\u003e(?s:(.*?))\u003c/strong\u003e\u003c/p\u003e`) if reg1 == nil { fmt.Println(\"MustCompile err\") return } if reg2 == nil { fmt.Println(\"MustCompile err\") return } //提取关键信息 result1 := reg1.FindAllStringSubmatch(buf, -1) result2 := reg2.FindAllStringSubmatch(buf, -1) //过滤\u003c\u003e\u003c/\u003e for _, text := range result1 { //过滤不带标签的 不带\u003c\u003e\u003c/\u003e fmt.Println(\"text[1] = \", text[1]) } //过滤\u003c\u003e\u003c/\u003e for _, text := range result2 { //过滤不带标签的 不带\u003c\u003e\u003c/\u003e fmt.Println(\"text[2] = \", text[1]) } } 输出如下: text[1] = About - 小炒肉 Blog | Jicki Blog text[2] = Your future depends on your dreams text[2] = 不要活在别人的眼里，不要活在别人的嘴里 text[2] = 要活在自己的心里，生活过的洒脱一点，不要为别人去活 ","date":"2000-01-01","objectID":"/golang-study-note-11/:3:0","tags":["golang"],"title":"Golang 正则表达式","uri":"/golang-study-note-11/"},{"categories":["golang"],"content":"GORM MYSQL","date":"2000-01-01","objectID":"/golang-web-note-5/","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-5/:0:0","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"GORM ORM（Object Relation Mapping）, 对象关系映射, 实际上就是对数据库的操作进行封装, 对上层开发人员屏蔽数据操作的细节, 开发人员看到的就是一个个对象, 大大简化了开发工作, 提高了生产效率 O = Object 对象 (程序中的对象/实例 如: Struct 结构体) R = Relational 关系 (关系型数据库 如: Mysql) M = Mapping 映射 结构体 –\u003e 数据表 结构体实例 –\u003e 数据库行 结构体字段 –\u003e 数据库字段 ORM 优缺点 优点 - 提高开发效率, 不需要使用SQL语句。 缺点 - 执行性能较差、灵活性较差、弱化了SQL能力。 ","date":"2000-01-01","objectID":"/golang-web-note-5/:1:0","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"GORM 概览 全功能 ORM (无限接近) 关联 (Has One, Has Many, Belongs To, Many To Many, 多态) 钩子 (在创建/保存/更新/删除/查找之前或之后) 预加载 事务 复合主键 SQL 生成器 数据库自动迁移 自定义日志 可扩展性, 可基于 GORM 回调编写插件 所有功能都被测试覆盖 开发者友好 ","date":"2000-01-01","objectID":"/golang-web-note-5/:1:1","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"gorm 模块用法 go get -u github.com/jinzhu/gorm gorm to mysql import ( \"github.com/jinzhu/gorm\" _ \"github.com/jinzhu/gorm/dialects/mysql\" ） var db *gorm.DB func init() { var err error db, err = gorm.Open(\"mysql\", \"\u003cuser\u003e:\u003cpassword\u003e/\u003cdatabase\u003e?charset=utf8\u0026parseTime=True\u0026loc=Local\") if err != nil { panic(err) } // 设置连接池 db.DB().SetMaxIdleConns(10) db.DB().SetMaxOpenConns(100) } ","date":"2000-01-01","objectID":"/golang-web-note-5/:1:2","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"GORM Model 在使用ORM工具时，通常我们需要在代码中定义模型（Models）与数据库中的数据表进行映射，在GORM中模型（Models）通常是正常定义的结构体、基本的go类型或它们的指针。 同时也支持sql.Scanner及driver.Valuer、接口interfaces。 GORM 内置了一个gorm.Model结构体。gorm.Model是一个包含了ID 、 CreatedAt 、UpdatedAt 、 DeletedAt四个字段的Golang结构体。 // gorm.Model 定义 type Model struct { ID uint `gorm:\"primary_key\"` CreatedAt time.Time UpdatedAt time.Time DeletedAt *time.Time } 嵌入到自己的结构体模型中 // 定义 数据模型 type UserInfo struct { gorm.Model // 内嵌 Model 模型 Name string Age sql.NullInt64 //零值类型 Birthday *time.Time Email string `gorm:\"type:varchar(100);unique_index\"` Role string `gorm:\"size:255\"` // 限制大小为最多255 MemberNumber *string `gorm:\"unique;not null\"` // 设置会员号唯一,且不能为空。 Num int `gorm:\"AUTO_INCREMENT\"` // 设置字段 num 自增类型。 Address string `gorm:\"index:addr\"` // 给address字段创建名为addr的索引 IgnoreMe int `gorm:\"-\"` // 忽略本字段 } GORM 结构体标识 (struct tag) 结构体标识(Tga) 描述 Column 指定列名 Type 指定列数据类型 Size 指定列大小, 默认值255 PRIMARY_KEY 将列指定为主键 UNIQUE 将列指定为唯一 DEFAULT 指定列默认值 PRECISION 指定列精度 NOT NULL 将列指定为非 NULL AUTO_INCREMENT 指定列是否为自增类型 INDEX 创建具有或不带名称的索引, 如果多个索引同名则创建复合索引 UNIQUE_INDEX 和 INDEX 类似，只不过创建的是唯一索引 EMBEDDED 将结构设置为嵌入 EMBEDDED_PREFIX 设置嵌入结构的前缀 - 忽略此字段 GORM 创建表时 set 的标识 创建表的标识 描述 MANY2MANY 指定连接表 FOREIGNKEY 设置外键 ASSOCIATION_FOREIGNKEY 设置关联外键 POLYMORPHIC 指定多态类型 POLYMORPHIC_VALUE 指定多态值 JOINTABLE_FOREIGNKEY 指定连接表的外键 ASSOCIATION_JOINTABLE_FOREIGNKEY 指定连接表的关联外键 SAVE_ASSOCIATIONS 是否自动完成 save 的相关操作 ASSOCIATION_AUTOUPDATE 是否自动完成 update 的相关操作 ASSOCIATION_AUTOCREATE 是否自动完成 create 的相关操作 ASSOCIATION_SAVE_REFERENCE 是否自动完成引用的 save 的相关操作 PRELOAD 是否自动完成预加载的相关操作 主键、表名、列名的约定 主键 (Primary Key) , GORM 默认会使用名为ID 这个字段名作为表的主键。 修改默认的 主键名为 其他, 需要使用结构体 tag grom:\"primary_key\" 来约定。 type User struct { ID string // 名为`ID`的字段会默认作为表的主键 Name string } // 使用`AnimalID`作为主键 type Animal struct { AnimalID int64 `gorm:\"primary_key\"` Name string Age int64 } 表名 (Table Name) GROM 创建数据表, 表名默认就是结构体名称的复数, 如: User = users , UserInfo = user_infos 修改默认的 数据表名称。 // 将 User 的表名设置为 `profiles` func (User) TableName() string { return \"profiles\" } // 多个条件判断 func (u User) TableName() string { if u.Role == \"admin\" { return \"admin_users\" } else { return \"users\" } } // 禁用默认表名的复数形式，如果置为 `true，则 `User` 的默认表名是 `user` db.SingularTable(true) // table 可以指定表名 // 使用User结构体创建名为`deleted_users`的表 db.Table(\"deleted_users\").CreateTable(\u0026User{}) // 修改默认表的规则 (格式为job_表明) gorm.DefaultTableNameHandler = func (db *gorm.DB, defaultTableName string) string { return \"job_\" + defaultTableName; } 数据列 名称 (Column Name) 列名 是由 结构体字段的名称组合,一个字段多个单词组成 会使用 _ 分割开。如: MemberNumber = member_number 修改默认的 数据列名称, 可使用结构体tag 中的 column 指定列名。 type Animal struct { AnimalId int64 `gorm:\"column:beast_id\"` Birthday time.Time `gorm:\"column:beast_day\"` Age int64 `gorm:\"column:beast_age\"` } 时间类型 (At) CreatedAt字段, 该字段的值将会是初次创建记录的时间。 UpdatedAt字段，该字段的值将会是每次更新记录的时间。 DeletedAt字段，调用Delete方法删除该记录时，将会设置DeletedAt字段为当前时间，而不是直接将记录从数据库中删除。 ","date":"2000-01-01","objectID":"/golang-web-note-5/:1:3","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"GORM CRUD CRUD 增、删、改、查 增 增加(create) // 定义 数据模型 type User struct { ID int64 // 使用 tag default 设置默认值 Name string `gorm:\"default:'小炒肉'\"` Age int64 } func main() { dsn := \"jicki:jicki123@tcp(127.0.0.1:3306)/jicki?charset=utf8mb4\u0026parseTime=true\" db, err := gorm.Open(\"mysql\", dsn) if err != nil { panic(err) } defer db.Close() // 创建 结构体User 对应的数据表 db.AutoMigrate(\u0026User{}) // 创建结构体实例 u1 := User{ Name: \"小炒肉\", Age: 20, } // 判断主键是否存在,可用于判断数据是否成功写入。 db.NewRecord(\u0026u1) // 创建(插入)数据 db.Create(\u0026u1) } 注意: 使用 结构体 tag default 设置默认值的时候, 所有字段的零值, 比如0, \"\", false或者其它零值，都不会保存到数据库内，但会使用他们的默认值。如需要零值/空值, 可以使用: Name *string gorm:“default:‘小炒肉’“`` // 定义 数据模型 type User struct { ID int64 // 使用 tag default 设置默认值 Name *string `gorm:\"default:'小炒肉'\"` Age int64 } func main(){ // 创建结构体实例 u1 := User{ Name: new(string), Age: 20, } // 判断主键是否存在,可用于判断数据是否成功写入。 db.NewRecord(\u0026u1) // 创建(插入)数据 db.Create(\u0026u1) } Name sql.NullString gorm:“default:‘小炒肉’“`` type User struct { ID int64 // 使用 tag default 设置默认值 Name sql.NullString `gorm:\"default:'小炒肉'\"` Age int64 } func main(){ // 创建结构体实例 u1 := User{ Name: sql.NullString{\"\", true}, Age: 20, } // 判断主键是否存在,可用于判断数据是否成功写入。 db.NewRecord(\u0026u1) // 创建(插入)数据 db.Create(\u0026u1) } 查询 查询 (First、Take、Last、Find) func main() { // 查询 select // 定义一个User结构体类型的变量用于存储查询返回的数据 var user User // 定义一个 User结构体类型的切片,用于存储查询返回的多条数据 var users []User // First 取第一条数据 db.First(\u0026user) fmt.Printf(\"DB First User: %v \\n\", user) // Take 随机取一条数据 db.Take(\u0026user) fmt.Printf(\"DB Take User: %v \\n\", user) // Last 根据主键 的顺序查询最后一条数据 db.Last(\u0026user) fmt.Printf(\"DB Last User: %v \\n\", user) // Find 查询所有的数据 db.Find(\u0026users) for _, user := range users { fmt.Printf(\"DB Find Users: %v \\n\", user) } // First 也可以查询指定的数据, 当主键为整型时可用 db.Debug().First(\u0026user, 2) fmt.Printf(\"DB First 2 User: %v \\n\", user) } 条件选取 (Where) func main() { // Where 条件查询 // 过滤查询单条的数据 db.Where(\"name = ?\", \"大炒肉\").First(\u0026user) fmt.Printf(\"Where First: %v \\n\", user) // 过滤查询所有的数据 db.Where(\"name = ?\", \"大炒肉\").Find(\u0026users) fmt.Printf(\"Where Find: %v \\n\", users) // 过滤查询所有的数据 \u003c\u003e 不等于 db.Where(\"name \u003c\u003e ?\", \"炒肉\").Find(\u0026users) fmt.Printf(\"Where Find \u003c\u003e : %v \\n\", users) // 多个条件 IN 查询所有数据 db.Where(\"name IN(?)\", []string{\"小炒肉\", \"大炒肉\"}).Find(\u0026users) fmt.Printf(\"Where Find IN : %v \\n\", users) // 模糊匹配 查询所有的数据 db.Where(\"name LIKE ?\", \"%小%\").Find(\u0026users) fmt.Printf(\"Where Find LIKE : %v \\n\", users) // AND 匹配 db.Where(\"name =? AND age = ?\", \"小炒肉\", \"20\").Find(\u0026users) fmt.Printf(\"Where Find AND : %v \\n\", users) // Time 时间匹配 db.Where(\"updated_at \u003e ?\", \"2020-03-02 06:41:44\").Find(\u0026users) fmt.Printf(\"Where Find Time : %v \\n\", users) // BETWEEN 区间时间 db.Where(\"created_at BETWEEN ? AND ?\", \"2020-01-01 09:09:01\", \"2020-03-02 06:41:50\").Find(\u0026users) fmt.Printf(\"Where Find BETWEEN : %v \\n\", users) // Where Struct 查询条件进行过滤查询 db.Where(\u0026User{Name: \"小炒肉\", Age: 20}).First(\u0026user) fmt.Printf(\"Where Struct : %v \\n\", user) // Where Map 查询条件进行过滤查询 db.Where(map[string]interface{}{\"name\": \"大炒肉\", \"age\": 30}).Find(\u0026users) fmt.Printf(\"Where Map : %v \\n\", users) // Where 主键切片 查询条件 进行过滤查询 db.Where([]int64{1, 2}).Find(\u0026users) fmt.Printf(\"Where 切片 : %v \\n\", users) } 提示：当通过结构体进行查询时，GORM将会只通过非零值字段查询，这意味着如果你的字段值为0，''，false或者其他零值时，将不会被用于构建查询条件。 条件排除 (Not) func main() { dsn := \"jicki:jicki123@tcp(127.0.0.1:3306)/jicki?charset=utf8mb4\u0026parseTime=true\" db, err := gorm.Open(\"mysql\", dsn) if err != nil { panic(err) } defer db.Close() // 创建 结构体User 对应的数据表 db.AutoMigrate(\u0026User{}) // 查询 select // 定义一个User结构体类型的变量用于存储查询返回的数据 var user User // 定义一个 User结构体类型的切片,用于存储查询返回的多条数据 var users []User // Not db.Not(\"name\", \"小小\").First(\u0026user) fmt.Printf(\"Not First : %v \\n\", user) // Not In db.Not(\"name\", []string{\"小炒肉\", \"大炒肉\"}).Find(\u0026users) fmt.Printf(\"Not In Find : %v \\n\", users) // Not In Slice 主键, 主键必须为 Int 类型 db.Not([]int64{2}).First(\u0026user) fmt.Printf(\"Not Slice First : %v \\n\", user) // Not Sql 写法 db.Not(\"name = ? \", \"小炒肉\").First(\u0026user) fmt.Printf(\"Not Sql First : %v \\n\", user) // Not Struct db.Not(\u0026User{Name: \"小炒肉\"}).First(\u0026user) fmt.Printf(\"Not Struct First : %v \\n\", user) } OR (或) func main() { d","date":"2000-01-01","objectID":"/golang-web-note-5/:1:4","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"高级查询 子查询 (QueryExpr) # 一个官方例子 db.Where(\"amount \u003e ?\", DB.Table(\"orders\").Select(\"AVG(amount)\").Where(\"state = ?\", \"paid\").QueryExpr()).Find(\u0026orders) // SELECT * FROM \"orders\" WHERE \"orders\".\"deleted_at\" IS NULL AND (amount \u003e (SELECT AVG(amount) FROM \"orders\" WHERE (state = 'paid'))); 选择字段 (Select) func main() { dsn := \"jicki:jicki123@tcp(127.0.0.1:3306)/jicki?charset=utf8mb4\u0026parseTime=true\" db, err := gorm.Open(\"mysql\", dsn) if err != nil { panic(err) } defer db.Close() // 创建 结构体User 对应的数据表 db.AutoMigrate(\u0026User{}) // 查询 select // 定义一个User结构体类型的变量用于存储查询返回的数据 //var user User var users []User // Select 选择字段 db.Select(\"name,age\").Find(\u0026users) fmt.Printf(\"Select Find : %v \\n\", users) // Select Map db.Select([]string{\"name\", \"age\"}).Find(\u0026users) fmt.Printf(\"Select Map Find : %v \\n\", users) } 排序 (Order) 定从数据库中检索出记录的顺序。设置第二个参数 reorder 为 true ，可以覆盖前面定义的排序条件。 // 排序 Order // desc: 降序 db.Order(\"age desc, name\").Find(\u0026users) fmt.Printf(\"Order : %v \\n\", users) // 多字段排序 db.Order(\"age desc\").Order(\"name\").Find(\u0026users) fmt.Printf(\"Order : %v \\n\", users) 数量 (Limit) 指定从数据库检索出的最大记录数。 // 限制数量 Limit db.Limit(3).Find(\u0026users) fmt.Printf(\"Limit 3 : %v \\n\", users) 偏移 (Offset) 指定开始返回记录前要跳过的记录数。 // 偏移 Offset db.Offset(3).Find(\u0026users) fmt.Printf(\"Offset 3 : %v \\n\", users) 总数 (Count) 获取记录的总数。 Count 必须是链式查询的最后一个操作 ，因为它会覆盖前面的 SELECT，但如果里面使用了 count 时不会覆盖。 func main() { var users []User var count int // 总数 Count db.Find(\u0026users).Count(\u0026count) fmt.Printf(\"Count %d \\n\", count) } 扫描 (Scan) 扫描结果记录到一个 Struct 实例中。 // 定义 数据模型 type User struct { gorm.Model // 使用 tag default 设置默认值 Name string `gorm:\"default:'小炒肉'\"` Age int64 `gorm:\"default:99\"` } type Result struct { Name string Age int64 } func main() { dsn := \"jicki:jicki123@tcp(127.0.0.1:3306)/jicki?charset=utf8mb4\u0026parseTime=true\" db, err := gorm.Open(\"mysql\", dsn) if err != nil { panic(err) } defer db.Close() // 创建 结构体User 对应的数据表 db.AutoMigrate(\u0026User{}) // 定义一个 Result 结构体类型的实例 result var result Result // 定义一个 Result切片类型的结构体 实例 results var results []Result // Scan 扫描单个结果 保存到 result 实例中。 db.Table(\"users\").Select(\"name, age\").Where(\"name = ?\", \"小炒肉\").Scan(\u0026result) fmt.Printf(\"Scan Result : %v \\n\", result) // Scan 扫描多个结构 保存到 results 实例中。 db.Table(\"users\").Select(\"name, age\").Where(\"id \u003e ?\", 0).Scan(\u0026results) fmt.Printf(\"Scan Results : %v \\n\", results) // 使用SQL 语句 db.Raw(\"SELECT name, age FROM users WHERE age \u003e ?\", 30).Scan(\u0026results) fmt.Printf(\"Scan Results : %v \\n\", results) } 立即执行方法 (Immediate Methods) 立即执行方法是指那些会立即执行SQL语句并发送到数据库的方法, 如: Create、First、Find、Take、Save、Update、Delete、Scan、Row 等. 多个立即执行方法，后一个立即执行方法会复用前面那个立刻执行方法的条件( 不包含内联条件 )。 func main() { var user User var count int64 // 立即执行方法 db.Debug().Select(\"name\").Where(\"name = ?\", \"小炒肉\").First(\u0026user) // 执行语句: SELECT name FROM `users` WHERE `users`.`deleted_at` IS NULL AND ((name = '小炒 肉')) ORDER BY `users`.`id` ASC LIMIT 1 // 多个立即执行条件, 后一个会复用前一个的条件. ( 内联条件除外 ) db.Debug().Where(\"name LIKE ?\", \"%炒肉\").Find(\u0026users, \"id IN (?)\", []int64{2, 3, 4}).Count(\u0026count) fmt.Printf(\"Users : %v \\n Count : %d \\n\", users, count) // 实际语句为以下两条: // SELECT * FROM `users` WHERE `users`.`deleted_at` IS NULL AND ((name LIKE '%炒肉') AND (id IN (2,3,4))) // SELECT count(*) FROM `users` WHERE `users`.`deleted_at` IS NULL AND ((name LIKE '%炒肉')) // Count 会复用 Where 后面的条件, Find 里面的是 内联条件,所以不会复用 Find 内的。 } 范围 (Scopes) Scope 是建立在链式操作基础上的方法。 可以让代码更加通用,逻辑更清晰。 package main import ( \"fmt\" \"github.com/jinzhu/gorm\" _ \"github.com/jinzhu/gorm/dialects/mysql\" ) // 定义一个全局的 DB 实例 var DB *gorm.DB // 定义 数据模型 type User struct { gorm.Model // 使用 tag default 设置默认值 Name string `gorm:\"default:'小炒肉'\"` Age int64 `gorm:\"default:99\"` } type Result struct { Name string Age int64 } // 查询年龄大于多少的用户 func AgeThan(age int64) func(db *gorm.DB) *gorm.DB { return func(db *gorm.DB) *gorm.DB { return db.Where(\"age \u003e ?\", age) } } // 查询返回指定用户 func NameThan(user []string) func(db *gorm.DB) *gorm.DB { return func(db *gorm.DB) *gorm.DB { return db.Where(\"name IN (?)\", user) } } func main() { dsn := \"jicki:jicki123@tcp(127.0.0.1:3306)/jicki?","date":"2000-01-01","objectID":"/golang-web-note-5/:1:5","tags":["golang"],"title":"GORM MYSQL","uri":"/golang-web-note-5/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-0/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-0/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-0/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-0/"},{"categories":["golang"],"content":"Go基础结构 // 程序的入口 package main // 导入的包 import \"fmt\" // main 函数的内容,每个程序必须包含 main 函数作为入口 func main() { // 单行注释写法 /* 多行注释,块注释 */ // 执行fmt包下的Println函数 fmt.Println(\"hello jicki!\") } go 程序执行 第一种 go run main.go 第二种(发布程序) go build , 然后再执行 ./main ","date":"2000-01-01","objectID":"/golang-study-note-0/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-0/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-1/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-1/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"标识符与关键字 ","date":"2000-01-01","objectID":"/golang-study-note-1/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"标识符 在编程语言中标识符就是程序员定义的具有特殊意义的词, 比如 变量名、常量名、函数名等. Go语言中标识符由字母数据和_(下划线)组成,并且只能以字母和_开头。比如 Abc、_Abc、a123 ","date":"2000-01-01","objectID":"/golang-study-note-1/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"关键字 关键字是指编程语言中预先定义了具有特殊含义的标识符。关键字和保留字都不建议用作变量名。 Go语言中有25个关键字: break default func interface select case defer go map struct chan else goto package switch const fallthrough if range type continue for import return var Go语言中的37个保留字: 常量: true false iota nil 类型: int int8 int16 int64 uint uint8 uint16 uint32 uint64 uintptr float32 float64 complex128 complex64 bool byte rune string error 方法(functions): make len cap new append copy close delete complex real imag panic recover ","date":"2000-01-01","objectID":"/golang-study-note-1/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"变量 变量的含义: 程序运行过程中的数据都是保存在内存中，我们想要在代码中操作某个数据时就需要去内存上找到这个变量，但是如果我们直接在代码中通过内存地址去操作变量的话，代码的可读性会非常差而且还容易出错，所以我们就利用变量将这个数据的内存地址保存起来，以后直接通过这个变量就能找到内存上对应的数据了。 变量类型: 变量 的功能是存储数据。不同的变量保存的数据类型可能会不一样。常见变量的数据类型有: 整型、浮点型、布尔型等。 Go语言中的每一个变量都有自己的类型，并且变量必须经过声明才能开始使用。 变量声明: Go语言中的变量需要声明后才能使用，同一作用域内不支持重复声明。 并且Go语言的变量声明后必须使用, 否则会报错。 变量声明格式: var 变量名 变量类型, 函数外的变量声明必须都是以 var 开头进行声明。 变量赋值: var 变量名 变量类型 = 值 var 变量名1,变量名2 = 变量1的值, 变量2的值。 package main import \"fmt\" func main() { // 标准声明 var name string var age int var isOk bool // 批量声明 var ( a string b int c bool ) // 变量赋值 name = \"名字\" age = 19 isOk = false a = \"A\" b = 12 c = true // 短变量声明,只能在函数内使用 d := \"dddd\" // 输出 fmt.Println(name, age, isOk, a, b, c, d) } ","date":"2000-01-01","objectID":"/golang-study-note-1/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"常量 相对于变量，常量是恒定不变的值，多用于定义程序运行期间不会改变的那些值。 常量的声明和变量声明非常类似，只是把var换成了const，常量在定义的时候必须赋值。 常量定义与赋值: const 常量名 = 值 // 常量定义 const a = 3333 const b = 4444 // 批量定义 const ( c = 5555 d = 6666 ) // 常量如果不写值会继承上一个的值 const ( e = 7777 f g ) iota 关键字: iota 是go语言中的常量计数器, 只能在常量表达式中使用。 iota 在const关键字出现时将被重置为0。const中每新增一行常量声明将使iota计数一次(iota 可理解为const语句块中的行索引)。 使用iota能简化定义，在定义枚举时很有用。 const ( a = iota //0 b //1 c //2 d //3 ) iota 在常量使用 _(忽略)仍然会占用一位。 const ( a = iota //0 b //1 _ d //3 ) const ( a1 = iota //0 a2 //1 a3 = 100 //100 a4 = iota //3 ) const ( b1, b2 = iota + 1, iota + 2 // 1, 2 b3, b4 // 2, 3 b5, b6 // 3, 4 ) ","date":"2000-01-01","objectID":"/golang-study-note-1/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"数据类型 Go语言中有丰富的数据类型，除了基本的整型、浮点型、布尔型、字符串外，还有数组、切片、结构体、函数、map、通道（channel）等。 ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"整型 整型分为以下两个大类： 按长度分为：int8、int16、int32、int64 对应的无符号整型：uint8、uint16、uint32、uint64 其中，uint8就是我们熟知的byte型，int16对应C语言中的short型，int64对应C语言中的long型。 类型 描述 int8 有符号 8位整型 (-128 到 127) int16 有符号 16位整型 (-32768 到 32767) int32 有符号 32位整型 (-2147483648 到 2147483647) int64 有符号 64位整型 (-9223372036854775808 到 9223372036854775807) uint8 无符号8位整型(0 到 255) uint16 无符号16位整型(0 到 65535) uint32 无符号32位整型(0 到 4294967295) uint64 无符号 64位整型 (0 到 18446744073709551615) ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"特殊类型 int, uint 在不同平台中有差异。 类型 描述 uint 32位操作系统上就是uint32，64位操作系统上就是uint64 int 32位操作系统上就是int32，64位操作系统上就是int64 uintptr 无符号整型，用于存放一个指针 获取对象的长度的内建len()函数返回的长度可以根据不同平台的字节长度进行变化。实际使用中，slice切片或 map 的元素数量等都可以用int来表示。在涉及到二进制传输、读写文件的结构描述时，为了保持文件的结构不会受到不同编译目标平台字节长度的影响，不要使用int和uint。 ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"数字字面量语法（Number literals syntax） 数字字面量语法，便于开发者以二进制、八进制或十六进制浮点数的格式定义数字。v := 0b00101101， 代表二进制的 101101，相当于十进制的 45。 v := 0o377，代表八进制的 377，相当于十进制的 255。 v := 0x1p-2，代表十六进制的 1 除以 2²，也就是 0.25。 而且还允许我们用 _ 来分隔数字, 比如说: v := 123_456 等于 123456。 ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"八进制和十六进制 fmt 包中可转换成这类进制的值。 // 十进制 var a int = 10 // %b 表示转换成二进制 fmt.Printf(\"%b \\n\", a) // %d 表示转换成十进制 fmt.Printf(\"%d \\n\", a) // 八进制 var b int = 077 // %o 表示转换成八进制 fmt.Printf(\"%o \\n\", b) // %d 表示转换成十进制 fmt.Printf(\"%d \\n\", b) // 十六进制 var c int = 0xff // 直接打印值 fmt.Println(c) // %x 表示转换成十六进制 fmt.Printf(\"%x \\n\", c) ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"浮点型 Go语言支持两种浮点型数: float32 和float64。这两种浮点型数据格式遵循IEEE 754标准: float32 的浮点数的最大范围约为 3.4e38, 可以使用常量定义: math.MaxFloat32. float64 的浮点数的最大范围约为 1.8e308, 可以使用一个常量定义: math.MaxFloat64。 func main() { // 打印 32位 最大浮点数 3.4028234663852886e+38 fmt.Println(math.MaxFloat32) // 打印 64位 最大浮点数 1.7976931348623157e+308 fmt.Println(math.MaxFloat64) } ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"复数 复数有实部和虚部, complex64的实部和虚部为32位, complex128的实部和虚部为64位。 var c1 complex64 c1 = 1 + 2i var c2 complex128 c2 = 2 + 3i fmt.Println(c1) fmt.Println(c2) ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"布尔类型 Go语言中以bool类型进行声明布尔型数据，布尔型数据只有true（真）和false（假）两个值。 布尔类型变量的默认值为false。布尔类型不能进行数值运算,也无法与其他类型进行转换。 func main() { var b bool // 默认值为 false fmt.Println(b) // 赋值 b = true fmt.Println(b) } ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"字符串(string) Go语言中的字符串以原生数据类型出现, 使用字符串就像使用其他原生数据类型（int、bool、float32、float64 等）一样. Go 语言里的字符串的内部实现使用UTF-8编码. 字符串的值为 双引号(\") 中的内容, 可以在Go语言的源码中直接添加非ASCII码字符. func main() { // 字符串 s1 := \"hello\" s2 := \"哈喽\" fmt.Println(s1) fmt.Println(s2) } 字符串转移 - Go 语言的字符串常见转义符包含回车、换行、单双引号、制表符等，如下表所示。 转义符 含义 \\r 回车符(返回行首) \\n 换行符(直接跳到下一行的同列位置) \\t 制表符 \\' 单引号 \\\" 双引号 \\\\ 反斜杠 func main() { // 转义 如下反斜杠 fmt.Println(\"c:\\\\golang\\\\bin\\\\go.exe\") } 多行字符串 - Go语言中要定义一个多行字符串时，就必须使用反引号字符 func main() { // 多行字符串 s3 := ` 第一行 第二行 第三行 \"\" '' \\ ` } 字符串常用操作 方法 介绍 len(str) 求长度 +或fmt.Sprintf 拼接字符串 strings.Split 分割 strings.contains 判断是否包含 strings.HasPrefix,strings.HasSuffix 前缀/后缀判断 strings.Index(),strings.LastIndex() 子串出现的位置 strings.Join(a[]string, sep string) join操作 func main() { // 字符串 s1 := \"hello\" s2 := \"哈喽\" s4 := fmt.Sprintf(\"%s -- %s\", s1, s2) s5 := \"how old are you\" // 返回字符串长度 fmt.Println(len(s5)) // 字符串拼接 fmt.Println(s1 + s2) fmt.Println(s4) // 字符串分割 fmt.Println(strings.Split(s5, \" \")) // 切割后变成 slice 切片 fmt.Printf(\"类型 = %T \\n\", strings.Split(s5, \" \")) // 判断字符串是否包含某个字符,返回 布尔值 fmt.Println(strings.Contains(s5, \"ow\")) // 判断前缀,与后缀, 返回 布尔值 fmt.Println(strings.HasPrefix(s5, \"how\")) fmt.Println(strings.HasSuffix(s5, \"you\")) // 判断字符串的位置 ,结果为 1 (从0开始) fmt.Println(strings.Index(s5, \"o\")) // 判断字符串最后出现的位置 , 结果为 13 fmt.Println(strings.LastIndex(s5, \"o\")) // join 操作, 以为定义的符号连接字符串 s6 := []string{\"how\", \"old\", \"are\", \"you\"} fmt.Println(s6) fmt.Println(strings.Join(s6, \"-\")) } ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"byte 与 rune 类型 组成每个字符串的元素叫做“字符”, 可以通过遍历或者单个获取字符串元素获得字符。 字符用单引号（’）包裹起来。 byte: 其实就是uint8类型，或者叫 byte 型，代表了ASCII码的一个字符。 rune: 其实就是 int32类型, 代表一个 UTF-8 字符。 func main() { var c1 byte = 'c' var c2 rune = 'c' fmt.Printf(\"c1 = %v type = %T \\nc2 = %v type = %T\\n\", c1, c1, c2, c2) s := \"hello 世界\" // for range 循环可以遍历 字符加符号的 for _, v := range s { fmt.Printf(\"%c \\n\", v) } } ","date":"2000-01-01","objectID":"/golang-study-note-1/:2:9","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-1/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-2/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-2/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"运算符 运算符用于在程序运行时执行数学或逻辑运算。 Go 语言内置的运算符有: 算术运算符 关系运算符 逻辑运算符 位运算符 赋值运算符 ","date":"2000-01-01","objectID":"/golang-study-note-2/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"算数运算符 ++, -- 不是运算符,是一个单独的语句. a++ a = a + 1, a-- a = a - 1 运算符 描述 + 相加 - 相减 * 相乘 / 相除 % 求余 func main() { // 算法运算符 a := 10 b := 20 fmt.Println(a + b) fmt.Println(b - a) fmt.Println(a * b) fmt.Println(b / a) fmt.Println(a % b) } ","date":"2000-01-01","objectID":"/golang-study-note-2/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"关系运算符 运算符 描述 == 检查两个值是否相等，如果相等返回 True 否则返回 False != 检查两个值是否不相等，如果不相等返回 True 否则返回 False \u003e 检查左边值是否大于右边值，如果是返回 True 否则返回 False \u003e= 检查左边值是否大于等于右边值，如果是返回 True 否则返回 False \u003c 检查左边值是否小于右边值，如果是返回 True 否则返回 False \u003c= 检查左边值是否小于等于右边值，如果是返回 True 否则返回 False func main() { a := 10 b := 20 // 关系运算符 (返回布尔值) fmt.Println(a \u003e b) fmt.Println(a != b) fmt.Println(a \u003c= b) } ","date":"2000-01-01","objectID":"/golang-study-note-2/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"逻辑运算符 运算符 描述 \u0026\u0026 逻辑 AND 运算符。 如果两边的操作数都是 True，则为 True，否则为 False。 || 逻辑 OR 运算符。 如果两边的操作数有一个 True，则为 True，否则为 False。 ! 逻辑 NOT 运算符。 如果条件为 True，则为 False，否则为 True。 func main() { // 逻辑运算符 (返回布尔值) fmt.Println(10 \u003c 5 \u0026\u0026 5 \u003e 3) fmt.Println(10 \u003c 5 \u0026\u0026 5 \u003e 10) // ! 表示取反 fmt.Println(!(20 \u003c 10)) fmt.Println(5 \u003c 10 || 10 \u003e 5) } ","date":"2000-01-01","objectID":"/golang-study-note-2/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"位运算符 位运算符对整数在内存中的二进制位进行操作。 运算符 描述 \u0026 参与运算的两数各对应的二进位相与。（两位均为1才为1） | 参与运算的两数各对应的二进位相或。（两位有一个为1就为1） ^ 参与运算的两数各对应的二进位相异或，当两对应的二进位不一样时，结果为1。 \u003c\u003c 左移n位就是乘以2的n次方。“a«b” 是把a的各二进位全部左移b位，高位丢弃，低位补0。 \u003e\u003e 右移n位就是除以2的n次方。“a»b” 是把a的各二进位全部右移b位。 func main() { //位运算符 c := 1 // 001 d := 5 // 101 // \u0026 上下两个都为1才为1 fmt.Println(c \u0026 d) // 001 // | 上下有1 就为1 fmt.Println(c | d) // 101 // ^ 上下不一样 就为1 fmt.Println(c ^ d) // 100 // \u003c\u003c 左移2位 1 = 001 左移2位 100 fmt.Println(1 \u003c\u003c 2) // 100 // \u003e\u003e 右移2位 4 = 100 右移2位 001 fmt.Println(4 \u003e\u003e 2) // 001 // 1 左移10位 10000000000 fmt.Println(1 \u003c\u003c 10) } ","date":"2000-01-01","objectID":"/golang-study-note-2/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"赋值运算符 运算符 描述 = 简单的赋值运算符，将一个表达式的值赋给一个左值 += 相加后再赋值 -= 相减后再赋值 *= 相乘后再赋值 /= 相除后再赋值 %= 求余后再赋值 \u003c\u003c= 左移后赋值 \u003e\u003e= 右移后赋值 \u0026= 按位与后赋值 |= 按位或后赋值 ^= 按位异或后赋值 func main() { // 赋值运算符 var e int // 赋值 e = 10 // e = e + 10 e += 10 fmt.Println(e) } ","date":"2000-01-01","objectID":"/golang-study-note-2/:1:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"流程控制 流程控制是每种编程语言控制逻辑走向和执行次序的重要部分, 流程控制可以说是一门语言的\"经脉”。 Go语言中最常用的流程控制有if和for, 而switch和goto主要是为了简化代码、降低重复代码而生的结构,属于扩展类的流程控制。 ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"if \u0026\u0026 else if条件判断基本写法: if 条件表达式1 { 执行语句1 } else if 条件表达式2 { 执行语句2 } else { 执行语句3 } 当条件表达式1的结果为true时, 执行语句1, 否则判断条件表达式2, 如果满足则执行语句2, 都不满足时, 则执行语句3。if判断中的else if和else都是可选的, 可以根据实际需要进行选择。 Go语言规定与if匹配的左括号{必须与if和表达式放在同一行, {放在其他位置会触发编译错误. 同理与else匹配的{也必须与else写在同一行, else也必须与上一个if或else , if右边的大括号在同一行。 // if else 判断 func main() { // 标准写法 age := 20 if age \u003e= 30 { fmt.Println(\"年龄大于等于30岁\") } else if age \u003c= 19 { fmt.Println(\"年龄小于等于19\") } else { fmt.Println(\"年龄介于20-30之间\") } // if 语句块中也可以定义变量，只在if块中生效 if score := 60; score \u003e= 70 { fmt.Println(\"大于等于70\") } else if score \u003c 60 { fmt.Println(\"小于60\") } else { fmt.Println(\"及格了\") } } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"for 循环 Go 语言中的所有循环类型均可以使用for关键字来完成。 条件表达式返回true时循环体不停地进行循环, 直到条件表达式返回false时自动退出循环。 for 初始语句;条件表达式;结束语句{ 循环体语句 } // for 循环 func main() { // 1. 标准的for循环语句 for i := 0; i \u003c 10; i++ { fmt.Println(i) } // 2. 循环语句块外定义变量，for语句中开头需要保留;号 i := 1 for ; i \u003c 10; i++ { fmt.Println(i) } // 3. 循环体内只有条件表达式 i = 10 for i \u003c 1 { fmt.Println(i) i-- } // 4. 无限循环 for { fmt.Println(i) } // 5. break 跳出循环 for i := 0; i \u003c 10; i++ { fmt.Println(i) if i == 5 { break } } // 6. continue 跳过本次循环,继续上一层循环 for i := 0; i \u003c 10; i++ { if i == 5 { continue } fmt.Println(i) } } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"for range 循环 Go语言中可以使用for range遍历数组、切片、字符串、map 及通道（channel）。 通过for range遍历的返回值有以下规律: 数组、切片、字符串返回索引和值。 map返回键和值。 通道（channel）只返回通道内的值。 // for range 循环 func main() { // 遍历数组, 返回 数组的 索引 与 值 a1 := [5]int{1, 2, 3, 4, 5} for i, v := range a1 { fmt.Printf(\"index = %v, value = %v \\n\", i, v) } // 遍历map, 返回 map 的 key 与 value m1 := map[string]string{\"第一列\": \"1\", \"第二列\": \"2\", \"第三列\": \"3\"} for k, v := range m1 { fmt.Printf(\"Map key = %v, value = %v \\n\", k, v) } } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"switch case 循环 switch语句可方便地对大量的值进行条件判断。 Go语言规定每个switch只能有一个default分支。 fallthrough语法可以执行满足条件的case的下一个case, 是为了兼容C语言中的case设计的。 func main() { score := 60 switch { case score \u003c 60: fmt.Println(\"不及格\") case score \u003e= 60 \u0026\u0026 score \u003c 80: fmt.Println(\"及格\") case score \u003e= 80: fmt.Println(\"优秀\") default: fmt.Println(\"无效的分数\") } // 判断多个条件 num := 5 switch num { case 1, 3, 5, 7, 9: fmt.Println(\"奇数\") case 2, 4, 6, 8, 10: fmt.Println(\"偶数\") default: fmt.Println(\"无效的输入\") } // fallthrough 满足条件以后会继续执行下一个 num = 10 switch { case num \u003c 20: fmt.Println(\"小于20\") fallthrough case num \u003e 5: fmt.Println(\"大于5\") case num \u003e 10: fmt.Println(\"大于10\") default: fmt.Println(\"其他\") } } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"goto语句 goto语句通过标签进行代码间的无条件跳转。goto语句可以在快速跳出循环、避免重复退出上有一定的帮助。Go语言中使用goto语句能简化一些代码的实现过程。 goto 语句必须配合 标签使用. // goto 语句 func main() { // for 循环多层嵌套 for i := 0; i \u003c 10; i++ { for j := 0; j \u003c 10; j++ { if j == 5 { goto breakTag } fmt.Printf(\"i = %v j = %v \\n\", i, j) } } return breakTag: fmt.Println(\"Goto跳转到此处\") } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"break(跳出循环) break语句可以结束for、switch和select的代码块。 break语句还可以在语句后面添加标签，表示退出某个标签对应的代码块，标签要求必须定义在对应的for、switch和 select的代码块上。 // break(跳出循环) func main() { // for 循环 for i := 0; i \u003c 10; i++ { fmt.Println(i) if i == 2 { // 跳出for循环 break } } // break 配合 标签 breakTag: for i := 0; i \u003c 10; i++ { for j := 0; j \u003c 10; j++ { if j == 2 { break breakTag } fmt.Printf(\"i = %v j = %v\\n\", i, j) } } fmt.Println(\"循环结束\") } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"continue(继续下次循环) continue 语句可以跳过前循环, 进行下一次的循环, 仅限在for循环内使用。 在continue语句后添加标签时, 表示开始标签对应的循环。 // continue 继续下一个循环 func main() { for i := 0; i \u003c 10; i++ { if i == 5 { continue } fmt.Println(i) } } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"九九乘法表 func main() { for i := 1; i \u003c 10; i++ { for j := 1; j \u003c= i; j++ { fmt.Printf(\"%v * %v = %v| \", j, i, i*j) } fmt.Println() } } ","date":"2000-01-01","objectID":"/golang-study-note-2/:2:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-2/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-3-1/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3-1/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-3-1/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3-1/"},{"categories":["golang"],"content":"引用类型与值类型 ","date":"2000-01-01","objectID":"/golang-study-note-3-1/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3-1/"},{"categories":["golang"],"content":"值类型 值类型有: 整型int、浮点型float、布尔值bool、字符串string、数组Array、结构体struct 值类型的特点是: 变量直接存储值, 内存通常在栈中分配. package main import \"fmt\" func main(){ // 定义一个数组 a , 数组是值类型 a := [5]int{2, 3, 4, 5, 6} // a 复制给 b 是 copy b := a fmt.Println(a,b) // 当b 发生改变 a 的值不会发生改变 b[2] = 77 fmt.Println(a,b) } 输出 [2,3,4,5,6] [2,3,4,5,6] [2,3,4,5,6] [2,3,77,5,6] ","date":"2000-01-01","objectID":"/golang-study-note-3-1/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3-1/"},{"categories":["golang"],"content":"引用类型 引用类型有: 指针pointer、切片slice、管道channel、接口interface、map、函数func 引用类型的特点是: 变量存储的是一个地址, 这个地址对应的空间里才是真正存储的值, 内存通常在堆中分配. package main import \"fmt\" func main(){ // 定义一个 切片 a , 切片是引用类型 a := []int{2, 3, 4, 5, 6} // a 复制给 b 是 指针的引用, a 与 b 指向同一个底层数组 b := a fmt.Println(a,b) // 当 b 发生改变时, a 的值也会跟着改变 b[2] = 77 fmt.Println(a,b) } 输出: [2,3,4,5,6] [2,3,4,5,6] [2,3,77,5,6] [2,3,77,5,6] ","date":"2000-01-01","objectID":"/golang-study-note-3-1/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3-1/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-3/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-3/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"数组Array 数组是同一种数据类型元素的集合。 在Go语言中，数组从声明时就确定，使用时可以修改数组成员，但是数组大小不可变化。 数组的定义: var 数组变量名 [元素数量]类型 (如: var a [3]int ) 数组的长度必须是常量, 并且长度是数组类型的一部分。一旦定义, 长度不能变。 数组可以通过索引进行访问,但是不能超过数组元素最大索引,否则会引发 panic ","date":"2000-01-01","objectID":"/golang-study-note-3/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"数组定义 func main() { // 数组的定义 var a1 [3]int var a2 [4]int // 数组的赋值, 未赋值的数组是对应类型元素的0值 a1[0] = 1 fmt.Println(a1) fmt.Println(a2) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"数组初始化 func main() { // 数组初始化 // 定义数组时初始化元素 var city = [4]string{\"北京\", \"上海\", \"广州\", \"深圳\"} fmt.Println(city) // 定义数组时使用[...]编译器动态配置数组长度 var number = [...]int{1, 2, 3, 4, 5, 6, 7, 8} fmt.Println(number) // 使用索引值方式初始化 var stu = [...]string{0: \"stu1\", 1: \"stu1\", 3: \"stu3\", 5: \"stu5\"} fmt.Printf(\"%#v\\n\", stu) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"数组遍历 数组遍历可使用 for 循环与 for range 循环进行遍历 func main() { // 数组遍历 a3 := [...]string{\"北\", \"上\", \"广\", \"深\"} // 利用 for 循环,使用数组索引进行遍历数组元素 for i := 0; i \u003c len(a3); i++ { fmt.Println(a3[i]) } // 利用 for range 循环进行遍历数组的索引与元素 for i, k := range a3 { fmt.Println(i, k) } } ","date":"2000-01-01","objectID":"/golang-study-note-3/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"多维数组 多维数组既,数组中嵌套数组 多维数组只有最外层数组可以使用[…]动态配置数组长度 数组是值类型, 赋值和传参会复制整个数组. 因此改变副本的值, 不会改变本身的值。 func main() { // 多维数组 a4 := [3][2]string{ {\"北京\", \"南京\"}, {\"深圳\", \"广州\"}, {\"上海\", \"杭州\"}, } fmt.Printf(\"%#v \\n\", a4) // 多维数组的遍历 for _, v1 := range a4 { for _, v2 := range v1 { fmt.Println(v2) } } // 数组是值类型 a5 := [...]int{1, 2, 3, 4, 5} y := a5 y[0] = 100 // 不会修改原数组的值 fmt.Println(a5) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"数组练习题 func main() { // 1. 求数组[1, 3, 5, 7, 8]所有元素的和 a1 := [...]int{1, 3, 5, 7, 8} sum := 0 for _, k := range a1 { sum = sum + k fmt.Println(sum) } // 2. 找出数组中和为指定值的两个元素的下标，比如从数组[1, 3, 5, 7, 8]中找出和为8的两个元素的下标 // 第一种 for range a2 := [...]int{1, 3, 4, 5, 7, 8, 9} for index, value := range a2 { for i := 0; i \u003c len(a2)/2; i++ { if value+a2[i] == 8 { // 元素值 //fmt.Println(value, a2[i]) // 下标 fmt.Println(index, i) } } } // 第二种 for // 第一次遍历数,取全部元素 for i := 0; i \u003c len(a2); i++ { // 第二次遍历数组,只取一半元素 for j := 0; j \u003c len(a2)/2; j++ { if a2[i]+a2[j] == 8 { fmt.Println(i, j) } } } } ","date":"2000-01-01","objectID":"/golang-study-note-3/:1:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片Slice 切片 (Slice) 是一个拥有相同类型元素的可变长度的序列。它是基于数组类型做的一层封装。它非常灵活，支持自动扩容。 切片 (slice) 是一个引用类型, 它的内部结构包含地址、长度和容量。切片一般用于快速地操作一块数据集合。 切片 (slice) 定义了以后, 必须初始化才能使用。 切片 (slice) 不能直接进行比较, 切片只能和 nil 进行比较. nil 的切片元素个数与容量都为 0。 ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片定义 var 变量名 []元素类型. func main() { // 切片的定义 var s1 []string var s2 []int // 定义时就初始化 var s3 = []bool{false, true} // 没有初始化的 切片元素为 nil fmt.Printf(\"s1 = %#v s2 = %#v s3 = %#v \\n\", s1, s2, s3) // 基于数组定义切片 a1 := [5]int{1, 2, 3, 4, 5} // 左包含右不包含, 既 元素下表为 a1[1],a1[2],a1[3] 的元素 s4 := a1[1:4] fmt.Printf(\"a1 = %#v s4 = %#v \\n\", a1, s4) // 基于 make 函数构造切片 (如下 5 = 元素个数, 10 = 容量) s5 := make([]int, 5, 10) fmt.Printf(\"s5 len = %d s5 cap = %d \\n\", len(s5), cap(s5)) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片本质 切片的本质就是对底层数组的封装, 它包含了三个信息: 底层数组的指针、切片的长度（len）和切片的容量（cap）。 func main() { // 定义了一个 元素个数8,容量为8 的切片(没有定义容量时,容量等于元素个数) s6 := []int{1, 2, 3, 4, 5, 6, 7, 8} fmt.Printf(\"s6 len = %d s6 cap = %d \\n\", len(s6), cap(s6)) // 在切片 s6 的基础上 定义的一个切片, 元素个数为 2 (s6[2],s6[3]) // 容量 等于 s6 容量 减去 前面2个元素 s7 容量为 6 s7 := s6[2:4] fmt.Printf(\"s7 len = %d s7 cap = %d \\n\", len(s7), cap(s7)) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片比较 func main() { // 切片不能直接比较,只能与 nil 值进行比较 var s8 []int // nil 值的切片 元素与容量都为 0 fmt.Printf(\"s8 len = %d s8 cap = %d \\n\", len(s8), cap(s8)) // 与 nil 比较, 判断为 true 进行初始化 if s8 == nil { fmt.Printf(\"%#v \\n\", s8) fmt.Println(\"s8 == nil\") s8 = make([]int, 5, 10) fmt.Printf(\"%#v \\n\", s8) } } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片的赋值 切片是 引用类型 func main() { // 切片的赋值与拷贝 // 定义个初始化的切片,元素个数为 5 s9 := make([]int, 5) // 这里 s9, s10 指向一个相同的 内存地址 s10 := s9 fmt.Println(s9) fmt.Println(s10) // 对 s10[0] 个元素进行赋值 s10[0] = 10 // 如下两个切片的元素都被修改了 fmt.Println(s9) fmt.Println(s10) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片的遍历 切片的遍历方式和数组是一致的, 支持索引for遍历和for range遍历。 func main() { // 切片的遍历 s11 := []string{\"北京\", \"上海\", \"广州\", \"深圳\"} // for 循环 for i := 0; i \u003c len(s11); i++ { fmt.Println(s11[i]) } // for range 循环 for i, k := range s11 { fmt.Println(i, k) } } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"append() Go语言的内建函数append()可以为切片动态添加元素。 每个切片会指向一个底层数组, 这个数组能容纳一定数量的元素。当底层数组不能容纳新增的元素时, 切片就会自动按照一定的策略进行\"扩容”, 此时该切片指向的底层数组就会更换。“扩容\"操作往往发生在append()函数调用时. func main() { // append() 方法 // 使用 append 方法 不需要额外初始化 var s12 []int s12 = append(s12, 100) fmt.Println(s12) // 切片自动扩容容量,相同容量情况下内存地址不变,扩容以后内存地址变化. for i := 0; i \u003c 5; i++ { fmt.Printf(\"s12 len = %d cap = %d ptr = %p\\n\", len(s12), cap(s12), s12) s12 = append(s12, i) } // append 在 切片中 追加 切片 s13 := []int{6, 7, 8} // 追加的切片后面必需要加 ... (s13... 表示将切片的元素一个一个取出来) s12 = append(s12, s13...) fmt.Println(s12) // 利用 append 实现删除切片某个元素 // 删除 s13 切片中 s13[1] 个元素 \"7\" s13 = append(s13[:1], s13[2:]...) fmt.Println(s13) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片扩容策略 切片容量长度小于1024时, 每次扩容的新容量就是旧容量的2倍。 如果新扩容的容量大于旧容量的2倍, 那么旧容量就等于新扩容的容量。 如果切片容量长度大于1024时,扩容的新容量是旧容量的1/4, 直到最终容量大于等于新申请的容量。 另外不同类型的切片, 扩容的策略也不相同。 ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"copy() 由于切片是引用类型, 所以切片(a = b) 实际指向的是同一个内存地址,修改切片b的元素时,a切片也会修改。 Go语言内建的copy()函数可以迅速地将一个切片的数据复制到另外一个切片空间中。 copy() 用法 copy(目标切片, 原切片)。 func main() { // copy() 切片的复制 s14 := []int{1, 2, 3, 4, 5} s15 := make([]int, 5) s16 := make([]int, 5) // 直接赋值 s15 = s14 // 使用 copy 函数 copy(s16, s14) // 修改 s15 的值 s15[0] = 20 // 修改 s16 的值 s16[0] = 10 // 打印 s14 的值 与 内存地址 fmt.Printf(\"s14 = %d s14 prt = %p \\n\", s14, s14) // 打印 s15 的值 与 内存地址 fmt.Printf(\"s15 = %d s15 prt = %p \\n\", s15, s15) // 打印 s16 的值 与 内存地址 fmt.Printf(\"s16 = %d s16 prt = %p \\n\", s16, s16) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片练习题 func main() { // 练习题 一 // 定义一个 string类型 的切片 a 元素长度5,容量10 var a = make([]string, 5, 10) // a = [ ] string类型 元素为 5个空值 for i := 0; i \u003c 10; i++ { // 循环追加元素到 a 切片中,(fmt.Sprintf 将int类型转换成 string类型) a = append(a, fmt.Sprintf(\"%v\", i)) } // [ 0 1 2 3 4 5 6 7 8 9] 前面5个空值 fmt.Println(a) // 练习题 二 // 使用内置的sort包对数组 var a1 = [...]int{3, 7, 8, 9, 1} 进行排序。 var a1 = [...]int{3, 7, 8, 9, 1} // 将 数组 转换成 切片,再使用 sort.Ints() 进行排序 sort.Ints(a1[:]) fmt.Println(a1) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:2:9","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"Map Go语言中提供的映射关系容器为map , 其内部使用散列表(hash)实现。 map是一种无序的基于key-value的数据结构, Go语言中的map是引用类型, 必须初始化才能使用。 map类型的变量默认初始值为nil，需要使用make()函数来分配内存。 ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"map定义 map[KeyType]ValueType (KeyType:表示键的类型, ValueType:表示键对应的值的类型) 如: map[string]int func main() { // map 定义 // 定义一个map,未初始化(nil) var m1 map[string]int // 定义个map并初始化 元素类型的零值 ({}) var m2 = make(map[string]int, 10) // 给 map 添加键值对(赋值), 必须初始化才能添加键值对 m2[\"name\"] = 1 m2[\"age\"] = 20 m2[\"score\"] = 80 // 定义一个map并初始化和赋值 ({\"age\":10, \"score\":90}) var m3 = map[string]int{ \"age\": 10, \"score\": 90, } fmt.Printf(\"%#v \\n\", m1) fmt.Printf(\"%#v \\n\", m2) fmt.Printf(\"%#v \\n\", m3) } 输出: map[string]int(nil) map[string]int{\"age\":20, \"name\":1, \"score\":80} map[string]int{\"age\":10, \"score\":90} ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"map 初始化 make(map[KeyType]ValueType, [cap]) 如: make(map[string]int,10) , 其中cap表示map的容量, 该参数虽然不是必须的, 但是我们应该在初始化map的时候就为其指定一个合适的容量。 func main(){ // 定义一个 map, 未初始化(nil) var m1 map[string]int // 初始化 map, 使用make函数,容量为 10 m1 = make(map[string]int, 10) fmt.Printf(\"%#v \\n\", m1) } 输出: map[string]int{} ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"判断map键值对是否存在 使用value, ok 的方式进行判断。 // 判断 map 某个键值 是否存在 m4 中,使用 value,ok 的方式判断 value, ok := m4[\"张三\"] if ok { fmt.Printf(\"张三 value = %d \\n\", value) } else { fmt.Println(\"查无此人\") } value, ok = m4[\"李六\"] if ok { fmt.Printf(\"李六 = %d \\n\", value) } else { fmt.Println(\"查无此人\") } 输出: 张三 value = 20 查无此人 ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"map 的遍历 使用 for range 循环遍历 map func main() { // map 的遍历 m5 := map[string]int{ \"哈哈\": 20, \"呵呵\": 30, \"嘻嘻\": 40, } // for range 循环 for key, value := range m5 { // 由于 map 是无序的, 所以输出的结果顺序与添加顺序会不一致。 fmt.Println(key, value) } // 只获取key或者value for key := range m5 { fmt.Println(key) } for _, value := range m5 { fmt.Println(value) } } 输出: 哈哈 20 呵呵 30 嘻嘻 40 ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"delete() 函数 map 可使用delete()内建函数 删除一组键值对, delete()函数。 格式: delete(map, map的key) func main() { // 使用 delete() 函数删除 map 中的键值对 m6 := map[string]int{ \"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4, } // 删除键值对 \"a\" delete(m6, \"a\") fmt.Println(m6) } 输出: map[b:2 c:3 d:4] ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"按照指定顺序遍历 map 将map中的 key 值存到切片中,利用 sort 对切片进行排序.然后利用 map[排序后的key] 对map进行排序. func main() { // 按照固定顺序遍历 map m7 := make(map[string]int, 50) // 使用 for 循环 增加 30个键值对 for i := 0; i \u003c 30; i++ { // 使用 Sprintf 生成 29个 key key := fmt.Sprintf(\"stu%02d\", i) // 使用 rand 生成 29个 随机数 value := rand.Intn(100) m7[key] = value } // 将 map 的 key 值保存到 slice 切片中 // 定义一个 []string 类型的切片, 长度为0,容量为50 keys := make([]string, 0, 50) // 使用 append()函数 将 map 中的 key 元素 存到 切片中 for key := range m7 { keys = append(keys, key) } // 利用 sort.Strings 对 string 类型 进行排序 sort.Strings(keys) // 利用排序后的 keys 对 map 排序 for _, key := range keys { // _ = keys切片中的 索引 // key = keys切片中的 键值 fmt.Println(key, m7[key]) } ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"切片与map结合 元素类型 为 map 的切片, s1 = make([]map[string]int, 0, 5)。 func main() { // 元素类型为 map 的 切片 // 对 切片 初始化 元素5, 容量5 s1 := make([]map[string]int, 5, 5) // 对切片 初始化后 还需要对 map 的每个元素(key) 进行初始化. s1[0] = make(map[string]int, 5) // 初始化后进行赋值 s1[0][\"age\"] = 6 // 可以利用 == nil 的方式进行判断初始化 for i := 0; i \u003c len(s1); i++ { if s1[i] == nil { s1[i] = make(map[string]int, 5) s1[i][\"age\"] = 5 } } fmt.Println(s1) } 输出: [map[age:6] map[age:5] map[age:5] map[age:5] map[age:5]] 元素值为切片的 map 如: m8 := make(map[string][]int, 8) func main(){ // 元素值为切片的 map // 定义一个 map 并初始化了 map, 容量为 8 m8 := make(map[string][]int, 8) // 对 map 初始化以后, 还需要对 元素值 []int 进行初始化 // 利用 v, ok 语法判断 map 的key是否存在 v, ok := m8[\"MapName\"] if ok { fmt.Println(v) } else { // 判断不存在 就创建键值对 // 由于这里 map 的 value 是 []int 所以还需要对 切片进行初始化 m8[\"MapName\"] = make([]int, 5, 5) // 赋值 m8[\"MapName\"][0] = 100 m8[\"MapName\"][1] = 200 } fmt.Println(m8) } 输出: map[MapName:[100 200 0 0 0]] ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"Map练习题 统计一个字符串中每个单词出现的次数。比如: “how do you do” 中how=1 do=2 you=1。 // 练习题 // 统计 \"how do you do\" 一个字符串中每个单词出现的次数。 // 定义一个 map key是 string 用于存放 字符串, value 是 int 类型用于记录出现的次数。 str := \"how do you do\" m9 := make(map[string]int, 10) // 利用 strings.Split 对整个字符串进行分割 words := strings.Split(str, \" \") // 遍历 分割后的切片,取到每个元素 for _, word := range words { // 利用 ok 来判断 map 中是否存在这个 key 值 v, ok := m9[word] if ok { // 如果 map 中存在这个键值, value 就 + 1 m9[word] = v + 1 } else { // 如果 map 中不存在这个 键值, 创建一个键值, value = 1 m9[word] = 1 } } fmt.Println(m9) 输出: map[do:2 how:1 you:1] ","date":"2000-01-01","objectID":"/golang-study-note-3/:3:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-3/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-4-1/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"类型别名和自定义类型 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"自定义类型 在Go语言中有一些基本的数据类型, 如string、整型、浮点型、布尔等数据类型, Go语言中可以使用type关键字来定义自定义类型。 自定义类型是定义了一个全新的类型。我们可以基于内置的基本类型定义, 也可以通过struct定义。 类型定义和类型别名的区别: 自定义类型,定义以后输出的类型会输出为 新定义的类型, 而类型别名,在定义以后输出的类型仍然是原类型。 // Myint 基于 int 的自定义类型 type Myint int func main() { // 定义一个 i 变量 类型为 Myint 类型 var i Myint fmt.Printf(\"type: %T value: %v \\n\", i, i) } 输出: type: main.Myint value: 0 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"类型别名 类型别名规定: TypeAlias只是Type的别名, 本质上TypeAlias与Type是同一个类型。 类型别名定义: type TypeAlias = Type。 rune和byte就是类型别名, 它们其实是 type byte = uint8, type rune = int32 // 类型别名 // Aliasint 是 int 的类型别名 type Aliasint = int func main(){ // 定义一个 a 变量 类型为 Aliasint 类型 var a Aliasint fmt.Printf(\"type: %T value: %v \\n\", a, a) } 输出: type: int value: 0 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体 Go语言中的基础数据类型可以表示一些事物的基本属性, 但是当我们想表达一个事物的全部或部分属性时, 这时候再用单一的基本数据类型明显就无法满足需求了, Go语言提供了一种自定义数据类型, 可以封装多个基本数据类型, 这种数据类型叫结构体, 英文名称struct。 也就是我们可以通过struct来定义自己的类型了。 Go语言中通过struct来实现面向对象。 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体的定义 使用type和struct关键字来定义结构体。 type 类型名 struct { 字段名 字段类型 字段名 字段类型 … } 说明: 类型名: 标识自定义结构体的名称, 在同一个包内不能重复。 字段名: 表示结构体字段名。结构体中的字段名必须唯一。 字段类型: 表示结构体字段的具体类型。 // 定义 struct 结构体 type person struct { name, city string age int8 } 如上定义了一个person的自定义类型, 它有name、city、age三个字段, 分别表示姓名、城市和年龄。这样我们使用这个person结构体就能够很方便的在程序中表示和存储人信息了。 语言内置的基础数据类型是用来描述一个值的, 而结构体是用来描述一组值的。比如一个人有名字、年龄和居住城市等, 本质上是一种聚合型的数据类型。 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体实例化 只有当结构体实例化时, 才会真正地分配内存。也就是必须实例化后才能使用结构体的字段。 结构体本身也是一种类型, 我们可以像声明内置类型一样使用var关键字声明结构体类型。 如: var 结构体实例 结构体类型 type person struct { name, city string age int8 } func main() { // 定义了一个 p1 变量,类型为 person 类型 var p1 person // 通过 . 的形式进行 赋值 p1.name = \"盘古\" p1.city = \"宇宙\" p1.age = 99 fmt.Printf(\"%#v \\n\", p1) // 通过 . 的形式进行 访问 fmt.Printf(\"名字: %v 城市: %v 年龄: %v \\n\", p1.city, p1.city, p1.age) } 输出: main.person{name:\"盘古\", city:\"宇宙\", age:99} 名字: 宇宙 城市: 宇宙 年龄: 99 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"匿名结构体 在定义一些临时数据结构等场景下可以使用匿名结构体。 func main() { // 定义一个匿名结构体 var p2 struct { name string age int8 } p2.name = \"女娲\" p2.age = 99 fmt.Printf(\"%#v \\n\", p2) } 输出: struct { name string; age int8 }{name:\"女娲\", age:99} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"指针类型的结构体 通过使用new关键字对结构体进行实例化, 得到的是结构体的内存地址。 //结构体指针 type person struct { name, city string age int8 } func main() { // 定义一个结构体指针 var p3 = new(person) fmt.Printf(\"%T \\n\", p3) // 结构体可以直接对结构体的指针使用,不需要使用*符号 p3.name = \"小企鹅\" p3.city = \"北极\" p3.age = 5 fmt.Printf(\"%#v \\n\", p3) } 输出: *main.person \u0026main.person{name:\"小企鹅\", city:\"北极\", age:5} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"取结构体的地址实例化 使用\u0026对结构体进行取地址操作相当于对该结构体类型进行了一次new实例化操作。 type person struct { name, city string age int8 } func main() { // 取结构体地址进行实例化 p4 := \u0026person{} fmt.Printf(\"p4 = %T \\n\", p4) p4.name = \"北极熊\" p4.city = \"北极\" p4.age = 10 fmt.Printf(\"%#v \\n\", p4) } 输出: p4 = *main.person \u0026main.person{name:\"北极熊\", city:\"北极\", age:10} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体初始化 没有初始化的结构体, 其成员变量都是对应其类型的零值。 使用键值对初始化 - 使用键值对对结构体进行初始化时, 键对应结构体的字段, 值对应该字段的初始值。也可以对结构体指针进行键值对初始化。当某些字段没有初始值的时候, 该字段可以不写。此时, 没有指定初始值的字段的值就是该字段类型的零值。 使用值的列表初始化 - 初始化结构体的时候可以简写, 也就是初始化的时候不写键, 直接写值。但是需要注意的是 (1. 必须初始化结构体的所有字段。2. 初始值的填充顺序必须与字段在结构体中的声明顺序一致。3. 方式不能和键值初始化方式混用。) 键值对初始化 type person struct { name, city string age int } func main() { // 1. 键值对初始化 p5 := person{ name: \"玉帝\", city: \"天宫\", age: 9999, } fmt.Printf(\"%#v \\n\", p5) // 取地址初始化 p6 := \u0026person{ name: \"王母\", city: \"天宫\", age: 9998, } fmt.Printf(\"%#v \\n\", p6) } 输出: main.person{name:\"玉帝\", city:\"天宫\", age:9999} \u0026main.person{name:\"王母\", city:\"天宫\", age:9998} 值的列表初始化 type person struct { name, city string age int } func main() { // 2. 值的列表初始化 // 值列表初始化 必须按照 struct 定义的顺序写, 必须写全部字段 p7 := person{ \"二郎神\", \"天宫\", 999, } fmt.Printf(\"%#v \\n\", p7) } 输出: main.person{name:\"二郎神\", city:\"天宫\", age:999} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体内存布局 结构体占用一块连续的内存。 type test struct { a int8 b int8 c int8 d int8 } func main() { n := test{ 1, 2, 3, 4, } fmt.Printf(\"n.a %p\\n\", \u0026n.a) fmt.Printf(\"n.b %p\\n\", \u0026n.b) fmt.Printf(\"n.c %p\\n\", \u0026n.c) fmt.Printf(\"n.d %p\\n\", \u0026n.d) } 输出: n.a 0xc0000a0004 n.b 0xc0000a0005 n.c 0xc0000a0006 n.d 0xc0000a0007 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体的匿名字段 结构体允许其成员字段在声明时没有字段名而只有类型, 这种没有名字的字段就称为匿名字段。 匿名字段不能有重复的类型. // 结构体的匿名字段 type Person struct { string int } func main() { // 调用以及访问结构体的匿名字段 p1 := Person{ \"张大仙\", 18, } // 通过访问 类型名字 来访问字段 fmt.Println(p1.string, p1.int) } 输出: 张大仙 18 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"嵌套结构体 一个结构体中可以嵌套包含另一个结构体或结构体指针。 // 嵌套结构体 type Address struct { province, city string } type Person struct { name, gender string age int // 嵌套 Address 结构体 // 第一个Address是名称 第二个Address是类型 Address Address } func main() { //嵌套结构体的初始化 p1 := Person{ name: \"张大仙\", gender: \"女\", age: 30, Address: Address{ province: \"广东\", city: \"深圳\", }, } fmt.Printf(\"%#v \\n\", p1) //嵌套结构体的字段访问 fmt.Println(p1.name, p1.Address) } 输出: main.Person{name:\"张大仙\", gender:\"女\", age:30, Address:main.Address{province:\"广东\", city:\"深圳\"}} 张大仙 {广东 深圳} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:9","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"嵌套匿名结构体 如上例子修改一下嵌套的结构体只写类型名 // 嵌套结构体 type Address struct { province, city string } type Person struct { name, gender string age int // 嵌套 匿名结构体 // 只写类型名 Address } func main() { //嵌套结构体的初始化 p1 := Person{ name: \"张大仙\", gender: \"女\", age: 30, Address: Address{ province: \"广东\", city: \"深圳\", }, } fmt.Printf(\"%#v \\n\", p1) //嵌套匿名结构体的访问, 可直接访问匿名结构体里的字段 // 如下两种方式都可以访问结构体里的字段 fmt.Println(p1.Address.city, p1.city) } 输出: main.Person{name:\"张大仙\", gender:\"女\", age:30, Address:main.Address{province:\"广东\", city:\"深圳\"}} 深圳 深圳 匿名结构体字段冲突 package main import \"fmt\" // 嵌套匿名结构体字段冲突 type Address struct { province, city string updateTime string } type Email struct { addr string updateTime string } type Person struct { name, gender string age int // 嵌套 匿名结构体 Address Address // 嵌套 匿名结构体 Email Email } func main() { // 初始化一个Person 实例 p1 := Person{ name: \"张大仙\", gender: \"女\", age: 20, Address: Address{ province: \"广东\", city: \"深圳\", updateTime: \"2019-10-30\", }, Email: Email{ addr: \"zhangdaxian@huya.com\", updateTime: \"2019-10-29\", }, } // 当匿名结构体存在冲突字段的时候,需要区分 fmt.Println(\"Address:\", p1.Address.updateTime) fmt.Println(\"Email:\", p1.Email.updateTime) } 输出: Address: 2019-10-30 Email: 2019-10-29 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:10","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"构造函数 Go语言的结构体是没有构造函数, 我们可以自己实现。构造函数就是 构造一个结构体实例的函数。 如下实现了一个person的构造函数。 因为struct是值类型, 如果结构体比较复杂的话, 值拷贝性能开销会比较大, 所以该构造函数返回的是结构体指针类型。 type person struct { name, city string age int } // 构造函数 // 构造函数一般约定名称前为 new 开头 // 这里返回值使用的是 取 person 内存地址指针的值 func newPerson(name, city string, age int) *person { // 返回 内存地址的 person return \u0026person{ name: name, city: city, age: age, } } func main() { // 构造多个 person p8 := newPerson(\"托塔天王\", \"天宫\", 999) fmt.Printf(\"%#v \\n\", p8) p9 := newPerson(\"太白金星\", \"天宫\", 9999) fmt.Printf(\"%#v \\n\", p9) } 输出: \u0026main.person{name:\"托塔天王\", city:\"天宫\", age:999} \u0026main.person{name:\"太白金星\", city:\"天宫\", age:9999} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:11","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体练习题 type student struct { name string age int } func main() { // 定义一个 map key 值为 string value 为 struct m := make(map[string]*student) // 定义一个 类型为 struct 的切片,里面有三组元素 stus := []student{ {name: \"小王子\", age: 18}, {name: \"娜扎\", age: 23}, {name: \"大王八\", age: 9000}, } for _, stu := range stus { // stu = {小王子 18} {娜扎 23} {大王八 9000} // stu.name 分别 = 小王子, 娜扎, 大王八 // \u0026stu 是取地址, slice 是引用类型, 指向同一个内存地址 m[stu.name] = \u0026stu // 所以这里 map[大王八:0xc000092020 娜扎:0xc000092020 小王子:0xc000092020] } for k, v := range m { // m = map[大王八:0xc000092020 娜扎:0xc000092020 小王子:0xc000092020] // k = \"大王八\", \"娜扎\", \"小王子\" // v = \"0xc000092020\" 同一个内存地址 fmt.Println(k, \"=\u003e\", v.name) } } 输出: 小王子 =\u003e 大王八 娜扎 =\u003e 大王八 大王八 =\u003e 大王八 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:2:12","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"方法和接收者 Go语言中的方法（Method）是一种作用于特定类型变量的函数。这种特定类型变量叫做接收者（Receiver) , 接收者的概念就类似于其他语言中的this或者self。 方法与函数的区别是, 函数不属于任何类型, 方法属于特定的类型。 方法的定义格式: 接收者变量: 接收者中的参数变量名在命名时, 官方建议使用接收者类型名的第一个小写字母, 而不是self、this之类的命名。例如, Person类型的接收者变量应该命名为 p, Connector类型的接收者变量应该命名为c等。 接收者类型: 接收者类型和参数类似, 可以是指针类型和非指针类型。 方法名、参数列表、返回参数: 具体格式与函数定义相同。 func (接收者变量 接收者类型) 方法名(参数列表) (返回参数) { 函数体 } // 方法与接受者 // Person 定义一个结构体 type Person struct { name string age int8 } // NewPerson Person 结构体的构造函数 func NewPerson(name string, age int8) *Person { return \u0026Person{ name: name, age: age, } } // Dream 为Person类型定义的方法 func (p Person) Dream() { fmt.Printf(\"%v 的梦想是做一条咸鱼 \\n\", p.name) } func main() { // 实例化 Person p1 := NewPerson(\"八戒\", 20) // 调用方法 p1.Dream() } 输出: 八戒 的梦想是做一条咸鱼 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"指针类型的接收者 指针类型的接收者由一个结构体的指针组成, 由于指针的特性, 调用方法时修改接收者指针的任意成员变量, 在方法结束后, 修改都是有效的。这种方式就十分接近于其他语言中面向对象中的this或者self。 例如 我们为Person添加一个SetAge方法, 来修改实例变量的年龄。 // Person 定义一个结构体 type Person struct { name string age int8 } // NewPerson Person 结构体的构造函数 func NewPerson(name string, age int8) *Person { return \u0026Person{ name: name, age: age, } } // 指针接受者 接受者的类型是指针类型 // SetAge 修改age 的方法 func (p *Person) SetAge(newAge int8) { p.age = newAge } func main() { // 实例化 Person p1 := NewPerson(\"八戒\", 20) // 修改前的值 fmt.Println(p1.age) // 调用指针类型的方法 p1.SetAge(99) // 修改后的值 fmt.Println(p1.age) } 输出: 20 99 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"值接收者 当方法作用于值类型接收者时, Go语言会在代码运行时将接收者的值复制一份。在值类型接收者的方法中可以获取接收者的成员值, 但修改操作只是针对副本, 无法修改接收者变量本身。 函数与方法传参的时候都是复制了一份, 所以修改只是修改了复制的那一份, 没有修改到本身。 // Person 定义一个结构体 type Person struct { name string age int8 } // NewPerson Person 结构体的构造函数 func NewPerson(name string, age int8) *Person { return \u0026Person{ name: name, age: age, } } // 值接收者 接收者的类型是值类型 // SetName 修改 name 的方法 func (p Person) SetName(newName string) { p.name = newName } func main() { // 实例化 Person p1 := NewPerson(\"八戒\", 20) // 修改前的值 fmt.Println(p1.name) // 调用值类型的修改方法 p1.SetName(\"悟空\") // 修改后的值 fmt.Println(p1.name) } 输出: 八戒 八戒 什么时候应该使用指针类型接收者 需要修改接收者中的值 接收者是拷贝代价比较大的大对象 保证一致性, 如果有某个方法使用了指针接收者, 那么其他的方法也应该使用指针接收者。 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"任意类型添加方法 在Go语言中, 接收者的类型可以是任何类型, 不仅仅是结构体, 任何类型都可以拥有方法。 举个例子: 我们基于内置的int类型使用type关键字可以定义新的自定义类型, 然后可以为我们的自定义类型添加方法。 注意事项: 非本地类型不能定义方法, 也就是说我们不能给别的包的类型定义方法。 // 任意类型添加方法 // 给任意类型定义方法必须是包内的类型 type myString string // 为 myString 类型定义一个方法 sayHello func (m myString) sayHello() { fmt.Println(\"Hello I'am myString!\") } func main() { //定义一个m 类型是 myString m := myString(\"string\") // 调用方法 m.sayHello() } 输出: Hello I'am myString! ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体\"继承” Go语言中是没有继承这个概念的, 但是使用结构体也可以实现其他编程语言中面向对象的继承。 // 结构体的 \"继承\" // 利用 结构体嵌套的方法实现 type Animal struct { Name string } // 定义一个 Animal Move 的方法 func (a *Animal) Move() { fmt.Printf(\"%s 走来走去 \\n\", a.Name) } // 定义一个 Dog 的结构体 type Dog struct { Feet int // 嵌套 匿名结构体 Animal的指针 *Animal } // 定义一个 Dog wang 的方法 func (d Dog) Wang() { fmt.Printf(\"%s 汪汪汪的叫 \\n\", d.Name) } // 定义一个 Cat 的结构体 type Cat struct { Feet int // 嵌套 匿名结构体 Animal的指针 *Animal } // 定义一个 Cat mao 的方法 func (c Cat) Mao() { fmt.Printf(\"%s 喵喵喵的叫 \\n\", c.Name) } func main() { // 实例化一个 dog dog1 := Dog{ Feet: 4, Animal: \u0026Animal{ Name: \"旺财\", }, } // 实例化一个 cat cat1 := Cat{ Feet: 4, Animal: \u0026Animal{ Name: \"小花\", }, } // 调用方法 dog1.Wang() cat1.Mao() // Dog 与 Cat 也可以调用 嵌套结构的方法 Move // 这就相当于实现了 继承 dog1.Move() cat1.Move() } 输出: 旺财 汪汪汪的叫 小花 喵喵喵的叫 旺财 走来走去 小花 走来走去 ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体字段的可见性 结构体中字段大写开头表示可公开访问, 小写表示私有（仅在定义当前结构体的包中可访问）。 type Person struct { // 字段首字母大写开头的表示外部可访问 Name string Age int } ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体JSON序列化 JSON (JavaScript Object Notation) 是一种轻量级的数据交换格式。易于人阅读和编写。同时也易于机器解析和生成。 JSON键值对是用来保存JS对象的一种方式, 键/值 对组合中的键名写在前面并用双引号\"\"包裹，使用冒号:分隔, 然后紧接着值, 多个键值之间使用英文,分隔。 package main import ( \"encoding/json\" \"fmt\" ) // 定义一个 Student 的结构体 type student struct { ID int Name string } // 定义一个 student 的构造函数 func newStudent(id int, name string) student { return student{ ID: id, Name: name, } } // 定义一个 class 的结构体 type class struct { Title string // 定义个student字段,类型是 student结构体类型的切片 Students []student } // 结构体 JSON 序列化 func main() { // 实例化一个 class 的对象 c1 c1 := class{ Title: \"王者荣耀\", // student 字段是一个切片,所以需要先初始化才能使用 Students: make([]student, 0, 20), } // 批量录入学生信息 for i := 1; i \u003c 10; i++ { tmpStu := newStudent(i, fmt.Sprintf(\"召唤师%02d\", i)) // 切片使用 append 函数追加 元素 c1.Students = append(c1.Students, tmpStu) } fmt.Printf(\"%#v \\n\", c1) // 利用 json.Marshal 包序列化 这一组数据 // 序列化因为调用的是外部的包，所以上面的 struct 跟 字段必须大写 data, err := json.Marshal(c1) // 格式化 json 输出 //data, err := json.MarshalIndent(c1, \"\", \"\") if err != nil { fmt.Println(\"json marshal failed err:\", err) return } // data 变量保存的数据类型为 byte 类型(uint8) // %s 会转换成 string 类型打印 fmt.Printf(\"%s \\n\", data) } 输出: main.class{Title:\"王者荣耀\", Students:[]main.student{main.student{ID:1, Name:\"召唤师01\"}, main.student{ID:2, Name:\"召唤师02\"}, main.student{ID:3, Name:\"召唤师03\"}, ma:4, Name:\"召唤师04\"}, main.student{ID:5, Name:\"召唤师05\"}, main.student{ID:6, Name:\"召唤师06\"}, main.student{ID:7, Name:\"召唤师07\"}, main.student{ID:8, Name:\"召唤师08t{ID:9, Name:\"召唤师09\"}}} {\"Title\":\"王者荣耀\",\"Students\":[{\"ID\":1,\"Name\":\"召唤师01\"},{\"ID\":2,\"Name\":\"召唤师02\"},{\"ID\":3,\"Name\":\"召唤师03\"},{\"ID\":4,\"Name\":\"召唤师04\"},{\"ID\":5,\"Name\":\"召唤师05\"},{\"ID\":6,\"Name\":\"召唤师06\"},{\"ID\":7,\"Name\":\"召唤师07\"},{\"ID\":8,\"Name\":\"召唤师08\"},{\"ID\":9,\"Name\":\"召唤师09\"}]} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体JSON 反序列化 JSON格式的字符串 反序列化成 结构体 // 定义一个 Student 的结构体 type student struct { ID int Name string } // 定义一个 student 的构造函数 func newStudent(id int, name string) student { return student{ ID: id, Name: name, } } // 定义一个 class 的结构体 type class struct { Title string // 定义个student字段,类型是 student结构体类型的切片 Students []student } // 结构体 JSON 反序列化 func main() { // Json 反序列化 Json 字符串 --\u003e 结构体 jsonStr := `{\"Title\": \"王者荣耀\",\"Students\": [{\"ID\": 1,\"Name\": \"召唤师01\"},{\"ID\": 2,\"Name\": \"召唤师02\"}]}` // 定义一个 class 实例化对象 c2 c2 := class{} // 这里需要传入 \u0026c2 必须要传入指针, 否则无法写入数据 err = json.Unmarshal([]byte(jsonStr), \u0026c2) if err != nil { fmt.Println(\"json Unmarshal failed err:\", err) } fmt.Printf(\"%#v \\n\", c2) } 输出: main.class{Title:\"王者荣耀\", Students:[]main.student{main.student{ID:1, Name:\"召唤师01\"}, main.student{ID:2, Name:\"召唤师02\"}}} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"结构体标签 (Tag) Tag 是结构体的元信息, 可以在运行的时候通过反射的机制读取出来。 Tag在结构体字段的后方定义, 由一对 `` 反引号包裹起来。 结构体标签由一个或多个键值对组成。键与值使用冒号:分隔, 值用双引号\"\"括起来。键值对之间使用一个空格分隔。 注意事项: 为结构体编写Tag时, 必须严格遵守键值对的规则。结构体标签的解析代码的容错能力很差, 一旦格式写错, 编译和运行时都不会提示任何错误, 通过反射也无法正确取值。例如不要在key和value之间添加空格。 // 定义一个 Student 的结构体 type student struct { ID int Name string } // 定义一个 student 的构造函数 func newStudent(id int, name string) *student { return \u0026student{ ID: id, Name: name, } } // 定义一个 class 的结构体 // 配置 tag, `输出格式:\"tag名\" 输出格式:\"tag名\"` type class struct { Title string `json:\"title\" db:\"title\"` // 定义个student字段,类型是 student结构体类型的切片 Students []student `json:\"student_slice\"` } func main() { // 实例化一个 class 的对象 c1 c1 := class{ Title: \"King glory\", // student 字段是一个切片,所以需要先初始化才能使用 Students: make([]student, 0, 20), } // 批量录入学生信息 for i := 1; i \u003c 10; i++ { tmpStu := newStudent(i, fmt.Sprintf(\"player%02d\", i)) // 切片使用 append 函数追加 元素 c1.Students = append(c1.Students, *tmpStu) } fmt.Printf(\"%#v \\n\", c1) // 利用 json.Marshal 包序列化 这一组数据 // 序列化因为调用的是外部的包，所以上面的 struct 跟 字段必须大写 data, err := json.Marshal(c1) // 格式化 json 输出 //data, err := json.MarshalIndent(c1, \"\", \"\") if err != nil { fmt.Println(\"json marshal failed err:\", err) return } // data 变量保存的数据类型为 byte 类型(uint8) // %s 会转换成 string 类型打印 fmt.Printf(\"%s \\n\", data) } 输出: main.class{Title:\"King glory\", Students:[]main.student{main.student{ID:1, Name:\"player01\"}, main.student{ID:2, Name:\"player02\"}, main.student{ID:3, Name:\"player03\"}, main.student{ID:4, Name:\"player04\"}, main.student{ID:5, Name:\"player05\"}, main.student{ID:6, Name:\"player06\"}, main.student{ID:7, Name:\"player07\"}, main.student{ID:8, Name:\"player08\"}, main.student{ID:9, Name:\"player09\"}}} // 如下是打了 tag 以后 json 序列化后的输出 {\"title\":\"King glory\",\"student_slice\":[{\"ID\":1,\"Name\":\"player01\"},{\"ID\":2,\"Name\":\"player02\"},{\"ID\":3,\"Name\":\"player03\"},{\"ID\":4,\"Name\":\"player04\"},{\"ID\":5,\"Name\":\"player05\"},{\"ID\":6,\"Name\":\"player06\"},{\"ID\":7,\"Name\":\"player07\"},{\"ID\":8,\"Name\":\"player08\"},{\"ID\":9,\"Name\":\"player09\"}]} ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"练习题 使用\"面向对象\"的思维方式编写一个学生信息管理系统。 学生有id、姓名、年龄、分数等信息 程序提供展示学生列表、添加学生、编辑学生信息、删除学生等功能 # student.go // 创建一个 student 的结构体 type student struct { id int name string class string } // 创建一个 student 的构造函数 func newStudent(id int, name, class string) *student { return \u0026student{ id: id, name: name, class: class, } } // 学员管理的结构体 type studentMgr struct { allStudents []*student } // 创建一个 studentMgr 的构造函数 func newStudentMgr() *studentMgr { return \u0026studentMgr{ allStudents: make([]*student, 0, 100), } } // 添加 学生的方法 func (s *studentMgr) addStudents(newStu *student) { // 使用 append 函数 往allStudents的切片中追加学生 s.allStudents = append(s.allStudents, newStu) } // 编辑 学生的方法 func (s *studentMgr) editStudents(newStu *student) { for i, v := range s.allStudents { // 判断传入的 id 是否等于 学员学号 if newStu.id == v.id { //根据切片的索引找到该学生信息,用新的学生信息覆盖 s.allStudents[i] = newStu fmt.Println(\"修改学员成功\") return } } fmt.Printf(\"没有找到 学号是: %d 的学生 \\n\", newStu.id) } // 删除 学生的方法 func (s *studentMgr) deleteStudents(newStu *student) { for i, v := range s.allStudents { // 判断传入的 id 是否等于 学员学号 if newStu.id == v.id { // 利用append 函数删除切片中的元素 append(slice[:i],slice[i+1:]...) s.allStudents = append(s.allStudents[:i], s.allStudents[i+1:]...) fmt.Println(\"删除学员成功\") return } } fmt.Printf(\"没有找到 学号是: %d 的学生 \\n\", newStu.id) } // 展示学生信息 func (s *studentMgr) showStudents() { // 使用 for range 循环 遍历 allStudents 切片 for _, v := range s.allStudents { fmt.Printf(\"学号: %d 姓名: %s 班级: %s \\n\", v.id, v.name, v.class) } } # main.go /* 学员管理系统需求: 1. 展示菜单 2. 添加学员信息 3. 编辑学员信息 4. 展示所有学员信息 5. 退出系统 */ func showMenu() { fmt.Println(\"欢迎来到学员管理系统\") fmt.Println(\"1. 添加学员信息\") fmt.Println(\"2. 编辑学员信息\") fmt.Println(\"3. 展示所有学员信息\") fmt.Println(\"4. 删除学员\") fmt.Println(\"5. 退出系统\") } // 获取用户输入的函数 func getInput() *student { var ( id int name string class string ) fmt.Println(\"请按照要求输入学员信息: \") fmt.Print(\"请输入学号:\") _, _ = fmt.Scanf(\"%d\\n\", \u0026id) fmt.Print(\"请输入姓名:\") _, _ = fmt.Scanf(\"%s\\n\", \u0026name) fmt.Print(\"请输入班级:\") _, _ = fmt.Scanf(\"%s\\n\", \u0026class) // 输入完以后~调用student的构造函数 stu := newStudent(id, name, class) fmt.Println(stu) return stu } func main() { // 实例化一个 studentMgr m1 := newStudentMgr() for { var input int // 1. 展示菜单 showMenu() // 2. 获取用户输入: fmt.Print(\"Please enter number:\") // 利用 fmt.Scan 捕获用户的输入 \u0026input 变量需要传入指针 _, err := fmt.Scanf(\"%d\\n\", \u0026input) if err != nil { fmt.Println(\"Scan Failed err:\", err) } fmt.Println(\"用户输入的是: \", input) // 3. 根据用户输入执行用户输入的操作 switch input { case 1: stu := getInput() m1.addStudents(stu) case 2: stu := getInput() m1.editStudents(stu) case 3: m1.showStudents() case 4: stu := getInput() m1.deleteStudents(stu) case 5: os.Exit(0) default: fmt.Println(\"无效输入,请重新输入\") continue } } } ","date":"2000-01-01","objectID":"/golang-study-note-4-1/:3:9","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-1/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-4-2/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"Go语言包(package) 在工程化的Go语言开发项目中, Go语言的源码复用是建立在包（package）基础之上的。 包（package）是多个Go源码的集合, 是一种高级的代码复用方案, Go语言为我们提供了很多内置包,如fmt、os、io等。 我们可以根据自己的需要创建自己的包。一个包可以简单理解为一个存放.go文件的文件夹。该文件夹下面的所有go文件都要在代码的第一行添加package 包名, 声明该文件归属的包。 一个文件夹下面只能有一个包, 同样一个包的文件不能在多个文件夹下。 包名可以不和文件夹的名字一样, 包名不能包含-符号。 包名为main的包为应用程序的入口包，编译时不包含main包的源代码时不会得到可执行文件。 ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"包(package)的可见性 如果想在一个包中引用另外一个包里的标识符（如变量、常量、类型、函数等）时, 该标识符必须是对外可见的（public）。在Go语言中只需要将标识符的首字母大写就可以让标识符对外可见了。 首字母小写 只能在当前包中调用。 ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"包的导入 要在代码中引用其他包的内容, 需要使用import关键字导入使用的包。具体语法 import \"包的路径\" 注意事项: import导入语句通常放在文件开头包声明语句的下面。 导入的包名需要使用双引号包裹起来。 包名是从$GOPATH/src/后开始计算的，使用/进行路径分隔。 Go语言中禁止循环导入包。 Go语言中导入的包必须要使用,否则会报错。 ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"包的别名 在导入包名的时候, 我们还可以为导入的包设置别名。具体语法格式 import 别名 \"包的路径\" import ( \"fmt\" // 导入自己写的包 js \"github.com/jicki/package/calc\" ) func main() { ret := js.Add(10, 20) fmt.Println(ret) } ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"匿名导入包 如果只希望导入包, 而不使用包内部的数据时, 可以使用匿名导入包。具体的格式 import _ \"包的路径\" ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"init()函数 在Go语言程序执行时导入包语句会自动触发包内部init()函数的调用。需要注意的是: init() 函数没有参数也没有返回值。 init()函数在程序运行时自动被调用执行, 不能在代码中主动调用它。 # 包初始化时执行的顺序 [ 全局声明 ] # 变量声明,既 var a = 100 等 ↓ [ init() ] ↓ [ main() ] ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"init()函数执行顺序 Go语言包会从main包开始检查其导入的所有包, 每个包中又可能导入了其他的包。Go编译器由此构建出一个树状的包引用关系, 再根据引用顺序决定编译顺序, 依次编译这些包的代码。 init() 多用于执行一些初始化工作,如: 日志格式化, 加载配置文件, 打开数据库连接等。 多个包含 init() 函数的 包在运行时, 被最后导入的包会最先初始化并调用其init()函数。 # 导入包的顺序如下: import import import [ main() ] ---\u003e [ 函数A ] ---\u003e [ 函数B ] ---\u003e [ 函数C ] # 初始化时执行顺序如下: [ main.init() ] \u003c--- [ A.init() ] \u003c--- [ B.init() ] \u003c--- [ c.init() ] 实际运行例子: package a import ( \"fmt\" _ \"github.com/jicki/golang基础/go包/package/b\" ) func init() { fmt.Println(\" 执行 a 包 的 init() 函数\") } package b import ( \"fmt\" _ \"github.com/jicki/golang基础/go包/package/c\" ) func init() { fmt.Println(\" 执行 b 包 的 init() 函数\") } package c import \"fmt\" func init() { fmt.Println(\" 执行 c 包 的 init() 函数\") } package main import ( \"fmt\" // 导入自己写的包 _ \"github.com/jicki/golang基础/go包/package/a\" ) func init() { fmt.Println(\"执行 main 包 的 init() 函数\") } func main() { fmt.Println(\"执行 main 包 的 main() 函数\") } 输出 执行 c 包 的 init() 函数 执行 b 包 的 init() 函数 执行 a 包 的 init() 函数 执行 main 包 的 init() 函数 执行 main 包 的 main() 函数 ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"练习题 编写一个clac包实现加减乘除四个功能函数, 在snow这个包中导入并使用加减乘除四个函数实现数学运算。 # clac 包 func Add(x, y int) int { return x + y } func Sub(x, y int) int { return x - y } func Multi(x, y int) int { return x * y } func Divide(x, y int) int { return x / y } # snow 包 package snow import ( \"fmt\" \"github.com/jicki/package/package_work/clac\" ) func Sum(char string, x, y int) int { sum := 0 switch char { case \"+\": sum = clac.Add(x, y) case \"-\": sum = clac.Sub(x, y) case \"*\": sum = clac.Multi(x, y) case \"/\": sum = clac.Divide(x, y) default: fmt.Println(\"char 输入有误!\") } return sum } # main.go import ( \"fmt\" \"github.com/jicki/package/package_work/snow\" ) func main() { ret := snow.Sum(\"+\", 100, 200) fmt.Println(ret) } ","date":"2000-01-01","objectID":"/golang-study-note-4-2/:1:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-2/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-4-3/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-3/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-4-3/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-3/"},{"categories":["golang"],"content":"文件操作 文件: 数据源（保存数据数据的地方） 文件在程序中是以流的形式来操作。 流是数据在数据源(文件)和程序(内存)之间经历的路径 输入流: 数据从数据源(文件)到程序(内存)的路径 文件 –\u003e Go程序（内存): 输入流[读文件] 输出流: 数据从程序(内存)到数据源(文件)的路径 Go程序（内存）–\u003e 文件: 输出流[写文件] ","date":"2000-01-01","objectID":"/golang-study-note-4-3/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-3/"},{"categories":["golang"],"content":"打开和关闭文件 os.Open()函数能够打开一个文件, 返回一个*File和一个err。对得到的文件实例调用close()方法能够关闭文件。 // 文件操作 func main() { // 调用 os.Open 方法打开 文件 file, err := os.Open(\"./temp.txt\") if err != nil { fmt.Println(\"open file failed!, err:\", err) return } // 关闭文件 defer file.Close() } ","date":"2000-01-01","objectID":"/golang-study-note-4-3/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-3/"},{"categories":["golang"],"content":"读取文件 file.Read() 方法 func (f *File) Read(b []byte) (n int, err error) 这里传入的变量是 []byte 字节切片, 返回n读取行数,与error错误。这里判断文件读完了, n 返回为0, error 返回 io.EOF 。 func osRead(f string) { file, err := os.Open(f) if err != nil { fmt.Println(\"open file failed err:\", err) return } defer file.Close() // 完整的元素的切片 var data []byte // 每一次读取的元素,临时存储的切片 var temp = make([]byte, 128) // 因为 temp 字节切片 有一定容量,一次读不完,所以可使用 for 循环读取 for { n, err := file.Read(temp) // 判断文件是否已经读取完, 用 io.EOF if err == io.EOF { fmt.Println(\"文件读完了\") break } if err != nil { fmt.Println(\"read file failed err:\", err) return } // 利用 append 函数 将临时切片读取到的内容 追加到完整切片中 data = append(data, temp[:n]...) } // 字节切片,打印的时候需要转换成 string 类型 fmt.Println(string(data)) } bufio 读取文件: bufio是在file的基础上封装了一层API, 支持更多的功能。 func buffIoRead(f string) { file, err := os.Open(f) if err != nil { fmt.Println(\"open file failed err:\", err) return } defer file.Close() // 定义一个 缓冲区,用于存放读取到的内容 reader := bufio.NewReader(file) for { // 这里 '\\n' 注意是字符, 这里意思是读取到什么字符就结束 line, err := reader.ReadString('\\n') if err == io.EOF { fmt.Println(line) fmt.Println(\"文件读完了\") break } if err != nil { fmt.Println(\"read file failed, err:\", err) return } fmt.Print(line) } } ioutil 读取整个文件: io/ioutil包的ReadFile方法能够读取完整的文件, 只需要将文件名作为参数传入。 func iouTileRead(f string) { // 使用 iouTil 读取整个文件 data, err := ioutil.ReadFile(f) if err != nil { fmt.Println(\"read file failed err:\", err) return } // 返回的是 []byte 也需要 string 转换 fmt.Println(string(data)) } ","date":"2000-01-01","objectID":"/golang-study-note-4-3/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-3/"},{"categories":["golang"],"content":"写文件 os.OpenFile()函数能够以指定模式打开文件, 从而实现文件写入相关功能。 打开文件格式: func OpenFile(name string, flag int, perm FileMode) (*File, error) { name: 要打开的文件名。 flag: 打开文件的模式。 perm: 文件权限, 一个八进制数。r（读）04 , w（写）02 , x（执行）01 。 模式有以下几种: 模式 含义 os.O_WRONLY 只写 os.O_CREATE 创建文件 os.O_RDONLY 只读 os.O_RDWR 读写 os.O_TRUNC 清空 os.O_APPEND 追加 // 写文件 func fileWrite(f, str string) { // 打开文件并配置权限 file, err := os.OpenFile(f, os.O_CREATE|os.O_WRONLY, 0644) if err != nil { fmt.Println(\"open file failed err:\", err) return } defer file.Close() // 写入文件内容 _, err = file.WriteString(str) if err != nil { fmt.Println(\"write file failed err:\", err) return } fmt.Println(str) } func main(){ fileWrite(\"./xx.txt\", \"测试写入\") } bufio.NewWriter 写文件。 func buffIoWrite(f, str string) { file, err := os.OpenFile(f, os.O_CREATE|os.O_TRUNC|os.O_WRONLY, 0644) if err != nil { fmt.Println(\"open file failed, err:\", err) return } defer file.Close() writer := bufio.NewWriter(file) // 循环写入多行数据 for i := 0; i \u003c 10; i++ { //将数据先写入缓存 _, err = writer.WriteString(str) if err != nil { fmt.Println(\"WriteString Failed err:\", err) return } } //因为写入的信息是先存在缓存中,所以需要将缓存中的内容写入文件 err = writer.Flush() if err != nil { fmt.Println(\"Flush Failed err:\", err) return } } func main(){ buffIoWrite(\"./xx.txt\", \"buffio 测试写入\\n\") } ioutil.WriteFile 写入文件。 func iouTilWrite(f, str string) { // 这里写入文件并不能配置 模式,只能一次写入,多次写入会覆盖 err := ioutil.WriteFile(f, []byte(str), 0666) if err != nil { fmt.Println(\"iouTilWrite Failed err:\", err) return } } func main(){ iouTilWrite(\"./xx.txt\", \"iouTileWrite 测试写入\\n\") } ","date":"2000-01-01","objectID":"/golang-study-note-4-3/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-3/"},{"categories":["golang"],"content":"实现Copy文件 利用文件操作,读取源文件,并存储到 切片中,再写入到 目标文件中。 func copyFile(dstName, srcName string) (n int, err error) { // 以只读的方式打开原文件 src, err := os.Open(srcName) if err != nil { fmt.Println(\"Open Src File Failed err:\", err) return } // 将文件读取出来,写入临时的切片中 // 完整的元素的切片 var data []byte // 每一次读取的元素,临时存储的切片 var temp = make([]byte, 128) // 因为 temp 字节切片 有一定容量,一次读不完,所以可使用 for 循环读取 for { n, err := src.Read(temp) // 判断文件是否已经读取完, 用 io.EOF if err == io.EOF { break } if err != nil { fmt.Println(\"Read src File Failed err:\", err) break } // 利用 append 函数 将临时切片读取到的内容 追加到完整切片中 data = append(data, temp[:n]...) } // 字节切片,打印的时候需要转换成 string 类型 defer src.Close() // 以 写入|创建 的方式打开 目标文件 dst, err := os.OpenFile(dstName, os.O_WRONLY|os.O_CREATE, 0644) if err != nil { fmt.Println(\"Open Dst File Failed err:\", err) return } n, err = dst.Write(data) if err != nil { fmt.Println(\"Write Dst File Failed err:\", err) return } defer dst.Close() fmt.Println(\"复制成功\") return } func main() { n, err := copyFile(\"./xx.txt\", \"./temp.txt\") if err != nil { fmt.Println(\"Copy File Failed err:\", err) return } fmt.Printf(\"复制了 %d 行\\n\", n) } ","date":"2000-01-01","objectID":"/golang-study-note-4-3/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-3/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-4-4/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"接口(interface) 接口(interface) 定义了一个对象的行为规范, 只定义规范不实现, 由具体的对象来实现规范的细节。 ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"接口类型 在Go语言中接口（interface）是一种类型, 一种抽象的类型。 interface是一组方法method的集合, 是duck-type programming的一种体现。接口做的事情就像是定义一个协议（规则）, 只要一台机器有洗衣服和甩干的功能, 我就称它为洗衣机。不关心属性（数据）, 只关心行为（方法）。 请牢记接口（interface）是一种类型。 我们编程过程中会经常遇到: 一个网上商城可能使用支付宝、微信、银联等方式去在线支付, 我们能不能把它们当成\"支付方式\"来处理呢？ 三角形, 四边形, 圆形都能计算周长和面积, 我们能不能把它们当成\"图形\"来处理呢？ Go语言中为了解决类似上面的问题, 就设计了接口这个概念。接口区别于我们之前所有的具体类型, 接口是一种抽象的类型。当你看到一个接口类型的值时, 你不知道它是什么, 唯一知道的是通过它的方法能做什么。 ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"接口的定义 Go语言提倡面向接口编程。 每个接口由数个方法组成, 接口的定义格式如下: 接口名: 使用type将接口定义为自定义的类型名。Go语言的接口在命名时, 一般会在单词后面添加er, 如有写操作的接口叫Writer, 有字符串功能的接口叫Stringer等。接口名最好要能突出该接口的类型含义。 方法名: 当方法名首字母是大写且这个接口类型名首字母也是大写时, 这个方法可以被接口所在的包（package）之外的代码访问。 参数列表、返回值列表: 参数列表和返回值列表中的参数变量名可以省略。 type 接口类型名 interface{ 方法名1( 参数列表1 ) 返回值列表1 方法名2( 参数列表2 ) 返回值列表2 … } ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"实现接口的条件 一个对象只要全部实现了接口中的方法, 那么就实现了这个接口。换句话说, 接口就是一个需要实现的方法列表。 // 定义两个结构体 type cat struct{} type dog struct{} // 定义一个 sayer 接口, 有一个say()方法 type sayer interface { say() } // 分别定义say方法,这里实现了接口sayer中的方法,那么就实现了接口 func (c cat) say() { fmt.Println(\"喵喵喵~\") } func (d dog) say() { fmt.Println(\"汪汪汪~\") } func main() { var s sayer // 定义一个接口 c1 := cat{} // 可以把c1这个实例直接赋值给接口 s = c1 s.say() d1 := dog{} // 可以把d1这个实例直接赋值给接口 s = d1 s.say() } 输出: 喵喵喵~ 汪汪汪~ ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"值接收者和指针接收者实现接口的区别 使用值接收者实现接口和使用指针接收者实现接口有什么区别呢？ 使用指针接收者实现接口: 只有类型指针能够保存到接口变量中。 使用值接收者实现接口: 类型的值类型和指针类型都能够保存到接口变量中。 // 定义一个接口 mover, 包含一个 move()方法 type mover interface { move() } // 定义一个结构体 type dog struct { name string } // 给 dog 实现一个方法 move // 使用值接收者实现接口: 类型的值类型和指针类型都能够保存到接口变量中。 func (d dog) move() { fmt.Printf(\"%s 在奔跑~\\n\", d.name) } // 使用指针接收者实现接口: 只有类型指针能够保存到接口变量中。 //func (d *dog) move() { // fmt.Printf(\"%s 在奔跑~\", d.name) //} func main() { // 定义一个接口变量 var m mover // 初始化一个d1 值类型的实例 d1 := dog{ name: \"旺财\", } // 初始化一个d2 指针类型的实例 d2 := \u0026dog{ name: \"旺财\", } // 值类型实例d1赋值给 接口变量m m = d1 // 指针类型实例d2赋值给 接口变量m m = d2 m.move() fmt.Println(m) } 输出: 旺财 在奔跑~ \u0026{旺财} ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"类型与接口的关系 一个类型实现多个接口 一个类型可以同时实现多个接口, 而接口间彼此独立, 不知道对方的实现。 例如: 狗可以叫, 也可以动。我们就分别定义Sayer接口和Mover接口. 多个类型实现同一接口 Go语言中不同的类型还可以实现同一接口 首先我们定义一个Mover接口, 它要求必须由一个move方法。 type mover interface { move() } type sayer interface { say() } // 定义一个结构体 type dog struct { name string } // 给 dog 实现一个方法 move func (d dog) move() { fmt.Printf(\"%s 在奔跑~\\n\", d.name) } // 给 dog 实现一个方法 say func (d dog) say() { fmt.Printf(\"%s 在叫~\\n\", d.name) } func main() { d1 := dog{ name: \"旺财\", } var m mover // 定义一个 mover 类型的变量 var s sayer // 定义一个 sayer 类型的变量 m = d1 // d1 可以赋值给 mover 接口 s = d1 // d1 可以赋值给 sayer 接口 m.move() s.say() } 输出: 旺财 在奔跑~ 旺财 在叫~ ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"接口嵌套 接口与接口间可以通过嵌套创造出新的接口。 // 接口的嵌套: 相当于实现了mover 与 sayer的接口方法 type animal interface { mover sayer } type mover interface { move() } type sayer interface { say() } // 定义一个结构体 type dog struct { name string } // 给 dog 实现一个方法 move func (d dog) move() { fmt.Printf(\"%s 在奔跑~\\n\", d.name) } // 给 dog 实现一个方法 say func (d dog) say() { fmt.Printf(\"%s 在叫~\\n\", d.name) } func main() { d1 := dog{ name: \"旺财\", } var a animal // 定义一个 animal 类型的变量 a = d1 // d1 可以赋值给 animal 接口 a.say() a.move() } 输出: 旺财 在叫~ 旺财 在奔跑~ ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"空接口 空接口的定义 空接口是指没有定义任何方法的接口。因此任何类型都实现了空接口。 空接口类型的变量可以存储任意类型的变量。 func main() { // 定义空接口 var x interface{} // 空接口 可以接受任意类型 x = \"string\" fmt.Printf(\"%#v\\n\", x) x = 100 fmt.Printf(\"%#v\\n\", x) x = false fmt.Printf(\"%#v\\n\", x) x = []string{} fmt.Printf(\"%#v\\n\", x) x = struct{}{} fmt.Printf(\"%#v\\n\", x) } 输出: \"string\" 100 false []string{} struct {}{} ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"空接口的应用 1.空接口作为函数的参数 使用空接口实现可以接收任意类型的函数参数。 // 空接口作为函数参数 func show(a interface{}) { fmt.Printf(\"type:%T value:%v\\n\", a, a) } 2.空接口作为map的值 使用空接口实现可以保存任意值的字典。 func main(){ // 定义一个 map value 的类型为 interface studentInfo := make(map[string]interface{}) // 这里可以赋值为 任何类型 studentInfo[\"name\"] = \"张大仙\" studentInfo[\"age\"] = 18 studentInfo[\"hobby\"] = []string{\"王者荣耀\", \"讲骚话\"} fmt.Printf(\"%#v\\n\", studentInfo) } 输出: map[string]interface {}{\"age\":18, \"hobby\":[]string{\"王者荣耀\", \"讲骚话\"}, \"name\":\"张大仙\"} ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"类型断言 空接口可以存储任意类型的值, 那我们如何获取其存储的具体数据呢? 接口值 一个接口的值（简称接口值）是由一个具体类型和具体类型的值两部分组成的。这两部分分别称为接口的动态类型和动态值。 var x interface{} x = false # 这里面 x 空接口包含两个部分 # 1. 动态类型 = bool 类型 # 2. 动态值 = false 想要判断空接口中的值这个时候就可以使用类型断言。 其语法格式: x.(T) , 该语法返回两个参数，第一个参数是x转化为T类型后的变量, 第二个值是一个布尔值, 若为true则表示断言成功, 为false则表示断言失败。 x: 表示类型为interface{}的变量 T: 表示断言x可能是的类型。 // 类型断言 func main() { var x interface{} x = false // 1. 使用 ok 来做类型断言 // 如果 !ok 的情况下,v的值是类型的零值 v, ok := x.(string) if !ok { fmt.Printf(\"不是String类型 v: %v\\n\", v) } else { fmt.Printf(\"是String类型 v: %v\\n\", v) } // 2. 使用switch case 来做类型断言 switch v := x.(type) { case string: fmt.Printf(\"String类型 v: %v\\n\", v) case int: fmt.Printf(\"Int类型 v: %v\\n\", v) case bool: fmt.Printf(\"布尔类型 v: %v\\n\", v) default: fmt.Printf(\"其他类型 v: %v\\n\", v) } } 输出: 不是String类型 v: 布尔类型 v: false ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:9","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"注意事项 关于接口需要注意的是, 只有当有两个或两个以上的具体类型必须以相同的方式进行处理时才需要定义接口。不要为了接口而写接口, 那样只会增加不必要的抽象, 导致不必要的运行时损耗。 ","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:10","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"接口练习题 使用接口的方式实现一个既可以往终端写日志也可以往文件写日志的简易日志库。 mylogger.go package mylogger import \"strings\" // 自定义一个 日志级别 的类型 type Level uint16 // 定义 日志级别 的常量 const ( DebugLevel Level = iota InfoLevel WarningLevel ErrorLevel FatalLevel ) // 定义一个 日志的 接口 type Logger interface { Debug(format string, args ...interface{}) Info(format string, args ...interface{}) Warning(format string, args ...interface{}) Error(format string, args ...interface{}) Fatal(format string, args ...interface{}) Close() } // 获取对应 日志级别的 字符串 func getLevelStr(level Level) string { switch level { case DebugLevel: return \"DEBUG\" case InfoLevel: return \"INFO\" case WarningLevel: return \"WARNING\" case ErrorLevel: return \"ERROR\" case FatalLevel: return \"FATAL\" default: return \"DEBUG\" } } // 获取用户传入的 字符串,解析日志级别 func parseLogLevel(levelStr string) Level { // 1. 将字符串转换成全小写 levelStr = strings.ToLower(levelStr) switch levelStr { case \"debug\": return DebugLevel case \"info\": return InfoLevel case \"warning\": return WarningLevel case \"error\": return ErrorLevel case \"fatal\": return FatalLevel default: return DebugLevel } } file.go package mylogger import ( \"fmt\" \"os\" \"path\" \"time\" ) // 往文件里写日志 // 定义 文件日志 的结构体 type FileLogger struct { // 定义日志记录的级别 level Level // 日志文件名称 filename string // 日志文件路径 filepath string // 文件句柄 file *os.File errFile *os.File // 日志大小 maxSize int64 } // 文件日志的 构造函数 并初始化打开日志文件以及句柄 func NewFileLogger(levelStr, filename, filepath string, logSize int64) *FileLogger { logLevel := parseLogLevel(levelStr) fl := \u0026FileLogger{ level: logLevel, filename: filename, filepath: filepath, maxSize: logSize, } // 初始化 传入的文件名字以及路径打开文件 fl.initFile() // 返回 return fl } // 初始化文件的句柄的方法 func (f *FileLogger) initFile() { // 1. 拼接 日志文件 的路径 logName := path.Join(f.filepath, f.filename) // 2. 打开文件,使用 os.OpenFile fileObj, err := os.OpenFile(logName, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil { panic(fmt.Errorf(\"打开文件 %s 失败, %v\", logName, err)) } // 3. 将文件句柄赋值给 FileLogger 结构体中的 f.file f.file = fileObj // 4. 错误日志 errLogName := fmt.Sprintf(\"%s.err\", logName) // 5. 打开错误日志文件 errFileObj, err := os.OpenFile(errLogName, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil { panic(fmt.Errorf(\"打开文件 %s 失败, %v\", errLogName, err)) } // 6. 将文件句柄赋值给 FileLogger 结构体中的 f.errFile f.errFile = errFileObj } func (f *FileLogger) checkSplit(file *os.File) bool { // 获取当前日志文件的大小 fileInfo, _ := file.Stat() fileSize := fileInfo.Size() // 判断传入的文件大小 return fileSize \u003e= f.maxSize } // 日志切割函数 func (f *FileLogger) splitLogFile(file *os.File) *os.File { // 获取 文件的完整路劲 fileName := file.Name() backupName := fmt.Sprintf(\"%s_%v.back\", fileName, time.Now().Unix()) // 关闭原来的文件句柄 file.Close() // 备份原来日志的文件 _ = os.Rename(fileName, backupName) // 重新打开日志文件 fileObj, err := os.OpenFile(fileName, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644) if err != nil { panic(fmt.Errorf(\"打开文件 %s 失败, %v\", fileName, err)) } return fileObj } // 日志记录功能函数 func (f *FileLogger) log(level Level, format string, args ...interface{}) { // 获取 日志需要记录的信息 // 0. 判断日志的级别, 判断传入参数level的日志级别 if f.level \u003e level { return } // 1. fmt.Sprint 字符串拼接 msg := fmt.Sprintf(format, args...) // 2. 定义 要写入日志的格式: ( [时间][文件:行号][函数名][日志级别] msg ) // 2.1 获取时间 nowStr := time.Now().Format(\"2006-01-02 15:04:05.000\") // 2.2 获取 文件,行号,函数名 等信息 fileName, link, funcName := getCallerInfo(3) // 2.3 获取 用户传入的日志级别 字符串 logLevelStr := getLevelStr(level) // 2.4 拼接成完整的日志信息 logMsg := fmt.Sprintf(\"[%s][%s:%d][%s][%s] %s\", nowStr, fileName, link, funcName, logLevelStr, msg) // 检查判断是否需要切分日志 if f.checkSplit(f.file) { f.file = f.splitLogFile(f.file) } // 3. fmt.Fprintf 写入 f.file 日志文件 _, err := fmt.Fprintln(f.file, logMsg) if err != nil { panic(fmt.Errorf(\"写入文件 %s 失败\", f.filename)) } // 4. 如果 level 等级是 error 或 fatal 级别还要写入 f.errFile 中 if level \u003e= ErrorLevel { if f.checkSplit(f.errFile) { f.errFile = f.splitLogFile(f.errFile) } _, err := fmt.Fprintln(f.errFile, logMsg) if err != nil { panic(fmt.Errorf(\"写入文件 %s 失败\", f.filename)) } } } // Debug 日志级别的方法 args ...表示 一个或多个参数 func (f *FileLogger) Debug(format string, args ...i","date":"2000-01-01","objectID":"/golang-study-note-4-4/:1:11","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4-4/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-4/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-4/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数 函数 是组织好的、可重复使用的、用于执行指定任务的代码块。 Go语言中支持函数、匿名函数和闭包, 并且函数在Go语言中属于 “一等公民” 。 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数定义 Go语言中定义函数使用func关键字. // 函数定义 (参数 与 返回值 可以是多个) func 函数名(参数)(返回值){ 函数体 } 名词解析: 函数名: 由字母、数字、下划线组成。但函数名的第一个字母不能是数字。在同一个包内, 函数名也称不能重名。 参数: 参数由参数变量和参数变量类型组成, 多个参数之间使用,分隔。 返回值: 返回值由返回值变量和其变量类型组成, 也可以只写返回值的类型, 多个返回值必须用()包裹, 并用,分隔。 函数体: 实现指定功能的代码块。 // 不包含参数与返回值的函数 func sayHello() { fmt.Println(\"Hello ~\") } func main() { // 函数的调用 sayHello() } 输出: Hello ~ ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数参数 函数参数由参数变量和参数变量类型组成, 如果相邻变量的类型相同, 则可以省略类型。 函数参数类型相同时可以简写, 如: func sum(x, y int) Go语言是没有 默认参数的 // 带参数的函数 func sayHi(name string) { fmt.Println(\"Hi \", name) } func main() { // 带参数的函数调用 sayHi(\"康师傅\") } 输出: Hi 康师傅 // 带多个参数和返回值的函数 func intSum(x, y int) int { sum := x + y // 使用 return 关键字 返回这个值 return sum } func main() { // 多个参数和返回值函数调用 ret := intSum(10, 11) fmt.Println(ret) } 输出: 21 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数可变参数 可变参数 是指函数的参数数量不固定, Go语言中的可变参数通过在参数名后加...来标识。 可变参数 要放在参数最后面, 如: func a(x int, y...int) // 带可变参数的函数 // 可变参数的类型为 切片 func allSum(a ...int) int { sum := 0 // 遍历 a 切片中的元素 for _, arg := range a { // 将所有传入的参数相加 sum = sum + arg } return sum } func main() { // 可变参数的函数调用 r1 := allSum() r2 := allSum(10, 20) r3 := allSum(20, 30, 40) fmt.Println(r1) fmt.Println(r2) fmt.Println(r3) } 输出: 0 30 90 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数返回值 Go语言中通过return关键字向外输出返回值。 Go语言中函数支持多返回值, 函数如果有多个返回值时必须用()将所有返回值包裹起来。 函数定义时可以给返回值命名, 并在函数体中直接使用这些变量, 最后通过return关键字返回。 多返回值也支持类型简写 // 带多个返回值的函数 func calc(x, y int) (sub, sum int) { sum = x + y sub = x - y return } func main() { // 多返回值的函数调用 (有多少个返回值就必须用多少个参数去接收) sub, sum := calc(100, 200) fmt.Println(sub, sum) } 输出: -100 300 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"defer语句 Go语言中的defer语句会将其后面跟随的语句进行延迟处理。在defer归属的函数即将返回时, 将延迟处理的语句按defer定义的逆序进行执行, 也就是说, 先被defer的语句最后被执行, 最后被defer的语句，最先被执行。 由于defer语句延迟调用的特性，所以defer语句能非常方便的处理资源释放问题。比如: 资源清理、文件关闭、解锁及记录时间等。 在Go语言的函数中return语句在底层并不是原子操作, 它分为给返回值赋值和RET指令两步。而defer语句执行的时机就在返回值赋值操作后, RET指令执行前。 // 函数 return 语句 底层实现 [返回值 = x] [return x] -\u003e ⇊ [RET 指令] // defer 语句执行 [返回值 = x] ⇊ [return x] -\u003e [运行 defer] ⇊ [RET 指令] // defer 延迟执行 func main() { // 多个 defer 是遵循 先入后出 的顺序 fmt.Println(\"Start --\") defer fmt.Println(\"第一条\") defer fmt.Println(\"第二条\") defer fmt.Println(\"第三条\") fmt.Println(\"End --\") } 输出: Start -- End -- 第三条 第二条 第一条 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"defer 题目分析 // defer 的题目 func calc(index string, a, b int) int { ret := a + b fmt.Println(index, a, b, ret) return ret } func main() { // defer 题目 // defer 先入后出 的顺序 x := 1 y := 2 // 第一个参数为 \"AA\" , 第二个参数为 x = 1 // 第三个参数为 calc(\"A\", 1, 2) 的返回值 = 3 // 最后整体的返回值是 1 + 3 = 4 // 输出 AA 1 3 4 // calc(\"A\", x, y) 这里的输出为 A 1 2 3 defer calc(\"AA\", x, calc(\"A\", x, y)) x = 10 // 第一个参数为 \"BB\", 第二个参数为 x = 10 // 第三个参数为 calc(\"B\", 10, 2) 的返回值 = 12 // 最后整体的返回值是 10 + 12 = 22 // 输出 BB 10 12 22 // calc(\"B\", x, y) 这里输出为 B 10 2 12 defer calc(\"BB\", x, calc(\"B\", x, y)) y = 20 } 输出: A 1 2 3 B 10 2 12 BB 10 12 22 AA 1 3 4 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数 变量作用域 函数 全局变量与局部变量同时存在时, 优先使用 函数内定义的局部变量, 然后在使用全局变量。 // 定义全局变量 var intG = 10 func testFunc() { // 定义一个局部变量 intG := 100 // 输出这个变量 (函数会优先使用 局部变量) fmt.Println(intG) } func main() { // 调用函数 testFunc() } 输出: 100 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数类型与变量 函数类型变量 func testFunc() { // 定义一个局部变量 intG := 100 // 输出这个变量 fmt.Println(intG) } func main() { // 调用函数 testFunc() // 函数可以作为一个变量来赋值 abc := testFunc fmt.Printf(\"%T \\n\", abc) abc() } 输出: func() 100 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:8","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"高阶函数 高阶函数分为函数作为参数和函数作为返回值两部分。 函数作为参数 func sum(x, y int) int { return x + y } func sub(x, y int) int { return x - y } // 函数变量也可以是 函数类型 func calc(x, y int, op func(int, int) int) int { return op(x, y) } func main() { // 将 sum 这个函数当成 calc 的变量传入 r1 := calc(100, 200, sum) fmt.Println(r1) // 将 sub 这个函数当成 calc 的变量传入 r2 := calc(100, 200, sub) fmt.Println(r2) } 输出: 300 -100 函数作为返回值 func sum(x, y int) int { return x + y } func sub(x, y int) int { return x - y } // 函数作为函数的返回值传入函数中 func do(s string) (func(int, int) int, error) { switch s { case \"+\": return sum, nil case \"-\": return sub, nil default: err := errors.New(\"无法识别的操作符\") return nil, err } } func main() { r2, err := do(\"+\") if err != nil { fmt.Println(err) } fmt.Println(r2(100, 200)) r3, err := do(\"-\") if err != nil { fmt.Println(err) } fmt.Println(r3(400, 200)) } 输出 300 200 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:9","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"匿名函数和闭包 匿名函数 - 匿名函数就是没有函数名的函数。 匿名函数多用于实现回调函数和闭包。 func(参数)(返回值){ 函数体 } // 匿名函数 func main() { // 定义匿名函数与调用 // 1. 匿名函数调用,赋值给变量 r1 := func() { fmt.Println(\"匿名函数1\") } r1() // 2. 匿名函数定义的时候就直接调用 func() { fmt.Println(\"匿名函数2\") }() } 输出: 匿名函数1 匿名函数2 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:10","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"闭包 闭包指的是一个函数和与其相关的引用环境组合而成的实体。简单来说: 闭包=函数+引用环境(外层变量的引用)。 // 函数闭包 // 定义一个函数,它的返回值是一个函数类型 func a() func() { name := \"闭包\" //把函数作为返回值,并传入一个变量 return func() { fmt.Println(\"func a:\", name) } } func main() { // 将a()这个函数赋值给 变量r1 // 闭包 = \"函数 + 外层变量的引用\" 所以 r1 就是一个闭包 r1 := a() // 此时调用r1 这个变量 相当于调用 a()函数内的匿名函数 r1() } 输出: func a: 闭包 // 定义一个函数并且包含一个变量name,返回值是一个函数类型 func b(name string) func() { return func() { fmt.Println(\"func b:\", name) } } func main() { // 此时调用 r2 这个变量 加上参数 r2 := b(\"闭包b\") r2() } 输出: func b: 闭包b 闭包的应用一 // 定义一个函数 返回值是一个匿名函数 // 匿名函数包含一个参数和一个返回值 func suffixFunc(suffix string) func(string) string { // 返回 这个匿名函数 return func(name string) string { // 如果 name 参数 不包含 suffix 这个上层参数 if !strings.HasSuffix(name, suffix) { // 返回 name 参数 + suffix 参数 return name + suffix } // 如果包含就直接返回这个 name 参数 return name } } func main() { //suffixFunc 赋值给 变量 ret, suffixFunc 传入一个参数 ret := suffixFunc(\".txt\") // 调用 suffixFunc 内部的匿名函数 r1 := ret(\"小说\") fmt.Println(r1) ret = suffixFunc(\".avi\") r2 := ret(\"视频\") fmt.Println(r2) } 输出: 小说.txt 视频.avi 闭包的应用二 // 闭包的应用 2 // 定义一个函数,有一个变量和两个函数返回值 func calc(base int) (func(int) int, func(int) int) { // 定义个匿名函数有一个参数和一个返回值 sum := func(i int) int { // 闭包: 引用外层变量 base // base = base + i base += i // 返回 base return base } // 定义个匿名函数有一个参数和一个返回值 sub := func(i int) int { // 闭包: 引用外层变量 base // base = sum 函数的返回值 base - i base -= i // 返回 base return base } // 返回如上两个匿名函数的变量 return sum, sub } func main() { //调用函数 有两个返回值x,y 并传入calc参数 base = 200 // x = sum 这个匿名函数 // y = sub 这个匿名函数 x, y := calc(200) // 调用 x 这个匿名函数,传入一个参数100 ret1 := x(100) // 调用 y 这个匿名函数,传入一个参数200 ret2 := y(200) // 分别打印 fmt.Printf(\"x:sum = %d y:sub = %d \\n\", ret1, ret2) } 输出: x:sum = 300 y:sub = 100 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:11","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"内置函数 内置函数 说明 close 主要用来关闭channel len 用来求长度，比如string、array、slice、map、channel new 用来分配内存，主要用来分配值类型，比如int、struct。返回的是指针 make 用来分配内存，主要用来分配引用类型，比如chan、map、slice append 用来追加元素到数组、slice中 panic和recover 用来做错误处理 panic/recover Go语言中 版本小于等于 1.12 是没有异常机制, 但是使用panic/recover模式来处理错误。panic可以在任何地方引发。 recover()必须搭配defer使用。 defer一定要在可能引发panic的语句之前定义。 引发 panic func a() { fmt.Println(\"func in a\") } func b() { panic(\"panic in b\") } func c() { fmt.Println(\"func in c\") } // panic \u0026\u0026 recover func main() { // 分别调用 a b c 函数 a() b() c() } 输出: func in a panic: panic in b goroutine 1 [running]: main.b(...) /Users/jicki/go/src/github.com/jicki/panic_recover/panic.go:10 main.main() /Users/jicki/go/src/github.com/jicki/panic_recover/panic.go:21 +0x96 exit status 2 使用 recover 恢复错误 func a() { fmt.Println(\"func in a\") } func b() { // recover 必须跟 defer 配合使用 // 定义一个匿名函数,使用 recover 尝试恢复错误 defer func() { err := recover() if err != nil { fmt.Println(\"recover error in b\") } }() // recover 必须要在可能引发 panic 之前执行 panic(\"panic in b\") } func c() { fmt.Println(\"func in c\") } // panic \u0026\u0026 recover func main() { // 分别调用 a b c 函数 a() b() c() } 输出: func in a recover error in b func in c ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:12","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"函数练习题 /* 你有50枚金币，需要分配给以下几个人：Matthew,Sarah,Augustus,Heidi,Emilie,Peter,Giana,Adriano,Aaron,Elizabeth。 分配规则如下： a. 名字中每包含1个'e'或'E'分1枚金币 b. 名字中每包含1个'i'或'I'分2枚金币 c. 名字中每包含1个'o'或'O'分3枚金币 d: 名字中每包含1个'u'或'U'分4枚金币 写一个程序，计算每个用户分到多少金币，以及最后剩余多少金币？ 程序结构如下，请实现 ‘dispatchCoin’ 函数 */ var ( coins = 50 users = []string{\"Matthew\", \"Sarah\", \"Augustus\", \"Heidi\", \"Emilie\", \"Peter\", \"Giana\", \"Adriano\", \"Aaron\", \"Elizabeth\"} distribution = make(map[string]int, len(users)) ) func dispatchCoin() int { // 遍历 users 切片,获取每个名字的元素 for _, name := range users { nameCoin := 0 // 遍历切片中的元素,获取到每个元素的字母的 byte for _, str := range name { // 循环判断 str 中的 byte switch str { case 'e', 'E': nameCoin += 1 case 'i', 'I': nameCoin += 2 case 'o', 'O': nameCoin += 3 case 'u', 'U': nameCoin += 4 } } // 将判断后的金币数量 赋值到 map 字典中 distribution[name] = nameCoin // 金币总数 = 金币总数 - 每个元素的 nameCoin coins -= nameCoin } // 最后返回这个 金币 coins 总数 fmt.Println(distribution) return coins } func main() { left := dispatchCoin() fmt.Println(\"剩下:\", left) } 输出: map[Aaron:3 Adriano:5 Augustus:12 Elizabeth:4 Emilie:6 Giana:2 Heidi:5 Matthew:1 Peter:2 Sarah:0] 剩下: 10 ","date":"2000-01-01","objectID":"/golang-study-note-4/:1:13","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"指针 Go语言中的指针区别于C/C++中的指针, 不能进行偏移和运算, 是安全指针。 Go语言中的指针需要先知道3个概念: 指针地址、指针类型和指针取值。 Go语言中的函数传参都是值拷贝, 当我们想要修改某个变量的时候, 我们可以创建一个指向该变量地址的指针变量。传递数据使用指针, 而无须拷贝数据。类型指针不能进行偏移和运算。Go语言中的指针操作非常简单, 只需要记住两个符号: \u0026（取地址） 和 *（根据地址取值）。 取地址操作符\u0026和取值操作符 *是一对互补操作符, \u0026取出地址, *根据地址取出地址指向的值。 变量、指针地址、指针变量、取地址、取值的相互关系和特性如下: 对变量进行取地址 \u0026 操作, 可以获得这个变量的指针变量。 指针变量的值是指针地址。 对指针变量进行取值*操作, 可以获得指针变量指向的原变量的值。 ","date":"2000-01-01","objectID":"/golang-study-note-4/:2:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"指针地址和指针类型 每个变量在运行时都拥有一个地址, 这个地址代表变量在内存中的位置。Go语言中使用\u0026字符放在变量前面对变量进行\"取地址\"操作。 Go语言中的值类型（int、float、bool、string、array、struct）都有对应的指针类型, 如: *int、*int64、*string 等。 取变量指针的语法: ptr := \u0026v v: 代表被取地址的变量, 类型为T ptr: 用于接收地址的变量, ptr的类型就为*T, 称做T的指针类型。*代表指针。 // 指针 func main() { // a int类型 a := 100 // \u0026 取地址 b *int类型 (int类型的指针) b := \u0026a // a 的值是 10 , a 的指针是 a 自己的指针 fmt.Printf(\"a 的值: [%v] a 的指针: [%p]\\n\", a, \u0026a) // b 的值是 a 的指针, b 的指针是 b 自己的地址 fmt.Printf(\"b 的值: [%v] b 的指针: [%p]\\n\", b, \u0026b) } 输出: a 的值: [100] a 的指针: [0xc0000aa008] b 的值: [0xc0000aa008] b 的指针: [0xc0000a6018] ","date":"2000-01-01","objectID":"/golang-study-note-4/:2:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"指针取值 在对普通变量使用\u0026操作符取地址后会获得这个变量的指针, 然后可以对指针使用*操作，也就是指针取值。 func main() { // a int类型 a := 100 // \u0026 取地址 b *int类型 (int类型的指针) b := \u0026a fmt.Printf(\"a 的值: [%v] a 的指针: [%p]\\n\", a, \u0026a) fmt.Printf(\"b 的值: [%v] b 的指针: [%p]\\n\", b, \u0026b) // 指针取值 c 的值 等于 b 指针的值 c := *b fmt.Printf(\"c 的值: [%v] c 的指针: [%p]\\n\", c, \u0026c) } 输出: a 的值: [100] a 的指针: [0xc0000aa008] b 的值: [0xc0000aa008] b 的指针: [0xc0000a6018] c 的值: [100] c 的指针: [0xc0000aa018] ","date":"2000-01-01","objectID":"/golang-study-note-4/:2:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"指针传值 // 定义一个函数包含一个int类型的参数 x func modify1(x int) { // 将 x 修改为 100 x = 100 } // 定义一个函数包含一个*int (指针类型)类型的参数 y func modify2(y *int) { // 将 指针地址的值 修改为 100 *y = 100 } func main() { // a int类型 a := 10 // 调用 modify1 将 a 传到 函数中 modify1(a) fmt.Printf(\"modify1 = %d \\n\", a) // 调用 modify2 将 a 的地址传到 函数中 modify2(\u0026a) fmt.Printf(\"modify2 = %d \\n\", a) } 输出: modify1 = 10 modify2 = 100 ","date":"2000-01-01","objectID":"/golang-study-note-4/:2:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"new和make Go语言中new和make是内建的两个函数, 主要用来分配内存。 new new是一个内置的函数，它的函数签名: func new(Type) *Type , Type表示类型, new函数只接受一个参数, 这个参数是一个类型. *Type表示类型指针, new函数返回一个指向该类型内存地址的指针。 new函数不太常用, 使用new函数得到的是一个类型的指针,并且该指针对应的值为该类型的零值。 func main() { a := new(int) b := new(bool) fmt.Printf(\"a 的类型: %T a 的值: %v \\n\", a, *a) fmt.Printf(\"b 的类型: %T b 的值: %v \\n\", b, *b) // 定义一个 指针类型 的变量 c var c *int // 定义了以后需要使用 new 来初始化这个变量 c = new(int) // 不初始化会报 nil 空指针的错误 *c = 100 fmt.Println(*c) } 输出: a 的类型: *int a 的值: 0 b 的类型: *bool b 的值: false 100 make 也是用于内存分配的，区别于new, 它只用于slice、map以及chan的内存创建, 而且它返回的类型就是这三个类型本身, 而不是他们的指针类型, 因为这三种类型就是引用类型, 所以就没有必要返回他们的指针了。 make 函数的函数签名 func make(t Type, size ...IntegerType) Type make 函数是无可替代的, 我们在使用slice、map以及channel的时候, 都需要使用make进行初始化, 然后才可以对它们进行操作。 func main() { // make // 定义一个 map var m map[string]int // 使用 make 进行初始化 m = make(map[string]int, 10) // 必须初始化以后才能操作,否则报错 m[\"Map\"] = 1 fmt.Println(m) } 输出: map[Map:1] ","date":"2000-01-01","objectID":"/golang-study-note-4/:2:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"new 与 make 的区别 二者都是用来做内存分配的。 make只用于slice、map以及channel的初始化, 返回的还是这三个引用类型本身。 new用于类型的内存分配, 并且内存对应的值为类型零值, 返回的是指向类型的指针。 ","date":"2000-01-01","objectID":"/golang-study-note-4/:2:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-4/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-5/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-5/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-5/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-5/"},{"categories":["golang"],"content":"1. 反射 反射是程序执行时检查其所拥有的结构. 尤其是类型的一种能力. 当 【类型断言】 无法满足时, 我们就使用 【反射】 获取 动态类型 和 动态值 . 反射一般应用于 各类 web 框架, 配置文件解析库, ORM框架 反射的优点: 反射是一个强大并富有表现力的工具, 可以让代码更加灵活. 反射的缺点: 基于反射的代码极其脆弱, 反射中类的类型错误会在程序运行时才会引发panic. 在代码中大量使用 反射, 会使代码很难理解. 反射的性能低下, 基于反射实现的代码通常比正常代码效率低一到两个数量级 在反射中 动态类型 还划分为 类型(Type) 与 种类(Kind) , 因为在 Go 语言中我们可以使用 type 关键字构造很多自定义类型, 种类(Kind) 是指最底层的类型, 在反射中, 需要区分 指针 结构体 等 大品种的类型时, 就会使用 种类(Kind) . // 种类(Kind) 的类型包括如下 type Kind uint const ( Invalid Kind = iota Bool Int Int8 Int16 Int32 Int64 Uint Uint8 Uint16 Uint32 Uint64 Uintptr Float32 Float64 Complex64 Complex128 Array Chan Func Interface Map Ptr //指针 Slice String Struct UnsafePointer ) ","date":"2000-01-01","objectID":"/golang-study-note-5/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-5/"},{"categories":["golang"],"content":"1. reflect 包 Go语言中反射的相关功能由内置的 reflect 包提供, 任意接口值在反射中都可以理解为由 reflect.Type 和 reflect.Value 两部分组成, 并且 reflect.TypeOf 和 reflect.ValueOf 两个函数来获取任意对象的Value和Type. TypeOf Go语言中, 使用 reflect.TypeOf() 函数可以获得任意值类型对象 (reflect.Type), 程序通过类型对象可以访问任意值的类型信息. // 获取 任意值 的类型 func reflectType(x interface{}) { v := reflect.TypeOf(x) // 获取 x 的动态信息 fmt.Printf(\"Type: %v\\n\", v) // 打印 v 的类型 } func main() { reflectType(11) // 输出 Type: int reflectType(\"字符串\") //输出 Type: string reflectType([10]string{}) //输出 Type: [10]string } // 使用 TypeOf(x).Name() 以及 TypeOf(x).Kind() 方法 // 获取动态类型的 具体类型(Type) 以及 种类(Kind) func reflectType(x interface{}) { v := reflect.TypeOf(x) // 获取 x 的动态信息 // 获取类型的 类型(Type) 以及 种类(Kind) fmt.Printf(\"Type name: %v, Kind: %v\\n\", v.Name(), v.Kind()) } type cat struct { name string } func main() { c1 := cat{ name: \"小花\", } i := 100 /* 输出如下: ( 引用类型的 name 为 空 ) Type name: cat, Kind: struct Type name: , Kind: ptr Type name: , Kind: slice Type name: , Kind: map */ reflectType(c1) reflectType(\u0026i) reflectType([]string{}) reflectType(map[string]int{}) } ValueOf reflect.ValueOf() 函数 返回的是 reflect.value 类型, 包含了原始值的相关信息. relect.value 与 原始值之间可以相互转换. relect.value 类型 获取原始值的方法有如下: 方法 说明 Interface() interface{} 将值以 interface{} 类型返回, 可通过类型断言转换指定类型 Int() int64 将值以 int 类型返回, 所有符号整型可以用此类型返回 Uint() uint64 将值以 uint64 类型返回, 所有无符号整型可以用此类型返回 Float() float64 将值以 float64 类型返回, 所有 浮点数 可用此类型返回 Bool() bool 将值以 (布尔值)bool 类型返回 Bytes() []bytes 将值以字节数组 []bytes 类型返回 String() string 将值以 (字符串)string 类型返回 func reflectValue(x interface{}) { v := reflect.ValueOf(x) // 获取 函数参数 x 的值信息 k := v.Kind() // 获取 值信息 对应的 种类(kind) // switch 类型断言 判断传入时的类型, 强制转换成 传入时的类型 // 当类型传入 接口时，类型会转换成 接口类型. // 想获取传入时的类型,就必须要进行如下判断并强制转换为原来的类型 switch k { case reflect.Int64: fmt.Printf(\"type = int64 , value = %d \\n\", int64(v.Int())) case reflect.Float32: fmt.Printf(\"type = float32 , value = %f \\n\", float32(v.Float())) case reflect.Float64: fmt.Printf(\"type = float64 , value = %f \\n\", float64(v.Float())) } } func main() { var a float32 = 3.14 var b int64 = 100 reflectValue(a) reflectValue(b) } 通过反射 修改设置变量的值 // 在反射中 使用 Elem() 方法获取指针对应的值 func reflectValue(x interface{}) { v := reflect.ValueOf(x) // 利用 类型断言 判断 Kind 种类 是否等于 传入的类型 if v.Elem().Kind() == reflect.Int64 { // 利用 SetInt 方法修改 传入的值 v.Elem().SetInt(190) } } func main() { var a int64 = 100 reflectValue(\u0026a) fmt.Println(a) } IsNil() 与 IsValid() IsNil() 常用于判断 指针是否为空, IsValid() 常用于判断返回值是否有效. func main() { var a *int var b *int c := 100 b = \u0026c // 输出 a IsNil: true fmt.Println(\"a IsNil:\", reflect.ValueOf(a).IsNil()) // 输出 b IsNil: false fmt.Println(\"b IsNil:\", reflect.ValueOf(b).IsNil()) } IsNil() func(v Value) IsNil() bool{} IsNil() 用来判断 v 的值是否为 nil ( 引用类型 在没有初始化的时候值都为 nil ), v 持有的值分类必须是 通道(channel)、函数、接口、映射、指针、切片之一; 否则会引起 panic IsValid() func(v Value) IsValid() bool{} IsValid() 返回 v 是否有一个值. 如果 v 是零值 返回 false, 此时 v 除了 IsValid、String、Kind 之外的方法都会引起 panic func main() { var a *int // 输出 a nil IsValid: true fmt.Println(\"a nil IsValid:\", reflect.ValueOf(a).IsValid()) // 创建一个匿名结构体 b b := struct{}{} // FieldByName() 判断结构体的 字段 是否 存在 // 输出 是否存在的结构体字段: false fmt.Println(\"是否存在的结构体字段:\", reflect.ValueOf(b).FieldByName(\"123\").IsValid()) // MethodByName() 判断结构体的 方法 是否存在 // 输出 是否存在的结构体方法: false fmt.Println(\"是否存在的结构体方法:\", reflect.ValueOf(b).MethodByName(\"123\").IsValid()) } ","date":"2000-01-01","objectID":"/golang-study-note-5/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-5/"},{"categories":["golang"],"content":"结构体反射 reflect.TypeOf() 获取反射对象信息,如果是类型是 结构体 , 可以通过反射 reflect.Type() 的 NumField() 和 Field() 方法获取结构体成员的详细信息. reflect.Type 获取结构体成员的方法: 方法 说明 Field(i int) StructField 根据索引,返回索引对应的结构体字段信息 NumField() int 返回结构体成员字段数量 FieldByName(name string) (StructField, bool) 根据指定字符串返回字符串对应结构体字段信息 FieldByIndex(index []int) StructField 多层成员访问, 根据 []int 提供的每个结构体的字段索引,返回字段信息 FieldByNameFunc(match func(string) bool) 根据传入的匹配函数查找需要的字段 NumMethod() int 返回该类型的方法 集中方法的数目 Method(int) Method 返回该方法集中的第N个方法 MethodByName(string)(Method, bool) 根据方法名返回该类型集中的方法 StructField 类型 StructField 类型用来描述结构体中的一个字段信息 StructField 的定义: type StructField struct { Name string // 字段名字 pkgPath string // 非导出字段的包路径 Type Type // 字段类型 Tag StructTag // 字段标签 Offset uintptr // 字段在结构体中的字节偏移量 Index []int // 用于Type.FieldByIndex 的索引切片 Anonymous bool // 是否为匿名字段 } 例子1 // 定义一个 student 的结构体 type student struct { Name string `json:\"name\"` //为 json 输出定义一个 tag Score int `json:\"score\"` } func main() { stu1 := student{ Name: \"小学生\", Score: 100, } // 反射 获取 stu1 的类型 t := reflect.TypeOf(stu1) // t.Name() 获取 stu1 类型, t.Kind() 获取 stu1 类型的种类. fmt.Println(t.Name(), t.Kind()) // 输出 student struct // 遍历结构体 // 1. 通过结构体索引获取所有字段信息. // t.NumField() 会返回结构体成员的数量 for i := 0; i \u003c t.NumField(); i++ { //t.Field() 根据索引返回索引的字段信息 field := t.Field(i) fmt.Printf(\"Key: %s index: %d Type: %v json Tag: %v \\n\", field.Name, field.Index, field.Type, field.Tag.Get(\"json\")) /* 输出: Key: Name index: [0] Type: string json Tag: name Key: Score index: [1] Type: int json Tag: score */ } // 2. 通过字段名的方式,获取指定结构体字段信息 // t.FieldByName(\"字段名\") 会返回指定字段名的结构体信息 if soreField, ok := t.FieldByName(\"Score\"); ok { fmt.Printf(\"Key: %s Index: %d Type: %v json tag: %v\\n\", soreField.Name, soreField.Index, soreField.Type, soreField.Tag.Get(\"json\")) /* 输出: Key: Score Index: [1] Type: int json tag: score */ } } 例子2 type student struct { Name string Score int } // Study 创建学习方法 func (s student) Study() string { msg := \"好好学习，天天向上\" fmt.Println(msg) return msg } func (s student) Sleep() string { msg := \"好好休息，劳逸结合\" fmt.Println(msg) return msg } // 创建一个函数 func printMethod(x interface{}) { // 反射 获取传入参数 x 的 动态类型 x 与 动态值 v t := reflect.TypeOf(x) v := reflect.ValueOf(x) // t.NumMethod() 返回该类型方法的数目 // 获取 Method 结构体类型必须是 值类型 fmt.Printf(\"Method Num: %d\\n\", t.NumMethod()) for i := 0; i \u003c v.NumMethod(); i++ { // v.Method(i).Type() 获取 方法 的类型 以及返回值 类型 methodType := v.Method(i).Type() // t.Method(i).Name 获取 方法的 名称 methodName := t.Method(i).Name fmt.Printf(\"Method Name: %s\\n\", methodName) fmt.Printf(\"Method Type: %v\\n\", methodType) /* 输出 Method Num: 2 Method Name: Sleep Method Type: func() string Method Name: Study Method Type: func() string */ } // 通过反射调用获取的方法,反射调用方法传递参数必须是 []reflect.Value 类型 var args = []reflect.Value{} // 通过 .Call 方法 调用方法. v.MethodByName(\"Study\").Call(args) // Method(i int) i 为方法的 index v.Method(0).Call(args) /* 输出: 好好学习，天天向上 好好休息，劳逸结合 */ } // 打印结构体里的方法 func main() { stu1 := student{ Name: \"张三丰\", Score: 100, } printMethod(stu1) } ","date":"2000-01-01","objectID":"/golang-study-note-5/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-5/"},{"categories":["golang"],"content":"解析 conf 配置文件 package main import ( \"errors\" \"fmt\" \"io/ioutil\" \"reflect\" \"strconv\" \"strings\" ) /* 1. 打开 xx.conf 文件 2. 一行一行读取文件内容, 根据tag 找结构体对应的字段 3. 根据找到的字段 赋值给结构体 指针(指针类型才可以修改) 4. 利用 反射 设置对应的值 */ //Config 配置文件 结构体 , 并设置 tag type Config struct { FilePath string `conf:\"file_path\"` FileName string `conf:\"file_name\"` MaxSize int64 `conf:\"max_size\"` } // 创建一个 赋值结构体的函数 func parseConf(fileName string, result interface{}) (err error) { // result 必须是 ptr // 利用反射 获取传入的变量的 动态类型 t t := reflect.TypeOf(result) // 利用反射 获取传入的变量的 动态值 v v := reflect.ValueOf(result) // 判断 (种类)Kind 是否为 Ptr (指针)类型 if t.Kind() != reflect.Ptr { err = errors.New(\"result 必须为指针(ptr)类型 \") return } // 利用 Elem() 获取 Ptr(指针) 对应的信息 telm := t.Elem() if telm.Kind() != reflect.Struct { err = errors.New(\"result 必须为指针结构体 \") return } // 读取文件 data = []byte 切片 data, err := ioutil.ReadFile(fileName) if err != nil { err = fmt.Errorf(\"打开配置文件[%s]失败\", fileName) return err } // 获取一个 []string 的切片 s1 := strings.Split(string(data), \"\\n\") //fmt.Println(s1) // 解析 []string 切片 for index, line := range s1 { // 去除 空格 line = strings.TrimSpace(line) // 忽略 空行 和 首字母为 # 的注释行 if len(line) == 0 || strings.HasPrefix(line, \"#\") { continue } // 判断配置文件项是否正确的 eqIndex := strings.Index(line, \"=\") if eqIndex == -1 { err = fmt.Errorf(\"line %d Config Error\", index+1) return } // 检查是否按 = 分割每一行 key := line[:eqIndex] value := line[eqIndex+1:] key = strings.TrimSpace(key) value = strings.TrimSpace(value) if len(key) == 0 { err = fmt.Errorf(\"line %d Config Error\", index+1) return } // 利用 反射 给 result 指针 赋值 // 遍历结构体的所有字段并且与 key 做比较 // telm.NumField() 返回结构体成员的数量 for i := 0; i \u003c telm.NumField(); i++ { // 根据索引返回索引的字段信息 field := telm.Field(i) // Get 结构体 指定 tag 的信息 tag := field.Tag.Get(\"conf\") // 判断配置文件中的 key 是否有结构体中的 tag 匹配 if key == tag { // 获取结构体中字段的类型 fieldType := field.Type // 类型断言, 并强制转换成 获取的字段类型 switch fieldType.Kind() { case reflect.String: v.Elem().Field(i).SetString(value) case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64: // strconv 转换类型 valueint, _ := strconv.ParseInt(value, 10, 64) v.Elem().Field(i).SetInt(valueint) } } } } return } func main() { // 初始化一个 结构体指针 var str1 = \u0026Config{} // 调用 parseConf() 函数,传入文件读取的数据到初始化的结构体中 err := parseConf(\"./xx.conf\", str1) if err != nil { fmt.Println(err) } // 打印赋值后的结构体 fmt.Printf(\"%#v\", str1) } ","date":"2000-01-01","objectID":"/golang-study-note-5/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-5/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-6/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-6/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"并发 并发与并行的区别: 并行 - 是指在一个处理器上“同时”处理多个任务。 并发 - 是指在多个处理器上同时处理多个任务。 ","date":"2000-01-01","objectID":"/golang-study-note-6/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"进程、线程、协程 进程: 一个程序启动后就是一个进程。 (进程是系统资源分配的最小单位.) 线程: 线程就是运行在进程上下文中的逻辑流。 (线程是操作系统能够进行运算调度的最小单位.) 协程: 协程又称微线程和纤程, 协没有线程的上下文切换消耗。协程的调度切换是用户(程序员)手动切换的,因此更加灵活,因此又叫用户空间线程。 ","date":"2000-01-01","objectID":"/golang-study-note-6/:2:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"goroutine Go 语言中创建 goroutine 使用 关键字 go 就可以为函数创建一个 goroutine 。 一个函数可以创建多个 goroutine 。 一个 goroutine 只能对应一个函数。 goroutine 调度是随机、无序的。 sync.WaitGroup 配合 goroutine 使用 sync.WaitGroup 包含三个方法 Add(i) Done() Wait() 例子1 package main import ( \"fmt\" \"sync\" ) // 定义一个锁等待 var wg sync.WaitGroup func say() { fmt.Println(\"This goroutine Func !\") // 执行完毕 wg.Add(n) 每执行完毕都 n-1 wg.Done() } func main() { // 使用关键字 go 启动一个 goroutine // 添加锁等待, (1) 数字为多少个 goroutine wg.Add(1) go say() fmt.Println(\"This Main Func !\") // 阻塞, 等待 goroutine 运行完. wg.Wait() } ","date":"2000-01-01","objectID":"/golang-study-note-6/:3:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"goroutine 与 线程 可增长的栈 OS线程(操作系统线程)一般都有固定的栈内存(通常2MB), 一个 goroutine 的栈在其生命周期开始时占用很小的内存(一般为2KB), goroutine 的栈并不是固定的, 它可以按需增加或缩小, goroutine 栈的大小限制最大可以达到1GB。 goroutine调度 OS线程是由OS内核来调度, goroutine 则是由 Go运行 runtime 自己的调度器调度的, goroutine调度器使用一个称为 m:n 调度技术(复用/调度 m 个 goroutine 到 n 个OS线程)。goroutine 调度不需要切换到OS内核环境, 所以调度一个 goroutine 比调度一个线程成本低很多。 (m:n m 是指 goroutine 数量 , n 是指 线程数量。) GOMAXPROCS Go运行时的调度器使用 GOMAXPROCS 参数来确定需要使用多少个OS线程来同时执行Go代码。默认是机器CPU总核数。 Go语言可以通过 runtime.GOMAXPROCS() 函数设置当前程序并发时占用的CPU逻辑核心数。 例子1 package main import ( \"fmt\" \"sync\" ) var wg sync.WaitGroup func a() { defer wg.Done() for i := 1; i \u003c 10; i++ { fmt.Printf(\"Func A 运行 %d 次\\n\", i) } } func b() { defer wg.Done() for i := 1; i \u003c 10; i++ { fmt.Printf(\"Func B 运行 %d 次\\n\", i) } } func main() { //runtime.GOMAXPROCS(1) // 设置程序运行占用 多少个 逻辑核数 wg.Add(10) for i := 1; i \u003c 10; i++ { go a() go b() } wg.Wait() } OS与goroutine的关系 一个操作系统线程对应用户态多个 goroutine。 Go 程序可以同时使用多个操作系统线程。 goroutine 与 OS 线程 是多对多的关系,既 m:n 。 goroutine退出 goroutine 什么时候退出? goroutine 是在 goroutine 启动所启动的那个 函数 退出的时候 就会退出. ","date":"2000-01-01","objectID":"/golang-study-note-6/:3:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"channel Go语言的并发模型是CSP, 提倡通过 通信共享内存 而不是通过 共享内存 而实现通信。 goroutine 是Go程序的并发执行体, channel 就是它们之间的连接。channel 是可以 让一个 goroutine 发送特定值到另一个 goroutine 的通信机制。 Go语言中的 通信(channel) 是一种特殊的类型。通道像一个 传送带或者队列, 总是遵循先入先出(First in First out) 的规则, 保证收发数据的顺序。每一个通道都是一个具体类型的导管, 也就是声明 channel 的时候需要为其指定 元素类型。 声明channel func main() { // channl 的定义 // channel 是引用类型 (引用类型需要 初始化, 未初始化值 nil) // var 变量名(ch1) chan(变量类型channel) // int(channel传递数据的类型) var ch1 chan int fmt.Printf(\"未初始化 ch1 = %v\\n\", ch1) // 输出 未初始化 ch1 = \u003cnil\u003e // var 变量名(ch2) chan(变量类型channel) // string(channel传递数据的类型) // 9 为容量,channel 中可接收多少个数据 var ch2 chan string ch2 = make(chan string, 9) fmt.Printf(\"初始化 ch2 = %v\\n\", ch2) // 输出 初始化 ch2 = 0xc0000a2060 // 直接定义以及初始化 ch3 := make(chan int, 10) // 操作 channel , 发送, 接收, 关闭 // 发送与接收 使用符号 \u003c- ch3 \u003c- 10 // 将 10 发送到 ch3 中 // \u003c-ch3 //接收值,并直接丢弃接收的值 ret := \u003c-ch3 //接收值,并保存到ret变量中. fmt.Println(ret) // 输出 10 // 关闭管道 // 1. 关闭的通道可以继续取值,值为传递类型的零值 // 2. 关闭的通道不允许发送值,会直接 panic // 3. 重复关闭已关闭的通道,会直接 panic close(ch3) } 无缓存与有缓存通道 无缓冲的与有缓冲channel有着重大差别：一个是同步的 一个是非同步的 无缓冲的 就是一个送信人去你家门口送信 ，你不在家 他不走，你一定要接下信，他才会走。无缓冲保证信能到你手上 有缓冲的 就是一个送信人去你家仍到你家的信箱 转身就走 ，除非你的信箱满了 他必须等信箱空下来。有缓冲的 保证 信能进你家的邮箱 ch1 := make(chan int) //无缓冲 ch2 := make(chan int,1) //有缓冲 例子 // 有缓冲通道 与 无缓冲通道 func recv(ch chan bool) { ret := \u003c-ch //接收 channel 数据, 阻塞 fmt.Println(\"recv 函数 通道接收数据 :\", ret) } func main() { ch := make(chan bool, 1) ch \u003c- false go recv(ch) ch \u003c- true fmt.Println(\"Main 函数结束\") } 判断通道是否被关闭 通道取值的时候如果通道被关闭,仍然取值, 就会 panic, 所以可以使用 value, ok := chan 中的OK来判断, 或者使用 for range 来循环取值 // 通道 接收值 的时候判断通道是否关闭 func send(ch chan int) { for i := 0; i \u003c 10; i++ { ch \u003c- i } close(ch) } func main() { ch1 := make(chan int) go send(ch1) // for 循环 去通道不断的取值 // 判断 接收值 的时候判断通道是否关闭 // 方法一: 利用 value 与 ok 判断 for { ret, ok := \u003c-ch1 if !ok { break } fmt.Println(ret) } // 方法二: 利用for range 循环取值 (推荐) for ret := range ch1 { fmt.Println(ret) } } 生产者消费者模型 func produce(ch chan int) { for i := 0; i \u003c 10; i++ { ch \u003c- i fmt.Println(\"Send:\", i) } close(ch) } func consumer(ch chan int) { for v := range ch { fmt.Println(\"Receive:\", v) } } func main() { // 无缓冲区,send 一个,接受一个. ch := make(chan int) // 有缓冲区,send 10个,接收10个. //ch := make(chan int, 10) go produce(ch) go consumer(ch) time.Sleep(1 * time.Second) } select 多路复用 select 可以同时响应多个通道的操作。 // select 语法 select{ case ch1 \u003c- 1: // 多通道的操作 发送或者接收值 ... case ch1 \u003c- 2: // 多通道的操作 发送或者接收值 ... case \u003c-ch1: ... case \u003c-ch2: ... default: ... } 例子1: func f1(ch chan string) { for i := 0; i \u003c 10000; i++ { ch \u003c- fmt.Sprintf(\"f1 Send -\u003e %d\", i) } } func f2(ch chan string) { for i := 0; i \u003c 10000; i++ { ch \u003c- fmt.Sprintf(\"f2 Send -\u003e %d\", i) } } func main() { ch1 := make(chan string, 100) ch2 := make(chan string, 100) go f1(ch1) go f2(ch2) for { select { case ret := \u003c-ch1: fmt.Println(ret) case ret := \u003c-ch2: fmt.Println(ret) default: fmt.Println(\"Null\") time.Sleep(time.Second * 1) } } } 例子2: func main() { // 有缓冲区,容量为1 ch := make(chan int, 1) for i := 0; i \u003c 10; i++ { select { // 1,3,5,7,9 写不进去,因为ch中容量为1 case ch \u003c- i: //当有1个值,未取时,下个值写不进去 case ret := \u003c-ch: // 输出 0 2 4 6 8 fmt.Println(ret) } } } 单向通道 单向通道 1. 让代码更加的清晰 2. 防止误操作 // 函数参数中包含 chan\u003c- 表示只能 发送 func send(ch chan\u003c- int) { ch \u003c- 1 } // 函数参数中包含 \u003c-chan 表示只能 接收 func receive(ch \u003c-chan int) { \u003c-ch } ","date":"2000-01-01","objectID":"/golang-study-note-6/:4:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"控制与锁 多个 goroutine 操作同一组数据的时候,会出现数据竞争, 这时候我们就要加锁. goroutine 竞争 var x int64 var wg sync.WaitGroup func add() { for i := 0; i \u003c 5000; i++ { // 全局变量 x // 每循环一次 x + 1 x = x + 1 } wg.Done() } func main() { wg.Add(2) // 启动2个 goroutine 去执行 x+1 // 此时会出现数据竞争 go add() go add() wg.Wait() // 返回的结果每次都不同 fmt.Println(x) } 互斥锁 互斥锁 是一种常用的控制共享资源访问的方法, 它能够保证同一时间只能有一个 goroutine 可以访问共享资源. Go语言 使用 sync 包 Mutex 类型来实现 互斥锁. 互斥锁能保证同一时间只有一个 goroutine 进入临界区, 其他的 goroutine 则在等待锁, 当互斥锁释放后, 等待的 goroutine 才可以获取锁进入临界区, 多个 goroutine 同时等待一个锁时, 唤醒的策略是随机的. 例子1 var x int64 var wg sync.WaitGroup // 定义一个互斥锁 var lock sync.Mutex func add() { for i := 0; i \u003c 5000; i++ { // 加锁 lock.Lock() x = x + 1 // 解锁 lock.Unlock() } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 读写锁 读写锁是一种特殊的自旋锁，它把对共享资源对访问者划分成了读者和写者，读者只对共享资源进行访问，写者则是对共享资源进行写操作。 一个读写锁同时只能存在一个写锁但是可以存在多个读锁，但不能同时存在写锁和读锁。 如果读写锁当前没有读者，也没有写者，那么写者可以立刻获的读写锁，否则必须自旋，直到没有任何的写锁或者读锁存在。 如果读写锁没有写锁，那么读锁可以立马获取，否则必须等待写锁释放。 var ( x int64 wg sync.WaitGroup // 互斥锁 lock sync.Mutex // 读写锁 rwlock sync.RWMutex ) func read() { rwlock.RLock() // 加读锁 time.Sleep(time.Millisecond) rwlock.RUnlock() // 解除 读锁 wg.Done() } func write() { rwlock.Lock() // 加写锁 x = x + 1 time.Sleep(10 * time.Millisecond) rwlock.Unlock() // 解除 写锁 wg.Done() } func main() { start := time.Now() // 开始时间 // 执行 50次 写的操作 for i := 0; i \u003c 10; i++ { wg.Add(1) go write() } // 执行 1000 次的 读操作 for i := 0; i \u003c 1000; i++ { wg.Add(1) go read() } wg.Wait() end := time.Now() // 结束时间 fmt.Printf(\"消耗了 %v 毫秒\\n\", end.Sub(start)) } sync.Map Go语言中内置的Map不是并发安全的。 fatal error: concurrent map writes sync.Map 不需要(make)初始化,直接可以使用. sync.Map 可为 map 加锁保证 map 的安全, 而且 sync.Map 还内置了 Store , Load , LoadOrStore , Delete , Range 等方法. 例子1 (线程不安全的map) var m = make(map[string]int) var wg sync.WaitGroup func get(key string) int { return m[key] } func set(key string, value int) { m[key] = value } func main() { // 当 goroutine 超过5个,会报错 // fatal error: concurrent map writes for i := 0; i \u003c 20; i++ { wg.Add(1) go func(n int) { //将int类型转换成 string类型 key := strconv.Itoa(n) set(key, n) fmt.Printf(\"k = %v,v = %v\\n\", key, m[key]) wg.Done() }(i) } wg.Wait() } 例子2 var m = sync.Map{} var wg sync.WaitGroup func main() { for i := 0; i \u003c 20; i++ { wg.Add(1) go func(n int) { key := strconv.Itoa(n) // sync.Map 用 Store 方法来设置值 m.Store(key, n) value, _ := m.Load(key) fmt.Printf(\"k = %v, v = %v\\n\", key, value) wg.Done() }(i) } wg.Wait() } 原子操作 在代码中加锁的操作性能会下降. 针对基本数据类型我们可以用 原子操作 来确保并发安全 原子操作 是Go语言的方法, 它在用户态 的时候就可以完成, 性能比加锁操作更好. Go语言的 原子操作 作为内置的标准库 sync/atomic 模块. atomic 原子操作,只支持 Int, Uint 的数据操作. // atomic 原子操作 var ( x int64 l sync.Mutex //锁 wg sync.WaitGroup //等待组 ) // 累加函数 func add() { x = x + 1 wg.Done() } // 加锁的累加函数 func mutexAdd() { l.Lock() x = x + 1 l.Unlock() wg.Done() } func atomicAdd() { // 给整数 x + 1 atomic.AddInt64(\u0026x, 1) wg.Done() } func main() { start := time.Now() for i := 0; i \u003c 10000; i++ { wg.Add(1) // 普通的数据累加操作 // 线程不是安全的,耗时 5.383036ms //go add() // 加锁版数据累加操作 // 线程安全的,耗时 5.628079ms // go mutexAdd() // 原子操作版数据累加 // 线程安全, 耗时 5.263185ms go atomicAdd() } wg.Wait() end := time.Now() fmt.Println(x) fmt.Println(end.Sub(start)) } ","date":"2000-01-01","objectID":"/golang-study-note-6/:5:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-6/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-7/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-7/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"单元测试 单元测试是指 开发完一个模块 之后自己进行测试. 单元测试 很重要, TDD: 测试驱动开发. Go语言内置 testing 包进行单元测试, 所有以 *_test.go为后缀名称的文件, 都可通过 go test 命令, 自动执行测试函数, 不会被go build 编译. 类型 格式 作用 测试函数 函数名前缀为Test 测试程序的一些逻辑行为是否正确 基准函数 函数名前缀为Benchmark 测试函数的性能 示例函数 函数名前缀为Example 为文档提供示例文档 例子1 // sum.go //SubAdd 相加 func SubAdd(a, b int) (sub int) { sub = a + b return } // SubMin 相减 func SubMin(a, b int) (sub int) { sub = a - b return } // sum_test.go func TestSubAdd(t *testing.T) { a := 10 b := 20 c := a + b result := SubAdd(a, b) if result != c { t.Fatalf(\"期望得到: %v, 实际得到: %v\\n\", c, result) } } func TestSubMin(t *testing.T) { a := 10 b := 20 c := a - b result := SubMin(a, b) if result != c { t.Fatalf(\"期望得到: %v, 实际得到: %v\\n\", c, result) } } // 执行 go test -v /* 输出如下: === RUN TestSubAdd --- PASS: TestSubAdd (0.00s) === RUN TestSubMin --- PASS: TestSubMin (0.00s) PASS ok github.com/jicki/testing02 0.004s */ ","date":"2000-01-01","objectID":"/golang-study-note-7/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"测试组 将多个测试用例 在一个函数中完成, 就是 测试组 // sum.go // Sub 计算 func Sub(a, b int, char string) (ret int) { switch char { case \"+\": ret = a + b case \"-\": ret = a - b case \"*\": ret = a * b case \"/\": ret = a / b } return } // sum_test.go func TestGroup(t *testing.T) { // 定义一个 存放测试数据 的结构体 type test struct { a int b int c int char string } // 创建一个存放所有测试用例的 map var tests = map[string]test{ \"Add\": test{10, 20, 30, \"+\"}, \"Min\": test{10, 20, -10, \"-\"}, \"Multi\": test{10, 20, 200, \"*\"}, \"Step\": test{10, 20, 0, \"/\"}, } for name, tc := range tests { // 调用 t.Run 子测试方法 // 可使用 go test -run=Sub/Add -v 这种方式调用单个测试 t.Run(name, func(t *testing.T) { ret := Sub(tc.a, tc.b, tc.char) if ret != tc.c { t.Errorf(\"期望的结果为: %#v, 实际结果为: %#v\", tc.c, ret) } }) } } /* 运行 go test -v 输出: === RUN TestGroup === RUN TestGroup/Add === RUN TestGroup/Min === RUN TestGroup/Multi === RUN TestGroup/Step --- PASS: TestGroup (0.00s) --- PASS: TestGroup/Add (0.00s) --- PASS: TestGroup/Min (0.00s) --- PASS: TestGroup/Multi (0.00s) --- PASS: TestGroup/Step (0.00s) PASS ok github.com/jicki/testing03 0.005s */ ","date":"2000-01-01","objectID":"/golang-study-note-7/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"测试覆盖率 测试覆盖率是指 代码被测试套件覆盖的百分比, 通常使用的都是语句覆盖率, 测试中至少被运行一次的代码占总代码的比例. Go 语言中使用 go test -cover 来查看测试覆盖率 ","date":"2000-01-01","objectID":"/golang-study-note-7/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"基准测试 (Benchmark) 基准测试就是 性能测试的一种. 默认情况下每次最少执行 1s,如果不足 1s 会重复执行,b.N的值会按1,2,5,10,20,50..增加 例子 func BenchmarkSub(b *testing.B) { b.Log(\"基准测试 !\") // b.N 是内置方法 for i := 0; i \u003c b.N; i++ { Sub(10, 20, \"*\") } } /* 执行 go test -bech=Sub (正则匹配名称) 输出: goos: darwin goarch: amd64 pkg: github.com/jicki/testing03 // 8 是指 8核(可用-cpu 指定) 300000000 是指 执行次数 // 4.16 ns/op 是指每次运行消耗的时间(ms) // 如果想查看具体的内存信息,可使用 -benchmem 参数查看运行 BenchmarkSub-8 300000000 4.17 ns/op --- BENCH: BenchmarkSub-8 sum_test.go:32: 基准测试 ! sum_test.go:32: 基准测试 ! sum_test.go:32: 基准测试 ! sum_test.go:32: 基准测试 ! sum_test.go:32: 基准测试 ! sum_test.go:32: 基准测试 ! PASS ok github.com/jicki/testing03 1.688s */ 性能比较函数 在基准测试的前提下,可以对测试进行比较 例子 //fib.go //Fib 斐波那契数列 func Fib(n int) int { if n \u003c 2 { return n } // 递归 return Fib(n-1) + Fib(n-2) } // 基准测试 func BenchmarkFib(b *testing.B) { for i := 0; i \u003c b.N; i++ { Fib(2) } } // 基准测试 - 性能比较函数 - (内部调用函数) func benchmarkFib(b *testing.B, n int) { for i := 0; i \u003c b.N; i++ { Fib(n) } } // 调用上面的内部函数 benchmarkFib n=2 func BenchmarkFib2(b *testing.B) { for i := 0; i \u003c b.N; i++ { benchmarkFib(b, 2) } } // 调用上面的内部函数 benchmarkFib n=20 func BenchmarkFib20(b *testing.B) { for i := 0; i \u003c b.N; i++ { benchmarkFib(b, 20) } } /* 执行 go test -bench=. (.表示执行当前的所有测试函数) 输出: goos: darwin goarch: amd64 pkg: github.com/jicki/testing04 BenchmarkFib-8 300000000 5.10 ns/op BenchmarkFib2-8 30000 157001 ns/op BenchmarkFib20-8 300 13065579 ns/op PASS ok github.com/jicki/testing04 11.643s */ 重置时间 函数内某些操作是不需要计算在函数性能内的,比如连接数据库,连接外部api等. 函数内可使用 b.ResetTimer() 进行重置时间. func BenchmarkJob(b *testing.B) { //1. 连接数据库 //2. 调用Api b.ResetTimer() //重置时间 // 测试的函数内容 for i := 0; i \u003c b.N; i++{ Job() } } 并行测试 开启并行测试，需要执行 b.RunParallel(func(pb *testing.PB) 方法，默认会以逻辑CPU个数来进行并行测试. 例子 // sub.go // SubAdd 加法 func SubAdd(a, b int) (sub int) { return a + b } // sub_test.go // 基准测试 func BenchmarkSub(b *testing.B) { for i := 0; i \u003c b.N; i++ { SubAdd(10, 20) } } // 并行测试 //函数名 约定包含 Parallel 表示 并行测试 //当函数包含很多 groutine 时 使用并行测试 func BenchmarkSubParallel(b *testing.B) { // 并行测试的函数 方法 b.RunParallel(func(pb *testing.PB) { for pb.Next() { SubAdd(10, 20) } }) } /* 执行 go test -bench=. 输出: goos: darwin goarch: amd64 pkg: github.com/jicki/testing05 BenchmarkSub-8 2000000000 0.57 ns/op BenchmarkSubParallel-8 2000000000 0.24 ns/op PASS ok github.com/jicki/testing05 1.706s */ ","date":"2000-01-01","objectID":"/golang-study-note-7/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"Setup与TearDown Setup 意指 在测试之前需要进行设置,如连接数据库等. TearDown 意指 在测试之后需要进行卸载 TestMain 在测试文件中调用 func TestMain(m *testing.M) 那么程序在生成测试之前会先调用 TestMain(m),然后在执行具体测试. TestMain 运行在 goroutine 中, 调用 m.Run 前后做 setup和teardown, 退出测试时使用 m.Run 的返回值做为参数调用os.Exit. 例子: // TestMain func TestMain(m *testing.M) { fmt.Println(\"测试之前的操作(Setup)\") //m.Run() 执行以上的测试,返回值为测试结果 ret := m.Run() fmt.Println(\"测试结束后执行的操作(teardown)\") // 调用系统退出,把测试结果传进去 os.Exit(ret) } /* 执行 go test -bench=. 输出: 测试之前的操作(Setup) goos: darwin goarch: amd64 pkg: github.com/jicki/testing06 BenchmarkSub-8 2000000000 0.57 ns/op BenchmarkSubParallel-8 2000000000 0.23 ns/op PASS 测试结束后执行的操作(teardown) ok github.com/jicki/testing06 1.689s */ 子测试(Setup/Teardown) 例子: func Sub(a, b int, char string) (ret int) { switch char { case \"+\": ret = a + b case \"-\": ret = a - b case \"*\": ret = a * b case \"/\": ret = a / b } return } // sum_test.go func TestGroup(t *testing.T) { // 定义一个 存放测试数据 的结构体 type test struct { a int b int c int char string } // 创建一个存放所有测试用例的 map var tests = map[string]test{ \"Add\": test{10, 20, 30, \"+\"}, \"Min\": test{10, 20, -10, \"-\"}, \"Multi\": test{10, 20, 200, \"*\"}, \"Step\": test{10, 20, 0, \"/\"}, } for name, tc := range tests { t.Run(name, func(t *testing.T) { // 在t.Run 这个子测试中,加入 Setup 与 Teardown t.Log(\"t.Run 子测试之前执行 Setup\") defer func() { t.Log(\"t.Run 子测试之后执行 Teardown\") }() ret := Sub(tc.a, tc.b, tc.char) t.Log(\"子测试实际执行过程!!\") if ret != tc.c { t.Errorf(\"期望的结果为: %#v, 实际结果为: %#v\", tc.c, ret) } }) } } /* 输出: === RUN TestGroup === RUN TestGroup/Multi === RUN TestGroup/Step === RUN TestGroup/Add === RUN TestGroup/Min --- PASS: TestGroup (0.00s) --- PASS: TestGroup/Multi (0.00s) sum_test.go:24: t.Run 子测试之前执行 Setup sum_test.go:29: 子测试实际执行过程!! sum_test.go:26: t.Run 子测试之后执行 Teardown --- PASS: TestGroup/Step (0.00s) sum_test.go:24: t.Run 子测试之前执行 Setup sum_test.go:29: 子测试实际执行过程!! sum_test.go:26: t.Run 子测试之后执行 Teardown --- PASS: TestGroup/Add (0.00s) sum_test.go:24: t.Run 子测试之前执行 Setup sum_test.go:29: 子测试实际执行过程!! sum_test.go:26: t.Run 子测试之后执行 Teardown --- PASS: TestGroup/Min (0.00s) sum_test.go:24: t.Run 子测试之前执行 Setup sum_test.go:29: 子测试实际执行过程!! sum_test.go:26: t.Run 子测试之后执行 Teardown PASS ok github.com/jicki/testing07 0.005s */ ","date":"2000-01-01","objectID":"/golang-study-note-7/:1:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"示例函数 示例函数 以 Example 为前缀. 它既没有参数也没有返回值. // sub.go // SubAdd 加法 func SubAdd(a, b int) (sub int) { return a + b } // sub_test.go // 示例函数 // 必须写注释 OutPut: 然后加上结果 func ExampleSubAdd() { fmt.Println(SubAdd(10, 20)) // OutPut: 30 } /* 输出: === RUN ExampleSubAdd --- PASS: ExampleSubAdd (0.00s) PASS ok github.com/jicki/testing08 0.005s */ ","date":"2000-01-01","objectID":"/golang-study-note-7/:1:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"Go网络编程 ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"OSI七层模型 互联网协议 # OSI 七层协议 --\u003e [应用层] [应用层] --\u003e [应用层] --\u003e [表示层] --\u003e [会话层] [传输层] --\u003e [传输层] --\u003e [传输层] [网络层] --\u003e [网络层] --\u003e [网络层] --\u003e [数据链路层] --\u003e [数据链路层] [网络接口层] --\u003e [物理层] --\u003e [物理层] ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"TCP/IP 协议 TCP协议 TCP(Transmission Control Protocol) 既传输控制协议/网络协议, 是一种面向连接(连接导向)的、可靠的、基于字节流的传输(Transport layer) 通信协议，因为是面向连接的协议，数据像水流一样传输，所以会存在黏包问题。 IP协议 IP(Internet Protocol) 因特网协议是为计算机网络相互连接进行通信而设计的协议。在因特网中，它规定了计算机在因特网中进行通信应该遵守的规则。 IP协议中还有一个非常重要的内容，给因特网上的每台计算机和设备都规定了一种地址，叫做\"IP地址”。 ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"Socket Socket 是 BSD UNIX 的进程通讯机制, 通常也称为 “套接字” , 用于描述IP地址和端口，是一个通讯链的句柄. Socket 可以理解为 TCP/IP 网络的API，它定义了很多函数或例程，程序员可以用它们来开发TCP/IP网络上的应用程序. Socket 是应用层与 TCP/IP 协议族通信的中间软件抽象层,在设计模式中，Socket 其实就是一个门面模式,它把复杂的 TCP/IP 协议族隐藏在 Socket 后面,对用户来说只需要调用 Socket 规定的相关函数，让 Socket 去组织符合指定的协议数据然后进行通信。 # sokcet 抽象层 位于 应用层 与 传输层之间 [应用层] | [Socket 抽象层] | [传输层] | [网络层] | [数据链路层] | [物理层] ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"TCP服务端/客户端 服务端 Go语言可通过 net 包实现 TCP服务端 server // 处理连接的函数 func process(conn net.Conn) { //从连接中接收数据 // 关闭客户端连接 defer conn.Close() // 创建一个 切片 buf := make([]byte, 1024) // n 表示读了多少数据(byte) n, err := conn.Read(buf) if err != nil { fmt.Println(\"接收客户端数据失败,err:\", err) } // buf 为字节切片，需要转换为 string fmt.Println(\"客户端发来的消息: \", string(buf[:n])) } func main() { listener, err := net.Listen(\"tcp\", \"127.0.0.1:8888\") if err != nil { fmt.Println(\"Listen 失败!\", err) return } fmt.Println(\"Listener: \", listener.Addr()) // 关闭 服务端连接 释放服务端绑定端口 defer listener.Close() //创建一个 for 循环，一直接收信息 for { // 如果没有连接，会一直等待客户端连接. conn, err := listener.Accept() if err != nil { fmt.Println(\"客户端连接失败,err:\", err) continue } // 客户端连接成功 使用 goroutine 创建 连接 go process(conn) } } Client //tcp 客户端 func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:8888\") if err != nil { fmt.Println(\"服务端连接失败,err:\", err) return } // 关闭连接 defer conn.Close() // 使用 bufio 获取 终端输入(os.Stdin) for { reader := bufio.NewReader(os.Stdin) // 读取终端输入的内容,以换行(\\n)为结束 input, err := reader.ReadString('\\n') if err != nil { fmt.Println(\"获取终端输入失败 err:\", err) return } data := []byte(input) _, err = conn.Write(data) if err != nil { fmt.Println(\"发送消息失败 err:\", err) return } } } ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:4","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"TCP黏包问题 Server func process(conn net.Conn) { defer conn.Close() reader := bufio.NewReader(conn) for { // 调用模块proto.Decode 函数 msg, err := proto.Decode(reader) if err == io.EOF { return } if err != nil { fmt.Println(\"消息解码失败 err: \", err) } fmt.Println(\"收到客户端发送的消息: \", msg) } } func main() { listen, err := net.Listen(\"tcp\", \"127.0.0.1:8888\") if err != nil { fmt.Println(\"启动服务端失败 err: \", err) return } for { conn, err := listen.Accept() if err != nil { fmt.Println(\"客户端连接失败, err\", err) continue } go process(conn) } } Client func main() { conn, err := net.Dial(\"tcp\", \"127.0.0.1:8888\") if err != nil { fmt.Println(\"连接服务端失败, err \", err) return } defer conn.Close() for i := 0; i \u003c 20; i++ { msg := \"Hello How are you ?\" // 调用模块 proto.Encode pkg, err := proto.Encode(msg) if err != nil { fmt.Println(\"Encode Error:\", err) return } conn.Write(pkg) } } 模块 //Encode 消息编码 func Encode(message string) ([]byte, error) { // length 将消息长度,转换成int32类型(int32 占4字节) var length = int32(len(message)) // 创建一个 字节类型的 缓冲区 var pkg = new(bytes.Buffer) // 写入信息头 pkg // binary.LittleEndian 内存 小端. // 小端: 内存读写的顺序,按照从左到右的顺序是小端. // 大端: 内存读写的顺序,按照从右到左的顺序是大端. if err := binary.Write(pkg, binary.LittleEndian, length); err != nil { return nil, err } // 写入消息内容 if err := binary.Write(pkg, binary.LittleEndian, []byte(message)); err != nil { return nil, err } return pkg.Bytes(), nil } // Decode 解码消息 func Decode(reader *bufio.Reader) (string, error) { // 读取消息头 // 读取消息前4个字节 lengthByte, _ := reader.Peek(4) // 缓冲区的内容 lengthBuff := bytes.NewBuffer(lengthByte) var length int32 // 读取消息 err := binary.Read(lengthBuff, binary.LittleEndian, \u0026length) if err != nil { return \"\", err } // 返回(缓冲区内)消息头中记录真正消息的(大小)字节数 if int32(reader.Buffered()) \u003c length+4 { return \"\", err } // 读取真正的消息内容 // 创建还有个切片,用于存放消息内容, 长度为头部4+length pack := make([]byte, int(4+length)) _, err = reader.Read(pack) if err != nil { return \"\", err } // 返回切数据中从第四位开始后面的数据 return string(pack[4:]), nil } ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:5","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"UDP服务端/客户端 UDP协议(User Datagram Protocol) 用户数据报协议, 是 OSI 参考模型中一种 无连接 的传输层协议, 不需要建立连接就可以直接进行数据发送和接收, 属于不可靠的、无时序的通信, UDP协议的实时性比较好,通常用于视频直播相关领域. Server服务端 func main() { listen, err := net.ListenUDP(\"udp\", \u0026net.UDPAddr{ IP: net.ParseIP(\"127.0.0.1\"), Port: 9999, }) if err != nil { fmt.Println(\"Listen 失败! err:\", err) return } defer listen.Close() // 循环收发数据 data := make([]byte, 1024) for { n, addr, err := listen.ReadFromUDP(data) if err != nil { fmt.Println(\"接收消息失败! err :\", err) return } fmt.Printf(\"接收到来自 %v 的消息 %v\\n\", addr, string(data[:n])) // 回复消息 n, err = listen.WriteToUDP([]byte(\"收到收到!\"), addr) if err != nil { fmt.Println(\"回复消息失败! err: \", err) return } } } Client客户端 //udp client func main() { conn, err := net.Dial(\"udp\", \"127.0.0.1:9999\") if err != nil { fmt.Println(\"连接服务端失败! err: \", err) } defer conn.Close() // 发送消息 n, err := conn.Write([]byte(\"哈喽\")) if err != nil { fmt.Println(\"发送消息失败 err: \", err) return } // 接收消息 buf := make([]byte, 1024) n, err = conn.Read(buf) if err != nil { fmt.Println(\"接收信息失败 err: \", err) return } fmt.Println(\"收到信息 :\", string(buf[:n])) } ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:6","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"Http HTTP 超文本传输协议(HTTP , HyperText Transfer Protocol) 是互联网上应用最为广泛的一种网络传输协议, 所有的 WWW 文件都必须遵守这个标准. 最初设计HTTP是为了提供一种发布和接收HTML页面的方法. HTTP数据传输图解 [HTTP数据] [应用层] - HTTP、FTP、SMTP | [TCP部首(HTTP数据)] [传输层] - TCP、UDP | [IP首部(TCP部首)(HTTP数据)] [网络层] - IP、ARP、路由器 | [以太网部首(IP部首)(TCP首部)(HTTP数据)] [数据链路层] - 以太网、网桥 物理层 模拟httpClient func main() { conn, err := net.Dial(\"tcp\", \"baidu.com:80\") if err != nil { fmt.Println(\"打开网站失败,err: \", err) } defer conn.Close() // 发送数据到网站 //fmt.Fprintf(conn, \"GET / HTTP/1.0\\r\\n\\r\\n\") conn.Write([]byte(\"GET / HTTP/1.0\\r\\n\\r\\n\")) //接收数据 buf := make([]byte, 1024) for { n, err := conn.Read(buf) if err == io.EOF { fmt.Printf(string(buf[:n])) return } if err != nil { fmt.Println(\"接收数据失败 err: \", err) return } fmt.Printf(string(buf[:n])) } } HttpServer 使用Go语言中的 http 包 func process(w http.ResponseWriter, r *http.Request) { fmt.Fprintln(w, \"\u003ch1\u003eMy Http Server!\u003c/h1\u003e\") } // http Server func main() { // 注册路由 http.HandleFunc(\"/\", process) // 创建连接 err := http.ListenAndServe(\":9999\", nil) if err != nil { fmt.Println(\"Listen Error:\", err) return } } ","date":"2000-01-01","objectID":"/golang-study-note-7/:2:7","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-7/"},{"categories":["golang"],"content":"Go学习笔记","date":"2000-01-01","objectID":"/golang-study-note-8/","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-8/"},{"categories":["golang"],"content":"Go语言基础 ","date":"2000-01-01","objectID":"/golang-study-note-8/:0:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-8/"},{"categories":["golang"],"content":"HTML 基础 超文本标记语言(Hypertext Markup Language, Html) 是一种用于创建网页的标记语言 本质上是浏览器可识别的规则,我们按照规则写网页,浏览器根据规则渲染我们的网页.对于不同的浏览器,对同一个标签可能会有不同的解析.(既兼容性) 网页文件的后缀(扩展名): html 或 htm ","date":"2000-01-01","objectID":"/golang-study-note-8/:1:0","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-8/"},{"categories":["golang"],"content":"Web本质 C/S 架构 C/S 架构 -\u003e 软件开发 优势: 可定制化高,用户体验好. 劣势: 开发成本高,适配不同的平台,有新功能需要客户端升级. B/S 架构 B/S 架构 -\u003e Web开发 优势: 开发成本低. 劣势: 复杂功能没办法很好的实现. ","date":"2000-01-01","objectID":"/golang-study-note-8/:1:1","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-8/"},{"categories":["golang"],"content":"HTML 文档结构 html基础结构 \u003c!DOCTYPE html\u003e \u003chtml lang=\"zh-CN\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e网页标题\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003ch1\u003e用户具体看到的内容\u003c/h1\u003e \u003c/body\u003e \u003c/html\u003e \u003c!DOCTYPE html\u003e 声明为 html5 文档,必须在HTML 文档的第一行,位于\u003chtml\u003e标签之前. \u003chtml\u003e\u003c/html\u003e 是文档的开始标记和结束标记,是HTML页面的根元素, 在它们之间是文档的 头部\u003chead\u003e 和 主体\u003cbody\u003e . \u003chead\u003e\u003c/head\u003e 定义了HTML文档的开头部分. 它们之间的内容不会在浏览器窗口中显示.包含了文档的元素meta 数据. \u003ctitle\u003e\u003c/title\u003e 定义了网页标题,在浏览器的标题栏中显示. \u003cbody\u003e\u003c/body\u003e 之间的文本是具体的WEB内容,在网页的主体中显示. \u003cmeta charset=\"UTF-8\"\u003e 定义了网页的编码,在中文网页中不定义有可能会出现乱码. HTML标签格式 HTML标签是由 尖括号\u003c\u003e 包裹的关键字,如 \u003chtml\u003e, \u003cdiv\u003e 等. HTML标签通常都是 成对的出现, 比如 \u003cdiv\u003e \u003c/div\u003e 第一个标签开始,第二个为标签结束,结束的标签语会带 斜杠/ . 有一部分标签是单独的, 比如 \u003cbr/\u003e , \u003chr/\u003e , \u003cimg src=\"1.jpg\" /\u003e 等. 标签里可以附带 若干个属性参数, 也可以不带属性. HTML标签语法 成对标签: \u003c标签名 A属性=\"A属性值1;A属性值2” B属性=\"B属性值1;B属性值2;B属性值3”…\u003e标签内容\u003c/标签名\u003e 单标签: \u003c标签名 属性1=\"属性值1” 属性2=\"属性值2”…/\u003e HTML重要属性 id: 定义标签的唯一ID, HTML标签文档数中唯一 class: HTML元素定义一个或多个类名 (classname) CSS样式类名 style: 规定元素的行内样式(CSS样式) HTML常用标签 标签 意义 \u003ctitle\u003e \u003c/title\u003e 定义网页标题 \u003cstyle\u003e \u003c/style\u003e 定义内部样式表 \u003cscrpit\u003e \u003c/scrpit\u003e 定义JS代码或引入外部JS文件 \u003clink/\u003e 引入外部样式表文件 \u003cmeta/\u003e 定义网页原信息 body内常用标签 标签 意义 \u003cb\u003e \u003c/b\u003e 加粗字体 \u003ci\u003e \u003c/i\u003e 斜体 \u003cu\u003e \u003c/u\u003e 下划线 \u003cs\u003e \u003c/s\u003e 删除线 \u003cp\u003e \u003c/p\u003e 段落标签 \u003ch1\u003e \u003c/h1\u003e 标题1 \u003ch2\u003e \u003c/h2\u003e 标题2 \u003ch3\u003e \u003c/h3\u003e 标题3 \u003ch4\u003e \u003c/h4\u003e 标题4 \u003ch5\u003e \u003c/h5\u003e 标题5 \u003ch6\u003e \u003c/h6\u003e 标题6 \u003cbr\u003e 换行 \u003chr\u003e 水平线 \u003c!-- --\u003e 注释 特殊字符 标签 意义 \u0026nbsp; 空格 \u0026gt; \u003e \u0026lt; \u003c \u0026amp; \u0026 \u0026yen; ¥ \u0026copy; 版权 \u0026reg; 注册 div标签 与 span标签 \u003cdiv\u003e 标签用来定义一个块级元素,并无实际的意义.主要通过CSS样式为其赋予不同的表现. \u003cspan\u003e 标签用来定义内联元素,并无实际意义.主要通过CSS样式为其赋予不同的表现. 块级元素 与 内联元素 的区别: 所谓块级元素, 是以另起一行开始渲染的元素, 内联元素则不需要另起一行.如果单独在网页中插入这两个元素,不会对页面产生任何的影响. 在标签嵌套中,通常块元素可以包含内联元素或某些块级元素 , 但是内联元素不能包含块级元素,它只能包含其它内联元素. \u003cp\u003e标签 不能包含 块级元素, \u003cp\u003e 标签内 也不能包含 \u003cp\u003e 标签. img标签 \u003cimg src=\"图片路径\" alt=\"图片未加载成功时的提示\" title=\"鼠标悬浮时提示信息\" width=\"宽\" height=\"高(宽高属性只设置一个会等比例缩放)\"\u003e a标签 超链接标签 所谓的超链接是指从一个网页指向一个目标的连接关系, 这个目标可以是另一个网页, 也可以是相同网页上的不同位置, 还可以是一个图片, 一个电子邮件地址, 一个文件, 甚至是一个应用程序. \u003ca href=\"https://baidu.com\" target=\"_blank\"\u003e显示的内容\u003c/a\u003e href 的几种属性: 绝对URL - 指向另一个站点(href=http://baidu.com) 相对URL - 指向当前站点内的路径 (href=\"index.html”) 锚URL - 指向页面中的锚(href=”#top”) target 属性: _blank 表示在浏览器打开新的标签显示网页. _self 表示在浏览器当前的标签中显示网页. 列表 \u003c!-- 无序列表: type属性: disc - 实心原点(默认) circle - 空心圆圈 square - 实心方块 none - 无样式 --\u003e \u003cul type=\"square\"\u003e \u003cli\u003e无序列表1\u003c/li\u003e \u003cli\u003e无序列表2\u003c/li\u003e \u003cli\u003e无序列表3\u003c/li\u003e \u003cli\u003e无序列表4\u003c/li\u003e \u003cli\u003e无序列表5\u003c/li\u003e \u003c/ul\u003e \u003c!-- 有序列表: type属性: 1 数字列表(默认) A 大写字母 a 小写字母 I 大写罗马数字 i 小写罗马数字 start属性: 从哪一个为开始,如 start=\"2\" --\u003e \u003col type=\"1\" start=\"2\"\u003e \u003cli\u003e有序列表1\u003c/li\u003e \u003cli\u003e有序列表2\u003c/li\u003e \u003cli\u003e有序列表3\u003c/li\u003e \u003cli\u003e有序列表4\u003c/li\u003e \u003cli\u003e有序列表5\u003c/li\u003e \u003c/ol\u003e \u003c!-- 标题列表: --\u003e \u003cdl\u003e \u003cdt\u003e标题列表1\u003c/dt\u003e \u003cdd\u003e标题列表1内容\u003c/dd\u003e \u003cdt\u003e标题列表2\u003c/dt\u003e \u003cdd\u003e标题列表2内容1\u003c/dd\u003e \u003cdd\u003e标题列表2内容2\u003c/dd\u003e \u003cdd\u003e标题列表2内容3\u003c/dd\u003e \u003c/dl\u003e 表格 表格是一个二维数据空间, 一个表格由若干行组成,一个行又有若干单元格组成,单元格里可以包含文件、列表、团案、表单、数字符号、预置文本和其它的表格等内容. 表格最重要的目的是显示表格类数据.表格类数据是指最适合组织为表格格式(既按行和列组织)的数据. \u003c!--表格的基本结构--\u003e \u003ctable border=\"2\"\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003e属性\u003c/th\u003e \u003cth\u003e意义\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003eborder\u003c/td\u003e \u003ctd\u003e表格边框\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003ecellpadding\u003c/td\u003e \u003ctd\u003e内边框\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003ecellspacing\u003c/td\u003e \u003ctd\u003e外边框\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003ewidth\u003c/td\u003e \u003ctd\u003e像素百分比\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003erowspan\u003c/td\u003e \u003ctd\u003e单元格横跨多少行\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003ecolspan\u003c/td\u003e \u003ctd\u003e单元格横跨多少列(合并单元格)\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e 属性 意义 border 表格边框 cellpadding 内边框 cellspacing 外边框 width 像素百分比 rowspan 单元格横跨多少行 colspan 单元格横跨多少列(合并单元格) form标签 表单 用于向服务器传输数据,从而实现用户与WEB服务器的交互. 表单 能够包含input系列标签, 比如 文本字段、复选框、提交按钮 等. 表单 可以包含 textarea、select、fieldset 和 label 标签. 想要在HTML里面通过点击 form 表单的 sumbit 按钮提交数据: 所有获取用户输入的标签必须放在 form 标签内. 所有获取用户输入的标签必须要有 name 属性. 必须要有 sumbit 按钮并且 form 表单要有 action 属性. 表单属性: 属性 描述 accept-charset 规定在被提交表单中使用的字符集(默认: 页面字符集) action 规定向何处提交表单的地址(URL)提交页面. autocomplete 规定浏览器应该自动完成表单(默认开启) enctype 规定被提交数据的编码(默认: url-encoded) method 规定在提交表单时所用的HTTP方法(默认: GET) name 规定识别表单的方法(对于 DOM 使用: document.forms.name) novalidate 规定浏览器不验证表单 target 规定 action 属性中地址的目标(默认: self) 表单元素 基本概念: HTML表单是HTML元素中较为复杂的部分,表单往往和脚本、动态页面、数据处理等功能相结合,因此它是制作动态网站很重要的内容. 表单一般用来收集用户的输入信息 表单工作原理: 访问者在浏览有表单的网页时,可填写必须的信息,然后按某个按钮提交.这些信息通过 Internet 传送到服务器上. 服务器上专门的程序对这些数据进行处理,如果有错误会返回错误信息,并要求纠正错误.当数据完整无误后,服务器反馈一个输入完成的信息. method: GET与POST方法的场景 GET: 获取页面 , 搜索引擎检索. POST: 提交form表单时, 有敏感数据时. input标签 \u003cinput\u003e 元素会根据不同的 type 属性, 变化","date":"2000-01-01","objectID":"/golang-study-note-8/:1:2","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-8/"},{"categories":["golang"],"content":"template 语法 模板渲染 本质就是一种字符串替换,一种高级的字符串替换. 模拟模板 func index(w http.ResponseWriter, r *http.Request) { w.Write([]byte(\"欢迎光临index\")) } func info(w http.ResponseWriter, r *http.Request) { data, err := ioutil.ReadFile(\"./info.html\") if err != nil { fmt.Println(\"open file err: \", err) return } // 设置一个随机数,模拟不同访问s num := rand.Intn(10) // 先转换成字符串 datastr := string(data) if num \u003e 5 { // 字符的替换 //{info} 是html中需要替换的变量 //\u003cli\u003e\u003c/li\u003e 是替换后写入 html 中的标签数据 //1 表示 替换次数 datastr = strings.Replace(datastr, \"{info}\", \"\u003cli\u003e《Golang》\u003c/li\u003e \\n \u003cli\u003e《Linux》\u003c/li\u003e \", 1) } else { datastr = strings.Replace(datastr, \"{info}\", \"\u003cli\u003e《三体》\u003c/li\u003e \\n \u003cli\u003e《大灰狼》\u003c/li\u003e \", 1) } w.Write([]byte(datastr)) } func main() { http.HandleFunc(\"/index\", index) http.HandleFunc(\"/info\", info) http.ListenAndServe(\"127.0.0.1:8888\", nil) } \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"ie=edge\"\u003e \u003ctitle\u003e个人中心\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cdiv\u003e \u003cul\u003e {info} \u003c/ul\u003e \u003c/div\u003e \u003c/body\u003e \u003c/html\u003e http/template 库 {% raw %} 模板语法: 所有模板语法都必须包含在 {{ }} 中间. {{.}} 中的 . 表示当前对象. 当我们传入一个结构体对象的时候,可以根据.来访问结构体对应的字段,如 {{.Name}} {{/* Go模板中的注释 */}} 执行时会忽略.可以多行,但是不能嵌套,需要紧贴分界符始止. pipeline: pipeline 是指产生数据的操作. 比如{{.}}、{{.Name}}等. Go的模板语法中支持使用管道符 | 链接多个命令,用法和Linux下的管道类似,将|前面命令运行的结果(返回值)传递给后面的命令. 注意: Go的模板语法中, pipeline 概念是传递数据,只要产生数据的都称为 pipeline. 变量: Action 里可以初始化一个变量来捕获管道的执行结果. 初始化语法: $variable := pipeline 其中 $variable 是变量名称. 声明变量的action不会产生任何输出. \u003cdiv\u003e {{/* 这里是注释 */}} {{ $age := .Age }} \u003ch1\u003e{{ $age }}\u003c/h1\u003e {{ $id := . }} \u003ch1\u003e{{ $id.ID }}\u003c/h1\u003e \u003c/div\u003e 条件判断 条件判断 条件判断 必须要以 {{end}} 来结束. {{if 条件判断 arg1 arg2}} 输出 {{end}} {{if 条件判断 arg1 arg2}} 输出 {{else}} 输出 {{end}} {{if 条件判断 arg1 arg2}} 输出 {{else if 条件判断 arg3 arg4}} 输出 {{end}} 比较函数 比较函数公式 布尔函数会将任何类型的 零值 视为 假, 其余视为 真 eq: 如果 arg1 == arg2 返回 真 ne: 如果 arg1 != arg2 返回 真 lt: 如果 arg1 \u003c arg2 返回 真 le: 如果 arg1 \u003c= arg2 返回 真 gt: 如果 arg1 \u003e arg2 返回 真 ge: 如果 arg1 \u003e= arg2 返回 真 为了简化 多参数 相等检测, eq 可接受2个或更多个参数, 会将第一个参数 分别于其他参数进行比较 {{eq arg1 arg2 arg3}} 这里既 arg1 分别于 arg2 arg3 分别比较. 注: 比较函数只适用于 基础类型 (或重定义的基本类型, 如: type Celsius float32). 例子: \u003cdiv\u003e {{/* 条件判断 */}} {{if gt .Age 20}} \u003c/div\u003e \u003cdiv\u003e \u003ch1\u003e大于20岁\u003c/h1\u003e \u003c/div\u003e {{else}} \u003cdiv\u003e \u003ch1\u003e小于20岁\u003c/h1\u003e \u003c/div\u003e {{end}} range循环 Go的模板语法中使用 range 关键字进行循环遍历,其中 pipeline 的值必须是数组、切片、字典或者通道. {{range $key, $value := .}} 取值可以直接取,也可以加 . 里面的值 例子: {{/* range 循环遍历 以下遍历Map */}} {{/* map[int]struct */}} \u003chr\u003e \u003ctable border=\"2\"\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003e序号\u003c/th\u003e \u003cth\u003eID\u003c/th\u003e \u003cth\u003e姓名\u003c/th\u003e \u003cth\u003e年龄\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e {{range $index, $user := .}} \u003ctr\u003e \u003ctd\u003e{{$index}}\u003c/td\u003e \u003ctd\u003e{{$user.ID}}\u003c/td\u003e \u003ctd\u003e{{$user.UserName}}\u003c/td\u003e \u003ctd\u003e{{$user.Age}}\u003c/td\u003e \u003c/tr\u003e {{end}} \u003c/tbody\u003e \u003c/table\u003e with(局部变量) with语句: 其含义就是创建一个封闭的作用域, 在其范围内, 可以使用.action, 而与外面的.无关，只与with的参数有关; {{ with arg }} 此时的点 . 就是arg {{ end }} 预定义函数 执行模板时, 函数从两个函数字典中查找: 首先是模板函数字典, 然后是全局函数字典. 一般不在模板内定函数,而是使用Funcs方法添加函数到模板里. 预定义的全局函数如下: and: 函数返回它的第一个 empty 参数或者最后一个参数; and x y等价于if x then y else x 所有参数都会执行. or: 函数返回它的第一个非 empty 参数或者最后一个参数; or x y等价于if x then x else y 所有参数都会执行. not: 返回它的单个参数的布尔值是否定. len: 返回它的参数的整数类型长度. index: 执行结果为第一个参数以剩下的参数为索引/键指向的值.如:index x 1 2 3 返回 x[1][2][3] 的值.每个被索引的主题必须是数组、切片、字典. print: 既 fmt.Sprint printf: 既 fmt.Sprintf println: 既 fmt.Sprintln html: 返回其参数文本表示的 html 逸码等价表示. urlquery: 返回其参数文本表示可嵌入URL查询的逸码等价表示. js: 返回其参数文本表示的 JavaScrpit 逸码等价表示. call: 执行结果是调用第一个参数的返回值,该参数必须是函数类型,其余参数作为调用该函数的参数;如: call .x .y 1 2 等价于Go语言里的 dot.x.y(1,2); 其中 y 是函数类型的字段或者字典的值,或者其他类似情况; call 的第一个参数的执行结果必须是函数类型的值(与预定函数print明显不同); 该函数类型值必须有1到2个返回值,如果有2个 则后一个必须是error接口类型;如果有2个返回值的方法返回error非nil,模板执行会中断并返回给调用模板执行者该错误; //构建一个 map userMap := map[int]user{ 1: {1, \"张三\", 20}, 2: {2, \"李四\", 10}, 3: {3, \"王五\", 30}, } t.Execute(w, userMap) } \u003cdiv\u003e {{/* 传入 . = userMap */}} {{/* 预定函数 */}} \u003cp\u003eMap长度: {{len .}}\u003c/p\u003e {{/* with 与 printf */}} \u003cp\u003e{{with index . 1}}\u003c/p\u003e {{printf \"ID: %d 姓名: %s 年龄: %d\" .ID .UserName .Age}} {{end}} \u003c/div\u003e 自定义模板函数 为模板添加一个自定义的 函数. 例子: type user struct { ID int UserName string Age int } func info(w http.ResponseWriter, r *htt","date":"2000-01-01","objectID":"/golang-study-note-8/:1:3","tags":["golang"],"title":"Go学习笔记","uri":"/golang-study-note-8/"},{"categories":["golang"],"content":"HTTP访问控制 CORS","date":"2000-01-01","objectID":"/golang-web-note-3/","tags":["golang"],"title":"HTTP 访问控制 CORS","uri":"/golang-web-note-3/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-3/:0:0","tags":["golang"],"title":"HTTP 访问控制 CORS","uri":"/golang-web-note-3/"},{"categories":["golang"],"content":"HTTP 访问控制 CORS CORS - 跨域资源共享 使用额外的 HTTP 头来告诉浏览器, 让运行在一个 origin (domain) 上的Web应用被准许访问来自不同源服务器上的指定的资源。 什么时候会出现跨域 当一个资源从与该资源本身所在的服务器不同的域(域名)、协议(http/https)或端口(80/443等)请求一个资源时, 资源会发起一个跨域 HTTP 请求。 如: https://www.jicki.cn 与 http://www.jicki.cn 属于协议不同, 跨域 如: http://www.jicki.cn 与 http://www.jicki.cn:81 属于 端口不同, 跨域 如: http://a1.jicki.cn 与 http://a2.jicki.cn 属于 域不同, 跨域 如何实现跨域请求 使用Ajax的 jsonp (使用该方式的缺点: 请求方式只能是GET请求) Nginx 反向代理 (将不同域 转换成 同域 的地址) CORS ( 跨域资源共享 ) ","date":"2000-01-01","objectID":"/golang-web-note-3/:1:0","tags":["golang"],"title":"HTTP 访问控制 CORS","uri":"/golang-web-note-3/"},{"categories":["golang"],"content":"CORS 解决跨域 浏览器会将Ajax请求分为两类，其处理方案略有差异：简单请求、特殊请求。 ","date":"2000-01-01","objectID":"/golang-web-note-3/:2:0","tags":["golang"],"title":"HTTP 访问控制 CORS","uri":"/golang-web-note-3/"},{"categories":["golang"],"content":"简单请求 简单请求: 只要同时满足以下两大条件，就属于简单请求。 请求方法是以(HEAD, GET, POST)三种方法之一 HTTP的头信息不超出(Accept, Accept-Language, Content-Language, Last-Event-ID, Content-Type)几种字段 浏览器发现发现的Ajax请求是简单请求时，会在请求头中携带一个字段Origin。 Origin 中会指出当前请求属于哪个域（协议+域名+端口）。服务会根据这个值决定是否允许其跨域。 如果服务器允许跨域，需要在返回的响应头中至少携带 Access-Control-Allow-Origin, Access-Control-Allow-Credentials, Content-Type 三个信息. Access-Control-Allow-Origin - 可接受的域，是一个具体域名或者 * *代表任意域. Access-Control-Allow-Credentials - 是否允许携带cookie，默认情况下, CORS 不会携带cookie, 除非这个值是true. 如果跨域请求要想操作cookie: 服务的响应头中需要携带Access-Control-Allow-Credentials 并且为 true . 浏览器发起Ajax需要指定withCredentials 并且为 true . 响应头中的 Access-Control-Allow-Origin 一定不能为*，必须是指定域名. ","date":"2000-01-01","objectID":"/golang-web-note-3/:2:1","tags":["golang"],"title":"HTTP 访问控制 CORS","uri":"/golang-web-note-3/"},{"categories":["golang"],"content":"特殊请求 不符合简单请求的条件, 会被浏览器判定为特殊请求, 例如请求方式为PUT 特殊请求会在正式通信之前, 增加一次HTTP查询请求, 称为\"预检” 请求（preflight） 浏览器先询问服务器, 当前网页所在的域名是否在服务器的许可名单之中, 以及可以使用哪些HTTP动词和头信息字段。只有得到肯定答复, 浏览器才会发出正式的XMLHttpRequest请求, 否则就报错。 特殊请求 - 与简单请求相比, 除了三种信息外, 还会携带 Access-Control-Request-Method , Access-Control-Request-Headers 和 Access-Control-Max-Age 三个信息. Access-Control-Request-Method - 特殊请求,的请求方式 如: PUT Access-Control-Request-Headers - 额外的 头信息. Access-Control-Max-Age - 本次许可的有效时长, 单位是秒, 过期之前的Ajax请求就无需再次进行预检了. 如果浏览器得到上述响应, 则认定为可以跨域, 后续就跟简单请求的处理是一样的了 ","date":"2000-01-01","objectID":"/golang-web-note-3/:2:2","tags":["golang"],"title":"HTTP 访问控制 CORS","uri":"/golang-web-note-3/"},{"categories":["golang","Go","web"],"content":"logrus 日志系统","date":"2000-01-01","objectID":"/golang-web-note-8/","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-8/:0:0","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"logrus Logrus is a structured logger for Go (golang), completely API compatible with the standard library logger. ","date":"2000-01-01","objectID":"/golang-web-note-8/:1:0","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"Logrus 特性 结构化日志 完全兼容标准日志库, 多个日志级别: Trace, Debug, Info, Warning, Error, Fataland, Panic 。 支持Field，可以输出附加信息 兼容golang 原生 Log-logger 支持 TextFormat 和 JsonFormat 输出, 也支持自定义格式化日志格式。 支持Hook 线程安全 ","date":"2000-01-01","objectID":"/golang-web-note-8/:1:1","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"安装 go get -u github.com/sirupsen/logrus ","date":"2000-01-01","objectID":"/golang-web-note-8/:1:2","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"import import \"github.com/sirupsen/logrus\" ","date":"2000-01-01","objectID":"/golang-web-note-8/:1:3","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"兼容log日志库方式 package main import ( \"net/http\" \"os\" // 模块的别名 log \"github.com/sirupsen/logrus\" ) func init() { // 设置日志格式为json格式, 并且配置 时间格式 log.SetFormatter(\u0026log.JSONFormatter{TimestampFormat: \"2006-01-02 15:04:05.000\"}) // 设置将日志输出到标准输出（默认的输出为stderr,标准错误） // 日志消息输出可以是任意的io.writer类型 log.SetOutput(os.Stdout) // 设置日志级别 (warn级别以上的日志才会输出) log.SetLevel(log.WarnLevel) // 记录函数名,以及行数(会极大的消耗性能) // log.SetReportCaller(true) } func main() { // Info 级别日志 log.WithFields(log.Fields{ \"Code\": http.StatusOK, \"Msg\": \"啦啦啦啦啦啦啦啦啦啦~~\", }).Info(\"这是一条INFO日志\") log.WithFields(log.Fields{ \"Code\": http.StatusOK, \"Msg\": \"啦啦啦啦啦啦啦啦啦啦~~\", }).Warn(\"这是一条 Warning 日志\") log.WithFields(log.Fields{ \"Code\": http.StatusOK, \"Msg\": \"啦啦啦啦啦啦啦啦啦啦~~\", }).Fatal(\"这是一条 Fatal 日志\") } 输出: {\"Code\":200,\"Msg\":\"啦啦啦啦啦啦啦啦啦啦~~\",\"level\":\"warning\",\"msg\":\"这是一条 Warning 日志\",\"time\":\"2019-12-16 14:35:18.164\"} {\"Code\":200,\"Msg\":\"啦啦啦啦啦啦啦啦啦啦~~\",\"level\":\"fatal\",\"msg\":\"这是一条 Fatal 日志\",\"time\":\"2019-12-16 14:35:18.164\"} ","date":"2000-01-01","objectID":"/golang-web-note-8/:1:4","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"自己创建Logger实例 logrus 可以配置多个 Logger 实例, 应对多个地方输出, 通常我们会定义全局的 Logger 实例。 package main import ( \"net/http\" \"os\" \"github.com/sirupsen/logrus\" ) // logrus提供了New() 函数来创建一个logrus的实例. // 可以创建任意数量的logrus实例. var log = logrus.New() func main() { // 设置logrus实例的输出到任意io.writer log.Out = os.Stdout // 配置输出格式以及时间格式 log.Formatter = \u0026logrus.JSONFormatter{TimestampFormat: \"2006-01-02 15:04:05.000\"} // 设置日志级别 log.SetLevel(logrus.WarnLevel) // 固定 Fields entry := log.WithFields(logrus.Fields{ \"Code\": http.StatusOK, \"Msg\": \"啦啦啦啦啦啦啦啦啦啦~~\", }) // Info 级别日志 entry.Info(\"这是一条 Info 日志\") // Warn 级别日志 entry.Warn(\"这是一条 Info 日志\") // Fatal 级别日志 entry.Fatal(\"这是一条 Info 日志\") } 输出: {\"Code\":200,\"Msg\":\"啦啦啦啦啦啦啦啦啦啦~~\",\"level\":\"warning\",\"msg\":\"这是一条 Info 日志\",\"time\":\"2019-12-16 15:08:39.459\"} {\"Code\":200,\"Msg\":\"啦啦啦啦啦啦啦啦啦啦~~\",\"level\":\"fatal\",\"msg\":\"这是一条 Info 日志\",\"time\":\"2019-12-16 15:08:39.459\"} ","date":"2000-01-01","objectID":"/golang-web-note-8/:1:5","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"Hook 接口方式 logrus最令人心动的功能就是其可扩展的 hook 机制了,通过在初始化时为logrus添加hook,logrus 可以实现各种扩展功能. logrus的hook接口定义如下,其原理是每次写入日志时拦截,修改logrus.Entry. // logrus在记录Levels()返回的日志级别的消息时会触发 hook , // 按照Fire方法定义的内容修改logrus.Entry. type Hook interface { Levels() []Level Fire(*Entry) error } hook的使用很简单,在初始化前调用log.AddHook(hook)添加相应的hook即可. logrus官方仅仅内置了syslog的hook. 但Github也有很多第三方的hook可供使用. ","date":"2000-01-01","objectID":"/golang-web-note-8/:1:6","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang","Go","web"],"content":"Gin 框架 使用 logrus logrus 在 Gin 框架中,可使用log方式将Gin框架输出的日志也输出到logrus实例中, 也可以使用Gin 中间件的方式灵活替代日志输出. package main import ( \"net/http\" \"os\" \"github.com/gin-gonic/gin\" \"github.com/sirupsen/logrus\" ) // 创建一个 全局 logrus 实例 var log = logrus.New() // 初始化 log 配置 func init() { // 配置 日志输出格式以及时间格式 log.Formatter = \u0026logrus.JSONFormatter{TimestampFormat: \"2006-01-02 15:04:05.000\"} // 打开文件 file, err := os.OpenFile(\"./logs/gin.log\", os.O_CREATE|os.O_APPEND|os.O_WRONLY, 0644) if err != nil { log.Fatalf(\"Open File Failed err: %v\", err) } // 将日志 输出到文件 log.Out = file // 配置 gin 框架的模式为 线上模式 gin.SetMode(gin.ReleaseMode) // 将 gin 框架默认输出 logrus 实例中 gin.DefaultWriter = log.Out // 配置 日志输出级别 log.SetLevel(logrus.InfoLevel) } func main() { // 创建一个 gin 实例 r := gin.Default() r.GET(\"/index\", func(c *gin.Context) { log.WithFields(logrus.Fields{ \"Code\": http.StatusOK, \"Path\": \"index\", }).Info(\"这是 Index 页面..\") // 页面 JSON 的数据 c.JSON(200, gin.H{ \"message\": \"This Index\", }) }) _ = r.Run(\":8888\") } ","date":"2000-01-01","objectID":"/golang-web-note-8/:2:0","tags":["golang","web"],"title":"logrus 日志系统","uri":"/golang-web-note-8/"},{"categories":["golang"],"content":"MVC 与 CLD 框架","date":"2000-01-01","objectID":"/golang-web-note-1/","tags":["golang"],"title":"MVC 与 CLD 框架","uri":"/golang-web-note-1/"},{"categories":["golang"],"content":"Go Web 编程 设计一个程序的结构，有一门专门的学问，叫做\"架构模式”（architectural pattern），属于编程的方法论。 ","date":"2000-01-01","objectID":"/golang-web-note-1/:0:0","tags":["golang"],"title":"MVC 与 CLD 框架","uri":"/golang-web-note-1/"},{"categories":["golang"],"content":"MVC 框架 MVC模式就是架构模式的一种。 MVC - 这个模式认为，程序不论简单或复杂，从结构上看，都可以分成三层。 Model（模型） Model 是Web应用中的最底层 用于处理数据逻辑的部分，包括Service层和Dao层。 Service层用于和数据库联动，放置业务逻辑代码，处理数据库的增删改查。 Dao层用于放各种接口，以备调用。 View（视图） View 是Web应用中的第一层 用于处理响应给客户的页面的部分，例如我们写的html静态页面，jsp动态页面，这些最终响应给浏览器的页面都是视图, 通常视图是依据模型数据来创建的。 Controller（控制） Controller 在Web应用中的中间一层，简而言之，就是Servlet。（实际上一个方法就相当于一个对应的Servlet）。 这三层是紧密联系在一起的，但又是互相独立的，每一层内部的变化不影响其他层。每一层都对外提供接口（Interface），供上面一层调用。这样一来，就实现了 模块化，修改外观或者变更数据都不用修改其他层，大大方便了维护和升级。 ","date":"2000-01-01","objectID":"/golang-web-note-1/:1:0","tags":["golang"],"title":"MVC 与 CLD 框架","uri":"/golang-web-note-1/"},{"categories":["golang"],"content":"Web经典三层架构 表现层，UI，User Interface： 主要接受用户的请求和把相应的页面响应给用户浏览器, 页面 对应MVC中的视图（View）, 逻辑 对应MVC中的控制器（Controller），即Servlet服务器。 业务逻辑层，BLL，Business Logic Layer: 对应MVC中模型（Model）中的Service层，与数据库联动处理增删改查。 数据访问层/持久层，DAL，Data Access Layer: 对应MVC中模型（Model）中的Dao层，提供接口支持。 ","date":"2000-01-01","objectID":"/golang-web-note-1/:1:1","tags":["golang"],"title":"MVC 与 CLD 框架","uri":"/golang-web-note-1/"},{"categories":["golang"],"content":"CLD 框架 Controller 层 服务的入口, 负责处理路由、参数校检、请求转发。 Logic 层 逻辑(服务)层, 负责处理业务逻辑。 Dao 层 负责数据与存储相关的服务。 ","date":"2000-01-01","objectID":"/golang-web-note-1/:2:0","tags":["golang"],"title":"MVC 与 CLD 框架","uri":"/golang-web-note-1/"},{"categories":["golang","Go","web"],"content":"PV/UV/PR/IP 网站分析指标","date":"2000-01-01","objectID":"/golang-web-note-7/","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-7/:0:0","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"PV/UV/PR/IP 网站流量指标 网站数据分析, 经常会统计一个页面或者一个网站或者其他情况的 PV/UV/PR/IP 量。 ","date":"2000-01-01","objectID":"/golang-web-note-7/:1:0","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"PV PV (page view), 即页面浏览量, 或点击量。 通常是衡量一个网站，一个栏目，一个页面，甚至一条信息的主要指标。 用户对一个页面进行一次访问, 刷新或者一次通过网址访问, 该页面的PV 就会计算为1。 一般以日为单位来衡量和计算。 ","date":"2000-01-01","objectID":"/golang-web-note-7/:1:1","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"UV UV (unique visitor), 即独立访客数。 访问某个站点或点击某个页面、某一条信息的不同访客人数。 一日内访问某个网站或者网页的不同用户数量。 同一个用户对一个网站或者一个页面的多次访问记为贡献一个uv。 一般以日为单位来衡量和计算。 ","date":"2000-01-01","objectID":"/golang-web-note-7/:1:2","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"IP IP(IPV4/IPV6) 。 同一时段内有多少IP连接。 同一时间内 那些 IP 访问次数最多。 ","date":"2000-01-01","objectID":"/golang-web-note-7/:1:3","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"PR PR(PageRank), 即网页的级别技术, 表示一个网页的重要程度。 级别从1到10级, 10级为满分。 PR值越高说明该网页越受用户喜爱。 ","date":"2000-01-01","objectID":"/golang-web-note-7/:1:4","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"PV 与 并发 计算公式: 并发连接数 = (( 80% * PV ) / ( 统计时间 * 40% ) * 页面衍生连接次数 * http响应时间 * 因数 ) / web服务器数量 80% 与 40%: 一天中有80% 的请求发生在一天的40%的时间内 (这个按照实际情况而定)。 页面衍生连接次数: 一个页面请求, 会有好几次http连接, 如外部的css, js,图片等, 这个根据实际情况而定。 http响应时间: 平均一个http请求的响应时间, 可以使用1秒或更少。 因数: 峰值流量 和平均流量的倍数, 一般使用 5 , 最好根据实际情况计算后得出。 例子: 10万PV的并发连接数: (( 80% * 100000PV ) / ( 86400 秒 * 40% ) * 50个页面衍生连接数 * 1秒内响应 * 5倍峰值) / 1台Web服务器 = 578 并发连接数。 如果我们能够测试出单机的并发连接数, 和 日 pv 数，那么我们也就大概估算出需要 web 的服务器数量。(服务器相同配置的情况下) ","date":"2000-01-01","objectID":"/golang-web-note-7/:1:5","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"PV 计算带宽 计算带宽大小需要关注两个指标: 峰值流量 和 页面平均大小。 计算公式: 网站带宽 = PV / 统计时间（换算到秒(s)）* 平均页面大小（单位KB）* 8 字节的单位是Byte, 而带宽的单位是bit, 1Byte = 8bit, 所以转换为带宽的时候, 要乘以 8。 在实际运行中，由于缓存、CDN、白天夜里访问量不同等原因，这个是计算下的算法。 在实际的网站运行过程中, 我们的网站必须要在峰值流量时保持正常的访问, 一般来说峰值流量是平均流量的5倍。 举例: 假设网站的平均日PV: 10w 的访问量, 页面平均大小0.4 M 。 网站带宽 = 10w / （24 * 60 * 60）* 0.4M * 8 = 3.7 Mbps 实际网站流量 计算流量 * 5倍 = 3.7 Mbps * 5 = 18.5Mbps ","date":"2000-01-01","objectID":"/golang-web-note-7/:1:6","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"其他概念 ","date":"2000-01-01","objectID":"/golang-web-note-7/:2:0","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"QPS QPS ( Queries Per Second ) 每秒处理的查询数（如果是数据库，就相当于读取） ","date":"2000-01-01","objectID":"/golang-web-note-7/:2:1","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"TPS TPS ( Transactions Per Second ) 每秒处理的事务数(如果是数据库，就相当于写入、修改) ","date":"2000-01-01","objectID":"/golang-web-note-7/:2:2","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang","Go","web"],"content":"IOPS IOPS ( Input/Output Operations Per Second ) 每秒磁盘进行的I/O操作次数 ","date":"2000-01-01","objectID":"/golang-web-note-7/:2:3","tags":["golang","web"],"title":"PV/UV/PR/IP 网站分析指标","uri":"/golang-web-note-7/"},{"categories":["golang"],"content":"RESTful 框架","date":"2000-01-01","objectID":"/golang-web-note-0/","tags":["golang"],"title":"RESTful 框架","uri":"/golang-web-note-0/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-0/:0:0","tags":["golang"],"title":"RESTful 框架","uri":"/golang-web-note-0/"},{"categories":["golang"],"content":"RESTful 框架 RESTful架构，就是目前最流行的一种互联网软件架构。它结构清晰、符合标准、易于理解、扩展方便，所以正得到越来越多网站的采用。 REST这个词，是Roy Thomas Fielding在他2000年的博士论文中提出的, Fielding 将他对互联网软件的架构原则，定名为REST，即Representational State Transfer的缩写。中文翻译为 表现层状态转化 。 如果一个架构符合REST原则，就称它为RESTful架构。 要理解RESTful架构，最好的方法就是去理解Representational State Transfer这个词组, REST的名称\"表现层状态转化\"中，省略了主语。“表现层\"其实指的是\"资源”（Resources）的\"表现层”。“资源\"是一种信息实体，它可以有多种外在表现形式。我们把\"资源\"具体呈现出来的形式，叫做它的\"表现层”（Representation）。状态转化（State Transfer）, 客户端想要操作服务器，必须通过某种手段，让服务器端发生\"状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是\"表现层状态转化”。 Resources(资源): 所谓\"资源”，就是网络上的一个实体，或者说是网络上的一个具体信息, 每种资源对应一个特定的URI, 当需要获取某种资源时,访问资源对应的 URL 既可。 Representation(表现层): 我们把\"资源\"具体呈现出来的形式，叫做它的 “表现层”（Representation）。URI只代表资源的实体，不代表它的形式。它的具体表现形式, 应该是在HTTP请求的头信息中用Accept和Content-Type字段指定，这两个字段才是对\"表现层\"的描述。 State Transfer(状态转化): 互联网通信协议HTTP协议，是一个无状态协议。这意味着，所有的状态都保存在服务器端。因此，如果客户端想要操作服务器，必须通过某种手段，让服务器端发生\"状态转化”（State Transfer）。而这种转化是建立在表现层之上的，所以就是\"表现层状态转化”。 总结: 每一个URI代表一种资源。 客户端和服务器之间，传递这种资源的某种表现层。 客户端通过四个HTTP动词，对服务器端资源进行操作，实现\"表现层状态转化”。 ","date":"2000-01-01","objectID":"/golang-web-note-0/:1:0","tags":["golang"],"title":"RESTful 框架","uri":"/golang-web-note-0/"},{"categories":["golang"],"content":"HTTP 四个动词-请求方法 GET 请求 用来获取资源 POST 请求 用来创建新的资源 PUT 请求 用来更新资源 DELETE 请求 用来删除资源 REST风格 的系统设计: 请求方法 URI 含义 GET /book 查询书籍信息 POST /book 创建书籍 PUT /book 更新书籍信息 DELETE /book 删除书籍 package main import ( \"fmt\" \"github.com/gin-gonic/gin\" \"net/http\" ) func main() { r := gin.Default() // HTTP 四个请求方式 // 基于 GET 请求 r.GET(\"/book\",func(c *gin.Context){ c.JSON(http.StatusOK, gin.H{ \"Message\":\"查询书籍\", }) }) // 基于 POST 请求 r.POST(\"/book\",func(c *gin.Context){ c.JSON(http.StatusOK,gin.H{ \"Message\":\"创建书籍\", }) }) // 基于 PUT 请求 r.PUT(\"/book\",func(c *gin.Context){ c.JSON(http.StatusOK,gin.H{ \"Message\":\"更新书籍\", }) }) // 基于 DELETE 请求 r.DELETE(\"/book\",func(c *gin.Context){ c.JSON(http.StatusOK,gin.H{ \"Message\":\"删除书籍\", }) }) // 启动服务 if err := r.Run(\":8888\");err!=nil{ fmt.Printf(\"Server Run Failed err: %v\\n\",err) return } } ","date":"2000-01-01","objectID":"/golang-web-note-0/:2:0","tags":["golang"],"title":"RESTful 框架","uri":"/golang-web-note-0/"},{"categories":["golang"],"content":"全局唯一ID生成器 Sonyflake","date":"2000-01-01","objectID":"/golang-web-note-10/","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"Go Web 编程 ","date":"2000-01-01","objectID":"/golang-web-note-10/:0:0","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"Sonyflake 官方地址 github.com/sony/sonyflake Sonyflake是Sony公司基于 Twitter snowflake 实现的的一个Golang的全局性唯一ID生成器的开源项目, 基本思路和snowflake 差不多, 但也稍有不同。 如下为 snowflake 的ID 组成 1 Bit Unused 41 Bit Timestamp 10 Bit Node ID 12 Bit Sequence ID 如下为 Sonyflake 的ID 组成 1 Bit Unused 39 Bit Timestamp 8 Bit Sequence ID 16 Bit Machine ID 名词解析 Unused: 未使用, 二进制中最高位为1的都是负数, 我们生成的id一般都是整数, 所以这个最高位固定是0。 Timestamp: 时间戳. Sequence ID: 由程序在运行期生成。 Node ID: 机器编号的位长。 Machine ID: 其实就是 Node ID。 ","date":"2000-01-01","objectID":"/golang-web-note-10/:1:0","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"安装 go get -u github.com/sony/sonyflake ","date":"2000-01-01","objectID":"/golang-web-note-10/:1:1","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"import import \"github.com/sony/sonyflake\" ","date":"2000-01-01","objectID":"/golang-web-note-10/:1:2","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"创建 Sonyflake 实例 func NewSonyflake(st Settings) *Sonyflake ","date":"2000-01-01","objectID":"/golang-web-note-10/:1:3","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"Setting 结构体 type Settings struct { StartTime time.Time MachineID func() (uint16, error) CheckMachineID func(uint16) bool } 说明: StartTime: 将 Sonyflake 的时间定义为开始的时间, 如果StartTime为0, 则将Sonyflake的开始时间设置为 2014-09-01 00:00:00 +0000 UTC 。 如果StartTime早于当前时间, 则不会创建Sonyflake。 MachineID: 返回Sonyflake实例的唯一ID。 如果MachineID 返回错误，则不会创建Sonyflake。如果MachineID为nil, 则使用默认的MachineID。默认的MachineID返回专用IP地址的低16位。 CheckMachineID: 验证计算机ID的唯一性。如果CheckMachineID返回false，则不会创建Sonyflake。如果CheckMachineID为nil，则不进行验证。 ","date":"2000-01-01","objectID":"/golang-web-note-10/:1:4","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"生成唯一ID 调用 NextID() 方法 func (sf *Sonyflake) NextID() (uint64, error) 从 StartTime 设置的时间开始, 执行 NextID 生成ID 可以生成约 174年。但是在Sonyflake时间超过限制后, NextID将返回错误。 ","date":"2000-01-01","objectID":"/golang-web-note-10/:1:5","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"模拟生成ID例子 package main import ( \"fmt\" \"log\" \"github.com/sony/sonyflake\" ) var ( sonyFlake *sonyflake.Sonyflake // 定义一个全局的 machineID 模拟获取 // 现实环境中应从 zk 或 etcd 中获取 machineID uint16 ) // 获取 机器编码ID的 回调函数 func getMachineID() (uint16, error) { // machineID 返回nil, 则返回专用IP地址的低16位 return machineID, nil } // 初始化 sonyFlake 配置 func Init(mID uint16) (err error) { machineID = mID st := sonyflake.Settings{} sonyFlake = sonyflake.NewSonyflake(st) return } // 获取全局 ID 的函数 func GetID() (id uint64, err error) { if sonyFlake == nil { err = fmt.Errorf(\"需要先初始化以后再执行 GetID 函数 err: %#v \\n\", err) return } return sonyFlake.NextID() } func main() { mID, err := getMachineID() if err != nil { log.Fatalf(\"getMachineID Failed Err: %#v\\n\", err) } if err = Init(mID); err != nil { log.Fatalf(\"Init Err: %#v\\n\", err) } id, err := GetID() if err != nil { log.Fatalf(\"GetID Failed Err: %#v\\n\", err) } fmt.Println(\"sonyFlake 生成 ID: \", id) } 输出: sonyFlake 生成 ID: 280652487722051811 ","date":"2000-01-01","objectID":"/golang-web-note-10/:1:6","tags":["golang"],"title":"全局唯一ID生成器 Sonyflake","uri":"/golang-web-note-10/"},{"categories":["golang"],"content":"数据传输协议 Protocol Buffer","date":"2000-01-01","objectID":"/golang-web-note-15/","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"Protocol Buffer ","date":"2000-01-01","objectID":"/golang-web-note-15/:0:0","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"数据传输协议 一般常用的数据传输协议为 json, xml, Protobuf 。 json 优势: 可读性强, 直观, 支持跨平台多语言。 劣势: 编解码很耗时。 xml 优势: 基于标签形式。 劣势: 局限性 一般在 前端/页面 中使用, 可读性不强, 冗余性不太好。 Protobuf 优势: 编解码速度快, 序列化后体积比json/xml 小, 适用于网络传输, 支持跨平台多语言。 劣势: 可读性不强, 二进制格式、不是明文传输, 注释描述差。 ","date":"2000-01-01","objectID":"/golang-web-note-15/:1:0","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"配置 Protobuf ","date":"2000-01-01","objectID":"/golang-web-note-15/:2:0","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"安装 protoc 直接下载官方编译好的二进制文件,解压后复制到 $GOPATH/bin 目录下 https://github.com/google/protobuf/releases # Win 下载以 protoc-3.11.2-win64.zip 的二进制文件 # Linux 下载以 protoc-3.11.2-linux-x86_64.zip 的二进制文件 # MacOS 下载以 protoc-3.11.2-osx-x86_64.zip 的二进制文件 # protoc --version libprotoc 3.11.2 ","date":"2000-01-01","objectID":"/golang-web-note-15/:2:1","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"安装 protoc-gen-go go get -u -v github.com/golang/protobuf/protoc-gen-go go get protoc-gen-go 以后会自动build 生成二进制文件放到 $GOPATH/bin 目录下 ","date":"2000-01-01","objectID":"/golang-web-note-15/:2:2","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"安装 protobuf 库 go get -u -v github.com/golang/protobuf/proto ","date":"2000-01-01","objectID":"/golang-web-note-15/:2:3","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"protobuf 数据类型 proto Go 说明 double float64 64位浮点数 float float32 32位浮点数 int32 int32 对于可变长编码,负数效率低, 请使用 sin64 代替. uint32 uint32 无符号32位整型 uint64 uint64 无符号64位整型 sint32 int32 存在负值时,效率比int32效率高很多 sint64 int64 存在负值时,效率比int64效率高很多 fixed32 uint32 总是4个字节, 如果总和比228大,这个类型比uint32高效 fixed64 uint64 总是8个字节, 如果总和比256打,这个类型比uint64高效 sfixed32 int32 总是4个字节 sfixed64 int64 总是8个字节 bool bool 布尔值 string string 字符串类型，并且必须是UTF-8编码或者7-bit ASCII编码 bytes []byte 包含任意顺序的字节数据 ","date":"2000-01-01","objectID":"/golang-web-note-15/:3:0","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"protobuf 语法 protobuf 文件一般约定为 proto 为后缀,如: Person.proto // 必须指定版本信息,否则报错 syntax = \"proto3\"; // 生成go文件的包名 package pb; // 定义一个消息类型,使用 message 关键字 message Person { string name = 1; int32 age = 2; // repeated 表示字段允许重复 repeated string emails = 3; repeated PhoneNumber phones = 4; } // 定义一枚举类型,使用 enum 关键字 enum PhoneType { MOBILE = 0; HOME = 1; WORK = 2; } // 消息类型嵌套 message PhoneNumber { string number = 1; // PhoneType 是 enum 定义的枚举类型, 只允许取枚举中定义的三个值。 PhoneType type = 2; } ","date":"2000-01-01","objectID":"/golang-web-note-15/:4:0","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"编译 protobuf 文件 protoc --proto_path=IMPORT_PATH --go_out=DST_PATH /path/file.proto # 这里也可以使用 protoc --go_out=. *.proto 简化操作. # 切换到 存放 proto 文件的目录下 protoc --go_out=. *.proto # 生成 .go 文件, 此文件不能修改。 Person.pb.go ","date":"2000-01-01","objectID":"/golang-web-note-15/:5:0","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"Golang 使用 protobuf package main import ( \"fmt\" // proto 文件生成 go 文件存放的路径 \"jicki/protobuf_demo/pb\" \"github.com/gogo/protobuf/proto\" ) // func main() { // 定义一个 Person 结构体 person := \u0026pb.Person{ Name: \"小炒肉\", Age: 20, Emails: []string{\"jicki@qq.com\", \"jicki@vip.qq.com\"}, Phones: []*pb.PhoneNumber{ \u0026pb.PhoneNumber{ Number: \"1333333333\", Type: pb.PhoneType_MOBILE, }, \u0026pb.PhoneNumber{ Number: \"85858585\", Type: pb.PhoneType_HOME, }, }, } // 序列化: // 将 person 结构体, 进行 proto 序列化, 得到一个二进制文件. // 如下: data 为需要传输的 数据 data, err := proto.Marshal(person) if err != nil { fmt.Println(\"proto Marshal err: \", err) return } // 反序列化: // 定义一个空结构体对象 newPerson := \u0026pb.Person{} err = proto.Unmarshal(data, newPerson) if err != nil { fmt.Println(\"proto Unmarshal err: \", err) return } fmt.Println(\"源数据: \", person) fmt.Println(\"序列化后的数据: \", data) fmt.Println(\"反序列化后的数据: \", newPerson) } 输出: 源数据: name:\"\\345\\260\\217\\347\\202\\222\\350\\202\\211\" age:20 emails:\"jicki@qq.com\" emails:\"jicki@vip.qq.com\" phones:\u003cnumber:\"1333333333\" \u003e phones:\u003cnumber:\"85858585\" type:HOME \u003e 序列化后的数据: [10 9 229 176 143 231 130 146 232 130 137 16 20 26 12 106 105 99 107 105 64 113 113 46 99 111 109 26 16 106 105 99 107 105 64 118 105 112 46 113 113 46 99 111 109 34 12 9 51 51 51 51 51 51 51 51 51 34 12 10 8 56 53 56 53 56 53 56 53 16 1] 反序列化后的数据: name:\"\\345\\260\\217\\347\\202\\222\\350\\202\\211\" age:20 emails:\"jicki@qq.com\" emails:\"jicki@vip.qq.com\" phones:\u003cnumber:\"1333333333\" \u003e phones:\u003cnumber:\"85858585\" type:HOME \u003e ","date":"2000-01-01","objectID":"/golang-web-note-15/:6:0","tags":["golang"],"title":"数据传输协议 Protocol Buffer","uri":"/golang-web-note-15/"},{"categories":["golang"],"content":"通讯协议 - TLV","date":"2000-01-01","objectID":"/golang-web-note-14/","tags":["golang"],"title":"通讯协议 - TLV","uri":"/golang-web-note-14/"},{"categories":["golang"],"content":"TLV 通信协议可以理解两个节点之间为了协同工作实现信息交换, 协商一定的规则和约定, 例如规定字节序, 各个字段类型, 使用什么压缩算法或加密算法等。常见的有tcp, udp,http, sip等常见协议。协议有流程规范和编码规范。流程如呼叫流程等信令流程, 编码规范规定所有信令和数据如何打包/解包。 ","date":"2000-01-01","objectID":"/golang-web-note-14/:0:0","tags":["golang"],"title":"通讯协议 - TLV","uri":"/golang-web-note-14/"},{"categories":["golang"],"content":"TLV 编码介绍 TLV 是指由数据的类型Tag, 数据的长度Length, 数据的值Value组成的结构体, 几乎可以描任意数据类型, TLV的Value也可以是一个TLV结构, 正因为这种嵌套的特性, 可以让我们用来包装协议的实现。 T L V Tag Length Value Tag - 描述Value的数据类型, TLV嵌套时可以用于描述消息的类型。 Length - 描述Value的长度, Value部分所占字节的个数, 编码格式分两类: 定长方式(DefiniteForm)和不定长方式(IndefiniteForm)。 Value - 描述数据的值, 由一个或多个值组成, 值可以是一个原始数据类型(Primitive Data), 也可以是一个TLV结构(Constructed Data) Primitive Data 编码 |Tag|Length|Value| Constructed Data 编码 |Tag|Length|[T][L][TLV][T|L|V]| ","date":"2000-01-01","objectID":"/golang-web-note-14/:1:0","tags":["golang"],"title":"通讯协议 - TLV","uri":"/golang-web-note-14/"},{"categories":["golang"],"content":"TLV 解决黏包 解决 TCP 黏包问题 一个完整的包,包含 head 和 body head body DataLen和id Data数据 第一次Read获取DataLen 第二次Read 根据DataLen 偏移读取消息数据 ","date":"2000-01-01","objectID":"/golang-web-note-14/:2:0","tags":["golang"],"title":"通讯协议 - TLV","uri":"/golang-web-note-14/"},{"categories":null,"content":"小炒肉 (技术博客)\" 小炒肉 (技术博客) 关于网站 | 小炒肉的个人技术博客 ","date":"0001-01-01","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"关于小炒肉 宅 热爱 DevOps, 半路出家写代码 搞搞 docker, 倒腾 kubernetes 强迫症患者 超级无敌热爱游戏 ","date":"0001-01-01","objectID":"/about/:1:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"一、 序 Your future depends on your dreams 不要活在别人的眼里，不要活在别人的嘴里 要活在自己的心里，生活过的洒脱一点，不要为别人去活 ","date":"0001-01-01","objectID":"/about/:1:1","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":"二、 工作状况 ","date":"0001-01-01","objectID":"/about/:1:2","tags":null,"title":"关于","uri":"/about/"}]